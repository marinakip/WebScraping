"{'location': 'India', 'stats_list': [], 'contributions': '100 contributions\n        in the last year', 'description': [""Linear-Regression-Model-for-House-Price-Prediction\nIn this tutorial you will learn how to create Machine Learning Linear Regression Model. You will be analyzing a house price predication dataset for finding out price of house on different parameters. You will do Exploratory Data Analysis, split the training and testing data, Model Evaluation and Predictions.\nProblem Statement\nA real state agents want the help to predict the house price for regions in the USA. He gave you the dataset to work on and you decided to use Linear Regressioon Model. Create a model which will help him to estimate of what the house would sell for.\nDataset contains 7 columns and 5000 rows with CSV extension. The data contains the following columns :\n\n'Avg. Area Income': Avg. Income of householder of the city house is located in.\n'Avg. Area House Age': Avg. Age of Houses in same city.\n'Avg. Area Number of Rooms': Avg. Number of Rooms for Houses in same city.\n'Avg. Area Number of Bedrooms': Avg. Number of Bedrooms for Houses in same city.\n'Area Population': Population of city.\n'Price': Price that the house sold at.\n'Address': Address of the houses.\n\n""], 'url_profile': 'https://github.com/huzaifsayed', 'info_list': ['2', 'Jupyter Notebook', 'Updated Mar 4, 2020', '36', 'Python', 'Updated Jun 11, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Python', 'Updated Feb 11, 2021', '17', 'Kotlin', 'Updated Dec 18, 2020', '7', 'Python', 'Updated Mar 9, 2020', '2', 'Python', 'MIT license', 'Updated Jan 4, 2021', '3', 'Python', 'Updated Mar 8, 2020', '1', 'HTML', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Dec 16, 2020']}","{'location': 'Seoul, South Korea', 'stats_list': [], 'contributions': '188 contributions\n        in the last year', 'description': ['Adaptive-Wing-Loss-for-Robust-Face-Alignment-via-Heatmap-Regression\n‚ùóongoing repo\nPytorch implementation of paper. official implementation can be found at official.\nblog post about the paper(korean) can be found here.\n\nüìù TODO\n\n prototype\n albumentation data augmentation\n evalutaion on 300W + data augmentation\n performance tuning\n dependency check\n provide pretrained weight\n apply different model (such as DLA, Unet)\n apply similar loss (such as Focal-loss)\n apply Integral regression moduel (AWing + Integral)\n\nPrerequisites\n\nPython 3.6 +\nPytorch 1.1.0\nScipy 0.19.1\ncv2 3.3.0\n\nUsage\nFirst, download dataset(Currently 300W supported).\n300W link\n\ndownload [part1] ~ [part2]\nlocate 300W images, pts files according to this structure\n\ndata\n|-- 300W\n|   |-- 01_Indoor\n|   |-- 02_Ourdoor\n\nTo train a model with downloaded dataset:\n$ python train.py\n\nTo test single image result:\n$ python detect.py\n\nModel overview\n\nmore detail about model\n\nloss function design\nAWing ‚Üí (lossMatrix) ‚Üí Loss_weighted\nevalutaion\nevalutaion on 300W testing dataset\nevaluation result will soon be updated\n\n\n\nmethod\nNME\nFR(10)\n\n\n\n\nthe paper\n3.07\nX\n\n\nthis repo\nx\n0.8\n\n\n\nReference\n\nCoordConv\nStacked Hourglass\nAdaptiveWingLoss\n\n'], 'url_profile': 'https://github.com/SeungyounShin', 'info_list': ['2', 'Jupyter Notebook', 'Updated Mar 4, 2020', '36', 'Python', 'Updated Jun 11, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Python', 'Updated Feb 11, 2021', '17', 'Kotlin', 'Updated Dec 18, 2020', '7', 'Python', 'Updated Mar 9, 2020', '2', 'Python', 'MIT license', 'Updated Jan 4, 2021', '3', 'Python', 'Updated Mar 8, 2020', '1', 'HTML', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Dec 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nPredictive Regression Modeling Workflow\nThis dataset was downloaded from Kaggle and contains information about used car sale listings.  We are trying to predict the price associated with the listing.\nFeatures (as described on Kaggle)\n\nCar_Name: The name of the car\nYear: The year in which the car was bought\nSelling_Price: The price the owner wants to sell the car at\nPresent_Price: The current ex-showroom price of the car\nKms_Driven: The distance completed by the car in km\nFuel_Type: The fuel type of the car (Petrol, Diesel, or Other)\nSeller_Type: Whether the seller is a dealer or an individual\nTransmission: Whether the car is manual or automatic\nOwner: The number of owners the car has previously had\n\nLooking at the original website, it looks like the prices are listed in lakhs, meaning hundreds of thousands of rupees.\ndf = pd.read_csv(""cars.csv"")\ndf.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nCar_Name\nYear\nSelling_Price\nPresent_Price\nKms_Driven\nFuel_Type\nSeller_Type\nTransmission\nOwner\n\n\n\n\n0\nritz\n2014\n3.35\n5.59\n27000\nPetrol\nDealer\nManual\n0\n\n\n1\nsx4\n2013\n4.75\n9.54\n43000\nDiesel\nDealer\nManual\n0\n\n\n2\nciaz\n2017\n7.25\n9.85\n6900\nPetrol\nDealer\nManual\n0\n\n\n3\nwagon r\n2011\n2.85\n4.15\n5200\nPetrol\nDealer\nManual\n0\n\n\n4\nswift\n2014\n4.60\n6.87\n42450\nDiesel\nDealer\nManual\n0\n\n\n\n\ndf.describe()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nYear\nSelling_Price\nPresent_Price\nKms_Driven\nOwner\n\n\n\n\ncount\n301.000000\n301.000000\n301.000000\n301.000000\n301.000000\n\n\nmean\n2013.627907\n4.661296\n7.628472\n36947.205980\n0.043189\n\n\nstd\n2.891554\n5.082812\n8.644115\n38886.883882\n0.247915\n\n\nmin\n2003.000000\n0.100000\n0.320000\n500.000000\n0.000000\n\n\n25%\n2012.000000\n0.900000\n1.200000\n15000.000000\n0.000000\n\n\n50%\n2014.000000\n3.600000\n6.400000\n32000.000000\n0.000000\n\n\n75%\n2016.000000\n6.000000\n9.900000\n48767.000000\n0.000000\n\n\nmax\n2018.000000\n35.000000\n92.600000\n500000.000000\n3.000000\n\n\n\n\ndf.isna().sum()\nCar_Name         0\nYear             0\nSelling_Price    0\nPresent_Price    0\nKms_Driven       0\nFuel_Type        0\nSeller_Type      0\nTransmission     0\nOwner            0\ndtype: int64\n\nsns.pairplot(df)\n<seaborn.axisgrid.PairGrid at 0x11c9e6f28>\n\n\nTrain-Test Split\nBefore performing any preprocessing or modeling, set aside a holdout test set\nX = df.drop(""Selling_Price"", axis=1)\ny = df[""Selling_Price""]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\nBaseline Model: Linear Regression with Numeric Features Only\nWe have four numeric features (Year, Present_Price, Kms_Driven, and Owner) and four non-numeric features (Car_Name, Fuel_Type, Seller_Type, Transmission).  Before doing any of the engineering work to be able to use those non-numeric features, let\'s just try using the numeric ones\nlin_reg_model = LinearRegression()\n\nX_train_numeric = X_train[[""Year"", ""Present_Price"", ""Kms_Driven"", ""Owner""]].copy()\nbaseline_cross_val_score = cross_val_score(lin_reg_model, X_train_numeric, y_train, cv=5)\nbaseline_cross_val_score\narray([0.67168559, 0.75626366, 0.88591659, 0.79241643, 0.84299344])\n\nOk, not too bad, we are getting somewhere between 0.67 and 0.89 r-squared for a linear regression with just the numeric features\nAdd One-Hot Encoded Features\nLet\'s see if adding in some of those non-numeric features helps\n# resetting the index so we can concatenate the one-hot encoded dfs more easily\nX_train_all_features = X_train.copy().reset_index().drop(""index"", axis=1)\ndef encode_and_concat_feature_train(X_train_all_features, feature_name):\n    """"""\n    Helper function for transforming training data.  It takes in the full X dataframe and\n    feature name, makes a one-hot encoder, and returns the encoder as well as the dataframe\n    with that feature transformed into multiple columns of 1s and 0s\n    """"""\n    # make a one-hot encoder and fit it to the training data\n    ohe = OneHotEncoder(categories=""auto"", handle_unknown=""ignore"")\n    single_feature_df = X_train_all_features[[feature_name]]\n    ohe.fit(single_feature_df)\n    \n    # call helper function that actually encodes the feature and concats it\n    X_train_all_features = encode_and_concat_feature(X_train_all_features, feature_name, ohe)\n    \n    return ohe, X_train_all_features\ndef encode_and_concat_feature(X, feature_name, ohe):\n    """"""\n    Helper function for transforming a feature into multiple columns of 1s and 0s. Used\n    in both training and testing steps.  Takes in the full X dataframe, feature name, \n    and encoder, and returns the dataframe with that feature transformed into multiple\n    columns of 1s and 0s\n    """"""\n    # create new one-hot encoded df based on the feature\n    single_feature_df = X[[feature_name]]\n    feature_array = ohe.transform(single_feature_df).toarray()\n    ohe_df = pd.DataFrame(feature_array, columns=ohe.categories_[0])\n    \n    # drop the old feature from X and concat the new one-hot encoded df\n    X = X.drop(feature_name, axis=1)\n    X = pd.concat([X, ohe_df], axis=1)\n    \n    return X\n# we will need each of these encoders later for transforming the test data\n\nfuel_type_ohe, X_train_all_features = encode_and_concat_feature_train(X_train_all_features, ""Fuel_Type"")\nseller_type_ohe, X_train_all_features = encode_and_concat_feature_train(X_train_all_features, ""Seller_Type"")\ntransmission_ohe, X_train_all_features = encode_and_concat_feature_train(X_train_all_features, ""Transmission"")\n# putting car name at the end just because there are the most categories\ncar_name_ohe, X_train_all_features = encode_and_concat_feature_train(X_train_all_features, ""Car_Name"")\nX_train_all_features.columns\nIndex([\'Year\', \'Present_Price\', \'Kms_Driven\', \'Owner\', \'CNG\', \'Diesel\',\n       \'Petrol\', \'Dealer\', \'Individual\', \'Automatic\', \'Manual\', \'800\',\n       \'Activa 3g\', \'Bajaj  ct 100\', \'Bajaj Avenger 150\',\n       \'Bajaj Avenger 150 street\', \'Bajaj Avenger 220\',\n       \'Bajaj Avenger 220 dtsi\', \'Bajaj Avenger Street 220\',\n       \'Bajaj Discover 100\', \'Bajaj Discover 125\', \'Bajaj Dominar 400\',\n       \'Bajaj Pulsar 135 LS\', \'Bajaj Pulsar 150\', \'Bajaj Pulsar 220 F\',\n       \'Bajaj Pulsar NS 200\', \'Bajaj Pulsar RS200\', \'Hero  Ignitor Disc\',\n       \'Hero Extreme\', \'Hero Glamour\', \'Hero Honda Passion Pro\', \'Hero Hunk\',\n       \'Hero Passion Pro\', \'Hero Passion X pro\', \'Hero Splender Plus\',\n       \'Hero Splender iSmart\', \'Hero Super Splendor\', \'Honda Activa 4G\',\n       \'Honda CB Hornet 160R\', \'Honda CB Shine\', \'Honda CB Unicorn\',\n       \'Honda CB twister\', \'Honda CBR 150\', \'Honda Karizma\', \'Hyosung GT250R\',\n       \'KTM 390 Duke \', \'KTM RC200\', \'KTM RC390\', \'Royal Enfield Bullet 350\',\n       \'Royal Enfield Classic 350\', \'Royal Enfield Classic 500\',\n       \'Royal Enfield Thunder 350\', \'Royal Enfield Thunder 500\',\n       \'Suzuki Access 125\', \'TVS Apache RTR 160\', \'TVS Apache RTR 180\',\n       \'TVS Jupyter\', \'TVS Sport \', \'TVS Wego\', \'Yamaha FZ  v 2.0\',\n       \'Yamaha FZ 16\', \'Yamaha FZ S \', \'Yamaha FZ S V 2.0\', \'alto 800\',\n       \'alto k10\', \'amaze\', \'baleno\', \'brio\', \'camry\', \'ciaz\', \'city\',\n       \'corolla altis\', \'creta\', \'dzire\', \'elantra\', \'eon\', \'ertiga\',\n       \'etios cross\', \'etios g\', \'etios gd\', \'etios liva\', \'fortuner\',\n       \'grand i10\', \'i10\', \'i20\', \'ignis\', \'innova\', \'jazz\', \'land cruiser\',\n       \'omni\', \'ritz\', \'swift\', \'sx4\', \'verna\', \'wagon r\', \'xcent\'],\n      dtype=\'object\')\n\nX_train_all_features\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nYear\nPresent_Price\nKms_Driven\nOwner\nCNG\nDiesel\nPetrol\nDealer\nIndividual\nAutomatic\n...\ninnova\njazz\nland cruiser\nomni\nritz\nswift\nsx4\nverna\nwagon r\nxcent\n\n\n\n\n0\n2017\n0.84\n5000\n0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1\n2015\n14.79\n12900\n0\n0.0\n0.0\n1.0\n1.0\n0.0\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\n2015\n0.32\n35000\n0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n3\n2015\n13.60\n21780\n0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n4\n2015\n5.90\n14465\n0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n220\n2013\n0.57\n18000\n0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n221\n2011\n12.48\n45000\n0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n222\n2014\n3.45\n16500\n1\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n223\n2011\n10.00\n69341\n0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n224\n2017\n1.78\n4000\n0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n225 rows √ó 96 columns\n\nLinear Regression with More Features\nlin_reg_model = LinearRegression()\n\nprint(""Old:"", baseline_cross_val_score)\nprint(""New:"", cross_val_score(lin_reg_model, X_train_all_features, y_train, cv=5))\nOld: [0.67168559 0.75626366 0.88591659 0.79241643 0.84299344]\nNew: [ 5.37672694e-01 -7.38091761e+12  9.16586477e-01  7.58859065e-01\n  7.52699829e-01]\n\n! conda list scikit-learn\n# packages in environment at /Users/ehoffman/.conda/envs/prework-labs:\n#\n# Name                    Version                   Build  Channel\nscikit-learn              0.22             py37h3dc85bc_1    conda-forge\n\nThat looks worse.  What if we don\'t use the car name, and just use the categories with 1-3 values?\nX_train_all_except_car_name = X_train_all_features[[\n                    ""Year"",\n                    ""Present_Price"",\n                    ""Kms_Driven"",\n                    ""Owner"",\n                    ""CNG"",\n                    ""Diesel"",\n                    ""Petrol"",\n                    ""Dealer"",\n                    ""Individual"",\n                    ""Automatic"",\n                    ""Manual""\n                ]].copy()\nX_train_all_except_car_name\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nYear\nPresent_Price\nKms_Driven\nOwner\nCNG\nDiesel\nPetrol\nDealer\nIndividual\nAutomatic\nManual\n\n\n\n\n0\n2017\n0.84\n5000\n0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n\n\n1\n2015\n14.79\n12900\n0\n0.0\n0.0\n1.0\n1.0\n0.0\n1.0\n0.0\n\n\n2\n2015\n0.32\n35000\n0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n\n\n3\n2015\n13.60\n21780\n0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n1.0\n\n\n4\n2015\n5.90\n14465\n0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n1.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n220\n2013\n0.57\n18000\n0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n\n\n221\n2011\n12.48\n45000\n0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n\n\n222\n2014\n3.45\n16500\n1\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n\n\n223\n2011\n10.00\n69341\n0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n1.0\n\n\n224\n2017\n1.78\n4000\n0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n\n\n\n225 rows √ó 11 columns\n\nlin_reg_model = LinearRegression()\n\nprint(""Old:"", baseline_cross_val_score)\nprint(""New:"", cross_val_score(lin_reg_model, X_train_all_except_car_name, y_train, cv=5))\nOld: [0.67168559 0.75626366 0.88591659 0.79241643 0.84299344]\nNew: [0.71773298 0.65008625 0.92349676 0.81151078 0.88264199]\n\nlin_reg_model.fit(X_train_all_except_car_name, y_train)\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n\nlin_reg_model.coef_\narray([ 3.66737867e-01,  4.24013125e-01, -5.41982063e-06, -1.01134981e+00,\n       -1.09963446e+00,  1.42995678e+00, -3.30322320e-01,  6.31302952e-01,\n       -6.31302952e-01,  7.77845019e-01, -7.77845019e-01])\n\nlin_reg_model.intercept_\n-736.3855028465609\n\nlin_reg_model.rank_\n8\n\nlin_reg_model.get_params()\n{\'copy_X\': True, \'fit_intercept\': True, \'n_jobs\': None, \'normalize\': False}\n\nOk, adding these categories improved r-squared for 4 out of 5 subsamples compared to just having numeric features, so let\'s keep them for our linear regression model\nbest_linreg_cross_val_score = cross_val_score(lin_reg_model, X_train_all_except_car_name, y_train)\nTry a More Advanced Model\nIt depends on our business case whether these numbers are sufficient.  We are explaining approximately somewhere between 65% and 92% of the variance in the sale price.  But let\'s try a more complicated model.\nFirst, just using the X_train values used in the linear regression:\nrandom_forest_regressor_model_1 = RandomForestRegressor(n_estimators=10, random_state=42)\n\nprint(""Old:"", best_linreg_cross_val_score)\nprint(""New:"", cross_val_score(random_forest_regressor_model_1, X_train_all_except_car_name, y_train, cv=5))\nOld: [0.71773298 0.65008625 0.92349676 0.81151078 0.88264199]\nNew: [0.83102658 0.66532476 0.90650579 0.81714334 0.92862009]\n\nOk, this more-sophisticated model is performing slightly better on 4 of 5 subsamples than the best linear regression score.  Let\'s see what happens if we add the car names back in:\nrandom_forest_regressor_model_2 = RandomForestRegressor(n_estimators=10, random_state=42)\n\nprint(""Old:"", cross_val_score(random_forest_regressor_model_1, X_train_all_except_car_name, y_train, cv=5))\nprint(""New:"", cross_val_score(random_forest_regressor_model_2, X_train_all_features, y_train, cv=5))\nOld: [0.83102658 0.66532476 0.90650579 0.81714334 0.92862009]\nNew: [0.8120682  0.7237103  0.90434184 0.80154837 0.92665771]\n\nOnly one of the subsamples improved with adding this feature, and everything else got worse\nHyperparameter Tuning the More Advanced Model\nLet\'s add some more ""power"" to the random forest regressor, since it\'s running reasonably quickly right now\nrandom_forest_regressor_model_3 = RandomForestRegressor(n_estimators=1000, random_state=42)\n\nprint(""Old:"", cross_val_score(random_forest_regressor_model_1, X_train_all_except_car_name, y_train, cv=5))\nprint(""New:"", cross_val_score(random_forest_regressor_model_3, X_train_all_except_car_name, y_train, cv=5))\nOld: [0.83102658 0.66532476 0.90650579 0.81714334 0.92862009]\nNew: [0.85347083 0.72854389 0.90506631 0.83936817 0.93520377]\n\nThat marginally improved 4 of the 5 subsamples (but was significantly slower to run).  Let\'s try including the car name again:\nrandom_forest_regressor_model_4 = RandomForestRegressor(n_estimators=1000, random_state=42)\n\nprint(""Old:"", cross_val_score(random_forest_regressor_model_3, X_train_all_except_car_name, y_train))\nprint(""New:"", cross_val_score(random_forest_regressor_model_4, X_train_all_features, y_train))\nOld: [0.85347083 0.72854389 0.90506631 0.83936817 0.93520377]\nNew: [0.82470832 0.71709424 0.89251424 0.8456903  0.93846238]\n\nAgain, that didn\'t really seem to help.  So if we\'re stopping right now, we can say that the third random forest regressor is the best model.\nModel Evaluation\nNow that we have chosen a best model, let\'s use the holdout set to see how well the final model does\nFirst, perform all of the same transformations on the test X that were performed on the train X\nX_test_all_except_car_name = X_test.reset_index().drop([""index"", ""Car_Name""], axis=1)\n\n# fuel_type_ohe, seller_type_ohe, and transmission_ohe were fitted on the training data\nX_test_all_except_car_name = encode_and_concat_feature(X_test_all_except_car_name, ""Fuel_Type"", fuel_type_ohe)\nX_test_all_except_car_name = encode_and_concat_feature(X_test_all_except_car_name, ""Seller_Type"", seller_type_ohe)\nX_test_all_except_car_name = encode_and_concat_feature(X_test_all_except_car_name, ""Transmission"", transmission_ohe)\n\nX_test_all_except_car_name\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nYear\nPresent_Price\nKms_Driven\nOwner\nCNG\nDiesel\nPetrol\nDealer\nIndividual\nAutomatic\nManual\n\n\n\n\n0\n2016\n0.57\n24000\n0\n0.0\n0.0\n1.0\n0.0\n1.0\n1.0\n0.0\n\n\n1\n2016\n13.60\n10980\n0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n1.0\n\n\n2\n2012\n9.40\n60000\n0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n\n\n3\n2011\n0.57\n35000\n1\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n\n\n4\n2013\n18.61\n40001\n0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n1.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n71\n2011\n8.01\n50000\n0\n0.0\n0.0\n1.0\n1.0\n0.0\n1.0\n0.0\n\n\n72\n2016\n7.90\n28569\n0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n1.0\n\n\n73\n2015\n7.27\n40534\n0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n1.0\n\n\n74\n2012\n4.43\n23709\n0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n1.0\n\n\n75\n2016\n1.40\n35000\n0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n\n\n\n76 rows √ó 11 columns\n\nFit our best model on all of the training data\nrandom_forest_regressor_model_3.fit(X_train_all_except_car_name, y_train)\nRandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion=\'mse\',\n                      max_depth=None, max_features=\'auto\', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_impurity_split=None, min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=1000, n_jobs=None, oob_score=False,\n                      random_state=42, verbose=0, warm_start=False)\n\nScore our best model on the test data\nrandom_forest_regressor_model_3.score(X_test_all_except_car_name, y_test)\n0.9706072528266274\n\nThat\'s pretty good!  We have a model that is able to explain 97% of the variance in the car sale list prices\nTo report something more applicable to a business audience, let\'s calculate the root mean square error\nfrom sklearn.metrics import mean_squared_error\nmean_squared_error(y_test, random_forest_regressor_model_3.predict(X_test_all_except_car_name))\n0.8075452595118476\n\nnp.sqrt(0.8075452595118476)\n0.8986352204937483\n\nTo interpret this: on average, our prediction of Selling_Price is off (either too high or too low) by about 0.9 lakh, i.e. about 90,000 rupees (about 1200 USD)\nAlso, here is a plot that shows the actual vs. predicted prices:\nfig, ax = plt.subplots()\n\nax.set_xlabel(""True Price (Lakhs)"")\nax.set_ylabel(""Predicted Price (Lakhs)"")\n\nax.scatter(y_test, random_forest_regressor_model_3.predict(X_test_all_except_car_name), alpha=0.5);\n\nYou can see that this model performs quite well on the lower end of price, then makes more mistakes as price increases, particularly beyond 10 lakhs.\nIf we had made this plot before the evaluation phase, maybe we could go back to figure out if there is any additional feature engineering that would help improve the predictions for higher priced cars (e.g. extracting the make from the car name).  But since this is the final evaluation phase, that would be ""cheating"" since it would incorporate information from the test data in the modeling phase.  Later we\'ll discuss overfitting and why this is a problem.\nSimilarities and Differences with Inferential Regression Workflow\nSame\n\nData understanding process (looking at what kinds of features we have, trying to understand the relationship to the target)\nStarting with a baseline model, then building more and checking how well they performed\nPreprocessing needed to prepare categorical variables (all features need to be numeric for the model to understand them)\nDoing a regression analysis and looking at r-squared to see how much of the variance is explained\nEvaluate model at the end\n\nDifferent\n\nChecking model performance along the way\n\nFor predictive, we only looked at the metric of interest (r-squared in this case) and not anything else\nAssumptions of linear regression (linearity, normality, homoscedasticiy, independence)\n\nChecked for these in inferential but not in predictive\nFor inferential, our FSM had only one feature because we were doing these checks at each phase.  For predictive, we added all numeric features into the FSM.\n\n\nCoefficients and their p-values\n\nFor inferential, checked these repeatedly in the modeling and evaluation process\nFor predictive, didn\'t really need to check coefficients at all (although we did towards the end out of curiousity), and never checked p-values\n\n\n\n\nModels used\n\nDifferent packages (sklearn for predictive, statsmodels for inferential) to run the linear regressions\n\nStatsmodels gives a lot more information in the model summary that we don\'t get from the sklearn\'s linear regression\n\n\nFor predictive, we used models other than a linear regression (RandomForestRegressor), inferential only used linear regression\n\nMore advanced models like RandomForestRegressor have hyperparameters we can tune\nHave to specify a random_state if we want the same behavior each time\n\n\n\n\nTrain-test split\n\nFor inferential, we just used all available rows every time we fit the model\nFor predictive, we had a test data set that was used for evaluation at the end\nFor predictive, we also used cross_val_score every time instead of just fitting to everything in X_train\n\n\nOne-hot encoding (I added this after the lesson ended)\n\nFor inferential, dropped the first one of any given category\nFor predictive, didn\'t drop any\n\n\n\n\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['2', 'Jupyter Notebook', 'Updated Mar 4, 2020', '36', 'Python', 'Updated Jun 11, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Python', 'Updated Feb 11, 2021', '17', 'Kotlin', 'Updated Dec 18, 2020', '7', 'Python', 'Updated Mar 9, 2020', '2', 'Python', 'MIT license', 'Updated Jan 4, 2021', '3', 'Python', 'Updated Mar 8, 2020', '1', 'HTML', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Dec 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['mbed-os-tf-m-regression-tests\nThis is a Mbed-flavored application which enables the user to run\nTF-M regression test suite (default) or PSA Compliance test suite\nwith the Mbed OS.\nNote: This repository supports regression and PSA compliance tests for\nTF-M v1.1 which is currently integrated by Mbed OS.\nPrerequisite\nDevelopment environment\nPlease refer to vagrant/bootstrap.sh and vagrant/bootstrap-user.sh for\ndetails on how to set up a development environment. These scripts can be run\nlocally on Linux, or you may use Vagrant to create a VM suitable for\ndevelopment (see vagrant/README.md for instructions).\nMbed initialization\nRun mbed deploy to obtain Mbed OS for this application.\nBuilding TF-M\nWe are building for the ARM Musca B1 (ARM_MUSCA_B1) in our example\nbelow, but other targets can be built for by changing the -m option.\nThis builds the ConfigCoreIPC.cmake config by default.\npython3 build_tfm.py -m ARM_MUSCA_B1 -t GNUARM\n\nNote: This step does not build any test suites, but the files and binaries\ngenerated are checked into Mbed OS repository at the time of release, which\nfurther supports the building of mbed-os-example-psa\nwithout the user requiring to go through the complex process.\nTo display help on supported options and targets:\npython3 build_tfm.py -h\n\nBuilding the TF-M Regression Test\nUse the -c option to specify the config to override the default.\npython3 build_tfm.py -m ARM_MUSCA_B1 -t GNUARM -c ConfigRegressionIPC.cmake\n\nBuilding the PSA Compliance Test\nFirst, run build_psa_compliance.py to build for a target. Different suites can\nbe built using the -s option.\npython3 build_psa_compliance.py -m ARM_MUSCA_B1 -s CRYPTO -t GNUARM\n\nThen run build_tfm.py with the PSA API config.\npython3 build_tfm.py -m ARM_MUSCA_B1 -t GNUARM -c ConfigPsaApiTestIPC.cmake -s CRYPTO\n\nNote: Make sure the TF-M Regression Test suite has PASSED on the board before\nrunning any PSA Compliance Test suite to avoid unpredictable behavior.\nTo display help on supported options and targets:\npython3 build_psa_compliance.py -h\n\nBuilding the Mbed OS application\nAfter building the TF-M regression or\nPSA compliance tests for the target, it should be\nfollowed by building a Mbed OS application. This will execute the test suites previously built.\nConfigure an appropriate test in the config section of mbed_app.json. If you want to\nflash and run tests manually, please set wait-for-sync to 0 so that tests start without\nwaiting.\nmbed compile -m ARM_MUSCA_B1 -t GCC_ARM\n\nRunning the Mbed OS application\n\nConnect your Mbed Enabled device to the computer over USB.\nCopy the binary or hex file to the Mbed device. The binary is located at ./BUILD/<TARGET>/<TOOLCHAIN>/mbed-os-tf-m-regression-tests.hex.\nConnect to the Mbed Device using a serial client application of your choice.\nPress the reset button on the Mbed device to run the program.\n\nNote: The default serial port baud rate is 115200 baud.\nExecute all tests suites\nThis will build and execute TF-M regression and PSA compliance tests with\nMbed OS application. Make sure the device is connected to your local machine.\npython3 test_psa_target.py -t GNUARM -m ARM_MUSCA_B1\n\nNotes:\n\nThis script cannot be executed in the vagrant\nenvironment because it does not have access to the USB of the host machine to\nconnect the target and therefore cannot run the tests, except it can only be\nused to build all the tests by -b option.\nIf you want to flash and run tests manually instead of automating them with Greentea,\nyou need to pass --no-sync so that tests start without waiting.\nThe PSA Crypto test suite is currently excluded from the automated run of all\ntests, because some Crypto tests are known to crash and reboot the target. This\ncauses the Greentea test framework to lose synchronization, and messes up the memory\nand prevents subsequent suites from running.\nYou can flash and run the Crypto suite standalone. Make sure to either pass --no-sync\nto test_psa_target.py when building tests, or build the Crypto suite manually with\nwait-for-sync set to 0 in mbed_app.json. And power cycle the target before and after\nthe run to clear the memory.\n\nTo display help on supported options and targets:\npython3 test_psa_target.py -h\n\n'], 'url_profile': 'https://github.com/ARMmbed', 'info_list': ['2', 'Jupyter Notebook', 'Updated Mar 4, 2020', '36', 'Python', 'Updated Jun 11, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Python', 'Updated Feb 11, 2021', '17', 'Kotlin', 'Updated Dec 18, 2020', '7', 'Python', 'Updated Mar 9, 2020', '2', 'Python', 'MIT license', 'Updated Jan 4, 2021', '3', 'Python', 'Updated Mar 8, 2020', '1', 'HTML', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Dec 16, 2020']}","{'location': 'St.Petersburg', 'stats_list': [], 'contributions': '380 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/zaleslaw', 'info_list': ['2', 'Jupyter Notebook', 'Updated Mar 4, 2020', '36', 'Python', 'Updated Jun 11, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Python', 'Updated Feb 11, 2021', '17', 'Kotlin', 'Updated Dec 18, 2020', '7', 'Python', 'Updated Mar 9, 2020', '2', 'Python', 'MIT license', 'Updated Jan 4, 2021', '3', 'Python', 'Updated Mar 8, 2020', '1', 'HTML', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Dec 16, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['Isotonic\nFrequently in data science, we have a relationship between X and y where (probabilistically) y increases as X does.\nThere is a classical algorithm for solving this problem nonparametrically, specifically Isotonic regression. This simple algorithm is also implemented in sklearn.isotonic. The classic algorithm is based on a piecewise constant approximation - with nodes at every data point - as well as minimizing (possibly weighted) l^2 error.\nI\'m a heavy user of isotonic regression, but unfortunately the version in sklearn does not meet my needs. Specific failings:\n\nMy data is frequently binary. This means that each y[i] is either 0 or 1, but the probability that y[i]==1 increasers as x[i] increases.\nMy data is often noisy with fatter than normal tails, which means that minimizing l^2 error overweights outliers.\nThe size of the isotonic model is large - O(N), in fact (with N the size of the training data).\nThe curves output by sklearn\'s isotonic model are piecewise constant with a large number of discontinuities (O(N) of them).\n\nThis library is an attempt to solve these problems once and for all.\nMore info on details and motivation is described in a companion blog post.\nUsage\nReal valued curves\nTo fit a curve to real valued data using mean squared error, it\'s it\'s pretty straightforward:\nfrom scipy.stats import norm, bernoulli\nimport numpy as np\n\nN = 10000\nx = norm(0,50).rvs(N) - bernoulli(0.25).rvs(N)*50\ny = -7+np.sqrt(np.maximum(x, 0)) + norm(0,0.5).rvs(N)\n\nfrom isotonic import LpIsotonicRegression\nfrom isotonic.curves import PiecewiseLinearIsotonicCurve, PiecewiseConstantIsotonicCurve\n\nimport pandas as pd\nfrom bokeh.plotting import figure, output_notebook, show\nfrom bokeh.models import Span, LinearAxis, Range1d, ColumnDataSource\noutput_notebook()\n\nplot = figure(\n    tools=""pan,box_zoom,reset,save,"",\n    y_axis_label=""y"", title=""isotonic"",\n    x_axis_label=\'x\'\n)\n\ncurve = LpIsotonicRegression(10, increasing=True, curve_algo=PiecewiseLinearIsotonicCurve).fit(x, y)\ncurve2 = LpIsotonicRegression(10, increasing=True, curve_algo=PiecewiseConstantIsotonicCurve).fit(x, y)\n#curve = PiecewiseIsotonicCurve(x_cuts, gamma_of_alpha(result.x))\nxx = np.arange(x.min(), x.max(), 0.01)\nplot.line(xx, curve.predict_proba(xx), color=\'red\', legend_label=\'piecewise linear\')\nplot.line(xx, curve2.predict_proba(xx), color=\'blue\', legend_label=\'piecewise constant\')\n\nplot.circle(x, y, color=\'black\', alpha=0.02, legend_label=\'raw data\')\n\nshow(plot)\n\n\nLike most regression methods based on l^2 loss, isotonic regression is sensitive to noise. As the name LpIsotonicRegression suggests, one can use alternate powers to accomodate greater degrees of noise. Consider the same example as above, but 5% of the samples are corrupted by high intensity Laplacian noise:\ny = -7+np.sqrt(np.maximum(x, 0)) + norm(0,0.5).rvs(N) + bernoulli(0.05).rvs(N) * laplace(scale=50).rvs(N)\n\nWe can compare the result of LpIsotonicRegression with different powers. Choosing an norm l^p for p nearly 1 yields a fit significantly less sensitive:\ncurve = LpIsotonicRegression(20, power=2, increasing=True, curve_algo=PiecewiseLinearIsotonicCurve).fit(x, y)\ncurve2 = LpIsotonicRegression(20, power=1.1, increasing=True, curve_algo=PiecewiseLinearIsotonicCurve).fit(x, y)\n\n\nIsotonic probability estimation\nIn many cases the data I wish to handle is binary, not real valued. That is, every y[i] is either 0 or 1. The value I wish to estimate is the probability that y == 1, given a value of x.\nIn isotonic, this is handled with the BinomialIsotonicRegression class. This fits a curve to binary data based on a binomial loss function.\nfrom isotonic import BinomialIsotonicRegression\nfrom isotonic.curves import PiecewiseLinearIsotonicCurve, PiecewiseConstantIsotonicCurve\n\nimport pandas as pd\nfrom bokeh.plotting import figure, output_notebook, show\nfrom bokeh.models import Span, LinearAxis, Range1d, ColumnDataSource\noutput_notebook()\n\nplot = figure(\n    tools=""pan,box_zoom,reset,save,"",\n    y_axis_label=""y"", title=""isotonic"",\n    x_axis_label=\'x\'\n)\n\nM = 10\nx_cuts = np.quantile(x, np.arange(0,1,1/M))\ndf = pd.DataFrame({\'x\': x, \'y\': y, \'x_c\': x_cuts[np.digitize(x, x_cuts)-1]})\ngrouped = df.groupby(\'x_c\')[\'y\'].mean().reset_index()\n\nplot.circle(grouped[\'x_c\'], grouped[\'y\'], color=\'green\', legend_label=\'true_frac\')\n\ncurve = BinomialIsotonicRegression(10, increasing=True, curve_algo=PiecewiseLinearIsotonicCurve).fit(x, y)\ncurve2 = BinomialIsotonicRegression(10, increasing=True, curve_algo=PiecewiseConstantIsotonicCurve).fit(x, y)\n\nxx = np.arange(x.min(), x.max(), 0.01)\nplot.line(xx, curve.predict_proba(xx), color=\'red\', legend_label=\'piecewise linear\')\nplot.line(xx, curve2.predict_proba(xx), color=\'blue\', legend_label=\'piecewise constant\')\n\nplot.circle(x, y, color=\'black\', alpha=0.01, legend_label=\'raw data\')\nshow(plot)\n\n\n'], 'url_profile': 'https://github.com/stucchio', 'info_list': ['2', 'Jupyter Notebook', 'Updated Mar 4, 2020', '36', 'Python', 'Updated Jun 11, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Python', 'Updated Feb 11, 2021', '17', 'Kotlin', 'Updated Dec 18, 2020', '7', 'Python', 'Updated Mar 9, 2020', '2', 'Python', 'MIT license', 'Updated Jan 4, 2021', '3', 'Python', 'Updated Mar 8, 2020', '1', 'HTML', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Dec 16, 2020']}","{'location': 'United States', 'stats_list': [], 'contributions': '44 contributions\n        in the last year', 'description': ['\n\n\n\n\n\nIntroduction\nLinear regression of the simple least-squares variety has been a canonical method used to characterize\nthe relation between two variables, but its utility is limited by the fact that it reduces full\npopulation statistics down to three numbers: a slope, normalization and variance/standard deviation.\nWith large empirical or simulated samples we can perform a more sensitive analysis\nusing a localized linear regression method (see, Farahi et al. 2018 and Anbajagane et al. 2020).\nThe KLLR method generates estimates of conditional statistics in terms of the local the slope, normalization,\nand covariance. Such a method provides a more nuanced description of population statistics appropriate\nfor the very large samples with non-linear trends.\nThis code is an implementation of the Kernel Localized Linear Regression (KLLR) method\nthat performs a localized Linear regression described in Farahi et al. (2018). It employs\nbootstrap re-sampling technique to estimate the uncertainties. We also provide a set of visualization\ntools so practitioners can seamlessly generate visualization of the model parameters.\nIf you use KLLR or derivates based on it, please cite the following papers which introduced the tool:\nLocalized massive halo properties in BAHAMAS and MACSIS simulations: scalings, lognormality, and covariance\nStellar Property Statistics of Massive Halos from Cosmological Hydrodynamics Simulations: Common Kernel Shapes\nDependencies\nnumpy, scipy,  matplotlib, pandas, sklearn, tqdm\nReferences\n[1]. A. Farahi, et al. ""Localized massive halo properties in BAHAMAS and MACSIS simulations: scalings, lognormality, and covariance."" Monthly Notices of the Royal Astronomical Society 478.2 (2018): 2618-2632.\n[2]. D. Anbajagane, et al. Stellar Property Statistics of Massive Halos from Cosmological Hydrodynamics Simulations: Common Kernel Shapes. No. arXiv: 2001.02283. 2020.\nAcknowledgment\nQuickstart\nTo start using KLLR, simply use from KLLR import kllr_model to\naccess the primary functions and class. The exact requirements for the inputs are\nlisted in the docstring of the kllr_model() class further below.\nAn example for using KLLR looks like this:\n    from kllr import kllr_model                                       \n                                                                      \n    lm = kllr_model(kernel_type = \'gaussian\', kernel_width = 0.2)     \n    xrange, yrange_mean, intercept, slope, scatter = lm.fit(x, y, nbins=11)                                   \n\n'], 'url_profile': 'https://github.com/afarahi', 'info_list': ['2', 'Jupyter Notebook', 'Updated Mar 4, 2020', '36', 'Python', 'Updated Jun 11, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Python', 'Updated Feb 11, 2021', '17', 'Kotlin', 'Updated Dec 18, 2020', '7', 'Python', 'Updated Mar 9, 2020', '2', 'Python', 'MIT license', 'Updated Jan 4, 2021', '3', 'Python', 'Updated Mar 8, 2020', '1', 'HTML', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Dec 16, 2020']}","{'location': 'patna', 'stats_list': [], 'contributions': '36 contributions\n        in the last year', 'description': ['linear_regression\nIn this repository i have attached code of linear regressing both by using machine learning library and without using it.\nI have attached three program for linear regression.\n\n\nlinear_regression in one variable:- in this i have taken no of data input from user,features and labels then i calculated most suitable slope and equation using formula of linear regression i plotted those points with a st. line which is the output of the linear regression calculation.Along with this i have displayed equation obtained for predicting .\n\n\nml2 and ml3 :- in these code i have used machine learning library for doing prediction on the basis of linear regression.. details are commented in code.\n\n\n'], 'url_profile': 'https://github.com/adarshbhagat', 'info_list': ['2', 'Jupyter Notebook', 'Updated Mar 4, 2020', '36', 'Python', 'Updated Jun 11, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Python', 'Updated Feb 11, 2021', '17', 'Kotlin', 'Updated Dec 18, 2020', '7', 'Python', 'Updated Mar 9, 2020', '2', 'Python', 'MIT license', 'Updated Jan 4, 2021', '3', 'Python', 'Updated Mar 8, 2020', '1', 'HTML', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Dec 16, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '52 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/deepanshu-goyal', 'info_list': ['2', 'Jupyter Notebook', 'Updated Mar 4, 2020', '36', 'Python', 'Updated Jun 11, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Python', 'Updated Feb 11, 2021', '17', 'Kotlin', 'Updated Dec 18, 2020', '7', 'Python', 'Updated Mar 9, 2020', '2', 'Python', 'MIT license', 'Updated Jan 4, 2021', '3', 'Python', 'Updated Mar 8, 2020', '1', 'HTML', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Dec 16, 2020']}","{'location': 'Hyderabad, Telengana, India', 'stats_list': [], 'contributions': '2,340 contributions\n        in the last year', 'description': ['Regressoin\nLinear Regression\n'], 'url_profile': 'https://github.com/azhar2ds', 'info_list': ['2', 'Jupyter Notebook', 'Updated Mar 4, 2020', '36', 'Python', 'Updated Jun 11, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Python', 'Updated Feb 11, 2021', '17', 'Kotlin', 'Updated Dec 18, 2020', '7', 'Python', 'Updated Mar 9, 2020', '2', 'Python', 'MIT license', 'Updated Jan 4, 2021', '3', 'Python', 'Updated Mar 8, 2020', '1', 'HTML', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Dec 16, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/animeshjanai', 'info_list': ['Updated Mar 4, 2020', 'MIT license', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Sep 15, 2020', 'Python', 'Updated Mar 4, 2020', 'Python', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'Python', 'Updated Mar 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '91 contributions\n        in the last year', 'description': ['Linear-Regression\nLinear Regression problems are generally used to solve problems in which data are related linearly and are collinear.\n'], 'url_profile': 'https://github.com/Nachiappan1808', 'info_list': ['Updated Mar 4, 2020', 'MIT license', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Sep 15, 2020', 'Python', 'Updated Mar 4, 2020', 'Python', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'Python', 'Updated Mar 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/meirm7', 'info_list': ['Updated Mar 4, 2020', 'MIT license', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Sep 15, 2020', 'Python', 'Updated Mar 4, 2020', 'Python', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'Python', 'Updated Mar 15, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Sumed0109', 'info_list': ['Updated Mar 4, 2020', 'MIT license', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Sep 15, 2020', 'Python', 'Updated Mar 4, 2020', 'Python', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'Python', 'Updated Mar 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ytserilo', 'info_list': ['Updated Mar 4, 2020', 'MIT license', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Sep 15, 2020', 'Python', 'Updated Mar 4, 2020', 'Python', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'Python', 'Updated Mar 15, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '93 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/WaiswaDonnie', 'info_list': ['Updated Mar 4, 2020', 'MIT license', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Sep 15, 2020', 'Python', 'Updated Mar 4, 2020', 'Python', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'Python', 'Updated Mar 15, 2020']}","{'location': 'Budapest', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['Regression\nThis is the repository for experimenting on Regression algorithms.\n'], 'url_profile': 'https://github.com/matescharnitzky', 'info_list': ['Updated Mar 4, 2020', 'MIT license', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Sep 15, 2020', 'Python', 'Updated Mar 4, 2020', 'Python', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'Python', 'Updated Mar 15, 2020']}","{'location': 'South Africa', 'stats_list': [], 'contributions': '88 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/BhekiMabheka', 'info_list': ['Updated Mar 4, 2020', 'MIT license', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Sep 15, 2020', 'Python', 'Updated Mar 4, 2020', 'Python', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'Python', 'Updated Mar 15, 2020']}","{'location': 'Kazan, Russia', 'stats_list': [], 'contributions': '266 contributions\n        in the last year', 'description': ['–†–µ–≥—Ä–µ—Å—Å–∏—è\n–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ —Å –≤–∏–∑—É–∞–ª—å–Ω—ã–º –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º —Å –ø–æ–º–æ—â—å—é Matplotlib\n–í —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –º—ã –æ–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ –∏–∑–≤–µ—Å—Ç–Ω—ã–º —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º –æ–±—ä–µ–∫—Ç–∞ –∑–Ω–∞—á–µ–Ω–∏–µ –∫–∞–∫–æ–≥–æ-–ª–∏–±–æ –µ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞.\n–í –±–∏–±–ª–∏–æ—Ç–µ–∫–µ sklearn –µ—Å—Ç—å –º–µ—Ç–æ–¥ –ª–∏–Ω–µ–π–Ω–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ LinearRegression, —Ç–∞–∫–∂–µ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–µ—Ç–æ–¥ k –±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–µ–π KNeighborsRegressor.\n–í –∫–æ–Ω—Ü–µ –ø—Ä–∏–º–µ—Ä–∞ —Ä–∞—Å—Å—á–∏—Ç–∞–Ω–∞ —Ç–æ—á–Ω–æ—Å—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –¥–ª—è –¥–∞—Ç–∞—Å–µ—Ç–∞.\n'], 'url_profile': 'https://github.com/MarkVoitov', 'info_list': ['Updated Mar 4, 2020', 'MIT license', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Sep 15, 2020', 'Python', 'Updated Mar 4, 2020', 'Python', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'Python', 'Updated Mar 15, 2020']}","{'location': 'London', 'stats_list': [], 'contributions': '50 contributions\n        in the last year', 'description': ['Machine-Learning\nTopic: Linear Regression, Logistic Regression, Classification\nproject 01 - SVM training in predicting the class of flowers\n'], 'url_profile': 'https://github.com/Robert-Ruilin', 'info_list': ['Updated Mar 4, 2020', 'MIT license', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Sep 15, 2020', 'Python', 'Updated Mar 4, 2020', 'Python', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated May 1, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'Python', 'Updated Mar 15, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['reg\nregression\n I will post some slides of regression in classes\n'], 'url_profile': 'https://github.com/Program-Wang', 'info_list': ['Updated Mar 4, 2020', '2', 'TypeScript', 'MIT license', 'Updated Mar 6, 2020', '4', 'Go', 'MIT license', 'Updated Aug 31, 2020', 'HTML', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Java', 'Updated Apr 1, 2020', 'Updated Jan 11, 2021', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Updated Mar 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Visual Regression Testing\nThe code has been extracted from our project, we haven\'t set up the build process in this repository yet, but it should be in working order if you want to get started on your own.\nUsage\nNote: In the index.tsx at the VRFixture, you\'ll need to replace the render function with your CSS in JS render function.\nit(\n    \'test name\',\n    // vrFixture sets everything up and passes you a fixture\n    vrFixture(async fixture => {\n        // fixture exposes takeSnapshot(reactElement, options)\n        await fixture.takeSnapshot(\n            wrapInRouter(<BreakingNews text=""Example words"" link=""/example-link"" />, thewest),\n            {\n                viewPortSize: \'iPad\'\n            }\n        )\n    })\n)\nOptions\nviewPortSize?: puppeteer.ViewPort | PuppeteerDevice\nA puppeteer viewport contains width, height, pixel density and other viewport related things\nYou can also pass a device which will emulate the viewport of that device\ngrowContainer?: { height?: number, width?: number }\nUseful when the component floats outside the container and you need to grow the snapshot container\npadding?: { left: number; right: number }\nAdds padding around a component, useful for detecting issues where container breakpoints would be handy but we only have viewport breakpoints\npreScreenshotHook?: (page: puppeteer.Page) => Promise\nGives you access to the page before the screenshot is taken, this is useful to force hover/focus states etc\n'], 'url_profile': 'https://github.com/sevenwestmedia-labs', 'info_list': ['Updated Mar 4, 2020', '2', 'TypeScript', 'MIT license', 'Updated Mar 6, 2020', '4', 'Go', 'MIT license', 'Updated Aug 31, 2020', 'HTML', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Java', 'Updated Apr 1, 2020', 'Updated Jan 11, 2021', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Updated Mar 5, 2020']}","{'location': 'Brazil-Brasilia-DF', 'stats_list': [], 'contributions': '1,073 contributions\n        in the last year', 'description': ['signalutils\n\nEvent/Signal processing utilities lib for Golang. Online moving averager, linear regression, timed value, worker frequency control in Golang etc\nSee API documentation at https://pkg.go.dev/github.com/flaviostutz/signalutils?tab=doc\nUsage\npackage main\nimport (\n\t""fmt""\n\t""github.com/flaviostutz/signalutils""\n)\n\nfunc main() {\n\tfmt.Printf(""Moving Average\\n"")\n\tma := signalutils.NewMovingAverage(5)\n\tma.AddSample(0.00)\n\tma.AddSample(99999.00)\n\tfmt.Printf(""Average is %f\\n"", ma.Average())\n\tma.AddSample(1000.00)\n\tma.AddSample(2000.00)\n\tfmt.Printf(""Average is %f\\n"", ma.Average())\n\tma.AddSample(3000.00)\n\tma.AddSample(4000.00)\n\tfmt.Printf(""Average is %f\\n"", ma.Average())\n\tma.AddSample(5000.00)\n\tma.AddSample(6000.00)\n\tfmt.Printf(""Average is %f\\n"", ma.Average())\n}\nResults\nMoving Average\nAverage is 49999.500000\nAverage is 25749.750000\nAverage is 21999.800000\nAverage is 4000.000000\n\nUtilities\n\nMovingAverage - add values to an array with a fixed max size and query for the average of values in this fixed size array\n\n\tma := NewMovingAverageTimeWindow(1*time.Second, 10)\n\tma.AddSample(1000)\n\tma.AddSample(2000)\n\tma.AddSample(3000)\n\tma.AddSample(4000)\n\tma.AddSample(3000)\n\tma.AddSample(2000)\n\tma.AddSample(3000)\n\tma.AddSample(2000)\n\tassert.Equal(t, 1000.0, ma.Average())\n\ttime.Sleep(100 * time.Millisecond)\n\tma.AddSample(3000)\n\tassert.Equal(t, 2000.0, ma.Average())\n\tma.AddSample(2000)\n\tassert.Equal(t, 2000.0, ma.Average())\n\nSchmittTrigger - set current values and track current up/down state based on schmitt trigger algorithm\n\n\tst, _ := NewSchmittTrigger(10, 20, false)\n\tassert.False(t, st.IsUpperRange())\n\tst.SetCurrentValue(11)\n\tassert.False(t, st.IsUpperRange())\n\tst.SetCurrentValue(15)\n\tassert.False(t, st.IsUpperRange())\n\n\tst.SetCurrentValue(21)\n\tassert.True(t, st.IsUpperRange())\n\tst.SetCurrentValue(25)\n\tassert.True(t, st.IsUpperRange())\n\tst.SetCurrentValue(18)\n\tassert.True(t, st.IsUpperRange())\n\n\nDynamicSchmittTrigger - set current values and track current up/down state based on schmitt trigger algorithm. Trigger points are set dynamically set in a timelly manner.\n\n\nStateTracker - set state identifications and if state has lots of successive repetitions, perform a state transition. Useful to filter out noises from state changes.\n\n\n\tst := NewStateTracker(""state1"", 3, onChange, 0, nil, true)\n\tst.SetTransientState(""state2"")\n\tst.SetTransientState(""state2"")\n\tassert.Equal(t, ""state1"", st.CurrentState.Name)\n\tst.SetTransientState(""state2"")\n\tassert.Equal(t, ""state2"", notifiedNewState.Name)\n\tst.SetTransientState(""state3"")\n\tassert.Equal(t, ""state2"", st.CurrentState.Name)\n\tst.SetTransientState(""state3"")\n\tst.SetTransientState(""state2"")\n\tst.SetTransientState(""state3"")\n\tst.SetTransientState(""state3"")\n\tassert.Equal(t, ""state2"", st.CurrentState.Name)\n\nTimeseries - time/value array with max time span for keeping size at control. If you try to get values between time points, interpolation will occur.\n\n\tts := NewTimeseries(1000 * time.Millisecond)\n\n\tts.AddSample(-100)\n\ttime.Sleep(500 * time.Millisecond)\n\tts.AddSample(-1000)\n\n\tnv, ok := ts.GetValue(time.Now().Add(-250 * time.Millisecond))\n\tassert.True(t, ok)\n\tassert.InDeltaf(t, float64(-555), nv.Value, float64(20), """")\n\nTimeseriesCounterRate - add counter values to a timeseries and query for rate at any time range. Something that ressembles ""rate(metric_name[1m])"" on Prometheus queries, for example.\n\n\tts := NewTimeseriesCounterRate(1 * time.Second)\n\n\t_, ok := ts.Rate(1 * time.Second)\n\tassert.False(t, ok)\n\n\tts.Inc(100000) //100000\n\ttime.Sleep(300 * time.Millisecond)\n\tts.Inc(100000) //200000\n\ttime.Sleep(300 * time.Millisecond)\n\tts.Inc(200000) //400000\n\ttime.Sleep(300 * time.Millisecond)\n\tts.Inc(100000) //500000\n\ttime.Sleep(300 * time.Millisecond)\n\n\t_, ok = ts.Rate(2 * time.Second)\n\tassert.False(t, ok)\n\nWorker - useful for workloads that works on a ""while true"" loop. It launches a Go routine with a function, limits the loop frequency, measures actual frequency and alerts if frequency is outside desired limits.\n\n\tw := StartWorker(context.Background(), ""test1"", func() error {\n\t\t//do some real work here\n\t\ttime.Sleep(15 * time.Millisecond)\n\t\treturn nil\n\t}, 1, 5, true)\n\ttime.Sleep(200 * time.Millisecond)\n\tassert.True(t, w.active)\n\ttime.Sleep(2000 * time.Millisecond)\n\tassert.InDeltaf(t, 5, w.CurrentFreq, 2, """")\n\tassert.InDeltaf(t, 15, w.CurrentStepTime.Milliseconds(), 5, """")\n\tw.Stop()\n\ttime.Sleep(300 * time.Millisecond)\n\tassert.False(t, w.active)\n'], 'url_profile': 'https://github.com/flaviostutz', 'info_list': ['Updated Mar 4, 2020', '2', 'TypeScript', 'MIT license', 'Updated Mar 6, 2020', '4', 'Go', 'MIT license', 'Updated Aug 31, 2020', 'HTML', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Java', 'Updated Apr 1, 2020', 'Updated Jan 11, 2021', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Updated Mar 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['for self-study only\n'], 'url_profile': 'https://github.com/CheesecakeMonoxide', 'info_list': ['Updated Mar 4, 2020', '2', 'TypeScript', 'MIT license', 'Updated Mar 6, 2020', '4', 'Go', 'MIT license', 'Updated Aug 31, 2020', 'HTML', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Java', 'Updated Apr 1, 2020', 'Updated Jan 11, 2021', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Updated Mar 5, 2020']}","{'location': 'Battaramulla,Srilanka', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Banukakdu', 'info_list': ['Updated Mar 4, 2020', '2', 'TypeScript', 'MIT license', 'Updated Mar 6, 2020', '4', 'Go', 'MIT license', 'Updated Aug 31, 2020', 'HTML', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Java', 'Updated Apr 1, 2020', 'Updated Jan 11, 2021', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Updated Mar 5, 2020']}","{'location': 'Portland, Oregon', 'stats_list': [], 'contributions': '447 contributions\n        in the last year', 'description': ['predict-heights-weights\nlogistic regression\n'], 'url_profile': 'https://github.com/laurenalexandra999', 'info_list': ['Updated Mar 4, 2020', '2', 'TypeScript', 'MIT license', 'Updated Mar 6, 2020', '4', 'Go', 'MIT license', 'Updated Aug 31, 2020', 'HTML', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Java', 'Updated Apr 1, 2020', 'Updated Jan 11, 2021', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Updated Mar 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['NONE'], 'url_profile': 'https://github.com/cwilder27PVH', 'info_list': ['Updated Mar 4, 2020', '2', 'TypeScript', 'MIT license', 'Updated Mar 6, 2020', '4', 'Go', 'MIT license', 'Updated Aug 31, 2020', 'HTML', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Java', 'Updated Apr 1, 2020', 'Updated Jan 11, 2021', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Updated Mar 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['reg_linear\nLinear Regression\n'], 'url_profile': 'https://github.com/shuzlee', 'info_list': ['Updated Mar 4, 2020', '2', 'TypeScript', 'MIT license', 'Updated Mar 6, 2020', '4', 'Go', 'MIT license', 'Updated Aug 31, 2020', 'HTML', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Java', 'Updated Apr 1, 2020', 'Updated Jan 11, 2021', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Updated Mar 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/archanaravi21', 'info_list': ['Updated Mar 4, 2020', '2', 'TypeScript', 'MIT license', 'Updated Mar 6, 2020', '4', 'Go', 'MIT license', 'Updated Aug 31, 2020', 'HTML', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Java', 'Updated Apr 1, 2020', 'Updated Jan 11, 2021', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Updated Mar 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ArinB', 'info_list': ['Updated Mar 4, 2020', '2', 'TypeScript', 'MIT license', 'Updated Mar 6, 2020', '4', 'Go', 'MIT license', 'Updated Aug 31, 2020', 'HTML', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Java', 'Updated Apr 1, 2020', 'Updated Jan 11, 2021', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Updated Mar 5, 2020']}"
"{'location': 'Pune', 'stats_list': [], 'contributions': '176 contributions\n        in the last year', 'description': ['Internet-Case-Study-Analysis-\nRegression model\n'], 'url_profile': 'https://github.com/agasheaditya', 'info_list': ['R', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '2', 'Jupyter Notebook', 'Updated Mar 2, 2020', '2', 'Updated Mar 3, 2020', 'R', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'R', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020']}","{'location': 'Bangalore, India.', 'stats_list': [], 'contributions': '110 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sibu1234', 'info_list': ['R', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '2', 'Jupyter Notebook', 'Updated Mar 2, 2020', '2', 'Updated Mar 3, 2020', 'R', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'R', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/archanaravi21', 'info_list': ['R', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '2', 'Jupyter Notebook', 'Updated Mar 2, 2020', '2', 'Updated Mar 3, 2020', 'R', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'R', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '61 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/jainammm', 'info_list': ['R', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '2', 'Jupyter Notebook', 'Updated Mar 2, 2020', '2', 'Updated Mar 3, 2020', 'R', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'R', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '39 contributions\n        in the last year', 'description': [""Linear-Regression\nLinear Regression implementation on different datasets.\nAll the datasets are kept in the dataset file\nimport the respective dataset to work on that and predict whatever you want,for this repository I worked on google colab\nso if u don't have anaconda or jupyter don't worry google have all problem solution.\nSo, open google colab and create new file and write the code to implement for prediction\n""], 'url_profile': 'https://github.com/SeniorDev009', 'info_list': ['R', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '2', 'Jupyter Notebook', 'Updated Mar 2, 2020', '2', 'Updated Mar 3, 2020', 'R', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'R', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': ['ot_regression\nCode for the paper Regression via Implicit Models and Optimal Transport Cost Minimization\nRefactoring the code, will commit the updated code soon. Please contact Saurav Manchanda (manch043@umn.edu) for any questions.\n'], 'url_profile': 'https://github.com/gurdaspuriya', 'info_list': ['R', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '2', 'Jupyter Notebook', 'Updated Mar 2, 2020', '2', 'Updated Mar 3, 2020', 'R', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'R', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '31 contributions\n        in the last year', 'description': ['logisticregression\npredictive logistic regression\n'], 'url_profile': 'https://github.com/jan3011', 'info_list': ['R', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '2', 'Jupyter Notebook', 'Updated Mar 2, 2020', '2', 'Updated Mar 3, 2020', 'R', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'R', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020']}","{'location': 'Delhi', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': [""logistic-regression\nIn this project we will be working with a fake advertising data set, indicating whether or not a particular internet user clicked on an Advertisement. We will try to create a model that will predict whether or not they will click on an ad based off the features of that user.\nThis data set contains the following features:\n'Daily Time Spent on Site': consumer time on site in minutes\n'Age': cutomer age in years\n'Area Income': Avg. Income of geographical area of consumer\n'Daily Internet Usage': Avg. minutes a day consumer is on the internet\n'Ad Topic Line': Headline of the advertisement\n'City': City of consumer\n'Male': Whether or not consumer was male\n'Country': Country of consumer\n'Timestamp': Time at which consumer clicked on Ad or closed window\n'Clicked on Ad': 0 or 1 indicated clicking on Ad\n""], 'url_profile': 'https://github.com/SimpsonStark', 'info_list': ['R', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '2', 'Jupyter Notebook', 'Updated Mar 2, 2020', '2', 'Updated Mar 3, 2020', 'R', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'R', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '151 contributions\n        in the last year', 'description': ['KLR\nKernel Logistic Regression\nInstallation\nInstall the devtools package.\nInstall this package using:\ndevtools::install_github(""fontaine618/KLR"")\nExample\nFit the model:\nlibrary(KLR)\nKLRobj = KLR(y, x, kernel=""gaussian"", lambda=0.001, sigma2=2.0)\nKLRobj = KLR(y, x, kernel=""polynomial"", lambda=0.001, sigma2=2.0, d=3)\nObtain predictions on the original data or on new data:\npredict(KLRobj)\npredict(KLRobj, newx)\nObtain level curves (contours):\ncontours(KLRobj, dims=1:2, res=100, levels=0.5)\nPerform cross-validation for the tuning parameters:\ncv.KLR(y, x, n_folds=10, lambda=c(0.001, 0.01, 0.1, 0.1), sigma2=c(0.1,0.5,1.0,2.0))\n'], 'url_profile': 'https://github.com/fontaine618', 'info_list': ['R', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '2', 'Jupyter Notebook', 'Updated Mar 2, 2020', '2', 'Updated Mar 3, 2020', 'R', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'R', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/RF37tes', 'info_list': ['R', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '2', 'Jupyter Notebook', 'Updated Mar 2, 2020', '2', 'Updated Mar 3, 2020', 'R', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'R', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ExponentialDS', 'info_list': ['Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Sep 28, 2020', 'Python', 'Updated Mar 4, 2020', 'Updated Mar 3, 2020', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Sep 4, 2020', 'JavaScript', 'Updated Mar 2, 2021', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 3, 2020']}","{'location': 'Yogyakarta, ID', 'stats_list': [], 'contributions': '105 contributions\n        in the last year', 'description': ['linear_regression\nName : Nadhifa Sofia\nThe assignment is about determining the linear regression of 2 features on the scraped data. The dataset itself was taken from  https://www.the-numbers.com/movie/budgets/all/2501.\nOn this assignment, columns that are used : Production Budget and Worldwide Gross.\nLinear regression is a linear approach to modeling the relationship between a scalar response (or dependent variable) and one or more explanatory variables (or independent variables). The case of one explanatory variable is called simple linear regression. For more than one explanatory variable, the process is called multiple linear regression.\nAnother advantages of Linear Regression :\n\nEvaluating Trends and Sales Estimates\nAnalyzing the Impact of Price Changes\nAssessing Risk\n\nFor example\nA movie producer might conduct a linear regression plotting number of production budget per previous film against worldwide gross and discover that older budget tend to make different worldwide revenue. The results of such an analysis might guide important business decisions made to account for risk.\n'], 'url_profile': 'https://github.com/dhifaaans', 'info_list': ['Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Sep 28, 2020', 'Python', 'Updated Mar 4, 2020', 'Updated Mar 3, 2020', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Sep 4, 2020', 'JavaScript', 'Updated Mar 2, 2021', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['Test-Score-Regression\nDescription\nThis project aims to answer the question:\n\n""Can we use student demographics and neighborhood characteristics to predict academic success for a school in a given zip code?""\n\nThis model aims to provide the following insights\n- Inform district leaders how to better support specific school sites \n- Provide insight into where to build a new school\n- Shed light to a larger system at play in the way success is defined that determine how resources and funds are allocated\n\nIn our model, Academic Success is defined as English Language Arts/Literacy (CAASPP) Scores for high schools in California. This model can easily be replicated for\nother test groups and scores for the CAASPP.\nData was primarily collected from the California Department of Education website\n- data bank: https://caaspp-elpac.cde.ca.gov/caaspp/\n- data key: https://caaspp-elpac.cde.ca.gov/caaspp/research_fixfileformat19\n\nFile Explanations\n\nCA Public School Success: This jupyter notebook contains all code for this project [this was my first individual data science project, please be understanding!]\nMy process as can be seen in the code was collecting data(combining files & webscraping) > cleaning data > EDA > modeling > feature selection > model tuning\nProject 2 Presentation: The powerpoint I created for this project can be found here with slides and visualizations describing the tools I used, my process, and findings\nmisc: The raw data files I found online or scraped, dataframes I pickled in my code, and additional visuals can be found in their labeled folders.\n\n'], 'url_profile': 'https://github.com/elliot-tam', 'info_list': ['Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Sep 28, 2020', 'Python', 'Updated Mar 4, 2020', 'Updated Mar 3, 2020', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Sep 4, 2020', 'JavaScript', 'Updated Mar 2, 2021', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['Train-model-linear-regression\nCh·∫°y file .py ƒë·ªÉ training model k·∫øt h·ª£p 3 th√¥ng s·ªë\nFile data.csv ch·ª©a d·ªØ li·ªáu theo c·∫•u tr√∫c: K√≠ch ho·∫°t, nh·ªãp tim, gia t·ªëc, k√™u c·ª©u\n\nV√≠ d·ª•:\n- 1,150,2000,94 : c√≥ nghƒ©a l√† nh·ªãp tim = 150, gia t·ªëc = 2000, k√™u c·ª©u = 94 th√¨ 1 = k√≠ch ho·∫°t\n- 0,89,400,34 : c√≥ nghƒ©a l√† nh·ªãp tim = 89, gia t·ªëc = 400, k√™u c·ª©u = 34 th√¨ 0 = kh√¥ng k√≠ch ho·∫°t\nCh√∫ √Ω c·∫ßn c√†i:\n+ tensorflow phi√™n b·∫£n 1.13.1\n+ keras m·ªõi nh·∫•t\nC√†i tensorflow: pip install tensorflow==1.13.1\nC√†i keras: pip install keras\n'], 'url_profile': 'https://github.com/duongvanphung', 'info_list': ['Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Sep 28, 2020', 'Python', 'Updated Mar 4, 2020', 'Updated Mar 3, 2020', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Sep 4, 2020', 'JavaScript', 'Updated Mar 2, 2021', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 3, 2020']}","{'location': 'Ahmedabad', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['Regression-Models-Course-Project\nRegression Models Course Project\n'], 'url_profile': 'https://github.com/nehapandya2811', 'info_list': ['Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Sep 28, 2020', 'Python', 'Updated Mar 4, 2020', 'Updated Mar 3, 2020', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Sep 4, 2020', 'JavaScript', 'Updated Mar 2, 2021', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['NONE'], 'url_profile': 'https://github.com/Arpita-Neha', 'info_list': ['Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Sep 28, 2020', 'Python', 'Updated Mar 4, 2020', 'Updated Mar 3, 2020', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Sep 4, 2020', 'JavaScript', 'Updated Mar 2, 2021', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '300 contributions\n        in the last year', 'description': ['Prediction of Cardiovascular Disease through Shrinkage Methods\nThe data consists of 70,000 observations with 11 relevant features to predict cardiovascular disease. I included all possible interaction terms in my design matrix. Through LASSO regression, I achieved an accuracy of 70%. 5-fold cross-validation was utilized to optimize the model.\n'], 'url_profile': 'https://github.com/teddythepooh', 'info_list': ['Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Sep 28, 2020', 'Python', 'Updated Mar 4, 2020', 'Updated Mar 3, 2020', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Sep 4, 2020', 'JavaScript', 'Updated Mar 2, 2021', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 3, 2020']}","{'location': 'Cambridge, UK', 'stats_list': [], 'contributions': '39 contributions\n        in the last year', 'description': ['Elm regression testing\n\n'], 'url_profile': 'https://github.com/decioferreira', 'info_list': ['Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Sep 28, 2020', 'Python', 'Updated Mar 4, 2020', 'Updated Mar 3, 2020', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Sep 4, 2020', 'JavaScript', 'Updated Mar 2, 2021', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '44 contributions\n        in the last year', 'description': ['Math423\nlinear regression\nFinal project was to build a linear regression model in prediction the age of ablones. By apply class-based knowledge and on line research skills, simply build an regression model by using the physical features as variables. Keep improving the precision of the model by adding more realiable features into the model.\nsource: https://archive.ics.uci.edu/ml/datasets/Abalone\nThe folder contains a final report and the R markdown of it.\n'], 'url_profile': 'https://github.com/Cassiel-H', 'info_list': ['Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Sep 28, 2020', 'Python', 'Updated Mar 4, 2020', 'Updated Mar 3, 2020', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Sep 4, 2020', 'JavaScript', 'Updated Mar 2, 2021', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 3, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/maheshwarang88', 'info_list': ['Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Sep 28, 2020', 'Python', 'Updated Mar 4, 2020', 'Updated Mar 3, 2020', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Sep 4, 2020', 'JavaScript', 'Updated Mar 2, 2021', 'Updated Mar 13, 2020', 'Python', 'Updated Mar 3, 2020']}"
"{'location': 'Istanbul/ Turkey', 'stats_list': [], 'contributions': '372 contributions\n        in the last year', 'description': ['Survivors of Titanicüõ≥\nI make a survival prediction of Titanic. The dataset is from a competition of Kaggle.\nFirst of all, there are different types of columns in the dataset. Therefore, I try to get some insight into the columns. Some columns have float type, some have integer type, and others have object type.\nThese data types were really helpful for the next step because the next step is data preparation. I prepared data by cleaning the dataset from redundant elements. I determined the empty elements, and then, either I filled it or removed it completely out of the dataset. To decide which one I should choose, I consider various factors such as the number of null elements, or what kind of information that the column has.\nWith data transformation, I meant the transformation of object data to encoded data. We cannot use object elements to train our model, so I used the Label Encoder. There are also other encoder methods. One of the most popular is OneHotEncoder. You can also check out below.\nI used two different classifier methods which are Random Forest Classifier, and Gradient Boosting Classifier. Random Forest algorithm is also not bad, but Gradient Boosting gave a better result than that.\nProcedure\n\nData Preparation\n\nData cleaning\nData transformation\n\n\nModel & Evaluation\n\nRandom Forest Classifier\nGradient Boosting Classifier\n\n\nSubmission\n\nTo learn more aboutüîçüß©\n\nEncoding labels: Label Encoder and One Hot Encoder\nClassifier:  Random Forest Classifier and Gradient Boosting Classifier\n\n'], 'url_profile': 'https://github.com/afraarslan', 'info_list': ['Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 4, 2020', 'Python', 'Updated Mar 13, 2020', 'R', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'R', 'Updated Mar 2, 2020', '2', 'Python', 'Updated Mar 6, 2020', '2', 'Python', 'Updated Mar 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Starwars63', 'info_list': ['Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 4, 2020', 'Python', 'Updated Mar 13, 2020', 'R', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'R', 'Updated Mar 2, 2020', '2', 'Python', 'Updated Mar 6, 2020', '2', 'Python', 'Updated Mar 6, 2020']}","{'location': 'Gujarat, India.', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['Linear-Regression-Analysis\nMethods for linear regression\n'], 'url_profile': 'https://github.com/Mayank-Kabra', 'info_list': ['Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 4, 2020', 'Python', 'Updated Mar 13, 2020', 'R', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'R', 'Updated Mar 2, 2020', '2', 'Python', 'Updated Mar 6, 2020', '2', 'Python', 'Updated Mar 6, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '53 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/shashikant18596', 'info_list': ['Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 4, 2020', 'Python', 'Updated Mar 13, 2020', 'R', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'R', 'Updated Mar 2, 2020', '2', 'Python', 'Updated Mar 6, 2020', '2', 'Python', 'Updated Mar 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/radhikarangu123', 'info_list': ['Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 4, 2020', 'Python', 'Updated Mar 13, 2020', 'R', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'R', 'Updated Mar 2, 2020', '2', 'Python', 'Updated Mar 6, 2020', '2', 'Python', 'Updated Mar 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/amritasingh17304', 'info_list': ['Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 4, 2020', 'Python', 'Updated Mar 13, 2020', 'R', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'R', 'Updated Mar 2, 2020', '2', 'Python', 'Updated Mar 6, 2020', '2', 'Python', 'Updated Mar 6, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '793 contributions\n        in the last year', 'description': ['linear-regression\nperforming linear regression\n'], 'url_profile': 'https://github.com/vipul43', 'info_list': ['Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 4, 2020', 'Python', 'Updated Mar 13, 2020', 'R', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'R', 'Updated Mar 2, 2020', '2', 'Python', 'Updated Mar 6, 2020', '2', 'Python', 'Updated Mar 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '58 contributions\n        in the last year', 'description': ['Linear-Regression-Project\nLinear Regression Project\nTeam members:\n\nTalmacel Sergiu-Victor\nIamandii Ana-Maria\n\n'], 'url_profile': 'https://github.com/iamandiianamariaa', 'info_list': ['Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 4, 2020', 'Python', 'Updated Mar 13, 2020', 'R', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'R', 'Updated Mar 2, 2020', '2', 'Python', 'Updated Mar 6, 2020', '2', 'Python', 'Updated Mar 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/w1449550206', 'info_list': ['Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 4, 2020', 'Python', 'Updated Mar 13, 2020', 'R', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'R', 'Updated Mar 2, 2020', '2', 'Python', 'Updated Mar 6, 2020', '2', 'Python', 'Updated Mar 6, 2020']}","{'location': 'Amman Jordan', 'stats_list': [], 'contributions': '6,081 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/IbrahimShamma99', 'info_list': ['Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 4, 2020', 'Python', 'Updated Mar 13, 2020', 'R', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'R', 'Updated Mar 2, 2020', '2', 'Python', 'Updated Mar 6, 2020', '2', 'Python', 'Updated Mar 6, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['\n\n\n\n\n\n\n\nonelearn: Online learning in Python\nDocumentation | Reproduce experiments |\nonelearn stands for ONE-shot LEARNning. It is a small python package for online learning\nwith Python. It provides :\n\nonline (or one-shot) learning algorithms: each sample is processed once, only a\nsingle pass is performed on the data\nincluding multi-class classification and regression algorithms\nFor now, only ensemble methods, namely Random Forests\n\nInstallation\nThe easiest way to install onelearn is using pip\npip install onelearn\n\nBut you can also use the latest development from github directly with\npip install git+https://github.com/onelearn/onelearn.git\n\nReferences\n@article{mourtada2019amf,\n  title={AMF: Aggregated Mondrian Forests for Online Learning},\n  author={Mourtada, Jaouad and Ga{\\""\\i}ffas, St{\\\'e}phane and Scornet, Erwan},\n  journal={arXiv preprint arXiv:1906.10529},\n  year={2019}\n}\n\n'], 'url_profile': 'https://github.com/onelearn', 'info_list': ['10', 'Python', 'BSD-3-Clause license', 'Updated Apr 30, 2020', '1', 'HTML', 'Apache-2.0 license', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', '1', 'Python', 'Updated Mar 18, 2020', 'Python', 'Updated Mar 3, 2020', 'R', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '39 contributions\n        in the last year', 'description': ['intro_regression\nIntroduction to Regression lecture materials\n'], 'url_profile': 'https://github.com/wyseguy7', 'info_list': ['10', 'Python', 'BSD-3-Clause license', 'Updated Apr 30, 2020', '1', 'HTML', 'Apache-2.0 license', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', '1', 'Python', 'Updated Mar 18, 2020', 'Python', 'Updated Mar 3, 2020', 'R', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/naveenijeri', 'info_list': ['10', 'Python', 'BSD-3-Clause license', 'Updated Apr 30, 2020', '1', 'HTML', 'Apache-2.0 license', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', '1', 'Python', 'Updated Mar 18, 2020', 'Python', 'Updated Mar 3, 2020', 'R', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '74 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kadirdeniz', 'info_list': ['10', 'Python', 'BSD-3-Clause license', 'Updated Apr 30, 2020', '1', 'HTML', 'Apache-2.0 license', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', '1', 'Python', 'Updated Mar 18, 2020', 'Python', 'Updated Mar 3, 2020', 'R', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': ['Logistic-Regression\nLogistic regression on diabetes dataset\n'], 'url_profile': 'https://github.com/JanetMacrina', 'info_list': ['10', 'Python', 'BSD-3-Clause license', 'Updated Apr 30, 2020', '1', 'HTML', 'Apache-2.0 license', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', '1', 'Python', 'Updated Mar 18, 2020', 'Python', 'Updated Mar 3, 2020', 'R', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/achyutganti', 'info_list': ['10', 'Python', 'BSD-3-Clause license', 'Updated Apr 30, 2020', '1', 'HTML', 'Apache-2.0 license', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', '1', 'Python', 'Updated Mar 18, 2020', 'Python', 'Updated Mar 3, 2020', 'R', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/naveenijeri', 'info_list': ['10', 'Python', 'BSD-3-Clause license', 'Updated Apr 30, 2020', '1', 'HTML', 'Apache-2.0 license', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', '1', 'Python', 'Updated Mar 18, 2020', 'Python', 'Updated Mar 3, 2020', 'R', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}","{'location': 'Baku, Azerbaijan', 'stats_list': [], 'contributions': '378 contributions\n        in the last year', 'description': ['logistic-regression-implementation\nUses student exams data set. It is a collection of exam scores and a binary value\nindicating whether student has passed the course\n'], 'url_profile': 'https://github.com/nihadatakishiyev', 'info_list': ['10', 'Python', 'BSD-3-Clause license', 'Updated Apr 30, 2020', '1', 'HTML', 'Apache-2.0 license', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', '1', 'Python', 'Updated Mar 18, 2020', 'Python', 'Updated Mar 3, 2020', 'R', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '84 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/guy-amir', 'info_list': ['10', 'Python', 'BSD-3-Clause license', 'Updated Apr 30, 2020', '1', 'HTML', 'Apache-2.0 license', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', '1', 'Python', 'Updated Mar 18, 2020', 'Python', 'Updated Mar 3, 2020', 'R', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}","{'location': 'M√©rida, Yucat√°n, M√©xico', 'stats_list': [], 'contributions': '197 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/alejandropuerto', 'info_list': ['10', 'Python', 'BSD-3-Clause license', 'Updated Apr 30, 2020', '1', 'HTML', 'Apache-2.0 license', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', '1', 'Python', 'Updated Mar 18, 2020', 'Python', 'Updated Mar 3, 2020', 'R', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated May 9, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/duongvanphung', 'info_list': ['Updated Mar 4, 2020', 'Updated Mar 7, 2020', 'HTML', 'Updated May 20, 2020', 'Python', 'Updated Mar 5, 2020', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'R', 'Updated Mar 3, 2020', '2', 'OCaml', 'LGPL-2.1 license', 'Updated Jul 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/chilochibi', 'info_list': ['Updated Mar 4, 2020', 'Updated Mar 7, 2020', 'HTML', 'Updated May 20, 2020', 'Python', 'Updated Mar 5, 2020', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'R', 'Updated Mar 3, 2020', '2', 'OCaml', 'LGPL-2.1 license', 'Updated Jul 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['MLRegression\nMachine Learning project for regression task\nThe dataset was taken from Kaggle: https://www.kaggle.com/agailloty/house-price-in-canada\nIn the dataset, there are 546 observations with 12 variables describing aspects of houses.\nGoal - predict the price of houses.\nThe applied Machine Learning algorithms:\n\nLinear Regression\nRidge Regression\nLASSO Regression\nKNN\n\nIn addition, there were applied feature selection methods:\n\nNear zero variance predictors,\nFilter-based variable importance,\nBackward elimination,\n\nand feature generation methods:\n\nBox Cox and Yeo-Johnson‚Äôs transformations,\nFunctional transformation.\n\nThe project is done in R (source code is in Regression.Rmd file), the ready project is knitted into HTML file: Regression.html.\n'], 'url_profile': 'https://github.com/inanarkevich', 'info_list': ['Updated Mar 4, 2020', 'Updated Mar 7, 2020', 'HTML', 'Updated May 20, 2020', 'Python', 'Updated Mar 5, 2020', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'R', 'Updated Mar 3, 2020', '2', 'OCaml', 'LGPL-2.1 license', 'Updated Jul 31, 2020']}","{'location': 'Portland, OR', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/JaredFitz', 'info_list': ['Updated Mar 4, 2020', 'Updated Mar 7, 2020', 'HTML', 'Updated May 20, 2020', 'Python', 'Updated Mar 5, 2020', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'R', 'Updated Mar 3, 2020', '2', 'OCaml', 'LGPL-2.1 license', 'Updated Jul 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '123 contributions\n        in the last year', 'description': ['linreg\nMy thoughts on linear regression\n'], 'url_profile': 'https://github.com/audiracmichelle', 'info_list': ['Updated Mar 4, 2020', 'Updated Mar 7, 2020', 'HTML', 'Updated May 20, 2020', 'Python', 'Updated Mar 5, 2020', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'R', 'Updated Mar 3, 2020', '2', 'OCaml', 'LGPL-2.1 license', 'Updated Jul 31, 2020']}","{'location': 'Istanbul ', 'stats_list': [], 'contributions': '56 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/keremguzel', 'info_list': ['Updated Mar 4, 2020', 'Updated Mar 7, 2020', 'HTML', 'Updated May 20, 2020', 'Python', 'Updated Mar 5, 2020', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'R', 'Updated Mar 3, 2020', '2', 'OCaml', 'LGPL-2.1 license', 'Updated Jul 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '122 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Nikhileshorg', 'info_list': ['Updated Mar 4, 2020', 'Updated Mar 7, 2020', 'HTML', 'Updated May 20, 2020', 'Python', 'Updated Mar 5, 2020', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'R', 'Updated Mar 3, 2020', '2', 'OCaml', 'LGPL-2.1 license', 'Updated Jul 31, 2020']}","{'location': 'M√©rida, Yucat√°n, M√©xico', 'stats_list': [], 'contributions': '197 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/alejandropuerto', 'info_list': ['Updated Mar 4, 2020', 'Updated Mar 7, 2020', 'HTML', 'Updated May 20, 2020', 'Python', 'Updated Mar 5, 2020', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'R', 'Updated Mar 3, 2020', '2', 'OCaml', 'LGPL-2.1 license', 'Updated Jul 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/umairrafique85', 'info_list': ['Updated Mar 4, 2020', 'Updated Mar 7, 2020', 'HTML', 'Updated May 20, 2020', 'Python', 'Updated Mar 5, 2020', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'R', 'Updated Mar 3, 2020', '2', 'OCaml', 'LGPL-2.1 license', 'Updated Jul 31, 2020']}","{'location': 'Japan', 'stats_list': [], 'contributions': '1,629 contributions\n        in the last year', 'description': ['oplsr\nOCaml wrapper to the pls R package\n'], 'url_profile': 'https://github.com/UnixJunkie', 'info_list': ['Updated Mar 4, 2020', 'Updated Mar 7, 2020', 'HTML', 'Updated May 20, 2020', 'Python', 'Updated Mar 5, 2020', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'R', 'Updated Mar 3, 2020', '2', 'OCaml', 'LGPL-2.1 license', 'Updated Jul 31, 2020']}"
"{'location': 'Ho Chi Minh City', 'stats_list': [], 'contributions': '55 contributions\n        in the last year', 'description': ['\n3D FACE RECONSTRUCTION AND MULTI-POSE FACIAL IMAGE SYNTHESIS FROM A SINGE IMAGE üéì\n\n\n\n\n\n\n'], 'url_profile': 'https://github.com/reidite', 'info_list': ['2', 'Python', 'MIT license', 'Updated Dec 4, 2020', 'MATLAB', 'GPL-2.0 license', 'Updated Dec 16, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 8, 2020', '2', 'Python', 'Updated Mar 6, 2020', 'R', 'Updated Mar 8, 2020', 'HTML', 'Updated Mar 7, 2020', 'R', 'Updated Mar 8, 2020', 'HTML', 'Updated Mar 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '85 contributions\n        in the last year', 'description': [""Bayes Factors for Model Selection in Multivariate Regression\n\nlinearReg_R2stat - a function to compute the Null-based\nBayes Factor (BF) for evidence quantification and model selection through\nBayesian hypothesis testing in Multivariate Regression designs.\nWritten in MATLAB by Mert T√ºrkol, (c) 2019.\n\nlinearReg_R2stat computes the BF (ratio between marginal likelihoods) by\ncomparing a target regression model to the Null (intercept only). It can be used\nwithout having access to the full dataset, by just utilizing the ordinary R^2\n(coeff. of determination) test statistic obtained as a result of the underlying\nregression design.\nMethodology: Laplace approximation to BF under Zellner-Siow prior\n(on model parameters) as a mixture of g-priors\n[ 1 ] is used. This results\nin an inverse-gamma prior: g ~ IG(1/2, s^2 * N/2), where 's' is the scale factor\nhyper-parameter and 'N' is the number of samples/observations\n[ 2 ].\nBased on user choice, Vectorized Adaptive Quadrature or high-precision numerical\nintegration using Variable-Precision Arithmetic is utilized to integrate the likelihood.\nTable of Contents\n\nUsage\n\nExamples\nInputs\nOptions\n\n\nDocumentation\n\nTheoretical Background\nFile Overview\n\n\n\nUsage\nCall linearReg_R2stat without input arguments to see its help documentation:\n>> linearReg_R2stat()\n\nCall linearReg_R2stat with the the function handle '@getOptions' to return\nthe default options setting within a Struct:\n>> [DefOpts] = linearReg_R2stat(@getOptions)\n\nCompute and return Bayes factor 'Bf' using 'Options' as a single Struct or\nname-value pairs passed as varargin:\n>> linearReg_R2stat(N, p, R2, varargin) \n\nPlease see linearReg_R2stat for a detailed list of input\narguments and run Options.\nExamples\nReturn the default Options in a Struct:\n>> [DefOpts] = linearReg_R2stat(@getOptions) \nDefOpts = \n  struct with fields:\n\n        s: 0.353553390593274\n   useVpa: 0\n   lvlTol: 'Mdefault'\n   relTol: 1e-06\n   absTol: 1e-10\n   simple: 0 \n\nCompute the Bayes Factor by reproducing the first entry of Table 1, [ 1 ] :\n\nMandatory inputs: #observations (175), #predictors (4), R-squared (0.7109),\nusing a prior scale factor of '1',\nutilizing the 'integral()' function for numerical integration using Vectorized Adaptive Quadrature with\nthe same relative and absolute tolerances as in the built-in 'integrate()' function found in R,\nto return the raw (simple) Bf value along with the Options of computation.\n\n>> [rawBf, Opts] = linearReg_R2stat(175, 4, 0.7109, ...\n                   's', 1, 'lvlTol', 'Rdefault', 'simple', true)\n                   \nrawBf =\n        3.54443032945501e+41\nOpts = \n  struct with fields:\n\n        s: 1\n   useVpa: 0\n   lvlTol: 'Rdefault'\n   relTol: 0.0001220703125\n   absTol: 0.0001220703125\n   simple: 1 \n\nAlternatively, following the mandatory inputs, we could pass a Struct for the\ndesired run Options ( i.e. using Model #6 with the covariates 'Local+Parasites' as in\n[ 1 ] ):\n>> [Bf60, RouderOpts] = linearReg_R2stat(175, 2, 0.2429, ...\n                        struct('s', 1, 'lvlTol', 'Rdefault', 'simple', true) )\n                   \nBf60 =\n          122607538.194857\nRouderOpts = \n  struct with fields:\n\n         s: 1\n    useVpa: 0\n    lvlTol: 'Rdefault'\n    relTol: 0.0001220703125\n    absTol: 0.0001220703125\n    simple: 1 \n\nInputs\n(required, in order):\n\n\n\nInput\nDescription\n\n\n\n\nN\n#data-points/observations, (scalar int) N >= 3.\n\n\np\n#predictors excluding the intercept, (scalar int) 1 <= p < N-1.\n\n\nR2\nOrdinary coeff. of determination, (real, scalar) 0 <= R2 < 1.\n\n\n\nCorresponds to the proportion of variance accounted for by the predictors, excluding the intercept.\n\n\n\nOptions\n(in any order, use DEFAULT value if not provided as argument):\n\n\n\nParameter\nDescription, Default Value. Type/Value Tags\n\n\n\n\n's'\nprior scale factor, DEFAULT - 'medium'.\n\n\n\n(real, positive, scalar) (0, 1]\n\n\n\nOR\n\n\n\n(char-array) preset as in {'medium', 'wide', 'ultrawide'}\n\n\n\n'medium': 0.3535, 'wide': 0.50, 'ultrawide': 0.7071\n\n\n'useVpa'\nLogical to utilize Variable-Precision Arithmetic in Bf computation, DEFAULT - 'false'.\n\n\n\ntrue  - vpaintegral() for High-Precision Numerical Integration (Symbolic Toolbox R2016b and above)\n\n\n\nfalse - integral() for Vectorized Adaptive Quadrature\n\n\n'lvlTol'\nPreset level of convergence tolerances in numerical integration, DEFAULT - 'Mdefault'.\n\n\n\n(char-array) preset as in {'Mdefault', 'Rdefault', 'medium', 'low', 'verylow'}\n\n\n\n'Mdefault' -> (relTol: 1e-6, absTol: 1e-10)\n\n\n\n'Rdefault' -> (relTol: eps('double')^0.25, absTol: eps('double')^0.25)\n\n\n\n'medium'   -> (relTol: 1e-10, absTol: 1e-12)\n\n\n\n'low'      -> (relTol: 50*eps, absTol: 1e-14)\n\n\n\n'verylow'  -> (relTol: 5*eps, absTol: 1e-15)\n\n\n'relTol'\nRelative tolerance value for convergence criterion in numerical integration, DEFAULT - 1e-6.\n\n\n\n(real, scalar double) [0, Inf]\n\n\n'absTol'\nAbsolute tolerance value for convergence criterion in numerical integration, DEFAULT - 1e-10.\n\n\n\n(real, scalar double) [0, Inf]\n\n\n'simple'\nLogical to return the Bf in raw form, DEFAULT - false.\n\n\n\ntrue  -  the raw Bayes factor 'Bf'\n\n\n\nfalse -  log(Bf) in order to prevent possible overflow\n\n\n\nDocumentation\nTheoretical Background\nFor a more theoretical understanding read the following publications:\n\n[1] Rouder, J.N. and Morey, R.D., 2012. Default Bayes factors for model selection in regression\n[2] Liang, F., Paulo, R., Molina, G., Clyde, M.A. and Berger, J.O., 2008. Mixtures of g priors for Bayesian variable selection\n\nFile Overview\n\n\n\nFile\nDescription\n\n\n\n\nlinearReg_R2stat.m\nMain function that computes the Bayes Factor\n\n\nlogUtility.m\nComputes accurately various logarithm/exponential related expressions which are prone to loss of precision\n\n\ndinvgamma.m\nComputes the derivatives of the log density of inverse gamma distribution\n\n\nlngamma.m\nComputes the complex ln(Gamma) function\n\n\nREADME.md\nThis README.md\n\n\nLICENSE\nLicense documentation\n\n\n\n""], 'url_profile': 'https://github.com/mturkol', 'info_list': ['2', 'Python', 'MIT license', 'Updated Dec 4, 2020', 'MATLAB', 'GPL-2.0 license', 'Updated Dec 16, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 8, 2020', '2', 'Python', 'Updated Mar 6, 2020', 'R', 'Updated Mar 8, 2020', 'HTML', 'Updated Mar 7, 2020', 'R', 'Updated Mar 8, 2020', 'HTML', 'Updated Mar 22, 2020']}","{'location': 'Mumbai, India', 'stats_list': [], 'contributions': '68 contributions\n        in the last year', 'description': ['House_prices_regression\nLasso Regression based notebook for the kaggle knowledge based House Prices: Advanced Regression Techniques competition\n'], 'url_profile': 'https://github.com/Vansh1010', 'info_list': ['2', 'Python', 'MIT license', 'Updated Dec 4, 2020', 'MATLAB', 'GPL-2.0 license', 'Updated Dec 16, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 8, 2020', '2', 'Python', 'Updated Mar 6, 2020', 'R', 'Updated Mar 8, 2020', 'HTML', 'Updated Mar 7, 2020', 'R', 'Updated Mar 8, 2020', 'HTML', 'Updated Mar 22, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['LinearRegression\nSolution of Kaggle Competition (Linear Regression Randomly created dataset for linear regression)\n'], 'url_profile': 'https://github.com/Santosh0236', 'info_list': ['2', 'Python', 'MIT license', 'Updated Dec 4, 2020', 'MATLAB', 'GPL-2.0 license', 'Updated Dec 16, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 8, 2020', '2', 'Python', 'Updated Mar 6, 2020', 'R', 'Updated Mar 8, 2020', 'HTML', 'Updated Mar 7, 2020', 'R', 'Updated Mar 8, 2020', 'HTML', 'Updated Mar 22, 2020']}","{'location': 'Brentwood, UK', 'stats_list': [], 'contributions': '302 contributions\n        in the last year', 'description': ['Regression-Demonstration\nLinear Regression Demonstration\nEnter x and y coordinates to table and watch how regression line tries to best fit the data.\nOption to add in residual lines and mean line for comparison\nR^2, MAE and RMSE returned below.\nActivities:\n\nEnter in 10 values and observe the metric scores underneath\nAdd more values to improve the R^2 score, what is happening?\nAdd/remove values to decrease the MAE and RMSE, what is happening?\n\n'], 'url_profile': 'https://github.com/Alastair-Tyson', 'info_list': ['2', 'Python', 'MIT license', 'Updated Dec 4, 2020', 'MATLAB', 'GPL-2.0 license', 'Updated Dec 16, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 8, 2020', '2', 'Python', 'Updated Mar 6, 2020', 'R', 'Updated Mar 8, 2020', 'HTML', 'Updated Mar 7, 2020', 'R', 'Updated Mar 8, 2020', 'HTML', 'Updated Mar 22, 2020']}","{'location': 'Kathmandu Nepal', 'stats_list': [], 'contributions': '78 contributions\n        in the last year', 'description': ['RealestatePredictionUsingLinearRegression\n'], 'url_profile': 'https://github.com/smaheshacharya', 'info_list': ['2', 'Python', 'MIT license', 'Updated Dec 4, 2020', 'MATLAB', 'GPL-2.0 license', 'Updated Dec 16, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 8, 2020', '2', 'Python', 'Updated Mar 6, 2020', 'R', 'Updated Mar 8, 2020', 'HTML', 'Updated Mar 7, 2020', 'R', 'Updated Mar 8, 2020', 'HTML', 'Updated Mar 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Anandu-Sanu', 'info_list': ['2', 'Python', 'MIT license', 'Updated Dec 4, 2020', 'MATLAB', 'GPL-2.0 license', 'Updated Dec 16, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 8, 2020', '2', 'Python', 'Updated Mar 6, 2020', 'R', 'Updated Mar 8, 2020', 'HTML', 'Updated Mar 7, 2020', 'R', 'Updated Mar 8, 2020', 'HTML', 'Updated Mar 22, 2020']}","{'location': 'Los Angeles', 'stats_list': [], 'contributions': '612 contributions\n        in the last year', 'description': ['linear_regression_rental_prediction\nLinear regression machine learning model for predicting bike rentals\nSimply enough, the application can be run by first installing all the dependencies:\npip3 install - requirements.txt\nThen you should be able to call the main.py script and log into yoour localhost/5000\npip3 main.py\nMessage me if you have any difficulties running the application\n'], 'url_profile': 'https://github.com/Chalmiller', 'info_list': ['2', 'Python', 'MIT license', 'Updated Dec 4, 2020', 'MATLAB', 'GPL-2.0 license', 'Updated Dec 16, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 8, 2020', '2', 'Python', 'Updated Mar 6, 2020', 'R', 'Updated Mar 8, 2020', 'HTML', 'Updated Mar 7, 2020', 'R', 'Updated Mar 8, 2020', 'HTML', 'Updated Mar 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Anandu-Sanu', 'info_list': ['2', 'Python', 'MIT license', 'Updated Dec 4, 2020', 'MATLAB', 'GPL-2.0 license', 'Updated Dec 16, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 8, 2020', '2', 'Python', 'Updated Mar 6, 2020', 'R', 'Updated Mar 8, 2020', 'HTML', 'Updated Mar 7, 2020', 'R', 'Updated Mar 8, 2020', 'HTML', 'Updated Mar 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '85 contributions\n        in the last year', 'description': ['Personal Info\n\na. Elmer Camargopena\nb. 2291367\nc. camargop@chapman.edu\nd. CPSC 392 Section 1\ne. Project 1\n\nNo known errors at this time\nhttps://www.datacamp.com/community/tutorials/understanding-logistic-regression-python\nhttps://towardsdatascience.com/better-heatmaps-and-correlation-matrix-plots-in-python-41445d0f2bec\nhttps://stackoverflow.com/questions/50909747/how-to-add-labelbinarizer-columns-to-dataframe\nhttps://stackoverflow.com/questions/43307156/sklearns-kfold-generates-nan-values\n'], 'url_profile': 'https://github.com/flapjackstan', 'info_list': ['2', 'Python', 'MIT license', 'Updated Dec 4, 2020', 'MATLAB', 'GPL-2.0 license', 'Updated Dec 16, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 8, 2020', '2', 'Python', 'Updated Mar 6, 2020', 'R', 'Updated Mar 8, 2020', 'HTML', 'Updated Mar 7, 2020', 'R', 'Updated Mar 8, 2020', 'HTML', 'Updated Mar 22, 2020']}"
"{'location': 'New York', 'stats_list': [], 'contributions': '107 contributions\n        in the last year', 'description': ['CarsML\nMachine Learning Linear Regression Using Tensorflow.js\nAbout\nCarsML is a machine learning project used to create a linear regression model to find the most accurate fit for the relationship between MPG and Weight of vehicles. In this example we use HTML, Tensorflow.js and Kaggle.json Data.\nPreview\n\n\n\nKey Takeaways - Machine Learning Model(Linear Regression)\nWhen using a machine learning model before inputting data it requires data cleaning to insert into the model‚Äôs inner layers. After the 2D tensor data/vector is placed into the neural network it is weighted through each connection using the weighted sum in the sigmoid function (0 not a match and greater than zero less than 1) (Fig.1). Then we train the model and see how the model performs and check the loss and mean squared error averaged over the whole dataset to the end of the epoch (Fig.2). Finally, we map the predicted output vs the original values and test for fit and check if the finding for minimization function has accurate output to form a linear regression (Fig.3) showing a strong positive correlation (when model is run for best fit).\nNegative Correlation - When Y increases then feature X decreases and vice versa.\nPositive Correlation - When Y decrease then feature X increase and vice versa.\nReferences:\nhttps://www.3blue1brown.com\nhttps://github.com/tensorflow/tfjs\nhttps://www.geeksforgeeks.org/mathematics-covariance-and-correlation/\nhttps://developers.google.com/machine-learning/crash-course\n'], 'url_profile': 'https://github.com/Brant-777', 'info_list': ['JavaScript', 'Updated Mar 26, 2020', 'Python', 'Updated Jun 4, 2020', 'HTML', 'Updated Mar 3, 2020', 'Java', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', '1', 'Python', 'Updated Mar 8, 2020', 'HTML', 'Updated Mar 23, 2020', '1', 'Python', 'Updated Nov 4, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '59 contributions\n        in the last year', 'description': ['DSLR\nA Machine Learning project using Logistic Regression.\nPurpose\nThis project\'s main goal is to solve a classification problem and by doing so, learning about data analysis, data visualization and Machine Learning techniques.\nI was given a training dataset and a testing dataset which are in the resources folder.\nIn them there are Hogwart students who need to be put in one of 4 houses according to their grades in many different subjects.\nIn order to do that, I first familiarized myself with basic data visualization techniques as well as data analysis in order to select the most interesting features. I then made a training program generating weights which will be used by the prediction program to assign students to their house.\nRequired packages\n\nNumpy\nPandas\nMatplotlib\nSeaborn\n\nUsage\nThere are many executable programs in this repository, here is what they do and how to use them:\nlogreg_train\nTrains the classification model.\n$> python logreg_train.py -h\nusage: logreg_train.py [-h] [-v] [-vi House N_feature1 N_feature2] dataset\n\nTrains the model with the specified dataset\n\npositional arguments:\n  dataset: dataset, needs to be a csv\n\noptional arguments:\n  -h, --help: show this help message and exit\n  -v, --verbose: display in real time actions of training\n  -vi House N_feature1 N_feature2: display data of one house in a separate windows\n\n$> python logreg_train.py ""./resources/dataset_train.csv"" -vi ""Ravenclaw"" 1 2\n\n\nThis will train the model by computing weights\n\n\nGenerated weights will be saved in weights.csv\n\n\nAdding the -vi option will display a graph on the training session in a one vs all format for the specified features and house\n\nlogreg_predict\nPredicts house belonging for a list of students.\n$> python logreg_predict.py -h\nusage: logreg_predict.py [-h] [-a] [-p] dataset weights\n\npredicts student\'s house with our model\n\npositional arguments:\n  dataset: dataset, needs to be a csv\n  weights: weights, needs to be a csv\n\noptional arguments:\n  -h, --help: show this help message and exit\n  -a, --accuracy: show accuracy for dataset_train\n  -p, --piechart: print a piechart for the results\n$> python logreg_predict.py ""./resources/dataset_test.csv"" ""./weights.csv"" -p\n\n\nThis will predict a house for each student in the test dataset\n\n\nAdding the -p option displays a pie chart of the repartition of each student\n\n\nYou can also use -a if you are making predictions on the training data to know the rate of correct predictions\n\ndescribe\nGives different metrics of the dataset given as an argument.\n$> python describe.py ""./resources/dataset_train.csv""\n\n\nThis will display a description of the training dataset similar to pandas.describe()\n\npair_plot\nDisplays a pair_plot of the data using seaborn. Very useful to determine which attributes to keep for the training.\n$> python pair_plot.py ./resources/dataset_train.csv\n\n\nThis will open a new window containing the pair plot, it may take a while to load considering there\'s a lot of computation to be done\n\nhistogram\nDisplays a histogram answering the question:\nWhich Hogwarts course has a homogeneous score distribution beween all four houses?\n$> python histogram.py ""./resources/dataset_train.csv""\n\n\nThis will open a new window containing the histogram\n\nScatter Plot\nDisplays a scatter plot answering the question:\nWhat are the two features that are similar?\n$> python scatter_plot.py ""./resources/dataset_train.csv""\n\n\nThis will open a new window containing the scatter plot\n\nCredit\nThis project was made by:\n\nJulien Dumay (https://github.com/ChokMania)\nmyself, Aleksi Gautier (https://github.com/Kelias-42)\n\n'], 'url_profile': 'https://github.com/Kelias-42', 'info_list': ['JavaScript', 'Updated Mar 26, 2020', 'Python', 'Updated Jun 4, 2020', 'HTML', 'Updated Mar 3, 2020', 'Java', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', '1', 'Python', 'Updated Mar 8, 2020', 'HTML', 'Updated Mar 23, 2020', '1', 'Python', 'Updated Nov 4, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/William-Wang-github', 'info_list': ['JavaScript', 'Updated Mar 26, 2020', 'Python', 'Updated Jun 4, 2020', 'HTML', 'Updated Mar 3, 2020', 'Java', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', '1', 'Python', 'Updated Mar 8, 2020', 'HTML', 'Updated Mar 23, 2020', '1', 'Python', 'Updated Nov 4, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020']}","{'location': 'Maroc', 'stats_list': [], 'contributions': '48 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ismailktami', 'info_list': ['JavaScript', 'Updated Mar 26, 2020', 'Python', 'Updated Jun 4, 2020', 'HTML', 'Updated Mar 3, 2020', 'Java', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', '1', 'Python', 'Updated Mar 8, 2020', 'HTML', 'Updated Mar 23, 2020', '1', 'Python', 'Updated Nov 4, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020']}","{'location': 'Ames, IA USA', 'stats_list': [], 'contributions': '77 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/scottkds', 'info_list': ['JavaScript', 'Updated Mar 26, 2020', 'Python', 'Updated Jun 4, 2020', 'HTML', 'Updated Mar 3, 2020', 'Java', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', '1', 'Python', 'Updated Mar 8, 2020', 'HTML', 'Updated Mar 23, 2020', '1', 'Python', 'Updated Nov 4, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Mariam-Hassan', 'info_list': ['JavaScript', 'Updated Mar 26, 2020', 'Python', 'Updated Jun 4, 2020', 'HTML', 'Updated Mar 3, 2020', 'Java', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', '1', 'Python', 'Updated Mar 8, 2020', 'HTML', 'Updated Mar 23, 2020', '1', 'Python', 'Updated Nov 4, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/farjon', 'info_list': ['JavaScript', 'Updated Mar 26, 2020', 'Python', 'Updated Jun 4, 2020', 'HTML', 'Updated Mar 3, 2020', 'Java', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', '1', 'Python', 'Updated Mar 8, 2020', 'HTML', 'Updated Mar 23, 2020', '1', 'Python', 'Updated Nov 4, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020']}","{'location': 'St. Petersburg', 'stats_list': [], 'contributions': '62 contributions\n        in the last year', 'description': ['StackOverflow developer survey analysis\nproject 1: will people born today will have a better life than their parents?\nTask: examine the relationship between belief in whether youth will have better life than their parents and several variables (no more than 4) using logistic regression.\n\nDo developers think that people born today will have a better life than their parents?\n\nData: StackOverflow Developer Survey 2019\nThe following variables were chosen as predictors:\n\nRegion\nBlockchainIs\nGender\nAge\n\nMethods:\n\nmodel: logistic regression\n\nResults:\n\ndevelopers who consider blockchain useful have more optimistic thoughts compared to those with negative attitude\nregion matters as well: most regions had higher odds of belief in better life compared to developers in Northern America\ngender and age: females and non-binary are more suspicious about the future, as well as people with higher age.\n\nView as:\n\nmarkdown\nhtml\n\n'], 'url_profile': 'https://github.com/actuallyykatie', 'info_list': ['JavaScript', 'Updated Mar 26, 2020', 'Python', 'Updated Jun 4, 2020', 'HTML', 'Updated Mar 3, 2020', 'Java', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', '1', 'Python', 'Updated Mar 8, 2020', 'HTML', 'Updated Mar 23, 2020', '1', 'Python', 'Updated Nov 4, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '186 contributions\n        in the last year', 'description': ['StarGAN for Regression on Arousal and Valence\nRaFD Dataset\nAffectNet Dataset\n'], 'url_profile': 'https://github.com/liyu10000', 'info_list': ['JavaScript', 'Updated Mar 26, 2020', 'Python', 'Updated Jun 4, 2020', 'HTML', 'Updated Mar 3, 2020', 'Java', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', '1', 'Python', 'Updated Mar 8, 2020', 'HTML', 'Updated Mar 23, 2020', '1', 'Python', 'Updated Nov 4, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/naveenijeri', 'info_list': ['JavaScript', 'Updated Mar 26, 2020', 'Python', 'Updated Jun 4, 2020', 'HTML', 'Updated Mar 3, 2020', 'Java', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', '1', 'Python', 'Updated Mar 8, 2020', 'HTML', 'Updated Mar 23, 2020', '1', 'Python', 'Updated Nov 4, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['House_Price_Predict\nKaggle test: House Prices: Advanced Regression Techniques\nPredict sales prices and practice feature engineering, RFs, and gradient boosting\n'], 'url_profile': 'https://github.com/BinCooker', 'info_list': ['Jupyter Notebook', 'Updated Mar 3, 2020', 'MATLAB', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Aug 11, 2020', 'Python', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 6, 2020', 'HTML', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 7, 2020', 'Python', 'MIT license', 'Updated Aug 27, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '243 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Jukgei', 'info_list': ['Jupyter Notebook', 'Updated Mar 3, 2020', 'MATLAB', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Aug 11, 2020', 'Python', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 6, 2020', 'HTML', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 7, 2020', 'Python', 'MIT license', 'Updated Aug 27, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}","{'location': 'Fullerton California', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/misterindie', 'info_list': ['Jupyter Notebook', 'Updated Mar 3, 2020', 'MATLAB', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Aug 11, 2020', 'Python', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 6, 2020', 'HTML', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 7, 2020', 'Python', 'MIT license', 'Updated Aug 27, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/harisprasovic', 'info_list': ['Jupyter Notebook', 'Updated Mar 3, 2020', 'MATLAB', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Aug 11, 2020', 'Python', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 6, 2020', 'HTML', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 7, 2020', 'Python', 'MIT license', 'Updated Aug 27, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Shrivas-7', 'info_list': ['Jupyter Notebook', 'Updated Mar 3, 2020', 'MATLAB', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Aug 11, 2020', 'Python', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 6, 2020', 'HTML', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 7, 2020', 'Python', 'MIT license', 'Updated Aug 27, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}","{'location': 'New York, NY', 'stats_list': [], 'contributions': '278 contributions\n        in the last year', 'description': ['poisson_regression\nWorking with count data and Poisson regression\n'], 'url_profile': 'https://github.com/Akbarnejadhn', 'info_list': ['Jupyter Notebook', 'Updated Mar 3, 2020', 'MATLAB', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Aug 11, 2020', 'Python', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 6, 2020', 'HTML', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 7, 2020', 'Python', 'MIT license', 'Updated Aug 27, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}","{'location': 'Los Angeles', 'stats_list': [], 'contributions': '612 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Chalmiller', 'info_list': ['Jupyter Notebook', 'Updated Mar 3, 2020', 'MATLAB', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Aug 11, 2020', 'Python', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 6, 2020', 'HTML', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 7, 2020', 'Python', 'MIT license', 'Updated Aug 27, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}","{'location': 'Chicago', 'stats_list': [], 'contributions': '119 contributions\n        in the last year', 'description': ['Rent prices in Paris:\n\nCollected data, engineered features\nDefined regression problem and built the pipeline with tree-based model\nMade visualizations that explained my model including shapley value plots\nDeployed interactive web app to Heroku\n\nInteractive web app  - https://rents-in-paris.herokuapp.com/\n'], 'url_profile': 'https://github.com/iuliastremciuc', 'info_list': ['Jupyter Notebook', 'Updated Mar 3, 2020', 'MATLAB', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Aug 11, 2020', 'Python', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 6, 2020', 'HTML', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 7, 2020', 'Python', 'MIT license', 'Updated Aug 27, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}","{'location': 'Delhi, India', 'stats_list': [], 'contributions': '26 contributions\n        in the last year', 'description': ['Wind-Power-Forecasting\nConsidering the escalating rates of exhaustion of non-renewable energy resources, coupled with the harmful environmental side effects of harnessing them (e.g. damage to public health via air pollution), the need for a near- complete transition to renewable energy production seems inevitable.\nIn recent times (as of mid-2018), renewable energy production has seen a strong support from investors, governmental initiatives, and industries across the world. Globally installed wind power capacity has seen an increase of 345.24% over the past decade. This increase brings along a need for robust power production management systems having a potential for predicting wind turbine power outputs primarily based on real-time input wind velocities.\nThis project is an aim to propose optimized robust regression models for forecasting the wind power generated through turbines based on wind velocity vector components. Theoretically, the forecasted output of models can be compared with a city‚Äôs daily average threshold power requirement in order to make informed decisions about either shutting down an appropriate number of turbines to avoid excessive power production and wastage, or to compensate forecasted shortcomings in production on less windy days via alternative energy generation methodologies.\n'], 'url_profile': 'https://github.com/arnavw96', 'info_list': ['Jupyter Notebook', 'Updated Mar 3, 2020', 'MATLAB', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Aug 11, 2020', 'Python', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 6, 2020', 'HTML', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 7, 2020', 'Python', 'MIT license', 'Updated Aug 27, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}","{'location': 'Melbourne', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': ['Indian-Liver-Patient-Records\nPredicting liver disease using logistic regression\n'], 'url_profile': 'https://github.com/roshanwahane', 'info_list': ['Jupyter Notebook', 'Updated Mar 3, 2020', 'MATLAB', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Aug 11, 2020', 'Python', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 6, 2020', 'HTML', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 7, 2020', 'Python', 'MIT license', 'Updated Aug 27, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}"
"{'location': 'Calgary, Alberta', 'stats_list': [], 'contributions': '444 contributions\n        in the last year', 'description': ['Logistic-Regression-Bank-ML-dataset-\nLogistic regression to predict yes or no\n'], 'url_profile': 'https://github.com/IjeomaOdoko', 'info_list': ['Jupyter Notebook', 'Updated Apr 6, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Python', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 3, 2020', 'HTML', 'Updated Jan 15, 2021', 'R', 'Updated Mar 5, 2020', 'R', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 2, 2020', 'Python', 'Updated Mar 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['insurance-cost-prediction\nPredict insurance cost using Linear Regression\nIn this project, we have a dataset of insurance costs of 1338 individuals along with their information like sex, age, bmi, smoking status and region.\nThe goal of this project is to use this data to train a logistic regression model that can predict the insurance of any individual provided the age, bmi and smoking status of the individual.\nThe algorithm going to be used for this project is Linear Regression since that is the most suitable for this kind of problems.\n'], 'url_profile': 'https://github.com/alphago7', 'info_list': ['Jupyter Notebook', 'Updated Apr 6, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Python', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 3, 2020', 'HTML', 'Updated Jan 15, 2021', 'R', 'Updated Mar 5, 2020', 'R', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 2, 2020', 'Python', 'Updated Mar 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['Random forest regression under severe data corruption.\nSeverely damaged digital data having 79 features;  4 output target parameters.\nAlmost each of the features has severe damage (includes from 20% to 80% undefined NAN values ), therefore it is impossible to immediately find primary features that uniquely determine the output. The Train_Data_200k training data has 200k entries. It is necessary to predict test data test_data_100k. It is also necessary to determine the top 10 significant tags.\nI found that top tags are Tag_01 to Tag_10 in a row\naccuracy on train set: 0.9957\naccuracy on test set: 0.9912\n'], 'url_profile': 'https://github.com/stkuzmin', 'info_list': ['Jupyter Notebook', 'Updated Apr 6, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Python', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 3, 2020', 'HTML', 'Updated Jan 15, 2021', 'R', 'Updated Mar 5, 2020', 'R', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 2, 2020', 'Python', 'Updated Mar 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['ML_LogisticRegression\nLogistic Regression for example movie prediction.\nEnv: Python 3.6.9\nfeature.py: use dict.txt to generate formatted_train.tsv and formatted_valid.tsv\n$ python feature.py train_data.tsv valid_data.tsv test_data.tsv dict.txt formatted_train.tsv formatted_valid.tsv formatted_test.tsv 1\nlr.py: use generated data to predict the movie according to the movie description.\n$ python lr.py formatted_train.tsv formatted_valid.tsv formatted_test.tsv dict.txt train_out.labels test_out.labels metrics_out.txt 60\n(number 60 is the epoches)\n'], 'url_profile': 'https://github.com/leo1357904', 'info_list': ['Jupyter Notebook', 'Updated Apr 6, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Python', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 3, 2020', 'HTML', 'Updated Jan 15, 2021', 'R', 'Updated Mar 5, 2020', 'R', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 2, 2020', 'Python', 'Updated Mar 4, 2020']}","{'location': 'the Netherlands', 'stats_list': [], 'contributions': '650 contributions\n        in the last year', 'description': [""Data Science Linear Regression\nHarvardX: PH125.7x | Data Science: Linear Regression\nAbstract\nThis is the seventh course in the HarvardX Professional Certificate in Data Science, a series of courses that prepare you to do data analysis in R, from simple computations to machine learning. We assume that you have either taken the preceding courses in the series or that you are already familiar with the content covered in them.\nLinear regression is commonly used to quantify the relationship between two or more variables. It is also used to adjust for confounding. In this course, we cover how to implement linear regression and adjust for confounding in practice using R.\nIn data science applications, it is very common to be interested in the relationship between two or more variables. The motivating case study we examine in this course relates to the data-driven approach used to construct baseball teams described in the book (and movie) Moneyball. We will try to determine which measured outcomes best predict baseball runs and to do this we'll use linear regression.\nWe will also examine confounding, where extraneous variables affect the relationship between two or more other variables, leading to spurious associations. Linear regression is a powerful technique for removing confounders, but it is not a magical process, and it is essential to understand when it is appropriate to use. You will learn when to use it in this course.\nThe bookdown-version of this course is available on this Github Page\n""], 'url_profile': 'https://github.com/1965Eric', 'info_list': ['Jupyter Notebook', 'Updated Apr 6, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Python', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 3, 2020', 'HTML', 'Updated Jan 15, 2021', 'R', 'Updated Mar 5, 2020', 'R', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 2, 2020', 'Python', 'Updated Mar 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/LittlePenguin-OvO', 'info_list': ['Jupyter Notebook', 'Updated Apr 6, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Python', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 3, 2020', 'HTML', 'Updated Jan 15, 2021', 'R', 'Updated Mar 5, 2020', 'R', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 2, 2020', 'Python', 'Updated Mar 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '39 contributions\n        in the last year', 'description': ['Linear-Regression\nLinear Regression with a R dataset\n'], 'url_profile': 'https://github.com/SusanRadu', 'info_list': ['Jupyter Notebook', 'Updated Apr 6, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Python', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 3, 2020', 'HTML', 'Updated Jan 15, 2021', 'R', 'Updated Mar 5, 2020', 'R', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 2, 2020', 'Python', 'Updated Mar 4, 2020']}","{'location': 'Honolulu, Hawaii', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': ['Regression_Practice\nBoston Housing Dataset regression and analysis\n'], 'url_profile': 'https://github.com/nbdavis', 'info_list': ['Jupyter Notebook', 'Updated Apr 6, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Python', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 3, 2020', 'HTML', 'Updated Jan 15, 2021', 'R', 'Updated Mar 5, 2020', 'R', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 2, 2020', 'Python', 'Updated Mar 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['Titanic\nRegression analysis for TITANIC data set using Random forest classifier with OOB score of 83.05%\n'], 'url_profile': 'https://github.com/SandipanPaul', 'info_list': ['Jupyter Notebook', 'Updated Apr 6, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Python', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 3, 2020', 'HTML', 'Updated Jan 15, 2021', 'R', 'Updated Mar 5, 2020', 'R', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 2, 2020', 'Python', 'Updated Mar 4, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/maheshwarang88', 'info_list': ['Jupyter Notebook', 'Updated Apr 6, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Python', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 3, 2020', 'HTML', 'Updated Jan 15, 2021', 'R', 'Updated Mar 5, 2020', 'R', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 2, 2020', 'Python', 'Updated Mar 4, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '124 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/saket007saket', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 10, 2020', 'R', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Java', 'Updated Jan 7, 2021', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['Music Genre Classification using Logistic Regression\nDataset Heirarchy\ngtzan_dataset\n\t\\blues\n\t\\classical\n\t\\country\n\t\\disco\n\t\\hiphop\n\t\\jazz\n\t\\metal\n\t\\pop\n\t\\reggae\n\t\\rock\n\nRun order of files:\npython3 wav_convert.py\n\nConverts music files to the needed .wav format and store into ./converted\npython3 wav_extract_fft.py\n\nExtracts fft values and saves numpy files to ./fft_extracted\npython3 log_reg.py fft_extracted/\n\nApplies Logistic Regression to train and classify; gives prediction accuracy, saves confusion matrix image and model\npython3 test.py ~/path/to/your/song/file\n\nLoads saved model to predict genre of new file\n'], 'url_profile': 'https://github.com/sandeepkramesan', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 10, 2020', 'R', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Java', 'Updated Jan 7, 2021', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/golaso17', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 10, 2020', 'R', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Java', 'Updated Jan 7, 2021', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020']}","{'location': 'Alajuela, Costa Rica', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ronaldjs', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 10, 2020', 'R', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Java', 'Updated Jan 7, 2021', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020']}","{'location': 'Berlin', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['Scalable ML: Algorithms for Ridge Regression\nHere will experiment with various algorithms for solving a regularized least square regression problem (ridge regression), and analyze the speed of convergence.\n'], 'url_profile': 'https://github.com/badalyaz', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 10, 2020', 'R', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Java', 'Updated Jan 7, 2021', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['RegressionDBA3803\n'], 'url_profile': 'https://github.com/tappyness1', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 10, 2020', 'R', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Java', 'Updated Jan 7, 2021', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '41 contributions\n        in the last year', 'description': ['Regression model:\ny_k = b_0 + b_1 t_k + x_k,\nin particular, noise x_k could fit an AR(1) model:\nx_k = a x_{k - 1} + sqrt(1 - a^2) e_k,  |a| < 1\nwhere e_k is a white noise.\n\nResults:\n1. Exact values of coverage factor K for normally distributed e_k\n2. Values of coverage factor K for e_k having two-side power (TSP) distribution (obtained by Monte Carlo simulation)\n3. Values of coverage factor K when x_k is 1/f^alpha noise (obtained by Monte Carlo simulation)\n(confidence level = 95%)\n'], 'url_profile': 'https://github.com/stepanov17', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 10, 2020', 'R', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Java', 'Updated Jan 7, 2021', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '73 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/AnahVeronica', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 10, 2020', 'R', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Java', 'Updated Jan 7, 2021', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['regression_model\nregression_model\n'], 'url_profile': 'https://github.com/diegoethi12', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 10, 2020', 'R', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Java', 'Updated Jan 7, 2021', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '125 contributions\n        in the last year', 'description': ['DataScience_Bootcamp_Udemy9\nDataScience Bootcamp by Jose Portilla on Udemy-Logistic Regression\n'], 'url_profile': 'https://github.com/GauriSaran', 'info_list': ['1', 'Jupyter Notebook', 'Updated Apr 21, 2020', 'Python', 'Updated Apr 10, 2020', 'R', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Java', 'Updated Jan 7, 2021', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Python', 'Updated Apr 5, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Linear Regression\n\nAssociation between Two Random Variables\nSimple Linear Regression Model\nMultiple Linear Regression Model\nEvaluation of model - Practical Standard\n\n'], 'url_profile': 'https://github.com/SmallBet', 'info_list': ['Jupyter Notebook', 'Updated Mar 5, 2020', 'Python', 'Updated Mar 6, 2020', 'R', 'Updated Mar 5, 2020', 'Python', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Apr 10, 2020', 'SAS', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Python', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Mariam-Hassan', 'info_list': ['Jupyter Notebook', 'Updated Mar 5, 2020', 'Python', 'Updated Mar 6, 2020', 'R', 'Updated Mar 5, 2020', 'Python', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Apr 10, 2020', 'SAS', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Python', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/LittlePenguin-OvO', 'info_list': ['Jupyter Notebook', 'Updated Mar 5, 2020', 'Python', 'Updated Mar 6, 2020', 'R', 'Updated Mar 5, 2020', 'Python', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Apr 10, 2020', 'SAS', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Python', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/jana1998', 'info_list': ['Jupyter Notebook', 'Updated Mar 5, 2020', 'Python', 'Updated Mar 6, 2020', 'R', 'Updated Mar 5, 2020', 'Python', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Apr 10, 2020', 'SAS', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Python', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}","{'location': 'Hyderabad', 'stats_list': [], 'contributions': '591 contributions\n        in the last year', 'description': ['Linear-Regression-from-scratch\nLinear Regression implemented from scratch in jupyter notebook\nTask:\n\nGiven a NASA data set, obtained from a series of aerodynamic and acoustic tests\nof two and three-dimensional airfoil blade sections. Implement a linear regression\nmodel from scratch using gradient descent to predict scaled sound pressure level.\nThe various attributes of the data are explained in the file description.txt.\nUsing appropriate plot show how number of iterations is affecting the mean squared\nerror for above model under below given conditions:\n(a) Using 3 different initial regression coefficients (weights) for fixed value of learn-\ning parameter (All 3 in single plot).\n(b) Using 3 different learning parameters for some fixed initial regression coeffi-\ncients. (All 3 in single plot)\nIf you want to apply regression on some dataset but one of it‚Äôs features has missing\nvalues under below given conditions, how will you approach the problem. (No need\nof Code Experimentation)\n(a) When 0-0.5% of values are missing of that feature\n(b) When 8-10% of values are missing of that feature\n(c) When 60-70% of values are missing of that feature\n\nSolution:\nImplemented everything in detailed manner in jupyter notebook\n'], 'url_profile': 'https://github.com/AshishKempwad', 'info_list': ['Jupyter Notebook', 'Updated Mar 5, 2020', 'Python', 'Updated Mar 6, 2020', 'R', 'Updated Mar 5, 2020', 'Python', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Apr 10, 2020', 'SAS', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Python', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}","{'location': 'Noida, India', 'stats_list': [], 'contributions': '109 contributions\n        in the last year', 'description': ['carprice_prediction\nA linear regression solution to predict car price.\n'], 'url_profile': 'https://github.com/singh-manvinder', 'info_list': ['Jupyter Notebook', 'Updated Mar 5, 2020', 'Python', 'Updated Mar 6, 2020', 'R', 'Updated Mar 5, 2020', 'Python', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Apr 10, 2020', 'SAS', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Python', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}","{'location': 'McLean, VA', 'stats_list': [], 'contributions': '41 contributions\n        in the last year', 'description': [""UVA Regression Analysis\nThis was written in an effort to analyze, predict, and model UVA's average points per game based on their previous seasons ranging from 2011 to the present. The program was written in SAS and features many common data analysis practice, such as explanatory data anaylsis and  modeling by step. The original data taken off of Kaggle was converted using code to create dummy variables to test qualitative variables.\nInteraction and higher-order terms were all tested by nested F-tests. After numerous testing, it was found that the most significant variables that affect points per game are field goal percentage, assists, 3 point percentage, free throws, and turnovers.\nThe data was compiled by Nate Duncan, a well recognized data analyst specializing in NBA game data.\nReferences\nwww.kaggle.com/nateduncan/2011current-ncaa-basketball-games/data\n""], 'url_profile': 'https://github.com/clifflinrichie', 'info_list': ['Jupyter Notebook', 'Updated Mar 5, 2020', 'Python', 'Updated Mar 6, 2020', 'R', 'Updated Mar 5, 2020', 'Python', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Apr 10, 2020', 'SAS', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Python', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/acarunkumar', 'info_list': ['Jupyter Notebook', 'Updated Mar 5, 2020', 'Python', 'Updated Mar 6, 2020', 'R', 'Updated Mar 5, 2020', 'Python', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Apr 10, 2020', 'SAS', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Python', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/tse-hou', 'info_list': ['Jupyter Notebook', 'Updated Mar 5, 2020', 'Python', 'Updated Mar 6, 2020', 'R', 'Updated Mar 5, 2020', 'Python', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Apr 10, 2020', 'SAS', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Python', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}","{'location': 'M√©rida, Yucat√°n, M√©xico', 'stats_list': [], 'contributions': '197 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/alejandropuerto', 'info_list': ['Jupyter Notebook', 'Updated Mar 5, 2020', 'Python', 'Updated Mar 6, 2020', 'R', 'Updated Mar 5, 2020', 'Python', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Apr 10, 2020', 'SAS', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Python', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '61 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ipalvis', 'info_list': ['R', 'Updated Mar 6, 2020', 'Updated Mar 2, 2020', 'Python', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Updated Mar 2, 2020', 'R', 'Updated Mar 6, 2020', '1', 'Python', 'GPL-3.0 license', 'Updated Mar 2, 2020']}","{'location': 'Chicago', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['King-County--Project-2\nBuilding a regression model on housing data from king county.\n'], 'url_profile': 'https://github.com/MixMaster1', 'info_list': ['R', 'Updated Mar 6, 2020', 'Updated Mar 2, 2020', 'Python', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Updated Mar 2, 2020', 'R', 'Updated Mar 6, 2020', '1', 'Python', 'GPL-3.0 license', 'Updated Mar 2, 2020']}","{'location': 'Munich', 'stats_list': [], 'contributions': '233 contributions\n        in the last year', 'description': ['Linear-Regression\nSimple linear regression! Using sklearn,pandas,matplotlib and pickle.\nI used the popular Student Performance Data Set (https://archive.ics.uci.edu/ml/datasets/Student+Performance) to predict students final grade based on a series of attributes.\n'], 'url_profile': 'https://github.com/SimonBurmer', 'info_list': ['R', 'Updated Mar 6, 2020', 'Updated Mar 2, 2020', 'Python', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Updated Mar 2, 2020', 'R', 'Updated Mar 6, 2020', '1', 'Python', 'GPL-3.0 license', 'Updated Mar 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '199 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/nilberthsouza', 'info_list': ['R', 'Updated Mar 6, 2020', 'Updated Mar 2, 2020', 'Python', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Updated Mar 2, 2020', 'R', 'Updated Mar 6, 2020', '1', 'Python', 'GPL-3.0 license', 'Updated Mar 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '598 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Fredpwol', 'info_list': ['R', 'Updated Mar 6, 2020', 'Updated Mar 2, 2020', 'Python', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Updated Mar 2, 2020', 'R', 'Updated Mar 6, 2020', '1', 'Python', 'GPL-3.0 license', 'Updated Mar 2, 2020']}","{'location': 'banglore , india', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['Linear_Regression\nbasics of linear regression model using sklearn library\n'], 'url_profile': 'https://github.com/Anshulmeshram', 'info_list': ['R', 'Updated Mar 6, 2020', 'Updated Mar 2, 2020', 'Python', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Updated Mar 2, 2020', 'R', 'Updated Mar 6, 2020', '1', 'Python', 'GPL-3.0 license', 'Updated Mar 2, 2020']}","{'location': 'Charlotte, NC', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/BonaventureR', 'info_list': ['R', 'Updated Mar 6, 2020', 'Updated Mar 2, 2020', 'Python', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Updated Mar 2, 2020', 'R', 'Updated Mar 6, 2020', '1', 'Python', 'GPL-3.0 license', 'Updated Mar 2, 2020']}","{'location': 'Nairobi', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': [""REGRESSION-IN-R\nA beginner's guide toward regression in R\n""], 'url_profile': 'https://github.com/Machocho254', 'info_list': ['R', 'Updated Mar 6, 2020', 'Updated Mar 2, 2020', 'Python', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Updated Mar 2, 2020', 'R', 'Updated Mar 6, 2020', '1', 'Python', 'GPL-3.0 license', 'Updated Mar 2, 2020']}","{'location': 'Juiz de Fora, Brazil', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ClecioFerreira', 'info_list': ['R', 'Updated Mar 6, 2020', 'Updated Mar 2, 2020', 'Python', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Updated Mar 2, 2020', 'R', 'Updated Mar 6, 2020', '1', 'Python', 'GPL-3.0 license', 'Updated Mar 2, 2020']}","{'location': 'Trento (Italy)', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': [""hierachical_bayesian_regression_pymc3_20200301\nHierachical Bayesian regression by PyMC3 + prediction performance evaluation\nPROJECT NAME: 'hierachical_bayesian_model_regression_20200102'\nAUTHOR: Paolo Ranzi\nREADME FILE VERSION (LAST UPDATE): 20200301\nPython 3.6.7 has been used. For each step the specific Python script has been mentioned, accordingly. At the begginning of each script we have to make sure of setting custom-made parameters/pieces of information:\n\nimport Python libraries (if you do not have a specific library you have to manually install it by using PIP within your virtual enviroment);\nsetting paths and keywords according to your storage location;\nset parameters (e.g. input file name, output file name etc.);\nall scripts have been optimized for using them by parallel computing. Please set number of CPUs by searching for 'cpu_count()' within each script according to your available resources;\nSYNOPSYS: script 01 + 02 corresponds at not hierachical logistic regression (used just for comparison\nwith Hierachical Bayesian Model (HBM)); instead script 03 + 04 + 05 have been used for the more interesting HBM logistic regression;\n\nSTEPS:\n\n\nLEARNING STEP (NOT HIERACHICAL)\n(it computes Bayesian + MCMC not-hierachical logistic regression by PyMC3):\nSCRIPT NAME: '01_logistic_regression_20200102.py'\nINPUT: .csv file;\nOUTPUT: out-of-bag .csv file; pickeled bayesian model ( .joblib); pickeled MCMC traces (to be used later for diagnositics and prediction);\n\n\nVALIDATION STEP BY CHECKING ALGORITHM'S PREDICTIIVE PERFORMANCE:\n(it computes MCC + precision + recall metrics on the out-of-bag data-set):\nSCRIPT NAME: '02_logistic_prediction_20200102.py'\nINPUT: '01_logistic_regression_20200102.py' + out-of-bag .csv file;\nOUTPUT: model validation metrics (i.e. MCC, precision and reacall);\n\n\nSPLITTING DATA-SET IN CHUNKS\n(it computes Bayesian + MCMC not-hierachical logistic regression by PyMC3):\nSCRIPT NAME: '03_split_dataset_20200102.py'\nINPUT: .csv file;\nOUTPUT: .csv files, one for each chunk;\n\n\nLEARNING STEP (HIERACHICAL)\n(it computes Bayesian + MCMC hierachical logistic regression by PyMC3):\nSCRIPT NAME: '04_HBM_analysis_20200102.py'\nINPUT: .csv files, one for each chunk;\nOUTPUT: out-of-bag .csv files; pickeled bayesian model ( .joblib); pickeled MCMC traces (to be used later for diagnositics and prediction);\n\n\nVALIDATION STEP BY CHECKING ALGORITHM'S PREDICTIIVE PERFORMANCE:\n(it computes MCC + precision + recall metrics on the out-of-bag data-set):\nSCRIPT NAME: '05_HBM_prediction_20200102.py'\nINPUT: '04_HBM_analysis_20200102.py' + out-of-bag .csv files;\nOUTPUT: model validation metrics (i.e. MCC, precision and reacall);\n\n\n""], 'url_profile': 'https://github.com/PaoloRanzi81', 'info_list': ['R', 'Updated Mar 6, 2020', 'Updated Mar 2, 2020', 'Python', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated May 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Updated Mar 2, 2020', 'R', 'Updated Mar 6, 2020', '1', 'Python', 'GPL-3.0 license', 'Updated Mar 2, 2020']}"
"{'location': 'Tempe, Arizona', 'stats_list': [], 'contributions': '438 contributions\n        in the last year', 'description': ['Naive-Bayes_and_Logistic-Regression\nNaive Bayes and Logistic Regression for MNIS Handwritten Data\n'], 'url_profile': 'https://github.com/asgaonkar', 'info_list': ['Python', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 3, 2020', 'Ruby', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Mar 2, 2020', 'R', 'Updated Mar 2, 2020']}","{'location': 'M√©rida, Yucat√°n, M√©xico', 'stats_list': [], 'contributions': '197 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/alejandropuerto', 'info_list': ['Python', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 3, 2020', 'Ruby', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Mar 2, 2020', 'R', 'Updated Mar 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/doublexu1209', 'info_list': ['Python', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 3, 2020', 'Ruby', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Mar 2, 2020', 'R', 'Updated Mar 2, 2020']}","{'location': 'Los Angeles', 'stats_list': [], 'contributions': '612 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Chalmiller', 'info_list': ['Python', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 3, 2020', 'Ruby', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Mar 2, 2020', 'R', 'Updated Mar 2, 2020']}","{'location': 'Dubai', 'stats_list': [], 'contributions': '63 contributions\n        in the last year', 'description': ['Logistic Regression Implementing perceptron using python\nLogistic regression implemetion using python\n'], 'url_profile': 'https://github.com/abidaks', 'info_list': ['Python', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 3, 2020', 'Ruby', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Mar 2, 2020', 'R', 'Updated Mar 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['M-estimator-non-linear-regression\n'], 'url_profile': 'https://github.com/maincover', 'info_list': ['Python', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 3, 2020', 'Ruby', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Mar 2, 2020', 'R', 'Updated Mar 2, 2020']}","{'location': 'Cincinnati', 'stats_list': [], 'contributions': '88 contributions\n        in the last year', 'description': ['Problem Statement\nA Chinese automobile company aspires to enter the US market by setting up their manufacturing unit there and producing cars locally to give competition to their US and European counterparts.\nThey want to understand the factors on which the pricing of cars depends. Specifically, they want to understand the factors affecting the pricing of cars in the American market, since those may be very different from the Chinese market. The company wants to know:\n\nWhich variables are significant in predicting the price of a car\nHow well those variables describe the price of a car\n\nBased on various market surveys, they have gathered a large dataset of different types of cars across the American market.\nBusiness Goal\nWe are required to model the price of cars with the available independent variables. It will be used by the management to understand how exactly the prices vary with the independent variables. They can accordingly manipulate the design of the cars, the business strategy etc. to meet certain price levels. Further, the model will be a good way for management to understand the pricing dynamics of a new market.\n'], 'url_profile': 'https://github.com/sidharthg', 'info_list': ['Python', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 3, 2020', 'Ruby', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Mar 2, 2020', 'R', 'Updated Mar 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Assignment-A43-Parking-Estimation\nArtificial Intelligence, Machine Learning - Supervised Machine Learning: Regression\n'], 'url_profile': 'https://github.com/BA-Software-Development-Ed', 'info_list': ['Python', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 3, 2020', 'Ruby', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Mar 2, 2020', 'R', 'Updated Mar 2, 2020']}","{'location': 'Munich', 'stats_list': [], 'contributions': '63 contributions\n        in the last year', 'description': ['Sentiment_analysis\nMovie Reviews classification ( Sentiment Analysis) using logistic regression\n'], 'url_profile': 'https://github.com/belgacemi', 'info_list': ['Python', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 3, 2020', 'Ruby', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Mar 2, 2020', 'R', 'Updated Mar 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['Predictive-Ananlysis\nA simple logistic regression on a football dataset.\n'], 'url_profile': 'https://github.com/Mrudhula0607', 'info_list': ['Python', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 3, 2020', 'Ruby', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Mar 2, 2020', 'R', 'Updated Mar 2, 2020']}"
"{'location': 'Kolkata ', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['River Non River image classification using Logistic Regression\n‚Ä¢ Four satellite Images of Hoogly River, Kolkata (Rband, Gband, Bband and Iband) are given to you with equal image size (512 * 512).\n‚Ä¢ The feature vector dimension is 4\n‚Ä¢ Each pixel location we have four values.\n‚Ä¢ Two Classes are given (River and NonRiver)\n‚Ä¢ Take 50 sample points (Pixel location‚Äôs corresponding pixel values) from river class for training for each band\n‚Ä¢ Take 100 sample points (Pixel location‚Äôs corresponding pixel values) from non river class for training for each band.\n‚Ä¢ Take (512 * 512) sample points (Pixel location‚Äôs corresponding pixel values) for testing for each band.\n‚Ä¢ Apply logistic regression to classify all the test sample either in river or nonriver class denoting 0 and 255 at corresponding pixel locations.\n‚Ä¢ Show the result in image form with black and white image (either 0 and 255)\n'], 'url_profile': 'https://github.com/SouravG', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Mar 4, 2020', '1', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 6, 2020', 'HTML', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Python', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020']}","{'location': 'Raleigh,NC', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['Polynomial_Regression\nA simple code to understand polynomial regression for curve fitting\n'], 'url_profile': 'https://github.com/Abhishek-EE', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Mar 4, 2020', '1', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 6, 2020', 'HTML', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Python', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['Battery-Life-Prediction-Polynomial-Regression-\nMachine Learning Project 2: Battery Life Prediction Using  Polynomial Regression\n'], 'url_profile': 'https://github.com/parameswar-kotari', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Mar 4, 2020', '1', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 6, 2020', 'HTML', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Python', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '121 contributions\n        in the last year', 'description': ['ML_Model_for_Predict_Big_Mart_Sale\nPredict Big Mart Sale with Simple Regression Model\nAuthor: Panuwat Ulis\nDate: 11/30/2019\nWe build a simple model using train_bigmart.csv and test_bigmart.csv data set for predicting Item_Outlet_Sales. This project is organized as follows: (a) Exploratory Data Analysis; (b) Data Preprocessing; (c) Machine Learning Modeling; and (d) Model Evaluation\ntrain_bigmart.csv and test_bigmart.csv: dataset used for model building.\nRegression with Big Mart Sale.ipynb: the jupyter notebook containing code.\n'], 'url_profile': 'https://github.com/PanuwatUlis', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Mar 4, 2020', '1', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 6, 2020', 'HTML', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Python', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Mariam-Hassan', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Mar 4, 2020', '1', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 6, 2020', 'HTML', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Python', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/MurderousMtnDew', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Mar 4, 2020', '1', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 6, 2020', 'HTML', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Python', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['Welcome to this Linear Regression App!\nto launch the app type ""flask run"" into your terminal while in the main directory\nOnce launched first navigate to the Uploader Section and upload an appropriately formated csv(only numerical columns with the target variable being a binary)\nonce you have uploaded your files, go to the selector tab and select which file and target variable you would like to run the Linear Regression on\nFinally hit submit\nSeveral test files have been provided for you already in the data folder\nBug report: on the selector tab you have to change which file is selected in order for the columns to populate, even if the file you want is already selected you will need change the selected file at least once.\n'], 'url_profile': 'https://github.com/NathanLaird', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Mar 4, 2020', '1', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 6, 2020', 'HTML', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Python', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '31 contributions\n        in the last year', 'description': ['Linear-Regression-PREDICTING-CAR-PRICE\nPREDICTING PRICE OF PRE-OWNED CARS using Linear Regression and Random Forest\n'], 'url_profile': 'https://github.com/rshankarsharma9', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Mar 4, 2020', '1', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 6, 2020', 'HTML', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Python', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020']}","{'location': 'M√©rida, Yucat√°n, M√©xico', 'stats_list': [], 'contributions': '197 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/alejandropuerto', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Mar 4, 2020', '1', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 6, 2020', 'HTML', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Python', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020']}","{'location': 'Melbourne, Australia', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/CarlosCastill', 'info_list': ['Jupyter Notebook', 'MIT license', 'Updated Mar 4, 2020', '1', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 6, 2020', 'HTML', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Python', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/laurenvvb', 'info_list': ['Jupyter Notebook', 'Updated Mar 2, 2020', '3', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', '1', 'JavaScript', 'Updated Sep 23, 2020', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '113 contributions\n        in the last year', 'description': ['predict-COVID19\nIn this notebook I tried to replicate the results of a study about the spread of COVID-19 virus in Italy using concepts from statistical regression analysis\n'], 'url_profile': 'https://github.com/pietroventurini', 'info_list': ['Jupyter Notebook', 'Updated Mar 2, 2020', '3', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', '1', 'JavaScript', 'Updated Sep 23, 2020', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ziqi-tan', 'info_list': ['Jupyter Notebook', 'Updated Mar 2, 2020', '3', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', '1', 'JavaScript', 'Updated Sep 23, 2020', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '169 contributions\n        in the last year', 'description': ['UrbanGradRAM Project\nA simple function that visualises the most salient region of a Keras VGG16 regression model. The code adapts from the gradient activation mapping method into a regression setting. The method will be applied on an urban scenicness model and will be evaluated against a simple pointing task.\nDependencies for running locally\n\nKeras\nnumpy\npandas\ncv2\n\n'], 'url_profile': 'https://github.com/booboo18', 'info_list': ['Jupyter Notebook', 'Updated Mar 2, 2020', '3', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', '1', 'JavaScript', 'Updated Sep 23, 2020', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020']}","{'location': 'Ho Chi Minh City', 'stats_list': [], 'contributions': '107 contributions\n        in the last year', 'description': ['AI CALCULATOR\n\nThe project is to build a web application that can recognize and perform mathematics on handwritten regression.\nThe training uses CNN model with image data preprocessed by OpenCV. The entire code can be found at model/main_nb.ipynb\nTo try web demo, run:\npython3 app/main.py\n\nImage data is received by using Javascript canvas, then processed with OpenCV before fit into pre-trained model for prediction.\n'], 'url_profile': 'https://github.com/nhanphan0411', 'info_list': ['Jupyter Notebook', 'Updated Mar 2, 2020', '3', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', '1', 'JavaScript', 'Updated Sep 23, 2020', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['EnergyOutputPredicition\nElectrical energy output prediction using regression techniques in Pyspark\n'], 'url_profile': 'https://github.com/zayedupal', 'info_list': ['Jupyter Notebook', 'Updated Mar 2, 2020', '3', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', '1', 'JavaScript', 'Updated Sep 23, 2020', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020']}","{'location': 'India, Bengaluru', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['Machine-learning-practice\npracticing the machine learning for regression and classification\n'], 'url_profile': 'https://github.com/lakshmipathips', 'info_list': ['Jupyter Notebook', 'Updated Mar 2, 2020', '3', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', '1', 'JavaScript', 'Updated Sep 23, 2020', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['LogisticRegression\nLogistic Regression with :-\n\n\nGradient descent using pima-indian-diabites dataset .\nReference of the dataset :- https://github.com/jbrownlee/Datasets/blob/master/pima-indians-diabetes.csv\n\n\nMaximum -likelihood- estimation.\nReference of the dataset used:-\nDua, D. and Graff, C. (2019). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California,      School of Information and Computer Science.\n\n\n'], 'url_profile': 'https://github.com/KaveriKR', 'info_list': ['Jupyter Notebook', 'Updated Mar 2, 2020', '3', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', '1', 'JavaScript', 'Updated Sep 23, 2020', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020']}","{'location': 'Arvada', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['Project 2 - Ames Housing Data and Kaggle Challenge\nWelcome to Project 2! It\'s time to start modeling.\nPrimary Learning Objectives:\n\nCreating and iteratively refining a regression model\nUsing Kaggle to practice the modeling process\nProviding business insights through reporting and presentation.\n\nYou are tasked with creating a regression model based on the Ames Housing Dataset. This model will predict the price of a house at sale.\nThe Ames Housing Dataset is an exceptionally detailed and robust dataset with over 70 columns of different features relating to houses.\nSecondly, we are hosting a competition on Kaggle to give you the opportunity to practice the following skills:\n\nRefining models over time\nUse of train-test split, cross-validation, and data with unknown values for the target to simulate the modeling process\nThe use of Kaggle as a place to practice data science\n\nAs always, you will be submitting a technical report and a presentation. You may find that the best model for Kaggle is not the best model to address your data science problem.\nSet-up\nBefore you begin working on this project, please do the following:\n\nSign up for an account on Kaggle\nIMPORTANT: Click this link (Regression Challenge Sign Up) to join the competition (otherwise you will not be able to make submissions!)\nReview the material on the DSI-US-10 Regression Challenge\nReview the data description.\n\nThe Modeling Process\n\nThe train dataset has all of the columns that you will need to generate and refine your models. The test dataset has all of those columns except for the target that you are trying to predict in your Regression model.\nGenerate your regression model using the training data. We expect that within this process, you\'ll be making use of:\n\ntrain-test split\ncross-validation / grid searching for hyperparameters\nstrong exploratory data analysis to question correlation and relationship across predictive variables\ncode that reproducibly and consistently applies feature transformation (such as the preprocessing library)\n\n\nPredict the values for your target column in the test dataset and submit your predictions to Kaggle to see how your model does against unknown data.\n\nNote: Kaggle expects to see your submissions in a specific format. Check the challenge\'s page to make sure you are formatting your CSVs correctly!\nYou are limited to models you\'ve learned in class. In other words, you cannot use XGBoost, Neural Networks or any other advanced model for this project.\n\n\nEvaluate your models!\n\nconsider your evaluation metrics\nconsider your baseline score\nhow can your model be used for inference?\nwhy do you believe your model will generalize to new data?\n\n\n\nSubmission\nMaterials must be submitted by the beginning of class on Friday, January 17.\nThe last day for the Kaggle competition will be Friday, January 17.\nYour technical report will be hosted on Github Enterprise. Make sure it includes:\n\nA README.md (that isn\'t this file)\nJupyter notebook(s) with your analysis and models (renamed to describe your project)\nAt least one successful prediction submission on DSI-US-10 Regression Challenge --  you should see your name in the ""Leaderboard"" tab.\nData files\nPresentation slides\nAny other necessary files (images, etc.)\n\nCheck with your local instructor for how they would like you to submit your repo for review.\n\nPresentation Structure\n\nMust be within time limit established by local instructor.\nUse Google Slides or some other visual aid (Keynote, Powerpoint, etc).\nConsider the audience. Check with your local instructor for direction.\nStart with the data science problem.\nUse visuals that are appropriately scaled and interpretable.\nTalk about your procedure/methodology (high level).\nTalk about your primary findings.\nMake sure you provide clear recommendations that follow logically from your analyses and narrative and answer your data science problem.\n\nBe sure to rehearse and time your presentation before class.\n\nRubric\nYour local instructor will evaluate your project (for the most part) using the following criteria.  You should make sure that you consider and/or follow most if not all of the considerations/recommendations outlined below while working through your project.\nScores will be out of 27 points based on the 9 items in the rubric. \n3 points per section\n\n\n\nScore\nInterpretation\n\n\n\n\n0\nProject fails to meet the minimum requirements for this item.\n\n\n1\nProject meets the minimum requirements for this item, but falls significantly short of portfolio-ready expectations.\n\n\n2\nProject exceeds the minimum requirements for this item, but falls short of portfolio-ready expectations.\n\n\n3\nProject meets or exceeds portfolio-ready expectations; demonstrates a thorough understanding of every outlined consideration.\n\n\n\nThe Data Science Process\nProblem Statement\n\nIs it clear what the student plans to do?\nWhat type of model will be developed?\nHow will success be evaluated?\nIs the scope of the project appropriate?\nIs it clear who cares about this or why this is important to investigate?\nDoes the student consider the audience and the primary and secondary stakeholders?\n\nData Cleaning and EDA\n\nAre missing values imputed appropriately?\nAre distributions examined and described?\nAre outliers identified and addressed?\nAre appropriate summary statistics provided?\nAre steps taken during data cleaning and EDA framed appropriately?\nDoes the student address whether or not they are likely to be able to answer their problem statement with the provided data given what they\'ve discovered during EDA?\n\nPreprocessing and Modeling\n\nAre categorical variables one-hot encoded?\nDoes the student investigate or manufacture features with linear relationships to the target?\nHave the data been scaled appropriately?\nDoes the student properly split and/or sample the data for validation/training purposes?\nDoes the student utilize feature selection to remove noisy or multi-collinear features?\nDoes the student test and evaluate a variety of models to identify a production algorithm (AT MINIMUM: linear regression, lasso, and ridge)?\nDoes the student defend their choice of production model relevant to the data at hand and the problem?\nDoes the student explain how the model works and evaluate its performance successes/downfalls?\n\nEvaluation and Conceptual Understanding\n\nDoes the student accurately identify and explain the baseline score?\nDoes the student select and use metrics relevant to the problem objective?\nIs more than one metric utilized in order to better assess performance?\nDoes the student interpret the results of their model for purposes of inference?\nIs domain knowledge demonstrated when interpreting results?\nDoes the student provide appropriate interpretation with regards to descriptive and inferential statistics?\n\nConclusion and Recommendations\n\nDoes the student provide appropriate context to connect individual steps back to the overall project?\nIs it clear how the final recommendations were reached?\nAre the conclusions/recommendations clearly stated?\nDoes the conclusion answer the original problem statement?\nDoes the student address how findings of this research can be applied for the benefit of stakeholders?\nAre future steps to move the project forward identified?\n\nOrganization and Professionalism\nProject Organization\n\nAre modules imported correctly (using appropriate aliases)?\nAre data imported/saved using relative paths?\nDoes the README provide a good executive summary of the project?\nIs markdown formatting used appropriately to structure notebooks?\nAre there an appropriate amount of comments to support the code?\nAre files & directories organized correctly?\nAre there unnecessary files included?\nDo files and directories have well-structured, appropriate, consistent names?\n\nVisualizations\n\nAre sufficient visualizations provided?\nDo plots accurately demonstrate valid relationships?\nAre plots labeled properly?\nAre plots interpreted appropriately?\nAre plots formatted and scaled appropriately for inclusion in a notebook-based technical report?\n\nPython Syntax and Control Flow\n\nIs care taken to write human readable code?\nIs the code syntactically correct (no runtime errors)?\nDoes the code generate desired results (logically correct)?\nDoes the code follows general best practices and style guidelines?\nAre Pandas functions used appropriately?\nAre sklearn methods used appropriately?\n\nPresentation\n\nIs the problem statement clearly presented?\nDoes a strong narrative run through the presentation building toward a final conclusion?\nAre the conclusions/recommendations clearly stated?\nIs the level of technicality appropriate for the intended audience?\nIs the student substantially over or under time?\nDoes the student appropriately pace their presentation?\nDoes the student deliver their message with clarity and volume?\nAre appropriate visualizations generated for the intended audience?\nAre visualizations necessary and useful for supporting conclusions/explaining findings?\n\nIn order to pass the project, students must earn a minimum score of 1 for each category.\n\nEarning below a 1 in one or more of the above categories would result in a failing project.\nWhile a minimum of 1 in each category is the required threshold for graduation, students should aim to earn at least an average of 1.5 across each category. An average score below 1.5, while it may be passing, means students may want to solicit specific feedback in order to significantly improve the project before showcasing it as part of a portfolio or the job search.\n\nREMEMBER:\nThis is a learning environment and you are encouraged to try new things, even if they don\'t work out as well as you planned! While this rubric outlines what we look for in a good project, it is up to you to go above and beyond to create a great project. Learn from your failures and you\'ll be prepared to succeed in the workforce.\n'], 'url_profile': 'https://github.com/meghazavar', 'info_list': ['Jupyter Notebook', 'Updated Mar 2, 2020', '3', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', '1', 'JavaScript', 'Updated Sep 23, 2020', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': ['Multi Stage Heatmap Regression\nDetecting Facial Keypoints using a Multi-Stage Heatmap Regression Network.\nNOTE: The notebook might be easier to read on the Kaggle kernel here.\nIntroduction\nThis github repo stores my notebook in which I introduced a way of predicting facial keypoints via heatmap regression. I utilize a popular deep learning architecture called: Convolutional Pose Machines which essentially takes RGB images as input and outputs a collection of heatmaps.\nThe heatmaps can be viewed as a probability distribution of the likelihood that a keypoint resides at every pixel. This technique is very popular in computer vision applications such as human pose estimation.\n'], 'url_profile': 'https://github.com/Fmak95', 'info_list': ['Jupyter Notebook', 'Updated Mar 2, 2020', '3', 'Jupyter Notebook', 'Updated Mar 31, 2020', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', '1', 'JavaScript', 'Updated Sep 23, 2020', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020']}"
"{'location': 'Arvada', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['Project 2 - Ames Housing Data and Kaggle Challenge\nWelcome to Project 2! It\'s time to start modeling.\nPrimary Learning Objectives:\n\nCreating and iteratively refining a regression model\nUsing Kaggle to practice the modeling process\nProviding business insights through reporting and presentation.\n\nYou are tasked with creating a regression model based on the Ames Housing Dataset. This model will predict the price of a house at sale.\nThe Ames Housing Dataset is an exceptionally detailed and robust dataset with over 70 columns of different features relating to houses.\nSecondly, we are hosting a competition on Kaggle to give you the opportunity to practice the following skills:\n\nRefining models over time\nUse of train-test split, cross-validation, and data with unknown values for the target to simulate the modeling process\nThe use of Kaggle as a place to practice data science\n\nAs always, you will be submitting a technical report and a presentation. You may find that the best model for Kaggle is not the best model to address your data science problem.\nSet-up\nBefore you begin working on this project, please do the following:\n\nSign up for an account on Kaggle\nIMPORTANT: Click this link (Regression Challenge Sign Up) to join the competition (otherwise you will not be able to make submissions!)\nReview the material on the DSI-US-10 Regression Challenge\nReview the data description.\n\nThe Modeling Process\n\nThe train dataset has all of the columns that you will need to generate and refine your models. The test dataset has all of those columns except for the target that you are trying to predict in your Regression model.\nGenerate your regression model using the training data. We expect that within this process, you\'ll be making use of:\n\ntrain-test split\ncross-validation / grid searching for hyperparameters\nstrong exploratory data analysis to question correlation and relationship across predictive variables\ncode that reproducibly and consistently applies feature transformation (such as the preprocessing library)\n\n\nPredict the values for your target column in the test dataset and submit your predictions to Kaggle to see how your model does against unknown data.\n\nNote: Kaggle expects to see your submissions in a specific format. Check the challenge\'s page to make sure you are formatting your CSVs correctly!\nYou are limited to models you\'ve learned in class. In other words, you cannot use XGBoost, Neural Networks or any other advanced model for this project.\n\n\nEvaluate your models!\n\nconsider your evaluation metrics\nconsider your baseline score\nhow can your model be used for inference?\nwhy do you believe your model will generalize to new data?\n\n\n\nSubmission\nMaterials must be submitted by the beginning of class on Friday, January 17.\nThe last day for the Kaggle competition will be Friday, January 17.\nYour technical report will be hosted on Github Enterprise. Make sure it includes:\n\nA README.md (that isn\'t this file)\nJupyter notebook(s) with your analysis and models (renamed to describe your project)\nAt least one successful prediction submission on DSI-US-10 Regression Challenge --  you should see your name in the ""Leaderboard"" tab.\nData files\nPresentation slides\nAny other necessary files (images, etc.)\n\nCheck with your local instructor for how they would like you to submit your repo for review.\n\nPresentation Structure\n\nMust be within time limit established by local instructor.\nUse Google Slides or some other visual aid (Keynote, Powerpoint, etc).\nConsider the audience. Check with your local instructor for direction.\nStart with the data science problem.\nUse visuals that are appropriately scaled and interpretable.\nTalk about your procedure/methodology (high level).\nTalk about your primary findings.\nMake sure you provide clear recommendations that follow logically from your analyses and narrative and answer your data science problem.\n\nBe sure to rehearse and time your presentation before class.\n\nRubric\nYour local instructor will evaluate your project (for the most part) using the following criteria.  You should make sure that you consider and/or follow most if not all of the considerations/recommendations outlined below while working through your project.\nScores will be out of 27 points based on the 9 items in the rubric. \n3 points per section\n\n\n\nScore\nInterpretation\n\n\n\n\n0\nProject fails to meet the minimum requirements for this item.\n\n\n1\nProject meets the minimum requirements for this item, but falls significantly short of portfolio-ready expectations.\n\n\n2\nProject exceeds the minimum requirements for this item, but falls short of portfolio-ready expectations.\n\n\n3\nProject meets or exceeds portfolio-ready expectations; demonstrates a thorough understanding of every outlined consideration.\n\n\n\nThe Data Science Process\nProblem Statement\n\nIs it clear what the student plans to do?\nWhat type of model will be developed?\nHow will success be evaluated?\nIs the scope of the project appropriate?\nIs it clear who cares about this or why this is important to investigate?\nDoes the student consider the audience and the primary and secondary stakeholders?\n\nData Cleaning and EDA\n\nAre missing values imputed appropriately?\nAre distributions examined and described?\nAre outliers identified and addressed?\nAre appropriate summary statistics provided?\nAre steps taken during data cleaning and EDA framed appropriately?\nDoes the student address whether or not they are likely to be able to answer their problem statement with the provided data given what they\'ve discovered during EDA?\n\nPreprocessing and Modeling\n\nAre categorical variables one-hot encoded?\nDoes the student investigate or manufacture features with linear relationships to the target?\nHave the data been scaled appropriately?\nDoes the student properly split and/or sample the data for validation/training purposes?\nDoes the student utilize feature selection to remove noisy or multi-collinear features?\nDoes the student test and evaluate a variety of models to identify a production algorithm (AT MINIMUM: linear regression, lasso, and ridge)?\nDoes the student defend their choice of production model relevant to the data at hand and the problem?\nDoes the student explain how the model works and evaluate its performance successes/downfalls?\n\nEvaluation and Conceptual Understanding\n\nDoes the student accurately identify and explain the baseline score?\nDoes the student select and use metrics relevant to the problem objective?\nIs more than one metric utilized in order to better assess performance?\nDoes the student interpret the results of their model for purposes of inference?\nIs domain knowledge demonstrated when interpreting results?\nDoes the student provide appropriate interpretation with regards to descriptive and inferential statistics?\n\nConclusion and Recommendations\n\nDoes the student provide appropriate context to connect individual steps back to the overall project?\nIs it clear how the final recommendations were reached?\nAre the conclusions/recommendations clearly stated?\nDoes the conclusion answer the original problem statement?\nDoes the student address how findings of this research can be applied for the benefit of stakeholders?\nAre future steps to move the project forward identified?\n\nOrganization and Professionalism\nProject Organization\n\nAre modules imported correctly (using appropriate aliases)?\nAre data imported/saved using relative paths?\nDoes the README provide a good executive summary of the project?\nIs markdown formatting used appropriately to structure notebooks?\nAre there an appropriate amount of comments to support the code?\nAre files & directories organized correctly?\nAre there unnecessary files included?\nDo files and directories have well-structured, appropriate, consistent names?\n\nVisualizations\n\nAre sufficient visualizations provided?\nDo plots accurately demonstrate valid relationships?\nAre plots labeled properly?\nAre plots interpreted appropriately?\nAre plots formatted and scaled appropriately for inclusion in a notebook-based technical report?\n\nPython Syntax and Control Flow\n\nIs care taken to write human readable code?\nIs the code syntactically correct (no runtime errors)?\nDoes the code generate desired results (logically correct)?\nDoes the code follows general best practices and style guidelines?\nAre Pandas functions used appropriately?\nAre sklearn methods used appropriately?\n\nPresentation\n\nIs the problem statement clearly presented?\nDoes a strong narrative run through the presentation building toward a final conclusion?\nAre the conclusions/recommendations clearly stated?\nIs the level of technicality appropriate for the intended audience?\nIs the student substantially over or under time?\nDoes the student appropriately pace their presentation?\nDoes the student deliver their message with clarity and volume?\nAre appropriate visualizations generated for the intended audience?\nAre visualizations necessary and useful for supporting conclusions/explaining findings?\n\nIn order to pass the project, students must earn a minimum score of 1 for each category.\n\nEarning below a 1 in one or more of the above categories would result in a failing project.\nWhile a minimum of 1 in each category is the required threshold for graduation, students should aim to earn at least an average of 1.5 across each category. An average score below 1.5, while it may be passing, means students may want to solicit specific feedback in order to significantly improve the project before showcasing it as part of a portfolio or the job search.\n\nREMEMBER:\nThis is a learning environment and you are encouraged to try new things, even if they don\'t work out as well as you planned! While this rubric outlines what we look for in a good project, it is up to you to go above and beyond to create a great project. Learn from your failures and you\'ll be prepared to succeed in the workforce.\n'], 'url_profile': 'https://github.com/meghazavar', 'info_list': ['Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'R', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'R', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': ['Multi Stage Heatmap Regression\nDetecting Facial Keypoints using a Multi-Stage Heatmap Regression Network.\nNOTE: The notebook might be easier to read on the Kaggle kernel here.\nIntroduction\nThis github repo stores my notebook in which I introduced a way of predicting facial keypoints via heatmap regression. I utilize a popular deep learning architecture called: Convolutional Pose Machines which essentially takes RGB images as input and outputs a collection of heatmaps.\nThe heatmaps can be viewed as a probability distribution of the likelihood that a keypoint resides at every pixel. This technique is very popular in computer vision applications such as human pose estimation.\n'], 'url_profile': 'https://github.com/Fmak95', 'info_list': ['Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'R', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'R', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '139 contributions\n        in the last year', 'description': ['Logistic_Regression_Projects\nTo Keep all logistic regression models and analysis\n'], 'url_profile': 'https://github.com/ArunRamji', 'info_list': ['Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'R', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'R', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '57 contributions\n        in the last year', 'description': ['Logistic-Regression\nAims to model, predict and visualize logistic regression in R.\nThis is done for diabetic data set in my case\n'], 'url_profile': 'https://github.com/bhargavi00', 'info_list': ['Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'R', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'R', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/shradha2103', 'info_list': ['Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'R', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'R', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '848 contributions\n        in the last year', 'description': ['Project 2: Predicting Housing Prices in Ames, Iowa.\nExecutive Summary\nIn this project, I explored housing data on individual properties sold in Ames, Iowa from 2006 to 2010 to build a model for predicting individual housing prices. I used feature engineering and linear regression models (multiple linear regression, lasso, and ridge) to build my model.\n\nProblem Statement\nMortgage rates are at an all time low, the lowest it\'s been since November 2016. That means, some individuals may be interested in refinancing their homes for a better mortgage rate. Banks may be interested in reaching these individuals to take a new loan with their bank. However, the process to refinance your home takes a long time. It requires an expert to come to your house and appraise its value before you know what mortgage rate you qualify for. This is an expensive and time-consuming process that people may not want to do.\nHow might a bank identify the targeted population who would qualify for lower mortgage rates, and outreach to those individuals?\n\nData\nProvided Data\nFor this project, I used the housing data from the Ames Assessor‚Äôs Office used to appraise the value for individual properties in 2006 to 2010. The data was split into training and testing data for the Kaggle competition. The data includes 81 features of each housing sale, including some ordinal (quality ratings), some nominal (classification of neighborhood, zone, sale type), and numeric (square feet, year built). The source for the Kaggle data is here.\n\nTraining Data\nKaggle Testing Data\n\nSubmission Data\nFor the Kaggle competition, I created multiple submissions to see how my models perform. Those can be found here.\nDescription of Data\n\n\n\nPredictive Variable\nData type\nDescription\n\n\n\n\noverall_qual\nScale of 1 to 10\nRates the overall material and finish of the house\n\n\nfull_bath\nint\nFull bathrooms above grade\n\n\nyear_built\nint\nOriginal construction date\n\n\ngr_liv_area_log\nlog of square feet\nLog of above grade (ground) living area square feet\n\n\nneighborhood_price_high\n0 or 1\nIs the neighborhood one standard deviation above the mean price per square feet of the total data set\n\n\nneighborhood_price_low\n0 or 1\nIs the neighborhood one standard deviation below the mean price per square feet of the total data set\n\n\nhas_pool\n0 or 1\nDoes the home have a pool\n\n\nms_zoning_FV\n0 or 1\nIs the property in a Floating Village Residential Zone\n\n\nms_zoning_RL\n0 or 1\nIs the property in a Residential Low Density Zone\n\n\nms_zoning_RM\n0 or 1\nResidential Medium Density\n\n\nexter_cond\nOrdinal 1 to 5\nEvaluates the present condition of the material on the exterior\n\n\nlot_area_log\nlog of square feet\nLot size\n\n\nbed_bath_ratio\nfloat\nRatio between number of bedrooms to number of bathrooms\n\n\ngarage_qual_cars\nfloat\nInteraction between the quality of the garage and the number of cars it fits\n\n\nbsmt_qual\nOrdinal 1 to 5\nEvaluates the height of the basement\n\n\nhas_remodel\n0 or 1\nWas the property recently remodeled\n\n\n\nData Cleaning\nThe data cleaning work was completed in python for the training data here. Then, I executed the cleaning code as a custom python function here to clean the training dataset in the exact same way.\nExploring the Data\nThe data exploratory work was completed in python here and in R here.\nTo get a baseline understanding of the data, I started with a series of exploratory graphs. To orient around the problem statement specifically, I wanted understand how each feature correlated to sale price.\nHere are the selected features that have an absolute correlation value above 0.5 to sale price.\n\nFrom this heatmap, we see that features with a strong correlation to sale price include overall quality (ordinal), number of bathrooms, kitchen quality (ordinal), price per square foot, year built, general living area (square feet), first floor area (square feet), and total rooms above grade.\nHow does neighborhood impact sale price?\nI wanted to see the impact of neighborhood on sale price, and find where the ""Beverly Hills"" like neighborhoods exist Ames. To do this, I calculated the average price per square foot for each neighborhood.\n\nThen I categorized each neighborhood as ""high,"" ""medium,"" or ""low.""\n\n\n\nCategory\nDescription\n\n\n\n\nHigh\nNeighborhoods with an average price per square foot one standard deviation above the population mean\n\n\nMedium\nNeighborhoods with an average price per square similar to the population mean\n\n\nLow\nNeighborhoods with an average price per square foot one standard deviation below the mean\n\n\n\nThere are three neighborhoods that classified as high value: GrnHill, NridgHt, StoneBr, and one neighborhood as low value: IDOTRR.\n\nSource of base map\nNext, I created distribution plots for the price per square foot by each neighborhood. I wanted to get a sense of how many properties were sold in each neighborhood.\n\nThe homes in high value neighborhoods did not have a lot of properties sold, compared to homes in the medium and low value neighborhoods.\n\nFeature Engineering\nI created some interaction features to capture the relationship between some features.\nRatio of Bedrooms to Bathrooms\nDivide the total number of full bedrooms by the total number of bathrooms (full bath + half bath). My assumption here is that houses with too many bathrooms over bedrooms may not be as valuable.\nGarage Quality to Number of Cars\nMultiply the overall garage quality (ordinal) to the number of cars that fit in the garage. My assumption here is that garages that fit more cars and have a higher quality would make the house more valuable.\n\nModels\n\n\n\nModel\nRMSE\nR2 Score Train Data\nR2 Score Test Data\n\n\n\n\nRidge\n19688.42\n0.91\n0.84\n\n\nLasso\n19702.55\n0.91\n0.84\n\n\nMultilinear Regression\n19814.38\n0.89\n0.91\n\n\n\nWhile I had a similar R2 score across my models, around 0.84-0.91, I had the least variance in my multilinear regression model where the R2 scores were closer between my training and data sets. Since Kaggle scores model performance on root mean squared error (RMSE), my ridge regression model performed best, making predictions within $1968.42. This means that on average, my predictions for housing prices are within $1968.42 of the actual housing price.\nHere is a graph of the predicted housing prices compared to actual housing prices for my ridge regression model.\n\nMy predictions are relatively close to the actual sale price. You can see that the higher the sale price the bigger the difference between my predictions and actual price.\nNext Steps\nTo make this model useful for banks, I would run this model on updated data. With updated predictions, we can see if a neighborhood has changed from a low or medium value to a high value neighborhood in the past 10 years. Those neighborhoods that have moved up in value would be of interest for banks to reach out to. Having a higher property value affects your mortgage rates, so if your home has increased in value since when you took out a loan, you may qualify for lower rates now.\nFrom there, we could build an online tool to help those in the targeted population group assess what kind of mortgage rates they may qualify for. This model predicted housing prices with just a handful of features. We can build an online tool that asks the homeowner questions about their home that they would know off the top of their head:\n\nZipcode\nSquare foot\nYear built\nGarage type\nNumber of bedrooms\nNumber of bathrooms\nNumber of cars that fit in your garage\nRemodel year\nOverall quality of your home (on a scale of 1 to 10)\n\nWith just a handful of questions, we could provide them an estimate of their home value now and what kind of mortgage rates they could qualify for at a specific bank. This lowers the barrier of entry for a homeowner to become interested in refinancing their home if they know that they have a good shot of having a lower mortgage rate.\n'], 'url_profile': 'https://github.com/veeps', 'info_list': ['Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'R', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'R', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'Charlotte, NC', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/BonaventureR', 'info_list': ['Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'R', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'R', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['Logistic-Regression\nLogistic Regression model to predict Heart Disease dataset using R.\n'], 'url_profile': 'https://github.com/nikkithags', 'info_list': ['Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'R', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'R', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '125 contributions\n        in the last year', 'description': ['DataScience_BootCamp_Udemy8\nDataScience Bootcamp by Jose Portilla on Udemy-Linear Regression\n'], 'url_profile': 'https://github.com/GauriSaran', 'info_list': ['Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'R', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'R', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '139 contributions\n        in the last year', 'description': ['Linear Regression Use cases\n#1. Housing price prediction using LR\n#2. Alchol vs Happiness\n'], 'url_profile': 'https://github.com/ArunRamji', 'info_list': ['Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated May 10, 2020', 'R', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'R', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated May 3, 2020']}"
"{'location': 'mumbai,maharastra', 'stats_list': [], 'contributions': '41 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/PATELVIMALV1', 'info_list': ['R', 'Updated Mar 2, 2020', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'HTML', 'Updated Mar 8, 2020', '1', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Updated Mar 2, 2020', 'Updated Mar 7, 2020', '1', 'Java', 'Updated Mar 22, 2020']}","{'location': 'Boston, MA', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/hgviss', 'info_list': ['R', 'Updated Mar 2, 2020', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'HTML', 'Updated Mar 8, 2020', '1', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Updated Mar 2, 2020', 'Updated Mar 7, 2020', '1', 'Java', 'Updated Mar 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '30 contributions\n        in the last year', 'description': ['LogisticRegression\n'], 'url_profile': 'https://github.com/amzkamble', 'info_list': ['R', 'Updated Mar 2, 2020', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'HTML', 'Updated Mar 8, 2020', '1', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Updated Mar 2, 2020', 'Updated Mar 7, 2020', '1', 'Java', 'Updated Mar 22, 2020']}","{'location': 'Boone, NC', 'stats_list': [], 'contributions': '48 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ongjk', 'info_list': ['R', 'Updated Mar 2, 2020', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'HTML', 'Updated Mar 8, 2020', '1', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Updated Mar 2, 2020', 'Updated Mar 7, 2020', '1', 'Java', 'Updated Mar 22, 2020']}","{'location': 'Hyderabad', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Saikri20', 'info_list': ['R', 'Updated Mar 2, 2020', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'HTML', 'Updated Mar 8, 2020', '1', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Updated Mar 2, 2020', 'Updated Mar 7, 2020', '1', 'Java', 'Updated Mar 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/git2ric', 'info_list': ['R', 'Updated Mar 2, 2020', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'HTML', 'Updated Mar 8, 2020', '1', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Updated Mar 2, 2020', 'Updated Mar 7, 2020', '1', 'Java', 'Updated Mar 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['LinearRegression\n'], 'url_profile': 'https://github.com/AnesCehic', 'info_list': ['R', 'Updated Mar 2, 2020', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'HTML', 'Updated Mar 8, 2020', '1', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Updated Mar 2, 2020', 'Updated Mar 7, 2020', '1', 'Java', 'Updated Mar 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/julianocorrea3', 'info_list': ['R', 'Updated Mar 2, 2020', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'HTML', 'Updated Mar 8, 2020', '1', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Updated Mar 2, 2020', 'Updated Mar 7, 2020', '1', 'Java', 'Updated Mar 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/S3R25', 'info_list': ['R', 'Updated Mar 2, 2020', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'HTML', 'Updated Mar 8, 2020', '1', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Updated Mar 2, 2020', 'Updated Mar 7, 2020', '1', 'Java', 'Updated Mar 22, 2020']}","{'location': 'Yekaterinburg, Russia', 'stats_list': [], 'contributions': '494 contributions\n        in the last year', 'description': ['SymbolicRegression\nAnnotation\nSymbolic regression is a way to find mathematical formula that satisfies given conditions,\nlike have specified values at given arguments, have derivative equal to specified function\n(i.e. formula will be equal to antiderivative of function)\nThis can be very useful when you have some data set like result of experiments and want to\nfind best formula to approximate those values and build new theory based on this formula\nDetails\nSymbolic regression generally uses genetic algorithm to create evolution of expressions.\nIn such scenario fitness function is presented by sum of squares of differences between results of expression\nand values from training set.\nCombinative variations presented by switching of subtrees of two trees.\nMutations are:\n\nchange of random operation\nfull replacement of random subtree\nremoval of some unary node in tree (this node is replaced by its child)\nnew root creation\nshuffle of subtrees of some node\nswitch between constant and variable (separate mutation as long as these are not ordinary operations\nreplacement of full tree\n\nImplementation\nTo incapsulate entity of expression/formula/function I use syntactic trees which have operation\nand list of subtrees which act as an operands for the operation\nSyntactic tree of expression works as a chromosome of expression and provides both combinative and mutative variability.\nAt each simulation step half of population that have highest sum of squared differences is\nremoved and every tree in other half is mutated and then combined pair by pair to restore population size.\nAfter population change the minimal deviation is evaluated. The tree that have this deviation is saved as minimal tree.\nIf after given maximum generation count there no expression that satisfies condition then minimal tree is returned.\nFeatures\nAlong with regression operators this project provides expression parsers which allow you store your expression in\ncompact form and restore their functionality at need.\n[NYFI] In order to make expression look better and evaluate faster you can use Optimizer which can find and evaluate\nconstant expressions and collapse expressions using rules of distributivity\n'], 'url_profile': 'https://github.com/FacelessLord', 'info_list': ['R', 'Updated Mar 2, 2020', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'HTML', 'Updated Mar 8, 2020', '1', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Updated Mar 2, 2020', 'Updated Mar 7, 2020', '1', 'Java', 'Updated Mar 22, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Decentralisedme', 'info_list': ['Jupyter Notebook', 'Updated Mar 4, 2020', 'Python', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Python', 'Updated Mar 3, 2020', 'Updated Mar 13, 2020', 'Java', 'Updated Mar 4, 2020', 'Python', 'Updated Mar 3, 2020', 'MATLAB', 'Updated Mar 6, 2020', '1', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020']}","{'location': 'E2R5', 'stats_list': [], 'contributions': '231 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/anroche', 'info_list': ['Jupyter Notebook', 'Updated Mar 4, 2020', 'Python', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Python', 'Updated Mar 3, 2020', 'Updated Mar 13, 2020', 'Java', 'Updated Mar 4, 2020', 'Python', 'Updated Mar 3, 2020', 'MATLAB', 'Updated Mar 6, 2020', '1', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020']}","{'location': 'Charlotte, NC', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/BonaventureR', 'info_list': ['Jupyter Notebook', 'Updated Mar 4, 2020', 'Python', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Python', 'Updated Mar 3, 2020', 'Updated Mar 13, 2020', 'Java', 'Updated Mar 4, 2020', 'Python', 'Updated Mar 3, 2020', 'MATLAB', 'Updated Mar 6, 2020', '1', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/GeorgeLiu617714', 'info_list': ['Jupyter Notebook', 'Updated Mar 4, 2020', 'Python', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Python', 'Updated Mar 3, 2020', 'Updated Mar 13, 2020', 'Java', 'Updated Mar 4, 2020', 'Python', 'Updated Mar 3, 2020', 'MATLAB', 'Updated Mar 6, 2020', '1', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020']}","{'location': 'London', 'stats_list': [], 'contributions': '47 contributions\n        in the last year', 'description': ['Logistic-regression\nS&P Stock Market Data\nDescription: Daily percentage returns for the S&P 500 stock index between 2001 and 2005.\nA data frame with 1250 observations on the following 9 variables:\nYear:\nThe year that the observation was recorded\nLag1:\nPercentage return for previous day\nLag2:\nPercentage return for 2 days previous\nLag3:\nPercentage return for 3 days previous\nLag4:\nPercentage return for 4 days previous\nLag5:\nPercentage return for 5 days previous\nVolume:\nVolume of shares traded (number of daily shares traded in billions)\nToday:\nPercentage return for today\nDirection:\nA factor with levels Down and Up indicating whether the market had a positive or negative return on a given day\nSource - Raw values of the S&P 500 were obtained from Yahoo Finance and then converted to percentages and lagged.\nReference - James, G., Witten, D., Hastie, T., and Tibshirani, R. (2013) An Introduction to Statistical Learning with applications in R, www.StatLearning.com, Springer-Verlag, New York\n'], 'url_profile': 'https://github.com/bokedara', 'info_list': ['Jupyter Notebook', 'Updated Mar 4, 2020', 'Python', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Python', 'Updated Mar 3, 2020', 'Updated Mar 13, 2020', 'Java', 'Updated Mar 4, 2020', 'Python', 'Updated Mar 3, 2020', 'MATLAB', 'Updated Mar 6, 2020', '1', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['regression-test\nÁÆóÊ≥ïÂõûÂΩíÊµãËØïÂπ≥Âè∞\n'], 'url_profile': 'https://github.com/dieson', 'info_list': ['Jupyter Notebook', 'Updated Mar 4, 2020', 'Python', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Python', 'Updated Mar 3, 2020', 'Updated Mar 13, 2020', 'Java', 'Updated Mar 4, 2020', 'Python', 'Updated Mar 3, 2020', 'MATLAB', 'Updated Mar 6, 2020', '1', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['Linear-Regression\n'], 'url_profile': 'https://github.com/YakariSaiPriya22', 'info_list': ['Jupyter Notebook', 'Updated Mar 4, 2020', 'Python', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Python', 'Updated Mar 3, 2020', 'Updated Mar 13, 2020', 'Java', 'Updated Mar 4, 2020', 'Python', 'Updated Mar 3, 2020', 'MATLAB', 'Updated Mar 6, 2020', '1', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['Logistic Regression\n\nlogistic evaluates the logistic regression model given an array of exemplars and the weight parameters\nlogisticNLP computes the negative log likelihood of the input data, and the gradient of the negative log likelihood\nlearnLogReg is a template for a gradient descent method, with line search, for minimizing the negative log likelihood of the data\n\n'], 'url_profile': 'https://github.com/jessicahwlau', 'info_list': ['Jupyter Notebook', 'Updated Mar 4, 2020', 'Python', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Python', 'Updated Mar 3, 2020', 'Updated Mar 13, 2020', 'Java', 'Updated Mar 4, 2020', 'Python', 'Updated Mar 3, 2020', 'MATLAB', 'Updated Mar 6, 2020', '1', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020']}","{'location': 'Tarragona, Spain', 'stats_list': [], 'contributions': '151 contributions\n        in the last year', 'description': ['torresxavier\n'], 'url_profile': 'https://github.com/torresxavier', 'info_list': ['Jupyter Notebook', 'Updated Mar 4, 2020', 'Python', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Python', 'Updated Mar 3, 2020', 'Updated Mar 13, 2020', 'Java', 'Updated Mar 4, 2020', 'Python', 'Updated Mar 3, 2020', 'MATLAB', 'Updated Mar 6, 2020', '1', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '27 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/vamsiteja142', 'info_list': ['Jupyter Notebook', 'Updated Mar 4, 2020', 'Python', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Python', 'Updated Mar 3, 2020', 'Updated Mar 13, 2020', 'Java', 'Updated Mar 4, 2020', 'Python', 'Updated Mar 3, 2020', 'MATLAB', 'Updated Mar 6, 2020', '1', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/GayaneKa', 'info_list': ['Jupyter Notebook', 'Updated Mar 12, 2020', 'C#', 'Updated Mar 4, 2020', 'Java', 'Updated Mar 4, 2020', 'Updated Mar 5, 2020', 'MATLAB', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 11, 2020', 'Python', 'Updated Apr 30, 2020']}","{'location': 'Saint-Petersburg', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['Linear\n'], 'url_profile': 'https://github.com/kalibrov520', 'info_list': ['Jupyter Notebook', 'Updated Mar 12, 2020', 'C#', 'Updated Mar 4, 2020', 'Java', 'Updated Mar 4, 2020', 'Updated Mar 5, 2020', 'MATLAB', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 11, 2020', 'Python', 'Updated Apr 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/tusharjoge', 'info_list': ['Jupyter Notebook', 'Updated Mar 12, 2020', 'C#', 'Updated Mar 4, 2020', 'Java', 'Updated Mar 4, 2020', 'Updated Mar 5, 2020', 'MATLAB', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 11, 2020', 'Python', 'Updated Apr 30, 2020']}","{'location': 'mumbai', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/devraj-patil', 'info_list': ['Jupyter Notebook', 'Updated Mar 12, 2020', 'C#', 'Updated Mar 4, 2020', 'Java', 'Updated Mar 4, 2020', 'Updated Mar 5, 2020', 'MATLAB', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 11, 2020', 'Python', 'Updated Apr 30, 2020']}","{'location': 'Saudi Arabia, Rabigh', 'stats_list': [], 'contributions': '273 contributions\n        in the last year', 'description': ['Linear Regression\nProject from Machine Learning course by Andrew Ng.\nThis Project is Divied Into Two Parts:\n\nLinear regression with one variable to predict profit of a food truck.\nLinear regression with multiable variables to predict prices of houses.\n\n'], 'url_profile': 'https://github.com/MohammedAljahdali', 'info_list': ['Jupyter Notebook', 'Updated Mar 12, 2020', 'C#', 'Updated Mar 4, 2020', 'Java', 'Updated Mar 4, 2020', 'Updated Mar 5, 2020', 'MATLAB', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 11, 2020', 'Python', 'Updated Apr 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '46 contributions\n        in the last year', 'description': ['Preview\nIn this lab, you‚Äôll use a linear regression machine learning algorithm to estimate a person‚Äôs medical insurance cost with his or her BMI.(Body Mass Index)\nGetting set up\nImport Libraries for Linear Regressions\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split \n\n\nPandas is a fast, powerful, flexible and easy to use open source data analysis tool, built within the Python programming language.\nNumpy is a library for the Python programming language, adding support for large, multi-dimensional arrays and matrices.\nMatplotlib is a plotting library for the Python programming language.\nImport these libraries as pd, np, and plt for simplification and efficiency.\nSklearn (Scikit-Learn) is a machine learning library for the Python programming language that features various classification, regression and clustering algorithms.\nLinearRegression and train_test_split is used for machine learning and splitting the raw data for training.\n\nImport Data File from Google Drive\n# Code to read csv file into Colaboratory:\n!pip install -U -q PyDrive\nfrom pydrive.auth import GoogleAuth\nfrom pydrive.drive import GoogleDrive\nfrom google.colab import auth\nfrom oauth2client.client import GoogleCredentials\'\n\n# Authenticate and create the PyDrive client.\nauth.authenticate_user()\ngauth = GoogleAuth()\ngauth.credentials = GoogleCredentials.get_application_default()\ndrive = GoogleDrive(gauth)\n\nGoogle Colab will ask you for an authorization of your google account. Click on the link and sign in with your google account. Copy and paste the verification code inside the box and press enter.\n\nlink = \'https://drive.google.com/open?id=1z3c7mVRAr-h0tdxlMp1EYxG5pU74Aybb\' # The shareable link\nfluff, id = link.split(\'=\')\n# Verify that you have everything after \'=\'\n\ndownloaded = drive.CreateFile({\'id\':id}) \ndownloaded.GetContentFile(\'insurance1.csv\')  \n\ndf3 = pd.read_csv(\'insurance1.csv\')\n# Dataset is now stored in a Pandas Dataframe\ndf3\n\nGoogle Colab will get the CSV data file from the link and read it in Pandas Dataframe. Now this data can be presented in a dataframe ""df3"".\nExpected Output:\n\nGenerate data-set\nx = df3.iloc[:, :-1].values\ny = df3.iloc[:, 1].values\n\nx variable is the 2D array from the first column until the second-last column. In this case, it is only the BMI.\ny variable is the 2D array of the prediction variable. In this case, it is the index 1 element of the data column.\nBoth variables use all the row data, as we need all the data to train the model.\n\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=1/3, random_state=0)\n\nSplit the x and y 2D arrays into training dataset and testing dataset.\nWe don\'t need testing dataset to create linear regression model\nPlot\ndef scatter():\n\n\tplt.scatter(X_test,y_test,s=1)\n\tplt.xlabel(\'BMI\')\n\tplt.ylabel(\'Financial Insurance Charge\')\n\tplt.title(\'Financial Insurance Charge with BMI\')\n\nScatter the original data point on the plot with a normal shape(s=1).\nLabel the x-axis as ""BMI"" and the y-axis as ""Financial Insurance Charge"".\nTitle the graph with ""Financial Insurance Charge with BMI"".\nRegression Model Building\nregression_model = LinearRegression()\n\nregression_model.fit(X_train, y_train)\ny_predicted = regression_model.predict(X_train)\n\nUsing regression_model as LinearRegression(), fit the model with x and y training data.\nDefine y_predicted(Predicted financial insurance charge) as a set of predictions of y variables from x training data.\nscatter()\nplt.plot(X_train, y_predicted, color=\'r\')\nplt.show()\n\nWe use the definition we created earlier, scatter(), to call a plot of original data points.\nUsing x training dataset and y prediction dataset, we can use ""plot"" to form a linear best fit line along the original datapoints using color red.\nPredict with customized BMI value\nprediction = float(input(""Enter your BMI: ""))\nuserpredict = regression_model.predict([[prediction]])\nprint(userpredict)\n\nAllow user to enter a BMI value to predict as a float.\nGenerate a new prediction point for the user with the corresponding BMI value and print the user-prediction.\nFinal Output\n\n'], 'url_profile': 'https://github.com/CCAGoogleColab', 'info_list': ['Jupyter Notebook', 'Updated Mar 12, 2020', 'C#', 'Updated Mar 4, 2020', 'Java', 'Updated Mar 4, 2020', 'Updated Mar 5, 2020', 'MATLAB', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 11, 2020', 'Python', 'Updated Apr 30, 2020']}","{'location': 'NIT MANIPUR', 'stats_list': [], 'contributions': '201 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sg140299', 'info_list': ['Jupyter Notebook', 'Updated Mar 12, 2020', 'C#', 'Updated Mar 4, 2020', 'Java', 'Updated Mar 4, 2020', 'Updated Mar 5, 2020', 'MATLAB', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 11, 2020', 'Python', 'Updated Apr 30, 2020']}","{'location': 'Coimbatore', 'stats_list': [], 'contributions': '142 contributions\n        in the last year', 'description': ['logistic-regression\n'], 'url_profile': 'https://github.com/Saroopashree', 'info_list': ['Jupyter Notebook', 'Updated Mar 12, 2020', 'C#', 'Updated Mar 4, 2020', 'Java', 'Updated Mar 4, 2020', 'Updated Mar 5, 2020', 'MATLAB', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 11, 2020', 'Python', 'Updated Apr 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['LinearRegression\nLinear Regerssion using gradient descent learning algorithm .\n'], 'url_profile': 'https://github.com/KaveriKR', 'info_list': ['Jupyter Notebook', 'Updated Mar 12, 2020', 'C#', 'Updated Mar 4, 2020', 'Java', 'Updated Mar 4, 2020', 'Updated Mar 5, 2020', 'MATLAB', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 11, 2020', 'Python', 'Updated Apr 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '56 contributions\n        in the last year', 'description': ['Text-Regression\nImplemented in Python 3.6.8 and Tensorflow Keras.\nModel is saved in .h5 format.\nThe label chosen for this project is ""continuous_target_1""\nThe trained model implements Bi-LSTM architecture.\nHow to run the code:\n\nCreate virtual environment\n\nvirtualenv [name_of_virtual_env]\n\nExample:\nvirtualenv venv\n\nActivate the virtual environment\nsource venv/bin/activate\n\n\nInstall the required packages\n\npip3 install -r requirements.txt\n\n\nRun Exploratory Data Analysis to summarize the data\n\npython3 EDA.py -c [config_file] -dir [directory_to_save_EDA_results] -i [which file from JSON config file to do EDA. must be either ""train_file"" or ""eval_file""]\n\nExample:\npython3 EDA.py -c config.json -dir EDA -i eval_file\n\nRun the training/evaluation/prediction:\n4. Set the required inputs in configuration JSON file.\n{\n    ""maxlen"" : [maximum sequence length. Use 72 for the pretrained model],\n    ""model_type"" : [model type. Use ""BILSTM"" for the pretrained model. The current available options are: ""BILSTM"" and ""CNN"".],\n    ""batch_size"" : [batch size for training. 128 for the pretrained model.],\n    ""epochs"" : [number of epochs],\n    ""word_embedding"": [word embedding file in txt format],\n    ""max_number_of_examples_per_class"": [maximum number of examples for each class for training purpose. This is to handle imbalanced data. Write ""None"" if you don\'t want to limit your data],\n    ""stopwords_file"" : [stopwords file in txt format],\n    ""train_file"" : [train file in csv format],\n    ""eval_file"" : [evaluation data in csv format],\n    ""predict_file"" : [data to predict in csv format],\n    ""input_column"" : [column name in data csv file that contains the text],\n    ""target_column"" : [column name in data csv file that contains the label],\n    ""model_dir"" : [directory where the model will be saved],\n    ""do_train"" : [write ""True"" if you want to do training. ],\n    ""do_eval"" : [write ""True"" if you want to do evaluation on additional dataset.""],\n    ""do_predict"" : [write ""True"" if you want to do prediction.]\n}\n\nFollow this configuration to do evaluation on the eval file using the pretrained model\n{\n    ""maxlen"" : 72,\n    ""model_type"" : ""BILSTM"",\n    ""batch_size"" : 128,\n    ""epochs"" : 30,\n    ""word_embedding"": ""./data/glove.6B.300d.txt"",\n    ""max_number_of_examples_per_class"": ""None"",\n    ""stopwords_file"" : ""./data/stopwords.txt"",\n    ""train_file"" : ""None"",\n    ""eval_file"" : [filename],\n    ""predict_file"" : ""None"",\n    ""input_column"" : ""features"",\n    ""target_column"" : ""continuous_target_1"",\n    ""model_dir"" : ""./model"",\n    ""do_train"" : ""False"",\n    ""do_eval"" : ""True"",\n    ""do_predict"" : ""False""\n}\n\nOther examples:\nExample to do training:\n{\n    ""maxlen"" : 72,\n    ""model_type"" : ""BILSTM"",\n    ""batch_size"" : 64,\n    ""epochs"" : 30,\n    ""word_embedding"": ""./data/glove.6B.300d.txt"",\n    ""max_number_of_examples_per_class"": ""None"",\n    ""stopwords_file"" : ""./data/stopwords.txt"",\n    ""train_file"" : ""./data/train.csv"",\n    ""eval_file"" : ""./data/eval.csv"",\n    ""predict_file"" : ""./data/prediction.csv"",\n    ""input_column"" : ""features"",\n    ""target_column"" : ""continuous_target_1"",\n    ""model_dir"" : ""./model"",\n    ""do_train"" : ""True"",\n    ""do_eval"" : ""True,\n    ""do_predict"" : ""True""\n}\n\nExample to do prediction only:\n{\n    ""maxlen"" : 72,\n    ""model_type"" : ""BILSTM"",\n    ""batch_size"" : 64,\n    ""epochs"" : 30,\n    ""word_embedding"": ""./data/glove.6B.300d.txt"",\n    ""max_number_of_examples_per_class"": ""None"",\n    ""stopwords_file"" : ""./data/stopwords.txt"",\n    ""train_file"" : ""None"",\n    ""eval_file"" : ""None"",\n    ""predict_file"" : ""./data/prediction.csv"",\n    ""input_column"" : ""features"",\n    ""target_column"" : ""continuous_target_1"",\n    ""model_dir"" : ""./model"",\n    ""do_train"" : ""False"",\n    ""do_eval"" : ""False"",\n    ""do_predict"" : ""True""\n}\n\nExtra Notes: \n1. The script will automatically split the data to train and validation set (stratified 20%) for training. \n2. If do_train is True, training will be performed and there will be evaluation done on test data which is obtained from stratified 20% of training data (train_file). This evaluation result will be saved in Evaluation_Report_on_Training.txt. \n3. If do_eval is also True, there will be another evaluation done on evaluation data (eval_file). In order to do this, provide the evaluation dataset in eval_file. Evaluation result will be saved in Evaluation_Report_on_Eval_Data.txt and Evaluation_Results.csv. If you don\'t have additional data to do evaluation, just set this flag to False. \n4. If you want to do evaluation without training, set do_train to False and do_eval to True. Provide the evaluation dataset in eval_file. \n5. If you want to do prediction, set do_predict to True. Provide the file to predict in predict_file. Result will be saved in Prediction_Results.csv. \n\nRun the main script\n\npython3 main.py -c [config_file]\n\nExample:\npython3 main.py -c config.json\n\nAdditional Notes:\n\nTensorboard events file that show training and eval loss, MAE, and MSE are saved in model directory. \nTo load: \n\ntensorboard --logdir=[events_file_directory]\n\n\nUncertainty measurement is implemented by setting Dropout layer Training to True in model architecture. The uncertainty measurement will be done on evaluation dataset when ""do_eval"" is set to ""True"". The result will be shown as Mean and Standard Deviation for each prediction. The uncertainty is measured 50 different predictions on each example. \n\nCitation \nThis model uncertainty measurement implementation is based on these blogs: \nhttps://www.depends-on-the-definition.com/model-uncertainty-in-deep-learning-with-monte-carlo-dropout/ \nhttps://medium.com/comet-ml/estimating-uncertainty-in-machine-learning-models-part-2-8711c832cc15\n'], 'url_profile': 'https://github.com/imayachita', 'info_list': ['Jupyter Notebook', 'Updated Mar 12, 2020', 'C#', 'Updated Mar 4, 2020', 'Java', 'Updated Mar 4, 2020', 'Updated Mar 5, 2020', 'MATLAB', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 11, 2020', 'Python', 'Updated Apr 30, 2020']}"
"{'location': 'Algiers, Algeria.', 'stats_list': [], 'contributions': '63 contributions\n        in the last year', 'description': ['logistic-regression\n'], 'url_profile': 'https://github.com/hanibounoua', 'info_list': ['Updated May 9, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2021', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'C#', 'Updated Mar 4, 2020', 'Java', 'Updated Mar 4, 2020', 'Updated Mar 5, 2020', 'MATLAB', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Updated Mar 6, 2020', '1', 'HTML', 'Updated Mar 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '375 contributions\n        in the last year', 'description': ['Linear_Regression\n'], 'url_profile': 'https://github.com/anahern4ndez', 'info_list': ['Updated May 9, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2021', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'C#', 'Updated Mar 4, 2020', 'Java', 'Updated Mar 4, 2020', 'Updated Mar 5, 2020', 'MATLAB', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Updated Mar 6, 2020', '1', 'HTML', 'Updated Mar 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/GayaneKa', 'info_list': ['Updated May 9, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2021', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'C#', 'Updated Mar 4, 2020', 'Java', 'Updated Mar 4, 2020', 'Updated Mar 5, 2020', 'MATLAB', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Updated Mar 6, 2020', '1', 'HTML', 'Updated Mar 8, 2020']}","{'location': 'Saint-Petersburg', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['Linear\n'], 'url_profile': 'https://github.com/kalibrov520', 'info_list': ['Updated May 9, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2021', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'C#', 'Updated Mar 4, 2020', 'Java', 'Updated Mar 4, 2020', 'Updated Mar 5, 2020', 'MATLAB', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Updated Mar 6, 2020', '1', 'HTML', 'Updated Mar 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/tusharjoge', 'info_list': ['Updated May 9, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2021', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'C#', 'Updated Mar 4, 2020', 'Java', 'Updated Mar 4, 2020', 'Updated Mar 5, 2020', 'MATLAB', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Updated Mar 6, 2020', '1', 'HTML', 'Updated Mar 8, 2020']}","{'location': 'mumbai', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/devraj-patil', 'info_list': ['Updated May 9, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2021', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'C#', 'Updated Mar 4, 2020', 'Java', 'Updated Mar 4, 2020', 'Updated Mar 5, 2020', 'MATLAB', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Updated Mar 6, 2020', '1', 'HTML', 'Updated Mar 8, 2020']}","{'location': 'Saudi Arabia, Rabigh', 'stats_list': [], 'contributions': '273 contributions\n        in the last year', 'description': ['Linear Regression\nProject from Machine Learning course by Andrew Ng.\nThis Project is Divied Into Two Parts:\n\nLinear regression with one variable to predict profit of a food truck.\nLinear regression with multiable variables to predict prices of houses.\n\n'], 'url_profile': 'https://github.com/MohammedAljahdali', 'info_list': ['Updated May 9, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2021', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'C#', 'Updated Mar 4, 2020', 'Java', 'Updated Mar 4, 2020', 'Updated Mar 5, 2020', 'MATLAB', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Updated Mar 6, 2020', '1', 'HTML', 'Updated Mar 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '46 contributions\n        in the last year', 'description': ['Preview\nIn this lab, you‚Äôll use a linear regression machine learning algorithm to estimate a person‚Äôs medical insurance cost with his or her BMI.(Body Mass Index)\nGetting set up\nImport Libraries for Linear Regressions\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split \n\n\nPandas is a fast, powerful, flexible and easy to use open source data analysis tool, built within the Python programming language.\nNumpy is a library for the Python programming language, adding support for large, multi-dimensional arrays and matrices.\nMatplotlib is a plotting library for the Python programming language.\nImport these libraries as pd, np, and plt for simplification and efficiency.\nSklearn (Scikit-Learn) is a machine learning library for the Python programming language that features various classification, regression and clustering algorithms.\nLinearRegression and train_test_split is used for machine learning and splitting the raw data for training.\n\nImport Data File from Google Drive\n# Code to read csv file into Colaboratory:\n!pip install -U -q PyDrive\nfrom pydrive.auth import GoogleAuth\nfrom pydrive.drive import GoogleDrive\nfrom google.colab import auth\nfrom oauth2client.client import GoogleCredentials\'\n\n# Authenticate and create the PyDrive client.\nauth.authenticate_user()\ngauth = GoogleAuth()\ngauth.credentials = GoogleCredentials.get_application_default()\ndrive = GoogleDrive(gauth)\n\nGoogle Colab will ask you for an authorization of your google account. Click on the link and sign in with your google account. Copy and paste the verification code inside the box and press enter.\n\nlink = \'https://drive.google.com/open?id=1z3c7mVRAr-h0tdxlMp1EYxG5pU74Aybb\' # The shareable link\nfluff, id = link.split(\'=\')\n# Verify that you have everything after \'=\'\n\ndownloaded = drive.CreateFile({\'id\':id}) \ndownloaded.GetContentFile(\'insurance1.csv\')  \n\ndf3 = pd.read_csv(\'insurance1.csv\')\n# Dataset is now stored in a Pandas Dataframe\ndf3\n\nGoogle Colab will get the CSV data file from the link and read it in Pandas Dataframe. Now this data can be presented in a dataframe ""df3"".\nExpected Output:\n\nGenerate data-set\nx = df3.iloc[:, :-1].values\ny = df3.iloc[:, 1].values\n\nx variable is the 2D array from the first column until the second-last column. In this case, it is only the BMI.\ny variable is the 2D array of the prediction variable. In this case, it is the index 1 element of the data column.\nBoth variables use all the row data, as we need all the data to train the model.\n\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=1/3, random_state=0)\n\nSplit the x and y 2D arrays into training dataset and testing dataset.\nWe don\'t need testing dataset to create linear regression model\nPlot\ndef scatter():\n\n\tplt.scatter(X_test,y_test,s=1)\n\tplt.xlabel(\'BMI\')\n\tplt.ylabel(\'Financial Insurance Charge\')\n\tplt.title(\'Financial Insurance Charge with BMI\')\n\nScatter the original data point on the plot with a normal shape(s=1).\nLabel the x-axis as ""BMI"" and the y-axis as ""Financial Insurance Charge"".\nTitle the graph with ""Financial Insurance Charge with BMI"".\nRegression Model Building\nregression_model = LinearRegression()\n\nregression_model.fit(X_train, y_train)\ny_predicted = regression_model.predict(X_train)\n\nUsing regression_model as LinearRegression(), fit the model with x and y training data.\nDefine y_predicted(Predicted financial insurance charge) as a set of predictions of y variables from x training data.\nscatter()\nplt.plot(X_train, y_predicted, color=\'r\')\nplt.show()\n\nWe use the definition we created earlier, scatter(), to call a plot of original data points.\nUsing x training dataset and y prediction dataset, we can use ""plot"" to form a linear best fit line along the original datapoints using color red.\nPredict with customized BMI value\nprediction = float(input(""Enter your BMI: ""))\nuserpredict = regression_model.predict([[prediction]])\nprint(userpredict)\n\nAllow user to enter a BMI value to predict as a float.\nGenerate a new prediction point for the user with the corresponding BMI value and print the user-prediction.\nFinal Output\n\n'], 'url_profile': 'https://github.com/CCAGoogleColab', 'info_list': ['Updated May 9, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2021', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'C#', 'Updated Mar 4, 2020', 'Java', 'Updated Mar 4, 2020', 'Updated Mar 5, 2020', 'MATLAB', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Updated Mar 6, 2020', '1', 'HTML', 'Updated Mar 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['regression_project\nThe study about cancer incidence rate in US.\nWe were hired by American Cancer Society (https://www.cancer.org/) to write a white paper exploring the factors related to cancer deaths in the US. They will use your analysis to identify regions (and associated partners) for cancer interventions across the US. The dataset provided includes information about all zipcodes in the United States.\n'], 'url_profile': 'https://github.com/Vcavalcanti1975', 'info_list': ['Updated May 9, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2021', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'C#', 'Updated Mar 4, 2020', 'Java', 'Updated Mar 4, 2020', 'Updated Mar 5, 2020', 'MATLAB', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Updated Mar 6, 2020', '1', 'HTML', 'Updated Mar 8, 2020']}","{'location': 'Banglore', 'stats_list': [], 'contributions': '71 contributions\n        in the last year', 'description': ['Logistic_regression\nContext\n""Predict behavior to retain customers. You can analyze all relevant customer data and develop focused customer retention programs."" [IBM Sample Data Sets]\nContent\nEach row represents a customer, each column contains customer‚Äôs attributes described on the column Metadata.\nThe data set includes information about:\nCustomers who left within the last month ‚Äì the column is called Churn Services that each customer has signed up for ‚Äì phone, multiple lines, internet, online security, online backup, device protection, tech support, and streaming TV and movies Customer account information ‚Äì how long they‚Äôve been a customer, contract, payment method, paperless billing, monthly charges, and total charges Demographic info about customers ‚Äì gender, age range, and if they have partners and dependents\n'], 'url_profile': 'https://github.com/chandana124', 'info_list': ['Updated May 9, 2020', 'Jupyter Notebook', 'Updated Feb 2, 2021', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'C#', 'Updated Mar 4, 2020', 'Java', 'Updated Mar 4, 2020', 'Updated Mar 5, 2020', 'MATLAB', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Updated Mar 6, 2020', '1', 'HTML', 'Updated Mar 8, 2020']}"
"{'location': 'Cardiff, United Kingdom', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['Logistic-Regression\n'], 'url_profile': 'https://github.com/milon101', 'info_list': ['Jupyter Notebook', 'Updated Mar 6, 2020', 'Python', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'R', 'Updated Mar 5, 2020', 'R', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '252 contributions\n        in the last year', 'description': ['linear_regression\nusing linear regression to fit points.\n'], 'url_profile': 'https://github.com/DistanceJiang', 'info_list': ['Jupyter Notebook', 'Updated Mar 6, 2020', 'Python', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'R', 'Updated Mar 5, 2020', 'R', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '349 contributions\n        in the last year', 'description': ['Regression_Night\n'], 'url_profile': 'https://github.com/Atsuhiko', 'info_list': ['Jupyter Notebook', 'Updated Mar 6, 2020', 'Python', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'R', 'Updated Mar 5, 2020', 'R', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['Panky\n'], 'url_profile': 'https://github.com/Pankajbora2010', 'info_list': ['Jupyter Notebook', 'Updated Mar 6, 2020', 'Python', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'R', 'Updated Mar 5, 2020', 'R', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['logistic-regression\n'], 'url_profile': 'https://github.com/Varsha-parth', 'info_list': ['Jupyter Notebook', 'Updated Mar 6, 2020', 'Python', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'R', 'Updated Mar 5, 2020', 'R', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 4, 2020']}","{'location': 'Teresina, Piau√≠', 'stats_list': [], 'contributions': '58 contributions\n        in the last year', 'description': ['linear_regression\nReposit√≥rio com prop√≥sitos de estudo sobre regress√£o linear.\n'], 'url_profile': 'https://github.com/magnoazneto', 'info_list': ['Jupyter Notebook', 'Updated Mar 6, 2020', 'Python', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'R', 'Updated Mar 5, 2020', 'R', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/viggy0', 'info_list': ['Jupyter Notebook', 'Updated Mar 6, 2020', 'Python', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'R', 'Updated Mar 5, 2020', 'R', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 4, 2020']}","{'location': 'Edmond ', 'stats_list': [], 'contributions': '78 contributions\n        in the last year', 'description': ['machine learning\nImplementing Logistic regression\nusing numpy,pandas and matplotlib\n'], 'url_profile': 'https://github.com/kowshikgunda71', 'info_list': ['Jupyter Notebook', 'Updated Mar 6, 2020', 'Python', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'R', 'Updated Mar 5, 2020', 'R', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/RaviRyuk', 'info_list': ['Jupyter Notebook', 'Updated Mar 6, 2020', 'Python', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'R', 'Updated Mar 5, 2020', 'R', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 4, 2020']}","{'location': 'Seoul', 'stats_list': [], 'contributions': '390 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/HowardHowonYu', 'info_list': ['Jupyter Notebook', 'Updated Mar 6, 2020', 'Python', 'Updated Apr 1, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'R', 'Updated Mar 5, 2020', 'R', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 4, 2020']}"
"{'location': 'Saudi Arabia, Rabigh', 'stats_list': [], 'contributions': '273 contributions\n        in the last year', 'description': ['Logistic Regression\nProject from Machine Learning course by Andrew Ng.\n\nThis model predict whether a student will get admitted to a university or not.\n\n'], 'url_profile': 'https://github.com/MohammedAljahdali', 'info_list': ['MATLAB', 'Updated Mar 8, 2020', '1', 'Jupyter Notebook', 'Updated May 13, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Python', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'HTML', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 4, 2020']}","{'location': 'Gr. NOIDA', 'stats_list': [], 'contributions': '553 contributions\n        in the last year', 'description': ['Linear-Regression\n'], 'url_profile': 'https://github.com/prakharR534', 'info_list': ['MATLAB', 'Updated Mar 8, 2020', '1', 'Jupyter Notebook', 'Updated May 13, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Python', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'HTML', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '39 contributions\n        in the last year', 'description': ['Linear-Regression\n'], 'url_profile': 'https://github.com/ketank77', 'info_list': ['MATLAB', 'Updated Mar 8, 2020', '1', 'Jupyter Notebook', 'Updated May 13, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Python', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'HTML', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['LogisticRegression\n'], 'url_profile': 'https://github.com/Ryo-tech-del', 'info_list': ['MATLAB', 'Updated Mar 8, 2020', '1', 'Jupyter Notebook', 'Updated May 13, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Python', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'HTML', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 4, 2020']}","{'location': 'Lagos-State, Nigeria', 'stats_list': [], 'contributions': '43 contributions\n        in the last year', 'description': ['""# Linear-Regression""\nIn statistics, linear regression is a linear approach to modeling the relationship between a scalar response (or dependent variable) and one or more explanatory variables (or independent variables). The case of one explanatory variable is called simple linear regression. For more than one explanatory variable, the process is called multiple linear regression.[1] This term is distinct from multivariate linear regression, where multiple correlated dependent variables are predicted, rather than a single scalar variable.\nIn linear regression, the relationships are modeled using linear predictor functions whose unknown model parameters are estimated from the data. Such models are called linear models\nMost commonly, the conditional mean of the response given the values of the explanatory variables (or predictors) is assumed to be an affine function of those values; less commonly, the conditional median or some other quantile is used. Like all forms of regression analysis, linear regression focuses on the conditional probability distribution of the response given the values of the predictors, rather than on the joint probability distribution of all of these variables, which is the domain of multivariate analysis.\n'], 'url_profile': 'https://github.com/afodamz', 'info_list': ['MATLAB', 'Updated Mar 8, 2020', '1', 'Jupyter Notebook', 'Updated May 13, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Python', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'HTML', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '395 contributions\n        in the last year', 'description': ['Polynomial_Regression\n'], 'url_profile': 'https://github.com/Santosh-Sah', 'info_list': ['MATLAB', 'Updated Mar 8, 2020', '1', 'Jupyter Notebook', 'Updated May 13, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Python', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'HTML', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 4, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '131 contributions\n        in the last year', 'description': ['linear-regression\n'], 'url_profile': 'https://github.com/preranadeshpande', 'info_list': ['MATLAB', 'Updated Mar 8, 2020', '1', 'Jupyter Notebook', 'Updated May 13, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Python', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'HTML', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 4, 2020']}","{'location': 'Krakow', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['logistic_regression\n'], 'url_profile': 'https://github.com/PaulinaAwr', 'info_list': ['MATLAB', 'Updated Mar 8, 2020', '1', 'Jupyter Notebook', 'Updated May 13, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Python', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'HTML', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['KNN-Regression\n'], 'url_profile': 'https://github.com/Tula2609', 'info_list': ['MATLAB', 'Updated Mar 8, 2020', '1', 'Jupyter Notebook', 'Updated May 13, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Python', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'HTML', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['PRODUCTION READY LASSO REGRESSION MODEL FOR PREDICTING HOUSE PRICES.\n'], 'url_profile': 'https://github.com/polumati-datamaverick', 'info_list': ['MATLAB', 'Updated Mar 8, 2020', '1', 'Jupyter Notebook', 'Updated May 13, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Python', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 15, 2020', 'HTML', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 4, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['LinearRegression\n'], 'url_profile': 'https://github.com/19PA1A0488', 'info_list': ['Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated May 16, 2020', 'HTML', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 4, 2020', 'Python', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'R', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Mar 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '58 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/alyabdulfatah', 'info_list': ['Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated May 16, 2020', 'HTML', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 4, 2020', 'Python', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'R', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Mar 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['KNN-Regression\n'], 'url_profile': 'https://github.com/Tula2609', 'info_list': ['Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated May 16, 2020', 'HTML', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 4, 2020', 'Python', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'R', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Mar 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['PRODUCTION READY LASSO REGRESSION MODEL FOR PREDICTING HOUSE PRICES.\n'], 'url_profile': 'https://github.com/polumati-datamaverick', 'info_list': ['Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated May 16, 2020', 'HTML', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 4, 2020', 'Python', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'R', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Mar 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '457 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/bsingh17', 'info_list': ['Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated May 16, 2020', 'HTML', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 4, 2020', 'Python', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'R', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Mar 7, 2020']}","{'location': 'Delhi, India', 'stats_list': [], 'contributions': '61 contributions\n        in the last year', 'description': ['Linear-Regression\n'], 'url_profile': 'https://github.com/Swati-90', 'info_list': ['Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated May 16, 2020', 'HTML', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 4, 2020', 'Python', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'R', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Mar 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/DwarakaAishwarya', 'info_list': ['Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated May 16, 2020', 'HTML', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 4, 2020', 'Python', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'R', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Mar 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '37 contributions\n        in the last year', 'description': ['logistic_regression\n'], 'url_profile': 'https://github.com/sgvvannabe', 'info_list': ['Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated May 16, 2020', 'HTML', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 4, 2020', 'Python', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'R', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Mar 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '50 contributions\n        in the last year', 'description': ['Linear-Regression\n'], 'url_profile': 'https://github.com/anishdulal', 'info_list': ['Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated May 16, 2020', 'HTML', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 4, 2020', 'Python', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'R', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Mar 7, 2020']}","{'location': 'San Jose, CA', 'stats_list': [], 'contributions': '92 contributions\n        in the last year', 'description': ['Annual Car Sales Prediction\nThis is a project that demonstrates the use of Machine Learning Linear Regression and Time Series to predict the total annual sales of cars from macroeconomic factors, while predicting the MSRP of a car based on its features, make, and model. This project is also aimed to demonstrate basic webscraping skills which i have used to scrape the most sold cars for the years.\nConclusions indicate that the unemployment rate as one of the major factors in the total annual sales of the cars. Also, it was found that during a recession, when the unemployment rate was at a peak, the car sales dropped and the top cars sold during a recession were the ones that were less expensive and gave the high gas mileage (i.e., high miles per gallon). During a good year when the unemployment rate was at its low, people still gravitated towards purchase the cars that were less expensive, while they were less concerned about the gas mileage of the car.\nTemplate Organization\n---------------------\n.\n‚îú‚îÄ‚îÄ README.md\n‚îî‚îÄ‚îÄ template_project\n    ‚îú‚îÄ‚îÄ LICENSE.txt\n    ‚îú‚îÄ‚îÄ README.txt\n    ‚îú‚îÄ‚îÄ code\n    ‚îÇ\xa0\xa0 ‚îú‚îÄ‚îÄ __init__.py\n    ‚îÇ\xa0\xa0 ‚îî‚îÄ‚îÄ hello_script.py\n    ‚îú‚îÄ‚îÄ data\n    ‚îÇ\xa0\xa0 ‚îú‚îÄ‚îÄ external\n    ‚îÇ\xa0\xa0 ‚îú‚îÄ‚îÄ interim\n    ‚îÇ\xa0\xa0 ‚îú‚îÄ‚îÄ processed\n    ‚îÇ\xa0\xa0 ‚îî‚îÄ‚îÄ raw\n    ‚îÇ\xa0\xa0     ‚îî‚îÄ‚îÄ example_data.txt\n    ‚îú‚îÄ‚îÄ docs\n    ‚îú‚îÄ‚îÄ example_notebook.ipynb\n    ‚îú‚îÄ‚îÄ references\n    ‚îî‚îÄ‚îÄ reports\n        ‚îî‚îÄ‚îÄ figures\n\nEnjoy!\nSamy Palaniappan\n'], 'url_profile': 'https://github.com/SamyPal', 'info_list': ['Jupyter Notebook', 'Updated Apr 4, 2020', 'Jupyter Notebook', 'Updated May 16, 2020', 'HTML', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 4, 2020', 'Python', 'Updated Apr 15, 2020', '1', 'Jupyter Notebook', 'Updated Jun 9, 2020', 'R', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Mar 7, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '40 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mhanuz', 'info_list': ['Jupyter Notebook', 'Updated Mar 1, 2021', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'HTML', 'Updated Mar 9, 2020', 'Python', 'Updated Mar 7, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Mar 8, 2020', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Python', 'Updated Mar 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '87 contributions\n        in the last year', 'description': ['Linear-Regression-Project\nUsing Linear Regression I predict the students success rate base on a number of variables\n'], 'url_profile': 'https://github.com/TyllerDaniel', 'info_list': ['Jupyter Notebook', 'Updated Mar 1, 2021', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'HTML', 'Updated Mar 9, 2020', 'Python', 'Updated Mar 7, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Mar 8, 2020', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Python', 'Updated Mar 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kamaleshsonu', 'info_list': ['Jupyter Notebook', 'Updated Mar 1, 2021', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'HTML', 'Updated Mar 9, 2020', 'Python', 'Updated Mar 7, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Mar 8, 2020', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Python', 'Updated Mar 4, 2020']}","{'location': 'Noida', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Manish2006', 'info_list': ['Jupyter Notebook', 'Updated Mar 1, 2021', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'HTML', 'Updated Mar 9, 2020', 'Python', 'Updated Mar 7, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Mar 8, 2020', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Python', 'Updated Mar 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': [""msds-regression-multivariate-analysis\nAbout\nThis repo contains Julia Rodd's assignments from the Northwestern University MSDS 410 Regression and Multivariate Analysis class. This class is the first modeling class within the program and was taught using R.\nFile structure\n\ncode contains all class code\ndata contains all relevant class data\noutput contains PDF summaries of each assignment\n\nAssignments\n\nAssignments 1-3,5: utilize the Ames housing data for comprehensive EDA and building/comparing various regression models to predict housing prices.\nAssignment 6: serves as an introduction to PCA using stock price data.\nAssignment 7: serves as an introduction to factor analysis using the study from 'A Factor Analysis of Liquor Preference' by Stoetzel (Journal of Marketing Research).\nAssignnment 8: serves as an introduction to cluster analysis using employment data from European countries.\n\nContributors\n\nJulia Rodd\n\n""], 'url_profile': 'https://github.com/juliarodd', 'info_list': ['Jupyter Notebook', 'Updated Mar 1, 2021', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'HTML', 'Updated Mar 9, 2020', 'Python', 'Updated Mar 7, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Mar 8, 2020', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Python', 'Updated Mar 4, 2020']}","{'location': 'Handong Global University,Pohang, Korea', 'stats_list': [], 'contributions': '146 contributions\n        in the last year', 'description': ['robo_RNN_regression\nrnn regresion\n'], 'url_profile': 'https://github.com/dlwlgus53', 'info_list': ['Jupyter Notebook', 'Updated Mar 1, 2021', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'HTML', 'Updated Mar 9, 2020', 'Python', 'Updated Mar 7, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Mar 8, 2020', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Python', 'Updated Mar 4, 2020']}","{'location': 'Chennai, India', 'stats_list': [], 'contributions': '75 contributions\n        in the last year', 'description': [""Sine-Regression\nMy first TF code without referring external sources\nCreated x and y, where y is sin(x)\nAdded noise to both x and y\nDefined variables, placeholders, model, error_cost, optimizer\nSince I have a low config system, I'd defined the variables slightly near the real values\nRun Session and plotted the prediction with original Data\n""], 'url_profile': 'https://github.com/ganeshramg', 'info_list': ['Jupyter Notebook', 'Updated Mar 1, 2021', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'HTML', 'Updated Mar 9, 2020', 'Python', 'Updated Mar 7, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Mar 8, 2020', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Python', 'Updated Mar 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '51 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/cwaldro5', 'info_list': ['Jupyter Notebook', 'Updated Mar 1, 2021', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'HTML', 'Updated Mar 9, 2020', 'Python', 'Updated Mar 7, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Mar 8, 2020', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Python', 'Updated Mar 4, 2020']}","{'location': 'Nairobi, Kenya', 'stats_list': [], 'contributions': '1,076 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/veldakarimi', 'info_list': ['Jupyter Notebook', 'Updated Mar 1, 2021', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'HTML', 'Updated Mar 9, 2020', 'Python', 'Updated Mar 7, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Mar 8, 2020', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Python', 'Updated Mar 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '72 contributions\n        in the last year', 'description': ['integrated_gradients_regression\nWritten Assignment 2\nUsing Integrated Gradients to Explain Neural Network predictions\nDependencies\nAll of the work was done in the conda environment assignment2_env.yml\nThe Boston housing dataset was loaded using scikit-learn, the Concrete dataset is in the folder concrete/yeh-concret-data/Concrete_Data_Yeh.csv\nThe proj2_base.py file contains the data loader functions that also preprocess the data so that each feature has 0 mean and unit variance\nTraining the network\nTo train the networks there are separate folders and files for each network:\n\nboston shallow: boston/shallow/bost_shallow_test.py\nboston deep: boston/deep/bost_deep_test.py\nconcrete shallow: concrete/shallow/conc_shallow_test.py\nconcrete deep: concrete/deep/conc_deep_test.py\nTo create the learning curve plots the scripts ending analysis.py in each folder can be used.\nThe folders also contain the binary files with the rms errors used for the plots\n\nAttributions and analysis\nThe file called attribute.py contains the script to calculate integrated gradietns\nDo different statistical analysis on them and create the plots in the paper\n'], 'url_profile': 'https://github.com/davkovacs', 'info_list': ['Jupyter Notebook', 'Updated Mar 1, 2021', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'HTML', 'Updated Mar 9, 2020', 'Python', 'Updated Mar 7, 2020', '1', 'Jupyter Notebook', 'MIT license', 'Updated Mar 8, 2020', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Python', 'Updated Mar 4, 2020']}"
"{'location': 'NYC', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/gjordj', 'info_list': ['Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Java', 'Apache-2.0 license', 'Updated Mar 3, 2021', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Updated Mar 8, 2020', 'Python', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '104 contributions\n        in the last year', 'description': ['Rainfall_Prediction_using_regression\nData set for project is taken form https://www.kaggle.com/grubenm/austin-weather\n'], 'url_profile': 'https://github.com/MaclaurinYudhisthira', 'info_list': ['Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Java', 'Apache-2.0 license', 'Updated Mar 3, 2021', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Updated Mar 8, 2020', 'Python', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020']}","{'location': 'Jakarta', 'stats_list': [], 'contributions': '112 contributions\n        in the last year', 'description': ['Linear-Regression---Ecommerce\nWe just got some contract work with an Ecommerce company based in New York City that sells clothing online but they also have in-store style and clothing advice sessions. Customers come in to the store, have sessions/meetings with a personal stylist, then they can go home and order either on a mobile app or website for the clothes they want.\n'], 'url_profile': 'https://github.com/dhaneswaramandrasa', 'info_list': ['Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Java', 'Apache-2.0 license', 'Updated Mar 3, 2021', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Updated Mar 8, 2020', 'Python', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1,167 contributions\n        in the last year', 'description': ['\n\nThis is a tool used to perform regression testing for Apache POI on a very large corpus of test-files.\nUse it\nPrepare requirements\nThe following installed packages are required\n\n\nOpenJDK 8 (newer JDKs should work, but Apache POI still supports JDK 8)\n\n\nfontconfig (to avoid a possible NPE in tests, see https://github.com/AdoptOpenJDK/openjdk-build/issues/693)\n\n\nA checkout of sources of Apache POI which is used during the tests, should be available\nat directory /opt/poi\n\n\nFor syncing the results to a page at people.apache.org, the following additional packages are required\n\nfuse\nrsync\nsshfs\n\nPrepare corpus\nThe corpus of files is not included here, but needs to be available on the machine where\nyou run the tool. By default, directory ../download is used, so make sure the corpus is\navailable there or a symbolic link points to a directory with files to test.\nFor testing, you can use the files included in the Apache POI source repository by running the following\nln -s /opt/poi/test-data /opt/download\nCompile Apache POI\nCompile the jars for Apache POI to use them as part of the test-run.\nFor Apache POI up to 4.1.2:\ncd /opt/poi\nant jar integration-test-jar\n\nFor Apache POi since 5.0.0:\ncd /opt/poi\nant jar compile-integration\n\nSet up test\n\nAdjust available memory in build.gradle, look for task processFiles and set the main\nmemory to what is available on your machine\nAdjust available threads in ProcessFiles.java, adjust it relative to how you adjust main memory\nAdjust version of the Apache POI jar-files in build.gradle, look for def poiVersion =\nAdjust version that should be recorded in the database in build.gradle, look for def poiVersionRC =\nAdd a new version in POIStatus.java, look for the two ways to write the version, e.g. 412 and 4.1.2,\nthis project currently requires a new column for every new version to provide efficient queries for reporting\nAdjust the version to report in Report.java in method createReport()\n\nRun tests\nMake sure the corpus files are available at ../download, then simply running the script should\nexecute the tests and produce the report\n./run.sh\nIf the run finishes successfully, results are available at build/reports and build/reportsAll.\nIf you also want to upload results automatically to people.apache.org, use ./run.sh --with-sync instead.\nHow it works\nThis tool performs a mass regression test which the Apache POI development team uses to verify changes\nin newer releases and compare results to previous released versions of Apache POI.\nBy executing the regression tests on the same large corpus of files for each release, it is possible\nto find regressions in file handling that may have been introduced in the latest changes before a\nnew release is provided.\nThe corpus of files currently consists of more than 2.3 million documents which ensures that\nmany different variations in the file-formats are covered.\nFor testing on each file, the tool makes use of the existing integration tests for Apache POI which\nopens files according to their file-type and performs a number of actions on them.\nSee https://github.com/apache/poi/blob/trunk/src/integrationtest/org/apache/poi/TestAllFiles.java for\nthese tests.\nThis can be adjusted for other projects which work with other types of files as long as there is an easy\nway to run tests on a certain file.\nExport to Elasticsearch\nSome additional tools are provided to export the data from the Derby database to text files\nand then import the information from test-runs into an Elasticsearch instance for performing\nmore advanced queries.\nThe following utilities are available for this:\n\nExportPOIStatus: Write the contents of the database in JSON format to /tmp/export.json.gz\nElasticsearchWriterFromJSON: Send the information form the json-file to an Elasticsearch instance\n\nChange it\nGrab it\ngit clone git://github.com/centic9/poi-regression-test\n\nBuild it and run tests\ncd poi-regression-test\n./gradlew check poiIntegrationTest jacocoTestReport\n\nLicensing\n\nThis project is licensed under the Apache 2 license\n\n'], 'url_profile': 'https://github.com/centic9', 'info_list': ['Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Java', 'Apache-2.0 license', 'Updated Mar 3, 2021', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Updated Mar 8, 2020', 'Python', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020']}","{'location': 'Montevideo, Uruguay', 'stats_list': [], 'contributions': '60 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mjhelal', 'info_list': ['Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Java', 'Apache-2.0 license', 'Updated Mar 3, 2021', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Updated Mar 8, 2020', 'Python', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kamaleshsonu', 'info_list': ['Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Java', 'Apache-2.0 license', 'Updated Mar 3, 2021', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Updated Mar 8, 2020', 'Python', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '31 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/SandaruThilakarathne', 'info_list': ['Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Java', 'Apache-2.0 license', 'Updated Mar 3, 2021', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Updated Mar 8, 2020', 'Python', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '31 contributions\n        in the last year', 'description': ['Linear-Regression-Ecommerce-Data\nAn Ecommerce company based in New York City that sells clothing online but they also have in-store style and clothing advice sessions. Customers come in to the store, have sessions/meetings with a personal stylist, then they can go home and order either on a mobile app or website for the clothes they want.  The company is trying to decide whether to focus their efforts on their mobile app experience or their website.\n'], 'url_profile': 'https://github.com/rshankarsharma9', 'info_list': ['Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Java', 'Apache-2.0 license', 'Updated Mar 3, 2021', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Updated Mar 8, 2020', 'Python', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Advance-Logistic-Regression\nA program to illustrate the advance logistic regression using XGBoost and parameter tuning with XGBoost\n'], 'url_profile': 'https://github.com/TheMaggots', 'info_list': ['Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Java', 'Apache-2.0 license', 'Updated Mar 3, 2021', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Updated Mar 8, 2020', 'Python', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '66 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/chiranjitpanda', 'info_list': ['Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Java', 'Apache-2.0 license', 'Updated Mar 3, 2021', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Updated Mar 8, 2020', 'Python', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 11, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '309 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/HwayoungYoon', 'info_list': ['SAS', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', '1', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', '1', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'C++', 'BSD-3-Clause license', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Jul 1, 2020']}","{'location': 'Saarbr√ºcken', 'stats_list': [], 'contributions': '88 contributions\n        in the last year', 'description': ['PCA_and_LogisticRegression with Pytorch\n#Torchvision #CIFAR_10\n'], 'url_profile': 'https://github.com/supreethmv', 'info_list': ['SAS', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', '1', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', '1', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'C++', 'BSD-3-Clause license', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Jul 1, 2020']}","{'location': 'Nigeria', 'stats_list': [], 'contributions': '148 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/femiogundare', 'info_list': ['SAS', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', '1', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', '1', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'C++', 'BSD-3-Clause license', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Jul 1, 2020']}","{'location': 'Toronto, Canada', 'stats_list': [], 'contributions': '117 contributions\n        in the last year', 'description': ['Salary_Compensation_Logistic_Regression\nThis repository contains Solution Notebook  for 2018 Kaggle ML & DS Survey Challenge.\n'], 'url_profile': 'https://github.com/agastidukare', 'info_list': ['SAS', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', '1', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', '1', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'C++', 'BSD-3-Clause license', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Jul 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '26 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Charles-Scott-Green', 'info_list': ['SAS', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', '1', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', '1', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'C++', 'BSD-3-Clause license', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Jul 1, 2020']}","{'location': 'Netherlands', 'stats_list': [], 'contributions': '332 contributions\n        in the last year', 'description': ['Prostate Cancer (modelling lpsa levels)\nProstate specific antigen is useful as a preoperative marker for patients with prostate cancer [Stamey].\nWe use the dataset from [Hastie] (and [Stamey]) to model the dependence of the logarithm of prostate specific antigen (lpsa) on the given (8) predictors.\nWe use multilinear regression, and choose what predictors to keep using backwards stepwise selection.\nMain results and argument:\nThe univariate plots suggest a linear relationship between many of the predictors and the response.\n\nUnsurprisingly, a multilinear model (using all the predictors) fits the data well, improving the base error (of the model which simply predicts the average) by 50.5%.\n\n(As explained in the IPYnotebook, the intercept is irrelevant in this graph: one must only observe the slope).\nHowever, some of the predictors turn out to be superfluous.\nBackwards selection tests the null-hypothesis using the Z-scores (which follow t-student distributions with 67-(p+1) d.o.f where p is the number of predictors) and provides a way to select the significant predictors.\n\nUsing backwards selection with a threshold of 5% on the p-value of the Z-scores actually improves our error on the test set: the base error is improved by 53.2%.\nAnalyzing the Z-scores more carefully, one sees that it may actually be better to cap the p-values at 6% instead.\n\nIndeed the resulting model (which drops the predictors gleason, age, lcp and pgg45) behaves the best, with an improvement of 56.7% w.r.t. the base error.\nHere is a table with the errors for the different models:\n\nReferences:\n\n[Hastie]: \'The Elements of Statistical Learning\' by Trevor Hastie et al.\n[Gareth]: \'Introduction to Statistical Learning\' by Gareth James et al.\n[Casella]: \'Statistical Inference\' by George Casella and Roger Berger.\n[Downey]: \'Think stats\' by Allen Downey.\n[Stamey]: Stamey, Thomas A., et al. ""Prostate specific antigen in the diagnosis and treatment of adenocarcinoma of the prostate. II. Radical prostatectomy treated patients."" The Journal of urology 141.5 (1989): 1076-1083.\n\n'], 'url_profile': 'https://github.com/francisco-simoes', 'info_list': ['SAS', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', '1', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', '1', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'C++', 'BSD-3-Clause license', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Jul 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['regression_with_scikit_learn\n'], 'url_profile': 'https://github.com/Ada-Kiekhaefer', 'info_list': ['SAS', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', '1', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', '1', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'C++', 'BSD-3-Clause license', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Jul 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': [""Neural Network from Scratch\nIntroduction\nA neural network model is implemented from scratch, using only NumPy, to solve a regression prediction problem. The data comes from the UCI Machine Learning Database https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset\nProblem\nStatement: Neural Network for predicting Bike Sharing Rides. Here NN will predict how many bikes a company needs because if they have too few they are losing money from potential riders and if they have too many they are wasting money on bikes that are just sitting around. So NN will predict from the hisitorical data how many bikes they will need in the near future.\nNetwork Description: The network has two layers, a hidden layer and an output layer. The hidden layer uses the sigmoid function for activations. The output layer has only one node and is used for the regression, the output of the node is the same as the input of the node. That is, the activation function is f(x)=xf(x)=x . A function that takes the input signal and generates an output signal, but takes into account the threshold, is called an activation function. We work through each layer of our network calculating the outputs for each neuron. All of the outputs from one layer become inputs to the neurons on the next layer. This process is called forward propagation. We use the weights to propagate signals forward from the input to the output layers in a neural network. We use the weights to also propagate error backwards from the output back into the network to update our weights. This is called backpropagation.\nProject structure\nneural_network.py - Neural Network implementation.\nneural_network_notebook.ipynb - Main notebook file.\nBike-Sharing-Dataset/day.csv - Information about each trip taken using the bike share system by day.\nBike-Sharing-Dataset/hour.csv - Information about each trip taken using the bike share system by hour.\nBike-Sharing-Dataset/Readme.txt - General dataset description.\nSteps to Run\n\n\nconda create -n test python=3\n\n\nconda activate test\n\n\nconda install numpy matplotlib pandas jupyter notebook\n\n\njupyter notebook neural_network.ipynb\n\n\nResults\n\nAll the code in the notebook runs in Python 3 without failing, and all unit tests pass.\nThe number of epochs is chosen such the network is trained well enough to accurately make predictions but is not overfitting to the training data.\nThe number of hidden units is chosen such that the network is able to accurately predict the number of bike riders, is able to generalize, and is not overfitting.\nThe learning rate is chosen such that the network successfully converges, but is still time efficient.\nThe number of output nodes is properly selected to solve the desired problem.\nThe training loss is below 0.09 and the validation loss is below 0.18.\n\nConclusion\nThe predictions given by the model are quite accurate. However, the model overestimes bike ridership in December because it hasn't had sufficient holiday season training examples. The model predicts well except for Thanksgiving and Christmas. We don't have enough examples of holidays for the network to learn about them (and the\xa0holiday\xa0variable is incorrectly labeled for the days around Christmas and Thanksgiving). For dealing with holidays with the small amount of training data available over the holiday time periods, we could use a\xa0RNN,\xa0time-lagged features\xa0or fix the variable for holiday/not holiday (they only label the 22nd and 25th of December a holiday). There's also the possibility of\xa0oversampling\xa0the data on Christmas and Thanksgiving holidays. The network also over-predicts for Thanksgiving if you look back further into the validation set.\n""], 'url_profile': 'https://github.com/BhabaniNayak', 'info_list': ['SAS', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', '1', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', '1', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'C++', 'BSD-3-Clause license', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Jul 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '35 contributions\n        in the last year', 'description': ['PHAS0100Assignment1 Linear Regression\nPurpose\nThe purpose of this project is to solve a linear regression of the form:\ny = theta1*x + theta0\nGiven a text document which contains a column of x values, and a column of y values.\nCredits\nThis project was developed by Griffin Bassman.\nBuild Instructions\nIf you would like to build this project from a tar file, please extract PHAS0100Assignment1. From here, open up your terminal and navigate to the same directory that PHAS0100Assignment1 is in and type:\nmkdir PHAS0100Assignment1-Build\ncd PHAS0100Assignment1-Build\ncmake ../PHAS0100Assignment1\nmake\n\nThis project can also be built from github with the following:\ngit clone https://github.com/bassmang/PHAS0100Assignment1/\nmkdir PHAS0100Assignment1-Build\ncd PHAS0100Assignment1-Build\ncmake ../PHAS0100Assignment1\nmake\n\nAt this point, the built project will be in the PHAS0100Assignment1-Build directory.\nHow to Use\nOnce the project has been properly built and you are in the PHAS0100Assignment1-Build directory, type:\ncd bin\n\nIn order to get in the same directory as the app. In this directory, you will find two files, TestData1.txt and TestData2.txt which are sample files to perform a linear regression. You can either perform a linear regression using the normal equation or gradient descent. If you would like the use the normal equation, type:\n./lrgLinReg -n [file_name.txt]\n\nIf you would like to use the gradient descent method, type:\n./lrgLinReg -g [file_name.txt] [epochs] [learning_rate] [theta0_guess] [theta1_guess]\n\nWhere [epochs] specifies the number of iterations, [learning_rate] specifies the learning rate of the algorithm, and [theta0_guess] and [theta1_guess] specify initial values for these parameter. If you would like more details please type:\n./lrgLinReg [-h/--help]\n\nAdditionally, this directory contains a executable to run all of my tests. To run this type:\n./lrgLeastSquaresSolverTests\n\nPlease check out the file Screenshot_of_test_results.pdf on the cover to see some example commands and expected output.\nEnjoy!\n'], 'url_profile': 'https://github.com/bassmang', 'info_list': ['SAS', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', '1', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', '1', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'C++', 'BSD-3-Clause license', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Jul 1, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '56 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Dharaniraj1997', 'info_list': ['SAS', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', '1', 'Jupyter Notebook', 'GPL-3.0 license', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020', '1', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'C++', 'BSD-3-Clause license', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Jul 1, 2020']}"
"{'location': 'Richmond Va', 'stats_list': [], 'contributions': '131 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/GoldinLocks', 'info_list': ['HTML', 'Updated Mar 3, 2020', 'R', 'Updated Mar 3, 2020', '1', 'HTML', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Python', 'Updated Mar 12, 2020', '1', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '164 contributions\n        in the last year', 'description': ['Simple-Linear-regression\npredict salary on the basis of years of experience\n'], 'url_profile': 'https://github.com/pawarshubham99', 'info_list': ['HTML', 'Updated Mar 3, 2020', 'R', 'Updated Mar 3, 2020', '1', 'HTML', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Python', 'Updated Mar 12, 2020', '1', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020']}","{'location': 'Banglore', 'stats_list': [], 'contributions': '71 contributions\n        in the last year', 'description': ['SGD-for-Linear-Regression\nProblem Statement\nDonorsChoose.org has funded over 1.1 million classroom requests through the support of 3 million donors, the majority of whom were making their first-ever donation to a public school. If DonorsChoose.org can motivate even a fraction of those donors to make another donation, that could have a huge impact on the number of classroom requests fulfilled.\nA good solution will enable DonorsChoose.org to build targeted email campaigns recommending specific classroom requests to prior donors. Part of the challenge is to assess the needs of the organization, uncover insights from the data available, and build the right solution for this problem. Submissions will be evaluated on the following criteria:\nPerformance - How well does the solution match donors to project requests to which they would be motivated to donate? DonorsChoose.org will not be able to live test every submission, so a strong entry will clearly articulate why it will be effective at motivating repeat donations.\nAdaptable - The DonorsChoose.org team wants to put the winning submissions to work, quickly. Therefore a good entry will be easy to implement in production.\nIntelligible - A good entry should be easily understood by the DonorsChoose.org team should it need to be updated in the future to accommodate a changing marketplace.\n'], 'url_profile': 'https://github.com/chandana124', 'info_list': ['HTML', 'Updated Mar 3, 2020', 'R', 'Updated Mar 3, 2020', '1', 'HTML', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Python', 'Updated Mar 12, 2020', '1', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020']}","{'location': 'Michigan', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['Global_Warming_Regression_Model\nThis is a project for my CMSE-201 class. I utilized Pandas and regression techniques to clean climate change data and create a model.\nThis is a very simple model, it is not meant to be 100% accurate by any means.\n'], 'url_profile': 'https://github.com/sittomat', 'info_list': ['HTML', 'Updated Mar 3, 2020', 'R', 'Updated Mar 3, 2020', '1', 'HTML', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Python', 'Updated Mar 12, 2020', '1', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020']}","{'location': 'izmir', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['LogisticRegressionExample\nHerkese selamlar bu veri √ßalƒ±≈ümasƒ±nda amacƒ±mƒ±z bir bisiklet firmasƒ±na ait bisiklet alƒ±p almayan insanlarƒ±n bilgilerini\ndeƒüerlendirip bu verileri kullanarak yeni insanlarƒ±n bisiklet alƒ±p almayacaƒüƒ± tahminleme modeli olu≈üturmaktƒ±r.\nTahminleme modelini olu≈ütururken tahminleme ba≈üarƒ±sƒ±nƒ± arttƒ±rmak i√ßin bir √ßok y√∂ntem denedim.\nSize katkƒ±m olabilir amacƒ±yla sonuca giderkenki maceramƒ± sizinle payla≈ümak istedim.\nEn sonunda ba≈üarƒ± oranƒ±nƒ± arttƒ±rabildim.√áalƒ±≈ümam ""bicycle_LogisticRegressionPrediction"" ve ""bicycle_LogisticRegressionPrediction2"" adlƒ±\n2 dosyadan olu≈üuyor.\n'], 'url_profile': 'https://github.com/Handreaa', 'info_list': ['HTML', 'Updated Mar 3, 2020', 'R', 'Updated Mar 3, 2020', '1', 'HTML', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Python', 'Updated Mar 12, 2020', '1', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/leochappon', 'info_list': ['HTML', 'Updated Mar 3, 2020', 'R', 'Updated Mar 3, 2020', '1', 'HTML', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Python', 'Updated Mar 12, 2020', '1', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '53 contributions\n        in the last year', 'description': ['Linear-Regression-Assumptions-Test\n'], 'url_profile': 'https://github.com/rahul765', 'info_list': ['HTML', 'Updated Mar 3, 2020', 'R', 'Updated Mar 3, 2020', '1', 'HTML', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Python', 'Updated Mar 12, 2020', '1', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/SanthoshVenu', 'info_list': ['HTML', 'Updated Mar 3, 2020', 'R', 'Updated Mar 3, 2020', '1', 'HTML', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Python', 'Updated Mar 12, 2020', '1', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020']}","{'location': 'Toronto, Ontario Canada', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['regression_on_keras\n'], 'url_profile': 'https://github.com/matthiasuave', 'info_list': ['HTML', 'Updated Mar 3, 2020', 'R', 'Updated Mar 3, 2020', '1', 'HTML', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Python', 'Updated Mar 12, 2020', '1', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '31 contributions\n        in the last year', 'description': ['Car_Price_Linear_Regression\nLinear Regression\nCarPrice Assignmment\nProblem Statement : A Chinese automobile company Geely Auto aspires to enter the US market.They want to understand,factors on which car price depends.Specifically, they want to understand the factors affecting the pricing of cars in the American market.\n'], 'url_profile': 'https://github.com/VishakhaKude', 'info_list': ['HTML', 'Updated Mar 3, 2020', 'R', 'Updated Mar 3, 2020', '1', 'HTML', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Python', 'Updated Mar 12, 2020', '1', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020']}"
"{'location': 'Romania', 'stats_list': [], 'contributions': '38 contributions\n        in the last year', 'description': ['ML-Matlab-Logistic-Regression\nMatlab implementation of logistic regression to predict the credit card application acceptance.\n'], 'url_profile': 'https://github.com/ionut-banu', 'info_list': ['1', 'MATLAB', 'Updated Jun 24, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Oct 25, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2021', '1', 'Jupyter Notebook', 'Updated Aug 14, 2020', 'R', 'Updated Mar 2, 2020', 'R', 'Updated Mar 4, 2020', 'Updated Mar 4, 2020', 'Updated Mar 5, 2020', 'Python', 'Updated Mar 4, 2020']}","{'location': 'Paris', 'stats_list': [], 'contributions': '585 contributions\n        in the last year', 'description': ['Datascience X Logistic Regression\nData Analysis\nDescribe\nStd\nQuantile\nData Visualisation\nHistogram\nScatter plot\nPair plot\nLogistic Regression\n'], 'url_profile': 'https://github.com/alngo', 'info_list': ['1', 'MATLAB', 'Updated Jun 24, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Oct 25, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2021', '1', 'Jupyter Notebook', 'Updated Aug 14, 2020', 'R', 'Updated Mar 2, 2020', 'R', 'Updated Mar 4, 2020', 'Updated Mar 4, 2020', 'Updated Mar 5, 2020', 'Python', 'Updated Mar 4, 2020']}","{'location': 'Kyiv', 'stats_list': [], 'contributions': '420 contributions\n        in the last year', 'description': ['ft_linear_regression\nSchool 42 Algorithm branch project\n\n'], 'url_profile': 'https://github.com/SashaKryzh', 'info_list': ['1', 'MATLAB', 'Updated Jun 24, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Oct 25, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2021', '1', 'Jupyter Notebook', 'Updated Aug 14, 2020', 'R', 'Updated Mar 2, 2020', 'R', 'Updated Mar 4, 2020', 'Updated Mar 4, 2020', 'Updated Mar 5, 2020', 'Python', 'Updated Mar 4, 2020']}","{'location': 'Mountain View, CA', 'stats_list': [], 'contributions': '83 contributions\n        in the last year', 'description': ['Apprentice Chef - A Machine Learning Regression Model\nThe company Apprentice Chef offers its busy customers the possibility to receive at their door a selection of\ndaily-prepared meals together with a selection of disposable cookware for easy cleanup. Each meal takes 30 minutes to\nfinish cooking at home, and the orders can be done through a user-friendly on-line platform and mobile app\nIn the current project I am aiming to create a Supervised Machine Learning Regression model that will predict the\nrevenue a client will bring to the company.\nFiles\n\nutils: Contains a reusable methods that are used across the entire project to do the analysis and build the\nmodels.\ndata/Apprentice_Chef_Dataset.xlsx: Excel spreadsheet that contains the data\nrequired to do the analysis and create de final model\nSupervisedLearning_Analysis.ipynb: Built in Jupyter Notebook, it contains the\nanalysis to create the model. The visual analysis has its independent file.\nSupervisedLearning_VisualAnalysis.ipynb: Built in Jupyter Notebook, it\ncontains the visual exploratory analysis necessary to create the model\nFinal_Model.py: A python script that creates the final model from the analysis made.\nmodel/ApprenticeChef_RegressionModel.pkl: Contains a series of\nsupplementary files that were used as support during the analysis.\n\n'], 'url_profile': 'https://github.com/AndRemy', 'info_list': ['1', 'MATLAB', 'Updated Jun 24, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Oct 25, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2021', '1', 'Jupyter Notebook', 'Updated Aug 14, 2020', 'R', 'Updated Mar 2, 2020', 'R', 'Updated Mar 4, 2020', 'Updated Mar 4, 2020', 'Updated Mar 5, 2020', 'Python', 'Updated Mar 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '52 contributions\n        in the last year', 'description': ['Ames Housing Data and Kaggle Challenge\n\nSummary\nIn this project, I used the AMES Housing data to create a linear regression model to predict the sale price of a house in Ames Iowa. Based on my Linear Regression, the model can predict the house\'s price is within $32,000 on average. Based on my model, the features with the most wight to sale price are overall quality, number of full baths, and kitchen quality.\n\nProblem Statement\nI was hired by Zillow to help determine if Ames Iowa is a favorable market for tech expansion. Zillow\'s factors that could suggest tech growth are:\n\nHousing affordability\nMarket ""hotness""\nDemographics & Labor Market\nTech availability\nLivability\n\nI am focusing on housing affordability to help determine if Ames is a favorable market for tech expansion. As Zillow suggests, no matter what a resident\'s income value is, ""tech-dominant markets [like San Francisco or Los Angeles] have become notoriously unaffordable."" With this in mind, where may the next Silicon Valley lie? Does Ames, Iowa have the desirable mix of affordable housing and the above listed factors? ‚Äì which, due to the scope of this project, the four other factors won\'t be explored in detail.\nSource\n\nProvided Data\nFor this project, we were provided with two datasets:\n\nTraining Data\nTesting Data\n\n\nData Cleaining & EDA\nModel Features Data Dictionary\n\n\n\nFeature\nType\nDescription\n\n\n\n\noverall_qual\nint64\nOverall material and finish quality\n\n\nyear_built\nint64\nOriginal construction date\n\n\nyear_remod/add\nint64\nRemodel date (same as construction date if no remodeling or additions)\n\n\nexter_qual\nint64\nExterior material quality\n\n\nbsmt_qual\nint64\nHeight of the basement\n\n\nbed_bath_ratio\nfloat64\nRatio of number of bedrooms above basement level and full bathrooms above grade\n\n\nbsmt_cond\nint64\nGeneral condition of the basement\n\n\nbsmtfin_sf_1\nint64\nType 1 (Quality of basement finished area) square feet\n\n\nfull_bath\nint64\nFull bathrooms above grade\n\n\nkitchen_qual\nint64\nKitchen quality\n\n\ntotrms_abvgrd\nint64\nTotal rooms above grade (does not include bathrooms)\n\n\ngarage_cars\nfloat64\nSize of garage in car capacity\n\n\ngarage_area\nfloat64\nSize of garage in square feet\n\n\ngarage_qual_cars\nfloat64\nRatio of size of garage in car capacity to size of garage in square feet\n\n\noverall_cond_bed_bath\nfloat64\nOverall condition rating and bed bath ratio\n\n\ngr_liv_area\nint64\nAbove grade (ground) living area square feet\n\n\n\nMissing Values\nMany of the missing values in the training dataset were because an observation did not have the specified feature. For example, the original column, PoolQC meaning pool quality contained the most missing values. Nan values indicated that a house did not have a pool, as indicated in the data dictionary for all 81 columns. I handled this by converting PoolQC into a dummy column named has_pool where a non-null value, maps to 1, indicating that a pool exists, and a null value maps to 0, indicating that there is no pool. Following this logical imputation process, along with the original data dictionary, I was able to impute parallel missing values with 0\'s.\nOrdinal & Nominal Values\nMany of the original columns in the training dataset contained values that were ordinal or nominal strings. To be able to plot these columns against sale price later - and determine correlation strength in model feature selection - I replaced ordinal values with integers using a dictionary replacements.\nExploratory Visualizations\nAfter managing null values, I looked at the distribution of numerical columns by plotting boxplots and histograms as well as the correlation of features and sale price by running scatter plots/heatmaps. By identifying features that had a stronger correlation with sale price, I created interaction columns that integrate features that are directly related or depend on one another. For instance, I created a new column bed_bath_ratio that finds the ratio of the number of bedrooms to number of bathrooms. These two features are more indicative when integrated because they are expected to be proportional, and therefore, their ratio relationship is more important when estimating price.\n\nModeling\nI establisbed my baseline prediction and found that the R2 score is 0. This means that indicates that the model explains none of the variability of the data. I was curious to see how taking the log of saleprice would affect the model so I added a column where I took the logarithm of the saleprice, that is, train[\'saleprice_log\'] = np.log(train[\'saleprice\']). I set the log of sale price to be my target vector and from that, my RMSE score dropped by $10,000. I instantiated, fit, and train/test/split my model to get a trainig R2 score of 85% and test R2 score of 85%. The R2 scores indicates that 85% of the variability in my data can be explained by the linear model. After predicing X_train, I was able to get and RMSE score of $31,987.88. This indicates that, based on my Linear Regression, the model can predict the house\'s price is within $31,987.88 on avereage.\nAs part of my Zillow assignment, I had to test three different models: Linear Regression, Ridge, and Lasso and compare R2. I used StandardScalar() to standardize my data. Standardizing and then fitting X_train learns the means and standard deviations of each of our features. Then fit and transform X_train and call that Z_train to distinguish z-scores. After transforming my features, I instantiated and fit the Ridge and Lasso models, and found that they produce the same R2 scores, meaning they also explain 85% of the variability in the data.\n\nBusiness Recommendations\nAlthough the Linear Model is not perfect and improving it is an iterative process, it performed well enough to predicted the house\'s price within $31,987.88 on average. Sale price in Ames, Iowa is, on average, much lower than tech-dominant markets such as Los Angeles or San Francisco. Based on the provided data and Zillow\'s estimator, SF Median Home Value and LA Median Home Value, I found that in each city, the median sale price values for a home are:\n\nAmes: $180,500\nLos Angeles: $717,583\nSan Francisco: $1,387,263\n\nGiven the sale price trends and predictions, Ames, Iowa marks off the housing affordability factor ‚Äì from which a tech company would benefit expanding to. Affordable housing appeals to outside talent and retains current talent. Although housing in Ames is within a dream price range for an expanding tech market, the other 4 factors must surely be investigated further.\nTech-Available Markets Factor: I researched the U.S. Bureau of Labor Statistics database and found that Ames unemployment rates were at 1.6% in 2018, compared to those in Los Angeles, which were at 4.2%. Zillow states that a labor market with lower unemployment rates can support a growing economy because it leaves room for outside tech talent to come in and take advantage of the unique work-in-tech-own-a-house opportunity.\n\nSources\n\n\nZillow Market Expansion\n\n\nU.S. Bureau of Labor Statistics\n\n\nU.S. Census, American Community Survey\n\n\n'], 'url_profile': 'https://github.com/mariaflores13', 'info_list': ['1', 'MATLAB', 'Updated Jun 24, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Oct 25, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2021', '1', 'Jupyter Notebook', 'Updated Aug 14, 2020', 'R', 'Updated Mar 2, 2020', 'R', 'Updated Mar 4, 2020', 'Updated Mar 4, 2020', 'Updated Mar 5, 2020', 'Python', 'Updated Mar 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['Predictive---Logistic-Regression\nJust a simple R code for Logistic Regression and a plot to visualize it..!\n'], 'url_profile': 'https://github.com/hash-7201', 'info_list': ['1', 'MATLAB', 'Updated Jun 24, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Oct 25, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2021', '1', 'Jupyter Notebook', 'Updated Aug 14, 2020', 'R', 'Updated Mar 2, 2020', 'R', 'Updated Mar 4, 2020', 'Updated Mar 4, 2020', 'Updated Mar 5, 2020', 'Python', 'Updated Mar 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['Regression-Models-Course-Project\nIn this analysis, we will be looking at the mtcars dataset in R. Let\'s pretend we work for Motor Trend, a magazine about the automobile industry. Looking at the collection of cars, we are interested in the relationship between a set of variables and miles per gallon (mpg), our outcome. We are interested in the two questions:\n\n‚ÄúIs an automatic or manual transmission better for MPG‚Äù\n""Quantify the MPG difference between automatic and manual transmissions""\n\nOur findings show that manual transmission has a larger mean output of mpg than that of automatic transmission. We also find that our linear model comparing mpg to transmission type is not as optimized as a model that compares mpg to transmission type plus other variables such as cylinders, horsepower, and weight included as regressors. This shows that our outcome is correlated with the other variables and can not be left out of our linear models.\n'], 'url_profile': 'https://github.com/Rangoonie', 'info_list': ['1', 'MATLAB', 'Updated Jun 24, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Oct 25, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2021', '1', 'Jupyter Notebook', 'Updated Aug 14, 2020', 'R', 'Updated Mar 2, 2020', 'R', 'Updated Mar 4, 2020', 'Updated Mar 4, 2020', 'Updated Mar 5, 2020', 'Python', 'Updated Mar 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kamaleshsonu', 'info_list': ['1', 'MATLAB', 'Updated Jun 24, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Oct 25, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2021', '1', 'Jupyter Notebook', 'Updated Aug 14, 2020', 'R', 'Updated Mar 2, 2020', 'R', 'Updated Mar 4, 2020', 'Updated Mar 4, 2020', 'Updated Mar 5, 2020', 'Python', 'Updated Mar 4, 2020']}","{'location': 'London', 'stats_list': [], 'contributions': '47 contributions\n        in the last year', 'description': ['Information about this dataset\nHousing Values in Suburbs of Boston\nThe Boston data frame has 506 rows and 14 columns.\nThis data frame contains the following columns:\n\nlstat: lower status of the population (percent).\nmedv: median value of owner-occupied homes in $1000s. - crim: per capita crime rate by town.\nzn: proportion of residential land zoned for lots over 25,000 sq.ft.\nindus : proportion of non-retail business acres per town.\nchas: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).\nnox: nitrogen oxides concentration (parts per 10 million).\nrm: average number of rooms per dwelling.\nage: proportion of owner-occupied units built prior to 1940.\ndis: weighted mean of distances to five Boston employment centres.\nrad: index of accessibility to radial highways.\ntax: full-value property-tax rate per $10,000.\nptratio: pupil-teacher ratio by town.\nblack: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.\n\nSource\n\nHarrison, D. and Rubinfeld, D.L. (1978) Hedonic prices and the demand for clean air. J. Environ. Economics and Management 5, 81‚Äì102.\nBelsley D.A., Kuh, E. and Welsch, R.E. (1980) Regression Diagnostics. Identifying Influential Data and Sources of Collinearity. New York: Wiley.\n\n'], 'url_profile': 'https://github.com/bokedara', 'info_list': ['1', 'MATLAB', 'Updated Jun 24, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Oct 25, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2021', '1', 'Jupyter Notebook', 'Updated Aug 14, 2020', 'R', 'Updated Mar 2, 2020', 'R', 'Updated Mar 4, 2020', 'Updated Mar 4, 2020', 'Updated Mar 5, 2020', 'Python', 'Updated Mar 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['linear-regression-with-relu\nopen hand_grad.py\nmodify path of train.csv and test.csv on line 26 and 45\nrun\n'], 'url_profile': 'https://github.com/get36', 'info_list': ['1', 'MATLAB', 'Updated Jun 24, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Oct 25, 2020', 'Jupyter Notebook', 'Updated Feb 10, 2021', '1', 'Jupyter Notebook', 'Updated Aug 14, 2020', 'R', 'Updated Mar 2, 2020', 'R', 'Updated Mar 4, 2020', 'Updated Mar 4, 2020', 'Updated Mar 5, 2020', 'Python', 'Updated Mar 4, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['Regression-Models-Course-Project\nIn this analysis, we will be looking at the mtcars dataset in R. Let\'s pretend we work for Motor Trend, a magazine about the automobile industry. Looking at the collection of cars, we are interested in the relationship between a set of variables and miles per gallon (mpg), our outcome. We are interested in the two questions:\n\n‚ÄúIs an automatic or manual transmission better for MPG‚Äù\n""Quantify the MPG difference between automatic and manual transmissions""\n\nOur findings show that manual transmission has a larger mean output of mpg than that of automatic transmission. We also find that our linear model comparing mpg to transmission type is not as optimized as a model that compares mpg to transmission type plus other variables such as cylinders, horsepower, and weight included as regressors. This shows that our outcome is correlated with the other variables and can not be left out of our linear models.\n'], 'url_profile': 'https://github.com/Rangoonie', 'info_list': ['R', 'Updated Mar 4, 2020', 'Updated Mar 4, 2020', 'Updated Mar 5, 2020', 'Python', 'Updated Mar 4, 2020', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', '1', 'MATLAB', 'Updated Jun 24, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Oct 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kamaleshsonu', 'info_list': ['R', 'Updated Mar 4, 2020', 'Updated Mar 4, 2020', 'Updated Mar 5, 2020', 'Python', 'Updated Mar 4, 2020', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', '1', 'MATLAB', 'Updated Jun 24, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Oct 25, 2020']}","{'location': 'London', 'stats_list': [], 'contributions': '47 contributions\n        in the last year', 'description': ['Information about this dataset\nHousing Values in Suburbs of Boston\nThe Boston data frame has 506 rows and 14 columns.\nThis data frame contains the following columns:\n\nlstat: lower status of the population (percent).\nmedv: median value of owner-occupied homes in $1000s. - crim: per capita crime rate by town.\nzn: proportion of residential land zoned for lots over 25,000 sq.ft.\nindus : proportion of non-retail business acres per town.\nchas: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).\nnox: nitrogen oxides concentration (parts per 10 million).\nrm: average number of rooms per dwelling.\nage: proportion of owner-occupied units built prior to 1940.\ndis: weighted mean of distances to five Boston employment centres.\nrad: index of accessibility to radial highways.\ntax: full-value property-tax rate per $10,000.\nptratio: pupil-teacher ratio by town.\nblack: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.\n\nSource\n\nHarrison, D. and Rubinfeld, D.L. (1978) Hedonic prices and the demand for clean air. J. Environ. Economics and Management 5, 81‚Äì102.\nBelsley D.A., Kuh, E. and Welsch, R.E. (1980) Regression Diagnostics. Identifying Influential Data and Sources of Collinearity. New York: Wiley.\n\n'], 'url_profile': 'https://github.com/bokedara', 'info_list': ['R', 'Updated Mar 4, 2020', 'Updated Mar 4, 2020', 'Updated Mar 5, 2020', 'Python', 'Updated Mar 4, 2020', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', '1', 'MATLAB', 'Updated Jun 24, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Oct 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['linear-regression-with-relu\nopen hand_grad.py\nmodify path of train.csv and test.csv on line 26 and 45\nrun\n'], 'url_profile': 'https://github.com/get36', 'info_list': ['R', 'Updated Mar 4, 2020', 'Updated Mar 4, 2020', 'Updated Mar 5, 2020', 'Python', 'Updated Mar 4, 2020', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', '1', 'MATLAB', 'Updated Jun 24, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Oct 25, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/SanthoshVenu', 'info_list': ['R', 'Updated Mar 4, 2020', 'Updated Mar 4, 2020', 'Updated Mar 5, 2020', 'Python', 'Updated Mar 4, 2020', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', '1', 'MATLAB', 'Updated Jun 24, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Oct 25, 2020']}","{'location': 'Toronto, Ontario Canada', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['regression_on_keras\n'], 'url_profile': 'https://github.com/matthiasuave', 'info_list': ['R', 'Updated Mar 4, 2020', 'Updated Mar 4, 2020', 'Updated Mar 5, 2020', 'Python', 'Updated Mar 4, 2020', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', '1', 'MATLAB', 'Updated Jun 24, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Oct 25, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '31 contributions\n        in the last year', 'description': ['Car_Price_Linear_Regression\nLinear Regression\nCarPrice Assignmment\nProblem Statement : A Chinese automobile company Geely Auto aspires to enter the US market.They want to understand,factors on which car price depends.Specifically, they want to understand the factors affecting the pricing of cars in the American market.\n'], 'url_profile': 'https://github.com/VishakhaKude', 'info_list': ['R', 'Updated Mar 4, 2020', 'Updated Mar 4, 2020', 'Updated Mar 5, 2020', 'Python', 'Updated Mar 4, 2020', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', '1', 'MATLAB', 'Updated Jun 24, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Oct 25, 2020']}","{'location': 'Romania', 'stats_list': [], 'contributions': '38 contributions\n        in the last year', 'description': ['ML-Matlab-Logistic-Regression\nMatlab implementation of logistic regression to predict the credit card application acceptance.\n'], 'url_profile': 'https://github.com/ionut-banu', 'info_list': ['R', 'Updated Mar 4, 2020', 'Updated Mar 4, 2020', 'Updated Mar 5, 2020', 'Python', 'Updated Mar 4, 2020', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', '1', 'MATLAB', 'Updated Jun 24, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Oct 25, 2020']}","{'location': 'Paris', 'stats_list': [], 'contributions': '585 contributions\n        in the last year', 'description': ['Datascience X Logistic Regression\nData Analysis\nDescribe\nStd\nQuantile\nData Visualisation\nHistogram\nScatter plot\nPair plot\nLogistic Regression\n'], 'url_profile': 'https://github.com/alngo', 'info_list': ['R', 'Updated Mar 4, 2020', 'Updated Mar 4, 2020', 'Updated Mar 5, 2020', 'Python', 'Updated Mar 4, 2020', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', '1', 'MATLAB', 'Updated Jun 24, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Oct 25, 2020']}","{'location': 'Kyiv', 'stats_list': [], 'contributions': '420 contributions\n        in the last year', 'description': ['ft_linear_regression\nSchool 42 Algorithm branch project\n\n'], 'url_profile': 'https://github.com/SashaKryzh', 'info_list': ['R', 'Updated Mar 4, 2020', 'Updated Mar 4, 2020', 'Updated Mar 5, 2020', 'Python', 'Updated Mar 4, 2020', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', '1', 'MATLAB', 'Updated Jun 24, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Mar 27, 2020', 'Jupyter Notebook', 'Updated Oct 25, 2020']}"
"{'location': 'Chicago, IL', 'stats_list': [], 'contributions': '56 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/daniellapombo', 'info_list': ['R', 'Updated Oct 22, 2020', 'Python', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 8, 2020', 'Python', 'Updated Mar 8, 2020', 'Python', 'Updated Nov 10, 2020', 'CSS', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Python', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '483 contributions\n        in the last year', 'description': ['Boston Housing Regression:\nIn this repository, a simple Tensorflow model was created using Tensorflow 1.14.0. This model was applied to a relatively small housing dataset. I tackled this project to enhance my versatility with respect to the platforms that I use to analyze data. This repository will serve as a benchmark for future data analysis projects. The training and testing functions are easy to replicate.\nDataset Description:\nThe dataset has 13 features and a single regressional output. The output represents the price of the house affiliated with that particular data entry. The produced model is a weak model given that the dataset has less than 1000 entries. Once again, the purpose of this project was to create a benchmark for future work.\nModel:\nA relatively simple 2 layer model was created. This model and it\'s parameters are illustrated by the code snippet below:\nlearning_rate = 0.001\nnum_epochs = 2000\nnum_models = 10\nbatch_size = 64\n\nhidden_layer_1 = 32\nhidden_layer_2 = 1\n\n# Placeholder for batch of inputs:\nx = tf.placeholder(tf.float32, [None, input_nodes])\n# Layer 1 variables:\nW1 = tf.Variable(tf.truncated_normal([input_nodes, hidden_layer_1], stddev=0.15))\nb1 = tf.Variable(tf.zeros([hidden_layer_1]))\ny1 = tf.math.sigmoid(tf.matmul(x, W1) + b1)\n# layer 2 variables:\nW2 = tf.Variable(tf.truncated_normal([hidden_layer_1, hidden_layer_2], stddev=0.15))\nb2 = tf.Variable(tf.zeros([hidden_layer_2]))\ny = tf.matmul(y1, W2) + b2\n# Placeholder for batch of targets:\ny_ = tf.placeholder(tf.float32, [None, 1])\nResults:\nThe application of this model to the dataset resulted in the following training curve:\n\nFigure 1: The loss curve associated with the training process\nAs illustrated by the above curve, the training process was smooth and gradual. There was no instability and the loss monotonically decreased.\nThe curve below delineates the quality of the model during testing:\n\nFigure 2: The approximated fit overlayed upon the actual data\nGiven the limited data, the failure of the model was expected, and has been configrmed by the above Figure. Many peaks and valleys were completed dismissed by the fit. Despite this, the model clearly demonstrated its capability to generally approximate the data.\nDuplication Instructions:\n\nClone the repository.\nEnsure Tensorflow 1.14 is installed (pip install tensorflow==1.14.0).\nCreate a ""/weights"" directory to store the weights.\nCreate ""/data_management/boston_housing"" directory and ""/data_management/cifar100"" directories.\nEdit and run the main file according to which set of weights are deemed optimal.\n\n'], 'url_profile': 'https://github.com/atkinssamuel', 'info_list': ['R', 'Updated Oct 22, 2020', 'Python', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 8, 2020', 'Python', 'Updated Mar 8, 2020', 'Python', 'Updated Nov 10, 2020', 'CSS', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Python', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '395 contributions\n        in the last year', 'description': ['Multiple-Linear-Regression\n'], 'url_profile': 'https://github.com/Santosh-Sah', 'info_list': ['R', 'Updated Oct 22, 2020', 'Python', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 8, 2020', 'Python', 'Updated Mar 8, 2020', 'Python', 'Updated Nov 10, 2020', 'CSS', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Python', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '31 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/SandaruThilakarathne', 'info_list': ['R', 'Updated Oct 22, 2020', 'Python', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 8, 2020', 'Python', 'Updated Mar 8, 2020', 'Python', 'Updated Nov 10, 2020', 'CSS', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Python', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 10, 2020']}","{'location': 'Riga, Latvia', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['Markov-modulated linear regression (MMLR) was firstly proposed by professor Alexander Andronov. This model is a special case of the Markov additive process (Y, J), where component J is called Markov, and component Y is additive and described by a linear regression. The component J is a continuous-time homogeneous irreducible Markov chain with the known transition intensities between the states. Usually this Markov component is called the external environment or background process. Unknown regression coefficients depend on external environment state, but regressors remain constant.\nMarkov-switching regression models or regime switching regression models share the same idea of varying the regression parameters randomly in accordance with external environment. However, these models have significant differences in terms of the analytical approach. Namely, MMLR describes the external environment as a continuous-time homogeneous irreducible Markov chain with known parameters. On the other hand, switching models consider Markov chain as unobservable and estimation procedure involves estimation of transition matrix.\nThe developed scripts can be applied to analyse statistical properties of MMLR model‚Äôs estimator based on simulated data. The examples below consider the influence of the sample parameters (e.g., sample size and distribution of the initial data), as well the influence of estimation method details (e.g., different weight matrices in OLS) on the consistency of model estimates.\n'], 'url_profile': 'https://github.com/Jackil1993', 'info_list': ['R', 'Updated Oct 22, 2020', 'Python', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 8, 2020', 'Python', 'Updated Mar 8, 2020', 'Python', 'Updated Nov 10, 2020', 'CSS', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Python', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ChaudhariShubham', 'info_list': ['R', 'Updated Oct 22, 2020', 'Python', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 8, 2020', 'Python', 'Updated Mar 8, 2020', 'Python', 'Updated Nov 10, 2020', 'CSS', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Python', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['Deeplearning-with-logistic-regression\n'], 'url_profile': 'https://github.com/pratikshekhar92', 'info_list': ['R', 'Updated Oct 22, 2020', 'Python', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 8, 2020', 'Python', 'Updated Mar 8, 2020', 'Python', 'Updated Nov 10, 2020', 'CSS', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Python', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 10, 2020']}","{'location': 'Washington D.C. - Baltimore', 'stats_list': [], 'contributions': '41 contributions\n        in the last year', 'description': [""Non-Linear-Regressions (position salaries)\nMade a polynomial linear model to predict what a new employee's salary should be based on what their previous salary was. The new employee requested $160,000 for their new role as this is how much they made at their preveious job. I have created a polynomial model to predict how this person should actually make based on their years of experience. This is a truth or bluff detector that HR would use to verify and justify salary negotiations.\nAlso experimented with SVR, Decision tree, and Random Forest regression models to fit the same dataset and predict the employee's salary.\n""], 'url_profile': 'https://github.com/justinyu1', 'info_list': ['R', 'Updated Oct 22, 2020', 'Python', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 8, 2020', 'Python', 'Updated Mar 8, 2020', 'Python', 'Updated Nov 10, 2020', 'CSS', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Python', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '395 contributions\n        in the last year', 'description': ['Simple_Linear_Regression\n'], 'url_profile': 'https://github.com/Santosh-Sah', 'info_list': ['R', 'Updated Oct 22, 2020', 'Python', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 8, 2020', 'Python', 'Updated Mar 8, 2020', 'Python', 'Updated Nov 10, 2020', 'CSS', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Python', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['NONE'], 'url_profile': 'https://github.com/Machine-Learning-AU-2020', 'info_list': ['R', 'Updated Oct 22, 2020', 'Python', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 8, 2020', 'Python', 'Updated Mar 8, 2020', 'Python', 'Updated Nov 10, 2020', 'CSS', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Python', 'Updated Mar 10, 2020', 'Python', 'Updated Mar 10, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': [""\nObjective\nThe objective of this project is twofold: first, to use data from the 2017 & 2018 NFL seasons to predict how many yards a player will run the ball after receiving a handoff; second, to determine the most important factors in the data collected by the NFL's newly implemented Next Gen Stats system in predicting rush yardage.\nThe Data\nThe data set was furnished by the NFL. It contains the 49 features listed below. Each row in the file corresponds to a single player's involvement in a single play. The player is assigned a unique PlayerId, and the play is assigned its own unique PlayId. Therefore, each play is comprised of 22 rows: one for each player on the field--both offense and defense. All the columns are contained in one large dataframe which is grouped by PlayId.\nThe data includes game-level circumstantial features, such as weather, temperature, playing surface, location, etc.; player-level features, such as the player's name, team, number, height, weight, position, etc.; and play-level features, such as time of snap, time of handoff, down number, and yardage gained on the play (our target). The Next Gen Stats system uses chips that are embedded into the shoulder pads of each player as well as the ball to provide additional measurements that are specific to each player for each play, like the player's, orientation, distance covered and acceleration (calculated from the last 0.1 seconds of play).\nA - acceleration in yards/second^2\nDefendersInTheBox - number of defenders lined up near the line of scrimmage, spanning the width of the offensive line\nDefensePersonnel - defensive team positional grouping\nDir - angle of player motion (deg)\nDis - distance traveled from prior time point, in yards\nDisplayName - player's name\nDistance - yards needed for a first down\nDown - the down (1-4)\nFieldPosition - which side of the field the play is happening on\nGameClock - time on the game clock\nGameId - a unique game identifier\nGameWeather - description of the game weather\nHomeScoreBeforePlay - home team score before play started\nHomeTeamAbbr - home team abbreviation\nHumidity - humidity\nJerseyNumber - jersey number\nLocation - city where the game is being played\nNflId - a unique identifier of the player\nNflIdRusher - the NflId of the rushing player\nOffenseFormation - offense formation\nOffensePersonnel - offensive team positional grouping\nOrientation - orientation of player (deg)\nPlayDirection - direction the play is headed\nPlayerBirthDate - birth date (mm/dd/yyyy)\nPlayerCollegeName - where the player attended college\nPlayerHeight - player height (ft-in)\nPlayerWeight - player weight (lbs)\nPlayId - a unique play identifier\nPosition - the player's position (the specific role on the field that they typically play)\nPossessionTeam - team with possession\nQuarter - game quarter (1-5, 5 == overtime)\nS - speed in yards/second\nSeason - year of the season\nStadium - stadium where the game is being played\nStadiumType - description of the stadium environment\nTeam - home or away\nTemperature - temperature (deg F)\nTimeHandoff - UTC time of the handoff\nTimeSnap - UTC time of the snap\nTurf - description of the field surface\nVisitorScoreBeforePlay - visitor team score before play started\nVisitorTeamAbbr - visitor team abbreviation\nWeek - week into the season\nWindDirection - wind direction\nWindSpeed - wind speed in miles/hour\nX - player position along the long axis of the field. See figure below.\nY - player position along the short axis of the field. See figure below.\nYardLine - the yard line of the line of scrimmage\nYards - the yardage gained on the play\n\n\n""], 'url_profile': 'https://github.com/WartimeHotTot', 'info_list': ['Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Python', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'R', 'Updated Mar 5, 2020', 'Python', 'Updated Mar 4, 2020', 'Updated Mar 8, 2020', 'R', 'MIT license', 'Updated Mar 27, 2020', 'Updated Mar 6, 2020']}","{'location': 'Pakistan', 'stats_list': [], 'contributions': '572 contributions\n        in the last year', 'description': ['Fit_Line_Using_Regression\n'], 'url_profile': 'https://github.com/MuhammadUsama100', 'info_list': ['Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Python', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'R', 'Updated Mar 5, 2020', 'Python', 'Updated Mar 4, 2020', 'Updated Mar 8, 2020', 'R', 'MIT license', 'Updated Mar 27, 2020', 'Updated Mar 6, 2020']}","{'location': 'Jakarta', 'stats_list': [], 'contributions': '112 contributions\n        in the last year', 'description': ['Logistic-Regression---Ads\nIn this project we will be working with a fake advertising data set, indicating whether or not a particular internet user clicked on an Advertisement. We will try to create a model that will predict whether or not they will click on an ad based off the features of that user.\n'], 'url_profile': 'https://github.com/dhaneswaramandrasa', 'info_list': ['Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Python', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'R', 'Updated Mar 5, 2020', 'Python', 'Updated Mar 4, 2020', 'Updated Mar 8, 2020', 'R', 'MIT license', 'Updated Mar 27, 2020', 'Updated Mar 6, 2020']}","{'location': ' Pittsburgh', 'stats_list': [], 'contributions': '375 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/BockSong', 'info_list': ['Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Python', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'R', 'Updated Mar 5, 2020', 'Python', 'Updated Mar 4, 2020', 'Updated Mar 8, 2020', 'R', 'MIT license', 'Updated Mar 27, 2020', 'Updated Mar 6, 2020']}","{'location': 'Puducherry, India', 'stats_list': [], 'contributions': '149 contributions\n        in the last year', 'description': ['Regression_Neural_Networks\nThis repository contains codes for Regression using Neural Networks, coded using Numpy\n'], 'url_profile': 'https://github.com/hari-19', 'info_list': ['Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Python', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'R', 'Updated Mar 5, 2020', 'Python', 'Updated Mar 4, 2020', 'Updated Mar 8, 2020', 'R', 'MIT license', 'Updated Mar 27, 2020', 'Updated Mar 6, 2020']}","{'location': 'NYC', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/gjordj', 'info_list': ['Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Python', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'R', 'Updated Mar 5, 2020', 'Python', 'Updated Mar 4, 2020', 'Updated Mar 8, 2020', 'R', 'MIT license', 'Updated Mar 27, 2020', 'Updated Mar 6, 2020']}","{'location': 'Yangon', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ayethantmay', 'info_list': ['Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Python', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'R', 'Updated Mar 5, 2020', 'Python', 'Updated Mar 4, 2020', 'Updated Mar 8, 2020', 'R', 'MIT license', 'Updated Mar 27, 2020', 'Updated Mar 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['Regression-On-Image-Data\n'], 'url_profile': 'https://github.com/PandeyVishal', 'info_list': ['Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Python', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'R', 'Updated Mar 5, 2020', 'Python', 'Updated Mar 4, 2020', 'Updated Mar 8, 2020', 'R', 'MIT license', 'Updated Mar 27, 2020', 'Updated Mar 6, 2020']}","{'location': 'France', 'stats_list': [], 'contributions': '139 contributions\n        in the last year', 'description': [""latent-variable-regressions\nCe r√©pertoire contient le code Rmarkdown ainsi que le document bookdown d'un projet que j'ai r√©alis√© dans le cadre du cours Data Mining 2 o√π j'applique les techniques de r√©gressions sur variables latentes sur la base de donn√©es AmesHousing.\n""], 'url_profile': 'https://github.com/agailloty', 'info_list': ['Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Python', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'R', 'Updated Mar 5, 2020', 'Python', 'Updated Mar 4, 2020', 'Updated Mar 8, 2020', 'R', 'MIT license', 'Updated Mar 27, 2020', 'Updated Mar 6, 2020']}","{'location': 'Madrid, Spain', 'stats_list': [], 'contributions': '564 contributions\n        in the last year', 'description': ['apprenticeChef-MachineLearningProject\n'], 'url_profile': 'https://github.com/dieko95', 'info_list': ['Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 12, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Python', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'R', 'Updated Mar 5, 2020', 'Python', 'Updated Mar 4, 2020', 'Updated Mar 8, 2020', 'R', 'MIT license', 'Updated Mar 27, 2020', 'Updated Mar 6, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '35 contributions\n        in the last year', 'description': [""SimpleRegression_cpp\nversion 1.Just a test.Make a simple regression model.The data was fixed.The result is True.\nversion 2.0 in the description of code file 'simpleReg.cpp'.Matrix class was finished.Reference by\nhttps ://blog.csdn.net/m0_37772174/article/details/83018940\nMatrix class problem:1.operator* return a local obj and before return, it get desturcted? So operator= get a ptr with nothin.\n2.If i make obj temp in function operator* ,when it return the result to main(),the static obj gone, not get destruced(trueÔºâ.But at the end of main(),when destructing, it still need to destruct temp.And its p has nothing.\n""], 'url_profile': 'https://github.com/zhukang01', 'info_list': ['C++', 'Updated Mar 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Python', 'Updated Aug 5, 2020', 'HTML', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '118 contributions\n        in the last year', 'description': ['Data-Science\nIn this Repository i have included only Linear Regression Projects which i worked on.\nI have uploaded all other Machine Learning Techniques in the Repository with their respective names.\nThis repository includes all the Machine learning algorithms and its respective R codes.\n'], 'url_profile': 'https://github.com/bantu07', 'info_list': ['C++', 'Updated Mar 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Python', 'Updated Aug 5, 2020', 'HTML', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/baptistemokas', 'info_list': ['C++', 'Updated Mar 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Python', 'Updated Aug 5, 2020', 'HTML', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': ['Machine Learning Pipeline\n\nPrepare data\nSelect algorithm(s)\nTrain model\nPackage model\nValidate model\nDeploy model\nMonitor model\n\nChoice of model(s):\n\nRidge\nLasso\nElasticNet\nSupport Vector Regressor (SVR)\nLightGBM\nXGBoost\nStack Regressor\nBlended Models (Combination of all models)\n\nEvaluation of the model(s):\nThe results of the trained model produced quite a close prediction to the actual train target variable. It has an accuracy score based on performance metrics RMSLE = 0.0292 and RMSE = 0.155. However, the best prediction score actually does not comes from our blended models. Instead, it comes from other algorithms trained models. We aim to achieve a test score that is close or even better than the train set score. If the difference is too far off, there could be overfitting in our train set which render the model to be less reliable for other datasets or future deployment. The following points will explain the differences of each trained models and their final prediction score (rounded).\n1. Ridge\nA regularization model to prevent model from overfitting. It reduces large coefficients but prevent it from reaching zero.\n\nRidgeCV regression with built-in cross-validation\nRobust scaler by scaling features using statistics that are robust to outliers\nDecrease model complexity while keeping all features in the model\nPenalizes the sum of squared coefficients\nChoice of Regularization Parameter by allocating list of alphas value to find the optimal value\n\nFinal prediction score:\n\nRMSLE: 0.781\nRMSE: 17094.348\n\n2. Lasso\nA regularization model to prevent model from overfitting. It reduces large coefficients and can bring it to 0.\n\nIt can be use for feature selection\nPenalizes the sum of their absolute values\nCorrelated features have a larger coefficient, while others are close to zero or zero\nChoice of Regularization Parameter by allocating list of alphas value to find the optimal value\n\nFinal prediction score:\n\nRMSLE: 0.782\nRMSE: 17204.302\n\n3. ElasticNet\nIt is a combination of both Ridge and Lasso regularization models.\n\nMix of both sum of square and absolute values\nTunable on the ratio of penalty\nChoice of Regularization Parameter and ratio values by allocating list to find the optimal value\n\nFinal prediction score:\n\nRMSLE: 0.782\nRMSE: 17200.410\n\n4. SVR\nSVR is different than other regression models. It uses the Support Vector Machine(SVM, a classification algorithm) to predict a continuous variable.\n\nFit the best line within a predefined or threshold error value\nHard to scale to datasets with more than a couple of 10,000 samples\n\nFinal prediction score:\n\nRMSLE: 0.687\nRMSE: 6839.904\n\n5. Light GBM\nThe size of dataset ranges from small to extremely large (big data) and is becoming available on many platforms. It becomes difficult for traditional data science algorithms to give faster results. Light GBM is prefixed as ‚ÄòLight‚Äô because of its high speed and efficiency.\n\nFaster training speed and higher efficiency\nHandle large size of data using lower memory to run\nFocuses on accuracy of results\nSupports parallel and GPU learning, good for data science application development\nPerformed accurately on the model resulting in overall high accuracy in predicted target\n\nFinal prediction score:\n\nRMSLE: 0.140\nRMSE: 563.641\n\n6. XGBoost\nXGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable.\n\nProvides a parallel tree boosting\nRuns on major distributed environment\nOne of the best algorithm for Execution Speed and Model Performance\nFinal prediction score:\nRMSLE: 0.028\nRMSE: 134.136\n\n7. Stack Regressor\nStacking is an ensemble method to combine multiple regression models via a meta-regressor. StackingCVRegressor extends the standard stacking algorithm to a level-2 regressor.\n\nCombines all models listed above to get the best results among them\nBest accuracy score among all other models, including the blended model\n\nFinal prediction score:\n\nRMSLE: 0.020\nRMSE: 89.751\n\n8. Blended Models\nBlending is similar to stacking but uses only a validation set from the train set to make predictions. The validation set and the predictions are used to build a model which is used for test set. It consists of following percentage of models prediction:\n\nRidge: 0.05\nLasso: 0.05\nElasticNet: 0.05\nSVR: 0.05\nLGB: 0.1\nXGB: 0.3\nStackReg: 0.4\n\nFinal prediction score:\n\nRMSLE: 0.151\nRMSE: 952.618\n\nSummary\nThe model selection process is based on several factors such as training performance, performance metrics, validation performance, final results and whether it solves the problem. If it solves the business problem in the most efficient and consistent manner, it will be highly favourable as the model to deploy.\nFor this business case, the scooter rental forecasting is trained on given dataset and requires the model to predict accurate results to predict their total users. The model that predicts the best score is the Stack Regressor model. It has the best RMSLE and RMSE scores. XGBoost is not far behind and could be chosen as the prefered model in other circumstances. For now the deployed model will be the Stack Regressor model.\n'], 'url_profile': 'https://github.com/dwszai', 'info_list': ['C++', 'Updated Mar 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Python', 'Updated Aug 5, 2020', 'HTML', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020']}","{'location': 'Tamil Nadu, India', 'stats_list': [], 'contributions': '102 contributions\n        in the last year', 'description': ['Logistic-Regression---Diabetes Prediction\nPrediction of Diabetes(0/1) from various features using Pima Indians Diabetes dataset : https://data.world/uci/pima-indians-diabetes/workspace/file?filename=pima-indians-diabetes.data.csv\n'], 'url_profile': 'https://github.com/Bharathi-A-7', 'info_list': ['C++', 'Updated Mar 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Python', 'Updated Aug 5, 2020', 'HTML', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020']}","{'location': 'Richmond Va', 'stats_list': [], 'contributions': '131 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/GoldinLocks', 'info_list': ['C++', 'Updated Mar 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Python', 'Updated Aug 5, 2020', 'HTML', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '56 contributions\n        in the last year', 'description': [""Keras+ELMo regression\nThis notebook is a sample code of using Keras and ELMo (https://tfhub.dev/google/elmo/2) to predict the informativeness score of sentences.\nNote: This code sample does not contain the sentence data or model weights. The notebook only contains the preliminary testing results of lager work.\nInput\nSingle sentence formatted for ELMo (e.g., using < BOS > and < EOS > tokens to indicate the beginning and end of each sentence). The model also assumes using a cloze sentence (i.e., a sentence with a blank) as input. The cloze target is indicated as blank (_____).\nModel\nThe notebook contains a simple DNN regression model.\nOne thing to notice is that unlike the original ELMo's LSTM layer outputs, we customized the ELMo embedding layer to extract the contextual information as a single vector, concatenating the L->R vector before the cloze target, and the L<-R vector after the target word.\nOutput\nThe dependant variable contains continuous numeric values. This is based on the human annotations that we did not include in this sample code repository. For the quick comparison, we used Spearman's r to evlauate the prediction performance.\n""], 'url_profile': 'https://github.com/sungjinnam', 'info_list': ['C++', 'Updated Mar 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Python', 'Updated Aug 5, 2020', 'HTML', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['linear_regression_tutorial\nMaterials from our first meetup where we focused on linear regression using the Boston housing dataset.\n'], 'url_profile': 'https://github.com/PyLadiesKarlsruhe', 'info_list': ['C++', 'Updated Mar 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Python', 'Updated Aug 5, 2020', 'HTML', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/akruskal', 'info_list': ['C++', 'Updated Mar 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Python', 'Updated Aug 5, 2020', 'HTML', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020']}","{'location': 'Washington D.C. - Baltimore', 'stats_list': [], 'contributions': '41 contributions\n        in the last year', 'description': ['Multiple-Linear-Regression-practice\nMade a multiple linear regression model with potential list of venture capital investments using backward elimination Ordinary least squares approach. There are 3 numerical features (R&D cost, Administration cost, Marketing cost) and one categorical feature (city). Just practicing using statsmodels package and the OLS class to visualize backward elimination process to claim features with highest influence.\n'], 'url_profile': 'https://github.com/justinyu1', 'info_list': ['C++', 'Updated Mar 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 20, 2020', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 16, 2020', 'Python', 'Updated Aug 5, 2020', 'HTML', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '680 contributions\n        in the last year', 'description': ['visual-regression-demo\n'], 'url_profile': 'https://github.com/aaron-humerickhouse', 'info_list': ['Updated Mar 3, 2020', 'HTML', 'Updated Mar 4, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Dec 9, 2020', '2', 'Python', 'MIT license', 'Updated Feb 2, 2021', 'Python', 'Updated Mar 3, 2020', 'Python', 'Updated Jun 21, 2020', 'Jupyter Notebook', 'Updated Sep 15, 2020', 'R', 'Updated Apr 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sunilkumarsukesan', 'info_list': ['Updated Mar 3, 2020', 'HTML', 'Updated Mar 4, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Dec 9, 2020', '2', 'Python', 'MIT license', 'Updated Feb 2, 2021', 'Python', 'Updated Mar 3, 2020', 'Python', 'Updated Jun 21, 2020', 'Jupyter Notebook', 'Updated Sep 15, 2020', 'R', 'Updated Apr 30, 2020']}","{'location': 'Daegu, South Korea', 'stats_list': [], 'contributions': '140 contributions\n        in the last year', 'description': ['Regression Methods From Scratch\nSARCOS Data Set\nAll models with be ran using the SARCOS data set and there performance assessed. SARCOS is an inverse dynamics problem for a seven degrees-of-freedom SARCOS anthropomorphic robot arm. The data set contains 21 input dimensions and one output dimension.\nhttp://www.gaussianprocess.org/gpml/data/\nToy Problem\nIn order to validate the regression models a toy problem was created.  The toy problem was designed to be a simple noise free data set so that the results can be easily predicted. It was designed to be non-linear and two dimensional in order to test the capability of all the models. The following equation was used to generate 500 data points for the toy data set.\n\nEach time any data was used, it was shuffled and then split in to test and train data set with an 80:20 split respectively. A short python code was written in order to automate and make this process. This code also contains a feature scaling option which allows for the normalisation and standardisation of the data.\nK-NN\nThe first step for validating the k-NN model was to find the optimum k values for the toy data. Firstly, the data was normalised between 0 and 1. Normalising each parameter is important for Euclidean distance, as for each parameter to have equal importance the ranges must be identical. A range of k values from 2 to 40 was tested and the root mean square error (RMSE) and mean absolute error (MAE) of each k value was calculated. For this process multi-threading was utilised using the Joblib Python module. Multi-threading allows for different k values to run simultaneously, reducing the overall compute time. The optimum k was found to be 13. The low MAE and RSME values of 0.0213 and 0.0262 show that k-NN algorithm functions as intended.\n\nLinear Regression\nRandom Forest\n'], 'url_profile': 'https://github.com/mbodenham', 'info_list': ['Updated Mar 3, 2020', 'HTML', 'Updated Mar 4, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Dec 9, 2020', '2', 'Python', 'MIT license', 'Updated Feb 2, 2021', 'Python', 'Updated Mar 3, 2020', 'Python', 'Updated Jun 21, 2020', 'Jupyter Notebook', 'Updated Sep 15, 2020', 'R', 'Updated Apr 30, 2020']}","{'location': 'San Diego, CA', 'stats_list': [], 'contributions': '147 contributions\n        in the last year', 'description': ['Linear and Ridge Regression\nThe purpose of this project was to analyze and predict housing prices using attributes or features such as square footage, number of bedrooms, number of floors, and so on.\nAfter a successful import of the data into a Jupyter Notebook, it is nice to see what types of data we are going to analyze and aggregate.\n\nWhen cleaning up the data, I found it helpful to drop certain columns, in order to provide a cleaner dataframe.\n\nUpon further inspection of the dataframe, I found that missing (or NaN) values existed for a few bedroom and bathroom entries. For this exercise, I decided that replacing the missing values with the mean values of the numbers of bedrooms and bathrooms, respectively, would aid in our analysis.\n\nWhile the notebook itself includes some visualizations from the seaborn library, I found it helpful to look at what features had a strong correlation to the price.\n\nTo better understand correlation, I fit a linear regression model to predict house price using a feature like longitude coordinates,\n\nwhich predictably had a low R^2 value,\nand a feature like square feet of living space, which had a much higher R^2 value.\n\nI also practiced fitting the linear regression model to predict house price with multiple features!\n\nWith a list of tuples containing estimators and model constructors,\nI was able to create a pipeline object,\nfit the object with the multiple features,\nthen fit the model in order to calculate correlation.\n\nImporting some models from sci-kit learn, I split the data into training and testing sets.\n\nI then created and fit a Ridge regression object, setting its regularization parameter and calculated its R^2 value.\nWith a value of 0.64, I then attempted to improve accuracy by performing a second-order polynomial transform on both training and testing data.\nAfter refitting the Ridge regression object, the recalculated R^2 value increased to 0.70.\n\n'], 'url_profile': 'https://github.com/jbizzlefoshizzle', 'info_list': ['Updated Mar 3, 2020', 'HTML', 'Updated Mar 4, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Dec 9, 2020', '2', 'Python', 'MIT license', 'Updated Feb 2, 2021', 'Python', 'Updated Mar 3, 2020', 'Python', 'Updated Jun 21, 2020', 'Jupyter Notebook', 'Updated Sep 15, 2020', 'R', 'Updated Apr 30, 2020']}","{'location': 'Canada', 'stats_list': [], 'contributions': '196 contributions\n        in the last year', 'description': ['House price prediction\nInitially, the repo was started as House Regression prediction submission containing experimental jupyter notebooks which\ncan be found in the jupyter_notebooks submodule.\n\nModel buidling without Feature Selection Technique: Base Model\nModel buidling with Feature Selection Technique\n\nNow, expanded the repo with following steps for deploying the model in production:\n‚úÖ Exploratory Analysis(Feature Engineering, Feature Selection, Hyperparameter optimization, Model development)\n‚úÖ Building Machine Learning Pipeline(Scikit-learn pipeline Architecture)\n‚úÖ Model API development(Serving model through RESTful API)\n‚úÖ Deploying to production(Web App)\nIf you want reproduce the project, you can use requirements text file to setup a conda environment in the local machine.\nApp is live in the link here.\nNote: Checkout the publishingApp branch to see the code used for deployment of the project. Project is live through free tier from PythonAnywhere. Only the end-point of API is deployed in the PythonAnywhere server.\nCode and Resources Used\nPython Version: 3.7\nLibraries: pandas, numpy, matplotlib, seaborn, scikit-learn, randomforest-regressor, RandomizedSearchCV, scikit-learn pipeline, pytest\nBackend: Flask\nFrontend: HTML,CSS,JS\nResources: Udemy: Deployment of Machine Learning Models\n'], 'url_profile': 'https://github.com/Mattobad', 'info_list': ['Updated Mar 3, 2020', 'HTML', 'Updated Mar 4, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Dec 9, 2020', '2', 'Python', 'MIT license', 'Updated Feb 2, 2021', 'Python', 'Updated Mar 3, 2020', 'Python', 'Updated Jun 21, 2020', 'Jupyter Notebook', 'Updated Sep 15, 2020', 'R', 'Updated Apr 30, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '647 contributions\n        in the last year', 'description': ['      \n\n\n\n\n\nNatural Language Inference (NLI)\n\n    Natural Language Inference (NLI) using SNLI dataset.\n    \nExplore the repository¬ª\n\n\nView Problem Statement\nView Report\n\n\n\ntags : natural language inferencing, natural language processing, NLI, SNLI, MNLI, deep learning, tensorflow\n\nTable of Contents\n\nAbout the Project\n\nBuilt With\n\n\nGetting Started\n\nPrerequisites\nInstructions to run\n\n\nModels overview\n\nLogistic regression\nDeep learning models\n\n\nResults\nLicense\nContact\nAcknowledgements\n\nAbout The Project\nThis project is an implementation of the task of Natural Language Inference (NLI). In this task, we are given two sentences called premise and hypothesis. We are supposed to determine whether the ‚Äùhypothesis‚Äù is true (entailment), false (contradiction), or undetermined (neutral) provided that the ‚Äùpremise‚Äù is true. For this project, we have used the Stanford Natural Language Inference (SNLI) dataset. From the dataset, we use the files snli_1.0_train.jsonl for training the model and snli_1.0_test.jsonl for testing the model. From each entry in these files, we consider the fields corresponding to gold_label, sentence1 and sentence2. sentence1 serves as the premise and sentence2 serves as the hypothesis and gold_label serves as the relationship label. The following models were implemented and the performance was evaluated.\n\nLogistic regression classifier using TF-IDF features\nDeep model classifiers for text such as GRU, LSTM and SumEmbeddings\nBERT-based classifiers (Only implementation code is available. Unable to train and test due to scarcity of resources)\n\nBuilt With\nThis project was built with\n\npython v3.7\ntensorflow v2.1\nThe list of libraries used for developing this project is available at requirements.txt.\n\nGetting Started\nClone the repository into a local machine using\ngit clone https://github.com/vineeths96/Natural-Language-Inference\nPrerequisites\nPlease install required libraries by running the following command (preferably within a virtual environment).\npip install -r requirements.txt\nThe Stanford Natural Language Inference dataset has to be downloaded from here. THe file has to be extracted and the files snli_1.0_train.jsonl and snli_1.0_test.jsonl have to be saved in the ./input/ directory.  generate_meta_input() function from utils package has to be executed to generate cleaned and processed data. This can be done by uncommenting the corresponding code snippet in the main function when the program is run for the first time. This function takes care of text preprocessing which is explained in detail in the report. The processed data is stored as pickled list files at ./input/data_pickles/ directory.\nGloVe embedding from Stanford has been used throughout this project to embed the words to vectors.  The GloVe embedding with 6 billion token is used for embedding. The corresponding file has to be download from here and extracted to /input/embedding/ directory. This is required only if we intend to retrain the model.\nNB: The SNLI dataset contains entries without any gold_labels (label of ‚Äú-‚Äù). As of writing this report, it was chosen to ignore those entries and delete them from the dataset as they do not add any information.\nInstructions to run\nThe main.py is the interface to the program. It is programmed to run in two modes ‚Äì train mode and test mode. The main.py file takes two optional command line argument, one to specify the mode of execution ‚Äì whether to train or test model,  and another one to specify the model architecture to be used. The main.py, when executed without any arguments, enters into testing the logistic regression model and the SumEmbeddings deep model, and produces the output files deep_model.txt and tfidf.txt respectively.\nThe main.py when executed with the (optional argument) --train-model enters into training mode and saves the models after training. The model to be used for training/testing can be specified with the (optional argument) --model-name.  Model name can be one among the following: BiGRU, BiLSTM, SumEmbeddings, and BERT.\nTrain mode\npython main.py --train-model --model-name <model_name>\nTest mode\npython main.py --model-name <model_name>\nModels overview\nLogistic regression\nLogistic regression model was trained using TF-IDF (Term Frequency-Inverse Document Frequency) features obtained using sklearn python library. The feature vector used to train the model is obtained by concatenating the TF-IDF vectors of premise and hypothesis. The model is trained (fit) using L-BFGS (Limited memory - Broyden‚ÄìFletcher‚ÄìGoldfarb‚ÄìShanno algorithm) solver with a maximum iteration limit of 1000. The trained (fit) model is saved at ./models/LR.pickle for future uses and testing. The model attains an accuracy of 63.38% and the results of prediction are written to a file at ./tfidf.txt.\nDeep learning models\nThe first step towards implementing a deep model for text is to convert each atomic discrete entity in the input (words or characters) into real vectors from $ \\mathbf{R}^{d} $ so that their semantics are captured meaningfully. For this purpose, GloVe embedding has been used in this project. Different pretrained GloVe embeddings are available and the embedding chosen for this project is the one with 6 billion tokens trained over Wikipedia corpus. Once the training is completed the model is stored at ./model/ directory as a h5 file.\nBiGRU (Bidirectional Gated Reccurent Unit)\nThe model attains an accuracy of 78.58% and the output text file and plots are saves at ./results/BiGRU/.\nBiLSTM (Bidirectional Long  Short Term Memory)\nThe model attains an accuracy of 76.38% and the output text file and plots are saves at ./results/BiLSTM/.\nSumEmbeddings\nA SumEmbedding lambda layer, which sums up all the embedding vectors in the sentence is used in this model. The model attains an accuracy of 80.38% and the output text file and plots are saves at ./results/SumEmbeddings/. Since this is the best performing model the output text files are stored at ./deep_model.txt as well\nBERT (Code-only)\nAn experimental Huggingface transformer based BERT is also implemented in the project. All the codes work and the model trains and tests, but the training process is computationally very expensive. Even after using Google Colab TPU and dividing the data into smaller parts, the training was unable to be completed due to Google usage time restrictions. Hence, the performance analysis or plots are not attached. With minor modifications to code, we can use any transformers based approach supported by Huggingface. Due to the scarcity of computing resources, I have not tried using other transformer based approaches.\nResults\n\n\n\nModel\nAccuracy\n\n\n\n\nLogistic regression\n63.38%\n\n\nBiGRU\n78.58%\n\n\nBiLSTM\n76.38%\n\n\nSumEmbeddings\n80.38%\n\n\n\nLicense\nDistributed under the MIT License. See LICENSE for more information.\nContact\nVineeth S  - vs96codes@gmail.com\nProject Link: https://github.com/vineeths96/Natural-Language-Inference\nAcknowledgements\n\n\nSNLI Dataset\n\nSamuel R. Bowman,  Gabor Angeli,  Christopher Potts,  and Christopher D. Manning.  2015.  A large annotated corpus for learning natural language inference.  In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP).\n\n\n\nGloVe Embeddings\n\nJeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014.  GloVe: Global Vectors for Word Representation\n\n\n\n'], 'url_profile': 'https://github.com/vineeths96', 'info_list': ['Updated Mar 3, 2020', 'HTML', 'Updated Mar 4, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Dec 9, 2020', '2', 'Python', 'MIT license', 'Updated Feb 2, 2021', 'Python', 'Updated Mar 3, 2020', 'Python', 'Updated Jun 21, 2020', 'Jupyter Notebook', 'Updated Sep 15, 2020', 'R', 'Updated Apr 30, 2020']}","{'location': 'banglore', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['Car-Data-Analysis\nConverted Categorical column into numerical values with the help of dummy variable and Performed Linear Regression\n'], 'url_profile': 'https://github.com/yasinthree', 'info_list': ['Updated Mar 3, 2020', 'HTML', 'Updated Mar 4, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Dec 9, 2020', '2', 'Python', 'MIT license', 'Updated Feb 2, 2021', 'Python', 'Updated Mar 3, 2020', 'Python', 'Updated Jun 21, 2020', 'Jupyter Notebook', 'Updated Sep 15, 2020', 'R', 'Updated Apr 30, 2020']}","{'location': 'Irvine, CA', 'stats_list': [], 'contributions': '62 contributions\n        in the last year', 'description': ['Multiple Linear Regression Web App\n'], 'url_profile': 'https://github.com/avinashpai', 'info_list': ['Updated Mar 3, 2020', 'HTML', 'Updated Mar 4, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Dec 9, 2020', '2', 'Python', 'MIT license', 'Updated Feb 2, 2021', 'Python', 'Updated Mar 3, 2020', 'Python', 'Updated Jun 21, 2020', 'Jupyter Notebook', 'Updated Sep 15, 2020', 'R', 'Updated Apr 30, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['transfermarkt\nA linear regression model predicting the value of a soccer player.\n'], 'url_profile': 'https://github.com/pkopka', 'info_list': ['Updated Mar 3, 2020', 'HTML', 'Updated Mar 4, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Dec 9, 2020', '2', 'Python', 'MIT license', 'Updated Feb 2, 2021', 'Python', 'Updated Mar 3, 2020', 'Python', 'Updated Jun 21, 2020', 'Jupyter Notebook', 'Updated Sep 15, 2020', 'R', 'Updated Apr 30, 2020']}","{'location': 'Bergen, Norway', 'stats_list': [], 'contributions': '422 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/emoen', 'info_list': ['Updated Mar 3, 2020', 'HTML', 'Updated Mar 4, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 13, 2020', 'Jupyter Notebook', 'Updated Dec 9, 2020', '2', 'Python', 'MIT license', 'Updated Feb 2, 2021', 'Python', 'Updated Mar 3, 2020', 'Python', 'Updated Jun 21, 2020', 'Jupyter Notebook', 'Updated Sep 15, 2020', 'R', 'Updated Apr 30, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '32 contributions\n        in the last year', 'description': ['R_SAS_Conversion\nSome R code I used in regression analysis for a similar output as SAS functions.\n'], 'url_profile': 'https://github.com/GinnyGoGo', 'info_list': ['Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'R', 'Updated Mar 3, 2020', 'Python', 'Updated Nov 4, 2020', 'Jupyter Notebook', 'Updated Sep 21, 2020', '1', 'Python', 'Updated Mar 7, 2020', 'MATLAB', 'Updated Mar 13, 2020', 'Updated Mar 7, 2020', '1', 'Jupyter Notebook', 'Updated Aug 7, 2020']}","{'location': 'New York', 'stats_list': [], 'contributions': '163 contributions\n        in the last year', 'description': ['Time Seriers Analysis\nTime Series Forecasting and Linear Regression Modeling analysis for Japanese Yen\n\nBackground\nThe financial departments of large companies often deal with foreign currency transactions while doing international business. As a result, they are always looking for anything that can help them better understand the future direction and risk of various currencies. Hedge funds, too, are keenly interested in anything that will give them a consistent edge in predicting currency movements.\nPrerequisites\nNumpy\nPandas\nPathlib\nsklearn.linear_model\nsklearn.metrics\nstatsmodels\narch\nTime-Series Forecasting\nloaded historical Dollar-Yen exchange rate futures data and ploted Yen futures settle prices.\n\nUsing a Hodrick-Prescott Filter, decomposed the Settle price into a trend and noise. and ploted settle Vs trend\n\nUsing futures Settle Returns, estimated an ARMA model using Sklearn.\n\nPlot for 5 Day Returns Forecast from ARMA model\n\nQuestion: Based on the p-value, is the model a good fit?\nAnswer: Based on the model summary table, none of the p-values are below 0.05, which suggests that we may need to add additional input variables or adjust the lag order.\nAssuming we were confident with this model\'s ability to predict, however, our model-based forecast for the next 5 days is of positive returns for the Yen.\nForecasting the Settle Price using an ARIMA Model\nUsing the raw Yen Settle Price, estimated an ARIMA model.\n\nPlot for 5 Day Futures Price Forecast\n\nVolatility Forecasting with GARCH\nUsing futures Settle Returns, estimated an GARCH model\n\nCovariance estimator: robust\nNote, our p-values for GARCH and volatility forecasts tend to be much lower than our ARMA/ARIMA return and price forecasts. In particular, here we have all p-values of less than 0.05, except for alpha(2), indicating overall a much better model performance.\nLinear Regression Forecasting\n\nbuilt a SKLearn linear regression model to predict Yen futures (""settle"") returns with lagged Yen futures returns.\nFitting a Linear Regression Model.\nMaking predictions using the testing data.\nOut-of-sample performance.\nIn-sample performance.\n\nThe first 20 predictions vs the true values\n\nPlease check conclusions.md to read the full Analysis\nBuilt With\nPython 3\n'], 'url_profile': 'https://github.com/mostafajoma', 'info_list': ['Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'R', 'Updated Mar 3, 2020', 'Python', 'Updated Nov 4, 2020', 'Jupyter Notebook', 'Updated Sep 21, 2020', '1', 'Python', 'Updated Mar 7, 2020', 'MATLAB', 'Updated Mar 13, 2020', 'Updated Mar 7, 2020', '1', 'Jupyter Notebook', 'Updated Aug 7, 2020']}","{'location': 'Guangzhou, China', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': ['Data-Analysis-wit-Python\nThis is all about examples of data analysis with python while building regression models\n'], 'url_profile': 'https://github.com/EDJOUKOU', 'info_list': ['Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'R', 'Updated Mar 3, 2020', 'Python', 'Updated Nov 4, 2020', 'Jupyter Notebook', 'Updated Sep 21, 2020', '1', 'Python', 'Updated Mar 7, 2020', 'MATLAB', 'Updated Mar 13, 2020', 'Updated Mar 7, 2020', '1', 'Jupyter Notebook', 'Updated Aug 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '47 contributions\n        in the last year', 'description': ['Logistic-Regression-Sample-in-R\nA small example of Logistic Regression in R for a data set.\n'], 'url_profile': 'https://github.com/ShreenidhiN', 'info_list': ['Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'R', 'Updated Mar 3, 2020', 'Python', 'Updated Nov 4, 2020', 'Jupyter Notebook', 'Updated Sep 21, 2020', '1', 'Python', 'Updated Mar 7, 2020', 'MATLAB', 'Updated Mar 13, 2020', 'Updated Mar 7, 2020', '1', 'Jupyter Notebook', 'Updated Aug 7, 2020']}","{'location': 'Paris', 'stats_list': [], 'contributions': '284 contributions\n        in the last year', 'description': ['DSLR (subject)\nIntroduction\nOn no! Since its creation, the famous school of wizards, Hogwarts, had never known such an offense. The forces of evil have bewitched the Sorting Hat. It no longer responds, and is unable to fulfill his role of sorting the students to the houses.\nThe new academic year is approaching. Gladly, the Professor McGonagall was able to take action in such a stressful situation, since it is impossible for Hogwarts not to welcome new students. . . She decided to call on you, a muggle ""datascientist"" who is able to create miracles with the tool which all muggles know how to use: a ""computer"".\nDespite the intrinsic reluctance of many wizards, the director of the school welcomes you to his office to explain the situation. You are here because his informant discovered that you are able to recreate a magic Sorting Hat using your muggle tools. You explain to him that in order for your ""muggle"" tools to work, you need students data. Hesitantly, Professor McGonagall gives you a dusty spellbook. Fortunately for you, a simple ""Digitalis!"" and the book turned into a USB stick.\nObjective\nIn this project DataScience x Logistic Regression, you will continue your exploration of Machine Learning by discovering different tools.\nThe use of the term DataScience in the title will be clearly considered by some to be abusive. That is true. We do not pretend to give you all the basics of DataScience in this topic. The subject is vast. We will only see here some bases which seemed to us useful for data exploration before sending it to the machine learning algorithm.\nYou will implement a linear classification model, as a continuation of the subject linear regression : a logistic regression. We also encourage you a lot to create a machine learning toolkit while you will move along the branch.\nSummarizing :\n‚Ä¢ You will learn how to read a data set, to visualize it in different ways, to select and\nclean unnecessary information from your data.\n‚Ä¢ You will train a logistic regression that will solve classification problem.\nFormulas\n\n\n\n\nInstall\ngit clone https://github.com/ChokMania/DSLR.git\npython -m pip install -r lib.txt\nUsage\nDescribe\nDescribes the dataset\npython describe.py ""./resources/dataset_train.csv""\n\n\nThis will display a description of the dataset, similar to pandas.describe\n\n\nHistogram\nDisplays a histogram answering the following question:\nWhich Hogwarts course has a homogeneous score distribution between all four houses ?\npython histogram.py ""./resources/dataset_train.csv""\n\n\nThis will open a new window with a histogram\n\n\nScatter Plot\nDisplays a scatter plot answering the following question:\nWhat are the two features that are similar ?\npython scatter_plot.py ""./resources/dataset_train.csv""\n\n\nThis will open a new window with a scatter plot\n\n\nPair Plot\nDisplays a pair plot or scatter plot matrix answering the following question:\nFrom this visualization, what features are you going to use for your logistic regression?\npython pair_plot.py ""./resources/dataset_train.csv""\n\n\nThis will open a new window with a pair plot using seaborn\n\n\nTrain\nTrains our model\npython logreg_train.py -h\nusage: logreg_train.py [-h] [-v] [-vi House N_feature1 N_feature2] dataset\n\nTrains our model with the specified dataset\n\npositional arguments:\n  dataset: dataset, needs to be a csv\n\noptional arguments:\n  -h, --help: show this help message and exit\n  -v, --verbose: display in real time actions of training\n  -vi House N_feature1 N_feature2: display data of one house in a separate windows\n\npython logreg_train.py ""./resources/dataset_train.csv"" -vi ""Ravenclaw"" 1 2\n\n\nThis will train our model with the dataset.\n\n\nAdding the -vi option will display a graph on the training session in a ""one vs all"" format\n\n\n\n\nPredict\nPredicts a house with our model\npython logreg_predict.py -h\nusage: logreg_predict.py [-h] [-a] [-p] dataset weights\n\npredicts student\'s house with our model\n\npositional arguments:\n  dataset: dataset, needs to be a csv\n  weights: weights, needs to be a csv\n\noptional arguments:\n  -h, --help: show this help message and exit\n  -a, --accuracy: show accuracy for dataset_train\n  -p, --piechart: print a piechart for the results\n\npython logreg_predict.py ""./resources/dataset_test.csv"" ""./weights.csv"" -p\n\n\nThis will predict a house for each student in the test dataset, and with -p option, display a pie chart\n\n\nAuthors\nAleksi Gautier et Julien Dumay\n'], 'url_profile': 'https://github.com/ChokMania', 'info_list': ['Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'R', 'Updated Mar 3, 2020', 'Python', 'Updated Nov 4, 2020', 'Jupyter Notebook', 'Updated Sep 21, 2020', '1', 'Python', 'Updated Mar 7, 2020', 'MATLAB', 'Updated Mar 13, 2020', 'Updated Mar 7, 2020', '1', 'Jupyter Notebook', 'Updated Aug 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/AhmedAbdAllah11', 'info_list': ['Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'R', 'Updated Mar 3, 2020', 'Python', 'Updated Nov 4, 2020', 'Jupyter Notebook', 'Updated Sep 21, 2020', '1', 'Python', 'Updated Mar 7, 2020', 'MATLAB', 'Updated Mar 13, 2020', 'Updated Mar 7, 2020', '1', 'Jupyter Notebook', 'Updated Aug 7, 2020']}","{'location': 'Mumbai', 'stats_list': [], 'contributions': '43 contributions\n        in the last year', 'description': ['PredictionModel\nThis contains code related to my selfproject on applying various regression Models on datasets.\n'], 'url_profile': 'https://github.com/Gmishra2000', 'info_list': ['Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'R', 'Updated Mar 3, 2020', 'Python', 'Updated Nov 4, 2020', 'Jupyter Notebook', 'Updated Sep 21, 2020', '1', 'Python', 'Updated Mar 7, 2020', 'MATLAB', 'Updated Mar 13, 2020', 'Updated Mar 7, 2020', '1', 'Jupyter Notebook', 'Updated Aug 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '150 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ameliorater', 'info_list': ['Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'R', 'Updated Mar 3, 2020', 'Python', 'Updated Nov 4, 2020', 'Jupyter Notebook', 'Updated Sep 21, 2020', '1', 'Python', 'Updated Mar 7, 2020', 'MATLAB', 'Updated Mar 13, 2020', 'Updated Mar 7, 2020', '1', 'Jupyter Notebook', 'Updated Aug 7, 2020']}","{'location': 'California', 'stats_list': [], 'contributions': '96 contributions\n        in the last year', 'description': ['  __   ___  ___  _    _  ____ ___      ___  ____ __   ___  ____ __  __  _  ___  _     \n / /\\ | |_)| |_)| |  | || |_ | | \\    | |_)| |_ / /`_| |_)| |_ ( (`( (`| |/ / \\| |\\ | \n/_/--\\|_|  |_|  |_|__|_||_|__|_|_/    |_| \\|_|__\\_\\_/|_| \\|_|___)_)_)_)|_|\\_\\_/|_| \\| \n\n\nSTAT5302 Projects\nCoursework for STAT 5302 (Applied Regression Analysis) at the University of Minnesota. Course details can be found at: http://classinfo.umn.edu/?subject=STAT&catalog_nbr=5302&term=1189. Taught with RMarkdown & Arc software. Course Syllabus details: http://users.stat.umn.edu/~rdcook/5302S11/syllabus.html. Course taken in Fall 2017 semester.\n'], 'url_profile': 'https://github.com/bperry09', 'info_list': ['Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'R', 'Updated Mar 3, 2020', 'Python', 'Updated Nov 4, 2020', 'Jupyter Notebook', 'Updated Sep 21, 2020', '1', 'Python', 'Updated Mar 7, 2020', 'MATLAB', 'Updated Mar 13, 2020', 'Updated Mar 7, 2020', '1', 'Jupyter Notebook', 'Updated Aug 7, 2020']}","{'location': 'San Francisco', 'stats_list': [], 'contributions': '36 contributions\n        in the last year', 'description': ['My first machine learning project.\nThis is my first machine learning project for my Machine Learning class at Hult International Business School.\nThis repository consists of 4 files in the respective order:\n\nDictionary for the different variables in the dataset.\nThe analysed dataset.\nRead Me\nThe Python code\nThe insight report.\n\nI will be redoing this analysis at some point, given the code is messy and sometimes unclear.\n'], 'url_profile': 'https://github.com/Wager95', 'info_list': ['Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'R', 'Updated Mar 3, 2020', 'Python', 'Updated Nov 4, 2020', 'Jupyter Notebook', 'Updated Sep 21, 2020', '1', 'Python', 'Updated Mar 7, 2020', 'MATLAB', 'Updated Mar 13, 2020', 'Updated Mar 7, 2020', '1', 'Jupyter Notebook', 'Updated Aug 7, 2020']}"
"{'location': 'Texas', 'stats_list': [], 'contributions': '37 contributions\n        in the last year', 'description': [""nlp-sentimentAnalysis\nLogistic Regression model to predict sentiment on sentences in a corpus and displays top k features\nInput: sentence, 0 or 1. per line in document.\nWe create tuples of (sentence, 0 or 1). 0 for negative 1 for positive sentiment.\nSpecial case for words that begin, end, or are between a single quote '. We match them with a regex pattern and handle by adding 'EDIT_' token in front of word.\nWe also have to match negative words using regex and tag these tokens with 'NOT_'. We do this after we encounter a negation token and until we encounter an end negation token.\nWe then transform these features into vector format from scratch to build X train matrix, Y train label vector, and X test matrix.\nWe normalize the X_train matrix and X_test matrix seperately.\nWe chose a Logistic Regression Model to train and predict the test set.\nWe used the following evaluation scores for predictions: Precision, Recall, and Fmeasure.\nThere is also a function at the end to display top K features for a trained model.\n""], 'url_profile': 'https://github.com/cwilden', 'info_list': ['Python', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', '1', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'JavaScript', 'MIT license', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Jun 12, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020']}","{'location': 'UK', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['SDG-Modelling\nPreliminary PhD Research: Using factor analyses and regression modelling on UN Global Goals data.\nThis submission consists of working with World Bank and UN data relating to the SDGs. Data wrangling, restructuring, visualisation, modelling, and Principal Component Analysis is performed.\nData\nPlease note that the raw data from the UN Database that was used in this repo can be accessed directly from here and is not included. This should be placed in /data/new.\nStructure\nIt is comprised of 6 components:\n\n3 Jupyter Notebooks which wrangle and visualise data [0,1,2]\n2 Jupyter Notebooks which perform predictions based on models previously developed for Child Mortality [3,4]\nOne R script which performs PCA factor analysis on different datasets\n\nPackages The only additional Python package used is the World Bank API:\n\nwbdata\n\nThe additional R packages used are:\n\nPerformanceAnalytics\nFactoMineR\nfactoextra\nlavaan\n\nReferences\nThe work is based on the following papers:\n\nSpaiser, V., Ranganathan, S., Bali Swain, R. and Sumpter, D.J.T. (2017): The sustainable development oxymoron: quantifying and modelling the incompatibility of sustainable development goals. International Journal of Sustainable Development & World Ecology, 24(6), 457-470.\nRanganathan, S., Nicolis, S.C., Bali Swain, R. and Sumpter, D.J.T. (2017): Setting development goals using stochastic dynamical system models. PLoS ONE, 12(2)\n\nData Sources\n\nOld data from the Spaiser - ‚Äúoxymoron‚Äù folder\nData provided by the UN directly regarding four countries (Bangladesh, Laos, Ethiopia, Tanzania) ‚Äì ‚Äúvito‚Äù folder\nThe World Bank - Accessed through API to fetch data for given targets\nThe World Bank - wb_income_groups.csv, wb_income_groups_2013.csv\nUNSTATS SDG Database - should be downloaded from UNSTATS and named SDG_all_countries_90_19.csv\nUN Data - UN_country_codes.csv\nThe Heritage Foundation Index of Economic Freedom - heritage_data_2019.csv\nOur World in Data - OWiD_co2-emissions-per-capita.csv\n\n'], 'url_profile': 'https://github.com/amiraliemami', 'info_list': ['Python', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', '1', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'JavaScript', 'MIT license', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Jun 12, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020']}","{'location': 'Baku, Azerbaijan', 'stats_list': [], 'contributions': '838 contributions\n        in the last year', 'description': ['Breast-Cancer-Detection-Python\nBreast Cancer Detection with Decision Tree, Logistic Regression, and Random Forest algorithms\n'], 'url_profile': 'https://github.com/elvinaqa', 'info_list': ['Python', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', '1', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'JavaScript', 'MIT license', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Jun 12, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '156 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/bql20000', 'info_list': ['Python', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', '1', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'JavaScript', 'MIT license', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Jun 12, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020']}","{'location': 'M√ºnchen, Bayern', 'stats_list': [], 'contributions': '102 contributions\n        in the last year', 'description': [""Yet another repository with book exercises\nSolutions to exercises from a great book Data analysis using regression and multilevel hierarchical models.\nThe book implies it to be in R, but due to python being my main tool for dealing with data, I wanted to work on both, the book exercises and improving my python statistical toolbox skills.\nAlso, the book is awesome, goes into subtle details of linear methods making them much more powerful and interpretable. Makes a lot of modern ML less magical :)\nI was doing it with jupyterlab. Some charts, formulae and layouts might be broken in github viewer or normal jupyter.\nInstallation and running the examples\n\nEnsure that your system has python 3.7 and pipenv installed. I'm pretty opinionated about environment management in python, pipenv provided me the best experience out of all tools I tried.\nClone the repository and enter the directory with it.\nRun pipenv --three install.\nAfter pipenv finishes creating the environment, enter it with pipenv shell while still being inside of the repository directory.\nRun jupyter lab and explore the notebooks.\n\nExercise completion\n\n\n\nChapter\nEx1\nEx2\nEx3\nEx4\nEx5\nEx6\nEx7\nEx8\nEx9\nEx10\nEx11\n\n\n\n\n1\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n\n\n2\n-\nV\nV\nV\nV\n-\n-\n-\n-\n-\n-\n\n\n3\nV\nV\nV\nV\nV\n-\n-\n-\n-\n-\n-\n\n\n4\nV\nV\nV\nV\nV\nV\n*\n*\n-\n-\n-\n\n\n5\nV\nV\nV\nV\nV\nV\nV\nV\nV\nV\n!\n\n\n6 N. 2\nV\n*\n-\n-\n-\n-\n-\n-\n-\n-\n-\n\n\n7\nV\nV\nV\nV\n-\n-\n-\n-\n-\n-\n-\n\n\n8\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n\n\n9\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n\n\n10\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n\n\n11\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n\n\n12\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n\n\n13\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n\n\n14\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n\n\n15\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n\n\n16\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n\n\n17\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n\n\n18\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n\n\n19\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n\n\n20\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n\n\n21\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n\n\n22\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n\n\n23\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n\n\n24\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n\n\n25\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n\n\n\nLegend:\nX - not started\n* - in progress\nV - completed\n! - blocked\n\nNotes\n\nEx 5.11 is blocked due to the dataset from the website has different structure from the examples in the book.\nSome models for chapter 6 are not present in statsmodels or sklearn. I'll skip it for now as the general approaches and ideas are clear so far.\nEx 7.4 to be reviewed and the sim function to be extracted separately\n\n""], 'url_profile': 'https://github.com/jezzarax', 'info_list': ['Python', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', '1', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'JavaScript', 'MIT license', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Jun 12, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['Battery-Life-Prediction\nMachine Learning Project 1: Battery Life Prediction Using Simple Linear Regression\n'], 'url_profile': 'https://github.com/parameswar-kotari', 'info_list': ['Python', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', '1', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'JavaScript', 'MIT license', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Jun 12, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['cancer-prediction\nClassify whether a cancer is malignant or benign using Logistic Regression\n'], 'url_profile': 'https://github.com/alphago7', 'info_list': ['Python', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', '1', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'JavaScript', 'MIT license', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Jun 12, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020']}","{'location': 'bit mesra ranchi india', 'stats_list': [], 'contributions': '153 contributions\n        in the last year', 'description': ['car-regression-in-tensorflow.js\nthis is a simple regression model in tensorflow js built for learning purpose.\nthis tensorflow.js based project which runs on the browser and preforn machine learning\n'], 'url_profile': 'https://github.com/rahulharlalka', 'info_list': ['Python', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', '1', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'JavaScript', 'MIT license', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Jun 12, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020']}","{'location': 'Berlin', 'stats_list': [], 'contributions': '46 contributions\n        in the last year', 'description': [""HousingPrices\nThis repository contains my solution to Kaggle's Housing Prices: Advanced Regression Techniques competition. My solution is focused on getting extensive insight into the data in order to develop meaningful strategies for filling in the missing data and fixing data abnormalities.\nAfter data cleaning is performed, an ensemble regression model is developed that consists of 5 individual regressors. Their predictions are averaged in order to obtain the final prediction. This tecnhique achieves reasonably good results on the dataset and got me placed in the top 12% amongst all competition participants.\nRepository contents:\n\ndata: training and test data files\nhousing_prices.ipynb: Jupyter notebbok containing the solution\n\nThe solution is structured as follows:\n\nEnvironment Setup\nData Exploration\nData Cleaning\nModel Training\nResults\n\nThe regressors that are ensembled are:\n\nLasso\nElasticNet\nRidge Regression\nGradient Boosting Regression\nXGBoost Regressor\nLightGBM Regressor\n\n""], 'url_profile': 'https://github.com/ssykiotis', 'info_list': ['Python', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', '1', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'JavaScript', 'MIT license', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Jun 12, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '363 contributions\n        in the last year', 'description': ['import pandas as pd\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(""ignore"")\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.rcParams[""figure.figsize""] = (14,7)\n1- Examine the distribution and importance of key variables including visual and statistical analysis.\nWe will begin by importing the data into a pandas data frame and examining the data\ndf = pd.read_csv(""Telecom-Usage-Details.csv"")\ndf.columns\nIndex([\'customerID\', \'gender\', \'SeniorCitizen\', \'Partner\', \'Dependents\',\n       \'tenure\', \'PhoneService\', \'MultipleLines\', \'InternetService\',\n       \'OnlineSecurity\', \'OnlineBackup\', \'DeviceProtection\', \'TechSupport\',\n       \'StreamingTV\', \'StreamingMovies\', \'Contract\', \'PaperlessBilling\',\n       \'PaymentMethod\', \'MonthlyCharges\', \'TotalCharges\', \'Churn\'],\n      dtype=\'object\')\n\ndf.info()\n<class \'pandas.core.frame.DataFrame\'>\nRangeIndex: 7043 entries, 0 to 7042\nData columns (total 21 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   customerID        7043 non-null   object \n 1   gender            7043 non-null   object \n 2   SeniorCitizen     7043 non-null   int64  \n 3   Partner           7043 non-null   object \n 4   Dependents        7043 non-null   object \n 5   tenure            7043 non-null   int64  \n 6   PhoneService      7043 non-null   object \n 7   MultipleLines     7043 non-null   object \n 8   InternetService   7043 non-null   object \n 9   OnlineSecurity    7043 non-null   object \n 10  OnlineBackup      7043 non-null   object \n 11  DeviceProtection  7043 non-null   object \n 12  TechSupport       7043 non-null   object \n 13  StreamingTV       7043 non-null   object \n 14  StreamingMovies   7043 non-null   object \n 15  Contract          7043 non-null   object \n 16  PaperlessBilling  7043 non-null   object \n 17  PaymentMethod     7043 non-null   object \n 18  MonthlyCharges    7043 non-null   float64\n 19  TotalCharges      7043 non-null   object \n 20  Churn             7043 non-null   object \ndtypes: float64(1), int64(2), object(18)\nmemory usage: 1.1+ MB\n\nWe notice somethings here:\n\nSeniorCitizen Column has the Dtype of int64 although it\'s a categorical variable\nTotalCharges values are not numerical\n\nWe will start analyzing the data and fixing these issues.\nIn the TotalCharges some records are "" "", we will replace them with 0 and change the column to numeric values.\nIn the SeniorCitizen column we will replace 1 values with ""yes"" and 0 values with ""no"".\nreplace_empty_with_0 = lambda x: ""0"" if x[0] =="" "" else x\ndf.TotalCharges = df.TotalCharges.apply(replace_empty_with_0)\ndf.TotalCharges = pd.to_numeric(df.TotalCharges)\n\nencode_SeniorCitizen = lambda x: ""yes"" if x ==1 else ""no""\ndf.SeniorCitizen = df.SeniorCitizen.apply(encode_SeniorCitizen)\ndf.describe()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\ntenure\nMonthlyCharges\nTotalCharges\n\n\n\n\ncount\n7043.000000\n7043.000000\n7043.000000\n\n\nmean\n32.371149\n64.761692\n2279.734304\n\n\nstd\n24.559481\n30.090047\n2266.794470\n\n\nmin\n0.000000\n18.250000\n0.000000\n\n\n25%\n9.000000\n35.500000\n398.550000\n\n\n50%\n29.000000\n70.350000\n1394.550000\n\n\n75%\n55.000000\n89.850000\n3786.600000\n\n\nmax\n72.000000\n118.750000\n8684.800000\n\n\n\n\nNow all the features are the correct type, we will then make sure that we don\'t have any duplicate customerID or any duplicate values\ndf = df.dropna()\nlen(df.customerID.value_counts())\n7043\n\nGood! that matches the counts above our data is in good shape. We will start analyzing.\nsns.countplot(x=""gender"", data=df)\n<matplotlib.axes._subplots.AxesSubplot at 0x7fd84d38d250>\n\n\nWe have more or less equal distribution of male and female\nsns.countplot(x=""SeniorCitizen"", data=df)\n<matplotlib.axes._subplots.AxesSubplot at 0x7fd84b44ac50>\n\n\nOnly a minority of our customers are senior citizens.\nsns.countplot(x=""gender"", hue=""SeniorCitizen"", data=df)\n<matplotlib.axes._subplots.AxesSubplot at 0x7fd84cfea150>\n\n\nbut we have the same distribution of ages across male and female customers\nsns.countplot(x=""Dependents"", data= df)\n<matplotlib.axes._subplots.AxesSubplot at 0x7fd84b38b190>\n\n\nWe have more customers with no dependents than without\nsns.countplot(x=""gender"", hue=""Dependents"", data=df)\n<matplotlib.axes._subplots.AxesSubplot at 0x7fd84b2dda90>\n\n\nBut again across the genders the distribution is equal.\nsns.countplot(x=""SeniorCitizen"", hue=""Dependents"", data=df)\n<matplotlib.axes._subplots.AxesSubplot at 0x7fd84b256910>\n\n\nWe notice here that nonsenior citizens are more likely to have dependents.\nNow that we have a good understanding if the demographic data of our customers we will start looking at the services they use\ndf_services = df.iloc[:,6:14]\ndf_services\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPhoneService\nMultipleLines\nInternetService\nOnlineSecurity\nOnlineBackup\nDeviceProtection\nTechSupport\nStreamingTV\n\n\n\n\n0\nNo\nNo phone service\nDSL\nNo\nYes\nNo\nNo\nNo\n\n\n1\nYes\nNo\nDSL\nYes\nNo\nYes\nNo\nNo\n\n\n2\nYes\nNo\nDSL\nYes\nYes\nNo\nNo\nNo\n\n\n3\nNo\nNo phone service\nDSL\nYes\nNo\nYes\nYes\nNo\n\n\n4\nYes\nNo\nFiber optic\nNo\nNo\nNo\nNo\nNo\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n7038\nYes\nYes\nDSL\nYes\nNo\nYes\nYes\nYes\n\n\n7039\nYes\nYes\nFiber optic\nNo\nYes\nYes\nNo\nYes\n\n\n7040\nNo\nNo phone service\nDSL\nYes\nNo\nNo\nNo\nNo\n\n\n7041\nYes\nYes\nFiber optic\nNo\nNo\nNo\nNo\nNo\n\n\n7042\nYes\nNo\nFiber optic\nYes\nNo\nYes\nYes\nYes\n\n\n\n7043 rows √ó 8 columns\n\nsns.countplot(x=""PhoneService"", hue=""MultipleLines"", data=df)\n<matplotlib.axes._subplots.AxesSubplot at 0x7fd84b1cab10>\n\n\nMost of our customers have phone service and a good percentage of them have multiple lines\nsns.countplot(x=""InternetService"", data=df)\n<matplotlib.axes._subplots.AxesSubplot at 0x7fd84b14fb50>\n\n\nHowever, a significant number of our customers don\'t have internet service with us. Fiber Optic service is more popular than DSL\ndf_services_no_internet = df_services[df_services.InternetService == ""No""]\nsns.countplot(x=""PhoneService"", hue=""MultipleLines"", data=df_services_no_internet)\n<matplotlib.axes._subplots.AxesSubplot at 0x7fd84b0a69d0>\n\n\nOur customers with no internet service mostly have single lines meaning they are likely to be low-value customers\ndf_internet = df_services[df_services.InternetService != ""No""]\nsns.countplot(x=""InternetService"", hue=""OnlineSecurity"", data=df_internet)\n<matplotlib.axes._subplots.AxesSubplot at 0x7fd84b020910>\n\n\nOur online security service is more popular with our DSL users compared to Fiber Optic users.\nsns.countplot(x=""InternetService"", hue=""OnlineBackup"", data=df_internet)\n<matplotlib.axes._subplots.AxesSubplot at 0x7fd84affad90>\n\n\nHowever our Online Backup service is more popular with Fiber optic users. Possibly because they can benefit from faster speeds\nsns.countplot(x=""InternetService"", hue=""DeviceProtection"", data=df_internet)\n<matplotlib.axes._subplots.AxesSubplot at 0x7fd84af62490>\n\n\nsimilar subsets of our customers buy the device protection service across DSL and Fiber optic\nsns.countplot(x=""InternetService"", hue=""TechSupport"", data=df_internet)\n<matplotlib.axes._subplots.AxesSubplot at 0x7fd84af4bc90>\n\n\nThe tech support service is much more popular with DSL customers. Possibly because fiber optic customers are more advanced users\nsns.countplot(x=""InternetService"", hue=""StreamingTV"", data=df_internet)\n<matplotlib.axes._subplots.AxesSubplot at 0x7fd84aeb8bd0>\n\n\nAgain Streaming is popular with optic fiber users possibly because of the faster internet speeds\nsns.countplot(x=""Contract"", data=df)\n<matplotlib.axes._subplots.AxesSubplot at 0x7fd84ae2a790>\n\n\nA significant portion of our customers are month-to-month customers. this is risky because they can cancel their service without breaching a contract\nsns.countplot(""PaymentMethod"", data=df)\n<matplotlib.axes._subplots.AxesSubplot at 0x7fd84ae17090>\n\n\nThere\'s an oppurtinuty here to convert customers using electronic or mailed checks to an automatic payment method\nsns.countplot(""Churn"", data= df)\n<matplotlib.axes._subplots.AxesSubplot at 0x7fd84ae34b50>\n\n\nThis is the distribution of customers from the prespective of churn\nsns.boxplot(y=""tenure"", x=""Churn"", data=df)\n<matplotlib.axes._subplots.AxesSubplot at 0x7fd84ad4f890>\n\n\nnewer customers are more likely to churn\nsns.boxplot(y=""TotalCharges"", x=""Churn"", data=df)\n<matplotlib.axes._subplots.AxesSubplot at 0x7fd84acd6e90>\n\n\nand lower value customers are more likely to churn. It\'s rare for our high-valued customers to churn since they are represented as outliers\nsns.scatterplot(x = ""tenure"", y=""TotalCharges"", hue=""Churn"", data=df)\n<matplotlib.axes._subplots.AxesSubplot at 0x7fd84ac46950>\n\n\nThis is the relatuinship between tenure and Totalcharges. We see that our lower valued customers are more likely to churn. less tenured customers are also likely to churn more.\nsns.countplot(x=""MultipleLines"", hue=""Churn"", data=df)\n<matplotlib.axes._subplots.AxesSubplot at 0x7fd84ab6ed50>\n\n\nIt doesn\'t appear that the type of phone service is significantly correlated to churn.\nsns.countplot(x=""InternetService"", hue=""Churn"", data=df)\n<matplotlib.axes._subplots.AxesSubplot at 0x7fd84ab19b10>\n\n\nThis graph is very informative. First of all, we see that our fiber optic customers are unsatisfied and likely to churn. we also see that our phone only customers are more satisfied and unlikely to churn\nSuppose we call \'OnlineSecurity\', \'OnlineBackup\', \'DeviceProtection\', \'TechSupport\',\'StreamingTV\', \'StreamingMovies\' Add on internet services\nWe will calculate the number of add-on services each customer has and study the relationship between that and the churn\ndf_addon = df[[\'OnlineSecurity\', \'OnlineBackup\', \'DeviceProtection\', \'TechSupport\',\'StreamingTV\', \'StreamingMovies\']]\nservice_counter = lambda x: 1 if x ==""Yes"" else 0\n\ndf_addon = df_addon.applymap(service_counter)\nimport numpy as np\nn_of_addon_services = np.sum(np.asarray(df_addon),axis=1)\ndf[""n_of_addon_services""] = n_of_addon_services\nsns.countplot(x=""n_of_addon_services"", hue=""Churn"", data=df)\n<matplotlib.axes._subplots.AxesSubplot at 0x7fd84926cf10>\n\n\nwe see that the more services our custoner has, the less likely they are to leave.\n2- Find out the best way to segment customers using K-means based on the Tenure and Total Charges variables in the dataset\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\n\nX = df[[""tenure"", ""TotalCharges""]]\ny = df[""customerID""]\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\nsns.scatterplot(X_scaled[:,0], X_scaled[:,1])\n<matplotlib.axes._subplots.AxesSubplot at 0x7fd8487776d0>\n\n\nWe begin by scaling Customer tenurity on the x-axis and customer total charges on the y-axis so our clustering could run more efficiently\nTo find the optimal number of clusters we will use the elbow method\nwe will plot the within cluster square sum of distance for each number of clusters\nWCSS = []\nfor i in range(1,11):\n    model = KMeans(n_clusters=i,random_state=0)\n    model.fit(X_scaled)\n    WCSS.append(model.inertia_)\nsns.lineplot(range(1,11), WCSS)\n<matplotlib.axes._subplots.AxesSubplot at 0x7fd84860eed0>\n\n\nWe will try 3, 4, 6 and 10 clusters\nmodel_3 = KMeans(n_clusters=3, init=""k-means++"")\ny = model_3.fit_predict(X_scaled)\nsns.scatterplot(x = ""tenure"", y=""TotalCharges"", hue=y, data=df)\n<matplotlib.axes._subplots.AxesSubplot at 0x7fd8481a1850>\n\n\nmodel_4 = KMeans(n_clusters=4, init=""k-means++"")\ny = model_4.fit_predict(X_scaled)\nsns.scatterplot(x = ""tenure"", y=""TotalCharges"", hue=y, data=df)\n<matplotlib.axes._subplots.AxesSubplot at 0x7fd848115610>\n\n\nmodel_6 = KMeans(n_clusters=6, init=""k-means++"")\ny = model_6.fit_predict(X_scaled)\nsns.scatterplot(x = ""tenure"", y=""TotalCharges"", hue=y, data=df,legend=""full"", palette=""muted"")\n<matplotlib.axes._subplots.AxesSubplot at 0x7fd8436bba90>\n\n\nmodel_10 = KMeans(n_clusters=10, init=""k-means++"")\ny = model_10.fit_predict(X_scaled)\nsns.scatterplot(x = ""tenure"", y=""TotalCharges"", hue=y, data=df,legend=""full"", palette=""muted"")\n<matplotlib.axes._subplots.AxesSubplot at 0x7f20e7589750>\n\n\nIn my opinion 6 clusters is the most logical option\nThe six clusters provide a good indication of customer\'s tenurity vs. their value we will begin applying labels to these clusters and we will analyze more\nmodel_6 = KMeans(n_clusters=6, init=""k-means++"")\ny = model_6.fit_predict(X_scaled)\nsns.scatterplot(x = ""tenure"", y=""TotalCharges"", hue=y, data=df,legend=""full"", palette=""muted"")\n<matplotlib.axes._subplots.AxesSubplot at 0x7fd84364ded0>\n\n\nmy_dict = {\n    0:""low Ten/low Val"",\n    1:""high Ten/high-int Val"",\n    2:""high Ten/low Val"",\n    3:""high Ten/High Val"",\n    4:""int Ten/int Val"",\n    5:""int Ten/low-int Val""\n}\n\ncustomerSegment = []\nfor i in y:\n    customerSegment.append(my_dict[int(i)])\n\ndf[""customerSegment""] = customerSegment\nsns.scatterplot(x = ""tenure"", y=""TotalCharges"", hue=""customerSegment"", data=df,legend=""full"", palette=""muted"")\n<matplotlib.axes._subplots.AxesSubplot at 0x7fd84355f750>\n\n\nsns.scatterplot(x = ""tenure"", y=""TotalCharges"", hue=""customerSegment"", data=df[df.Churn == ""Yes""],legend=""full"", palette=""muted"")\n<matplotlib.axes._subplots.AxesSubplot at 0x7fd843567110>\n\n\nThis plot shows customers who churned and their segment\nWe can see that some customer segments are more likely to churn than others\ndf_segmented = df.groupby([\'customerSegment\',\'Churn\']).agg({\'customerID\': \'count\'})\ndf_segmented\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\n\ncustomerID\n\n\ncustomerSegment\nChurn\n\n\n\n\n\nhigh Ten/High Val\nNo\n700\n\n\nYes\n89\n\n\nhigh Ten/high-int Val\nNo\n812\n\n\nYes\n134\n\n\nhigh Ten/low Val\nNo\n643\n\n\nYes\n20\n\n\nint Ten/int Val\nNo\n633\n\n\nYes\n233\n\n\nint Ten/low-int Val\nNo\n1076\n\n\nYes\n335\n\n\nlow Ten/low Val\nNo\n1310\n\n\nYes\n1058\n\n\n\n\ncalc_pct = lambda x: round(100 * x / float(x.sum()),2)\nChurn_percentages =df_segmented.groupby(level=0).apply(calc_pct)\nChurn_percentages\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\n\ncustomerID\n\n\ncustomerSegment\nChurn\n\n\n\n\n\nhigh Ten/High Val\nNo\n88.72\n\n\nYes\n11.28\n\n\nhigh Ten/high-int Val\nNo\n85.84\n\n\nYes\n14.16\n\n\nhigh Ten/low Val\nNo\n96.98\n\n\nYes\n3.02\n\n\nint Ten/int Val\nNo\n73.09\n\n\nYes\n26.91\n\n\nint Ten/low-int Val\nNo\n76.26\n\n\nYes\n23.74\n\n\nlow Ten/low Val\nNo\n55.32\n\n\nYes\n44.68\n\n\n\n\nWe can say here that our most at risk customers are:\n\nLow tenurity and low value customers (44.68% churn rate)\nIntermediate Tenurity and Intermediate value customers (26.91% churn rate)\nIntermediate tenurity and low-intermediate value customers (23.74% churn rate)\n\nOur safest customers are:\n\nHigh tenurity and low value customers (3.02% churn rate)\nHigh tenurity and High value customers (11.28% churn rate)\nHigh tenurity and high-intermediate value customers (14.16% churn rate)\n\nIt follows business logic that customers with the least churn are high tenurity and low value customers (ex. a sensior citizen who has only one line and has been with the company for a long time) and that customers with the highest churn rates are low tenurity and low value customers (ex. tourists buying single lines or people buying burner phones).\nit\'s positive that most of the churn rates are happening in intermediate to low value customers. it follows logic that customers who are deeply investing in our service are less likely to leave.\n3- Build simple models using Logistic Regression to predict customer churn behavior based on the most important variables in the provided dataset.\nFeature Engineering\ndf.columns\nIndex([\'customerID\', \'gender\', \'SeniorCitizen\', \'Partner\', \'Dependents\',\n       \'tenure\', \'PhoneService\', \'MultipleLines\', \'InternetService\',\n       \'OnlineSecurity\', \'OnlineBackup\', \'DeviceProtection\', \'TechSupport\',\n       \'StreamingTV\', \'StreamingMovies\', \'Contract\', \'PaperlessBilling\',\n       \'PaymentMethod\', \'MonthlyCharges\', \'TotalCharges\', \'Churn\',\n       \'n_of_addon_services\', \'customerSegment\'],\n      dtype=\'object\')\n\nencoded_df = df[[\'gender\', \'SeniorCitizen\', \'Partner\', \'Dependents\', \'PhoneService\', \'MultipleLines\', \'InternetService\',\n       \'OnlineSecurity\', \'OnlineBackup\', \'DeviceProtection\', \'TechSupport\',\n       \'StreamingTV\', \'StreamingMovies\', \'Contract\', \'PaperlessBilling\',\n       \'PaymentMethod\',\'n_of_addon_services\', \'customerSegment\']]\nfrom sklearn.preprocessing import OrdinalEncoder\nordinal_encoder = OrdinalEncoder()\nencoded_array = ordinal_encoder.fit_transform(encoded_df)\n\nfrom itertools import count\nj = count(start=0, step = 1)\nfor i in encoded_df.columns:\n    encoded_df[i] = encoded_array[:,next(j)]\nWe encoded all of our categorical Features\nWe give the categorical features numerical values so we can plug them in the model\nencoded_df\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\ngender\nSeniorCitizen\nPartner\nDependents\nPhoneService\nMultipleLines\nInternetService\nOnlineSecurity\nOnlineBackup\nDeviceProtection\nTechSupport\nStreamingTV\nStreamingMovies\nContract\nPaperlessBilling\nPaymentMethod\nn_of_addon_services\ncustomerSegment\n\n\n\n\n0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n2.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n2.0\n1.0\n5.0\n\n\n1\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n2.0\n0.0\n2.0\n0.0\n0.0\n0.0\n1.0\n0.0\n3.0\n2.0\n4.0\n\n\n2\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n2.0\n2.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n3.0\n2.0\n5.0\n\n\n3\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n2.0\n0.0\n2.0\n2.0\n0.0\n0.0\n1.0\n0.0\n0.0\n3.0\n2.0\n\n\n4\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n2.0\n0.0\n5.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n7038\n1.0\n0.0\n1.0\n1.0\n1.0\n2.0\n0.0\n2.0\n0.0\n2.0\n2.0\n2.0\n2.0\n1.0\n1.0\n3.0\n5.0\n4.0\n\n\n7039\n0.0\n0.0\n1.0\n1.0\n1.0\n2.0\n1.0\n0.0\n2.0\n2.0\n0.0\n2.0\n2.0\n1.0\n1.0\n1.0\n4.0\n0.0\n\n\n7040\n0.0\n0.0\n1.0\n1.0\n0.0\n1.0\n0.0\n2.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n2.0\n1.0\n5.0\n\n\n7041\n1.0\n1.0\n1.0\n0.0\n1.0\n2.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n3.0\n0.0\n5.0\n\n\n7042\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n2.0\n0.0\n2.0\n2.0\n2.0\n2.0\n2.0\n1.0\n0.0\n5.0\n0.0\n\n\n\n7043 rows √ó 18 columns\n\nscaler = StandardScaler()\nscaled_numeric = scaler.fit_transform(df[[""tenure"", ""MonthlyCharges"", ""TotalCharges""]])\nWe also scaled the numerical features\nWe will now make a new df called processed_df with all of these processed values\nprocessed_df = df\nprocessed_df[[\'gender\', \'SeniorCitizen\', \'Partner\', \'Dependents\', \'PhoneService\', \'MultipleLines\', \'InternetService\',\n       \'OnlineSecurity\', \'OnlineBackup\', \'DeviceProtection\', \'TechSupport\',\n       \'StreamingTV\', \'StreamingMovies\', \'Contract\', \'PaperlessBilling\',\n       \'PaymentMethod\',\'n_of_addon_services\', \'customerSegment\']] = encoded_df\nprocessed_df[[""tenure"", ""MonthlyCharges"", ""TotalCharges""]] = scaled_numeric\nWe will then encode our target feature (Churn)\nfrom sklearn.preprocessing import LabelEncoder\nlabelizer = LabelEncoder()\nprocessed_df[""Churn""] = labelizer.fit_transform(processed_df[""Churn""])\nMaking the train and test sets\nSince the data is not equally distributed across the Churn and the customerSgement columns we have to respect that when we sample the data. We want our training and test sets to have the same distribution as the original dataset. for this purpose we will use stratified split.\nfrom sklearn.model_selection import StratifiedShuffleSplit\nsplit = StratifiedShuffleSplit(n_splits=2, test_size=0.3, random_state=0)\nfor train_index, test_index in split.split(processed_df, processed_df[""customerSegment""], processed_df[""Churn""]):\n    strat_train_set = processed_df.loc[train_index]\n    strat_test_set = processed_df.loc[test_index]\nstrat_train_set.columns\nIndex([\'customerID\', \'gender\', \'SeniorCitizen\', \'Partner\', \'Dependents\',\n       \'tenure\', \'PhoneService\', \'MultipleLines\', \'InternetService\',\n       \'OnlineSecurity\', \'OnlineBackup\', \'DeviceProtection\', \'TechSupport\',\n       \'StreamingTV\', \'StreamingMovies\', \'Contract\', \'PaperlessBilling\',\n       \'PaymentMethod\', \'MonthlyCharges\', \'TotalCharges\', \'Churn\',\n       \'n_of_addon_services\', \'customerSegment\'],\n      dtype=\'object\')\n\nX_train = strat_train_set[[\'gender\', \'SeniorCitizen\', \'Partner\', \'Dependents\',\n       \'tenure\', \'PhoneService\', \'MultipleLines\', \'InternetService\',\n       \'OnlineSecurity\', \'OnlineBackup\', \'DeviceProtection\', \'TechSupport\',\n       \'StreamingTV\', \'StreamingMovies\', \'Contract\', \'PaperlessBilling\',\n       \'PaymentMethod\', \'MonthlyCharges\', \'TotalCharges\',\n       \'n_of_addon_services\', \'customerSegment\']]\ny_train = strat_train_set[\'Churn\']\n\nX_test = strat_test_set[[\'gender\', \'SeniorCitizen\', \'Partner\', \'Dependents\',\n       \'tenure\', \'PhoneService\', \'MultipleLines\', \'InternetService\',\n       \'OnlineSecurity\', \'OnlineBackup\', \'DeviceProtection\', \'TechSupport\',\n       \'StreamingTV\', \'StreamingMovies\', \'Contract\', \'PaperlessBilling\',\n       \'PaymentMethod\', \'MonthlyCharges\', \'TotalCharges\',\n       \'n_of_addon_services\', \'customerSegment\']]\ny_test = strat_test_set[\'Churn\']\nFeature Selection\nWe will use statsmodels logistic regression model and only keep features that are statistically significant to the model\nimport statsmodels.discrete.discrete_model as ds\n\n\nmodel= ds.MNLogit(y_train,X_train)\nresult=model.fit()\nresult.summary()\nOptimization terminated successfully.\n         Current function value: 0.420797\n         Iterations 8\n\n\nMNLogit Regression Results\n\nDep. Variable: Churn   No. Observations:     4930\n\n\nModel: MNLogit   Df Residuals:         4909\n\n\nMethod: MLE   Df Model:               20\n\n\nDate: Sun, 08 Mar 2020   Pseudo R-squ.:      0.2765\n\n\nTime: 06:41:09   Log-Likelihood:      -2074.5\n\n\nconverged: True   LL-Null:             -2867.4\n\n\nCovariance Type: nonrobust   LLR p-value:         0.000\n\n\n\n\nChurn=1 coef std err z P>|z| [0.025 0.975]\n\n\ngender    -0.0509     0.077    -0.663  0.507    -0.201     0.100\n\n\nSeniorCitizen     0.2352     0.099     2.366  0.018     0.040     0.430\n\n\nPartner    -0.0750     0.092    -0.819  0.413    -0.254     0.105\n\n\nDependents    -0.0568     0.106    -0.534  0.593    -0.265     0.152\n\n\ntenure    -1.7874     0.194    -9.194  0.000    -2.168    -1.406\n\n\nPhoneService    -0.2464     0.224    -1.101  0.271    -0.685     0.192\n\n\nMultipleLines     0.1492     0.050     2.981  0.003     0.051     0.247\n\n\nInternetService     1.1344     0.252     4.500  0.000     0.640     1.628\n\n\nOnlineSecurity    -0.7160     0.136    -5.249  0.000    -0.983    -0.449\n\n\nOnlineBackup    -0.6245     0.137    -4.572  0.000    -0.892    -0.357\n\n\nDeviceProtection    -0.5666     0.134    -4.217  0.000    -0.830    -0.303\n\n\nTechSupport    -0.7312     0.134    -5.465  0.000    -0.993    -0.469\n\n\nStreamingTV    -0.3346     0.115    -2.900  0.004    -0.561    -0.109\n\n\nStreamingMovies    -0.3471     0.117    -2.974  0.003    -0.576    -0.118\n\n\nContract    -0.7205     0.090    -7.973  0.000    -0.898    -0.543\n\n\nPaperlessBilling     0.3404     0.089     3.842  0.000     0.167     0.514\n\n\nPaymentMethod     0.0389     0.042     0.930  0.352    -0.043     0.121\n\n\nMonthlyCharges    -0.3358     0.270    -1.245  0.213    -0.865     0.193\n\n\nTotalCharges     0.1045     0.215     0.487  0.626    -0.316     0.525\n\n\nn_of_addon_services     1.1092     0.296     3.747  0.000     0.529     1.689\n\n\ncustomerSegment    -0.5256     0.109    -4.843  0.000    -0.738    -0.313\n\n\ndef exclude_irrelevant_features(X,y):\n    columns = list(X.columns)\n    while len(columns) > 0:\n        model= ds.MNLogit(y,X[columns])\n        result=model.fit(disp=0)\n        largest_pval = result.pvalues.nlargest(1,0)\n        if float(largest_pval.iloc[0])> .05:\n            col_name = largest_pval.index[0]\n            columns.remove(col_name)\n        else:\n            break\n    return columns\n\ngood_columns = exclude_irrelevant_features(X_train,y_train)\nThis function will run the logistic regression model iteratively and each run it will exclude the feature with the highest p value from the list of features. Once all features have p value less than 5% the function will return a list of the remaining features\ngood_columns\n[\'SeniorCitizen\',\n \'tenure\',\n \'MultipleLines\',\n \'InternetService\',\n \'OnlineSecurity\',\n \'OnlineBackup\',\n \'DeviceProtection\',\n \'TechSupport\',\n \'StreamingTV\',\n \'StreamingMovies\',\n \'Contract\',\n \'PaperlessBilling\',\n \'MonthlyCharges\',\n \'n_of_addon_services\',\n \'customerSegment\']\n\nmodel= ds.MNLogit(y_train,X_train[good_columns])\nresult=model.fit()\nresult.summary()\nOptimization terminated successfully.\n         Current function value: 0.421218\n         Iterations 7\n\n\nMNLogit Regression Results\n\nDep. Variable: Churn   No. Observations:     4930\n\n\nModel: MNLogit   Df Residuals:         4915\n\n\nMethod: MLE   Df Model:               14\n\n\nDate: Sun, 08 Mar 2020   Pseudo R-squ.:      0.2758\n\n\nTime: 06:41:09   Log-Likelihood:      -2076.6\n\n\nconverged: True   LL-Null:             -2867.4\n\n\nCovariance Type: nonrobust   LLR p-value:         0.000\n\n\n\n\nChurn=1 coef std err z P>|z| [0.025 0.975]\n\n\nSeniorCitizen     0.2398     0.097     2.463  0.014     0.049     0.431\n\n\ntenure    -1.8923     0.135   -14.025  0.000    -2.157    -1.628\n\n\nMultipleLines     0.1655     0.046     3.567  0.000     0.075     0.257\n\n\nInternetService     1.2774     0.207     6.177  0.000     0.872     1.683\n\n\nOnlineSecurity    -0.8110     0.104    -7.823  0.000    -1.014    -0.608\n\n\nOnlineBackup    -0.7171     0.105    -6.846  0.000    -0.922    -0.512\n\n\nDeviceProtection    -0.6573     0.105    -6.279  0.000    -0.863    -0.452\n\n\nTechSupport    -0.8228     0.102    -8.093  0.000    -1.022    -0.624\n\n\nStreamingTV    -0.4058     0.093    -4.344  0.000    -0.589    -0.223\n\n\nStreamingMovies    -0.4181     0.095    -4.415  0.000    -0.604    -0.232\n\n\nContract    -0.7438     0.089    -8.356  0.000    -0.918    -0.569\n\n\nPaperlessBilling     0.3321     0.088     3.781  0.000     0.160     0.504\n\n\nMonthlyCharges    -0.5512     0.183    -3.014  0.003    -0.910    -0.193\n\n\nn_of_addon_services     1.3240     0.213     6.224  0.000     0.907     1.741\n\n\ncustomerSegment    -0.6317     0.057   -11.015  0.000    -0.744    -0.519\n\n\nThese columns are the columns with the relevant pval (<5%) so we will only train our models on them\nWe also notice that the correlation coefficient of these features are significantly high\nMaking the three models\nA different random_state is used every time before we sample the training and test datasets. we use the resulting datasets to train each of our models.\nModel 1\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, roc_curve, auc\nX_train1 = X_train\nX_test1 = X_test\ny_train1 = y_train\ny_test1 = y_test\n\nmodel1 = LogisticRegression()\nmodel1.fit(X_train1[good_columns],y_train1)\ny_pred1 = model1.predict(X_test1[good_columns])\n\naccuracy_score(y_test1, y_pred1)\n0.8078561287269286\n\nModel 2\nsplit = StratifiedShuffleSplit(n_splits=2, test_size=0.3, random_state=1)\nfor train_index, test_index in split.split(processed_df, processed_df[""customerSegment""], processed_df[""Churn""]):\n    strat_train_set = processed_df.loc[train_index]\n    strat_test_set = processed_df.loc[test_index]\n\nX_train2 = strat_train_set[[\'gender\', \'SeniorCitizen\', \'Partner\', \'Dependents\',\n       \'tenure\', \'PhoneService\', \'MultipleLines\', \'InternetService\',\n       \'OnlineSecurity\', \'OnlineBackup\', \'DeviceProtection\', \'TechSupport\',\n       \'StreamingTV\', \'StreamingMovies\', \'Contract\', \'PaperlessBilling\',\n       \'PaymentMethod\', \'MonthlyCharges\', \'TotalCharges\',\n       \'n_of_addon_services\', \'customerSegment\']]\ny_train2 = strat_train_set[\'Churn\']\n\nX_test2 = strat_test_set[[\'gender\', \'SeniorCitizen\', \'Partner\', \'Dependents\',\n       \'tenure\', \'PhoneService\', \'MultipleLines\', \'InternetService\',\n       \'OnlineSecurity\', \'OnlineBackup\', \'DeviceProtection\', \'TechSupport\',\n       \'StreamingTV\', \'StreamingMovies\', \'Contract\', \'PaperlessBilling\',\n       \'PaymentMethod\', \'MonthlyCharges\', \'TotalCharges\',\n       \'n_of_addon_services\', \'customerSegment\']]\ny_test2 = strat_test_set[\'Churn\']\n\nmodel2 = LogisticRegression()\nmodel2.fit(X_train2[good_columns],y_train2)\ny_pred2 = model2.predict(X_test2[good_columns])\n\naccuracy_score(y_test2, y_pred2)\n0.8106956933270232\n\nModel 3\nsplit = StratifiedShuffleSplit(n_splits=2, test_size=0.3, random_state=2)\nfor train_index, test_index in split.split(processed_df, processed_df[""customerSegment""], processed_df[""Churn""]):\n    strat_train_set = processed_df.loc[train_index]\n    strat_test_set = processed_df.loc[test_index]\n\nX_train3 = strat_train_set[[\'gender\', \'SeniorCitizen\', \'Partner\', \'Dependents\',\n       \'tenure\', \'PhoneService\', \'MultipleLines\', \'InternetService\',\n       \'OnlineSecurity\', \'OnlineBackup\', \'DeviceProtection\', \'TechSupport\',\n       \'StreamingTV\', \'StreamingMovies\', \'Contract\', \'PaperlessBilling\',\n       \'PaymentMethod\', \'MonthlyCharges\', \'TotalCharges\',\n       \'n_of_addon_services\', \'customerSegment\']]\ny_train3 = strat_train_set[\'Churn\']\n\nX_test3 = strat_test_set[[\'gender\', \'SeniorCitizen\', \'Partner\', \'Dependents\',\n       \'tenure\', \'PhoneService\', \'MultipleLines\', \'InternetService\',\n       \'OnlineSecurity\', \'OnlineBackup\', \'DeviceProtection\', \'TechSupport\',\n       \'StreamingTV\', \'StreamingMovies\', \'Contract\', \'PaperlessBilling\',\n       \'PaymentMethod\', \'MonthlyCharges\', \'TotalCharges\',\n       \'n_of_addon_services\', \'customerSegment\']]\ny_test3 = strat_test_set[\'Churn\']\n\nmodel3 = LogisticRegression()\nmodel3.fit(X_train3[good_columns],y_train3)\ny_pred3 = model3.predict(X_test3[good_columns])\n\naccuracy_score(y_test3, y_pred3)\n0.8031235210601041\n\n4- Plot the ROC curve of the 3  models overlay them on same visual with the associated AUC result.\nfpr1, tpr1, thresholds1 = roc_curve(y_test1, model1.predict_proba(X_test1[good_columns])[:,1])\nauc1 = auc(fpr1, tpr1)\n\nfpr2, tpr2, thresholds2 = roc_curve(y_test2, model2.predict_proba(X_test2[good_columns])[:,1])\nauc2 = auc(fpr2, tpr2)\n\nfpr3, tpr3, thresholds3 = roc_curve(y_test3, model3.predict_proba(X_test3[good_columns])[:,1])\nauc3 = auc(fpr3, tpr3)\nhere we calculate the false positive rate and the true positive rate fore each of our models\nFPR is the rate of predictions that were classified as positive (in our case yes for churn) while they are actually negative. TPR are predictions that were classified as positive and they are actually positive.\nWe passed a list of probabilities for our positive (yes) class to the roc_curve function. the function calculates the FPR and TPR by setting different decision thresholds from 0 to 1 and classifying the points based on the probabilities we passed (above the threshold is positive and below the threshold is negative).\nthe auc is the area under the curve for each model. higher values indicate a better model\nplt.figure()\nlw = 2\nplt.plot(fpr1, tpr1, color=\'darkorange\',lw=lw, label=\'ROC curve for Model 1 (auc = %0.2f)\' % auc1)\nplt.plot(fpr2, tpr2, color=\'darkblue\',lw=lw, label=\'ROC curve for Model 2 (auc = %0.2f)\' % auc2)\nplt.plot(fpr3, tpr3, color=\'darkgreen\',lw=lw, label=\'ROC curve for Model 3 (auc = %0.2f)\' % auc3)\nplt.plot([0, 1], [0, 1], color=\'red\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic example\')\nplt.legend(loc=""lower right"")\n<matplotlib.legend.Legend at 0x7fd840192610>\n\n\n'], 'url_profile': 'https://github.com/mostafa-k-m', 'info_list': ['Python', 'Updated Mar 14, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', '1', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'JavaScript', 'MIT license', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Jun 12, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '55 contributions\n        in the last year', 'description': ['Air-Quality-Prediction\nML based project using model for linear regression with multiple features\n'], 'url_profile': 'https://github.com/satvik1998', 'info_list': ['Jupyter Notebook', 'Updated Mar 6, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'HTML', 'Updated Mar 6, 2020', 'Python', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'JavaScript', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Mar 3, 2020']}","{'location': 'Trento (Italy)', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': [""bayesian_networks_pymc3_20200302\nBayesian network exact linear regression by using polynomials for each edge by PyMC3\nPROJECT NAME: 'bayesian_networks_exact_regression_20200202'\nAUTHOR: Paolo Ranzi\nREADME FILE VERSION (LAST UPDATE): 20200301\nPython 3.6.7 has been used. For each step the specific Python script has been mentioned, accordingly. At the begginning of each script we have to make sure of setting custom-made parameters/pieces of information:\n\nimport Python libraries (if you do not have a specific library you have to manually install it by using PIP within your virtual enviroment);\nsetting paths and keywords according to your storage location;\nset parameters (e.g. input file name, output file name etc.);\nall scripts have been optimized for using them by parallel computing. Please set number of CPUs by searching for 'cpu_count()' within each script according to your available resources;\n\nSTEPS:\n\n\nLEARNING STEP BY EXACT POLYNOMIAL LINEAR REGRESSION FOR EACH EDGE:\n(it computes Bayesian + MCMC exact linear regression by PyMC3 for each edge twice. It is a completely data-driven approach, without the introduction\nof neither expert-knowledge nor conditional probability table):\nSCRIPT NAME: '01_analysis_20200202.py'\nINPUT: .csv file;\nOUTPUT:  pickled bayesian model ( .joblib); pickled MCMC traces (to be used later for diagnositics and prediction);\n\n\nPLOTTING THE DIRECTED ACYCLIC GRAPH (DAG) GRAPH:\n(it computes DAG from the results of single edge linear regressions. In a nutshell, it takes the slope for the interaction between 2 nodes and it plots\nit after selecting the strongest causal relationships by thresholding):\nSCRIPT NAME: '02_analysis_graph_20200202.py'\nINPUT: '01_analysis_20200202.py' + pickled bayesian model ( .joblib) + mask_edges_to_nodes.csv;\nOUTPUT: DAG in a .pdf format;\n\n\n""], 'url_profile': 'https://github.com/PaoloRanzi81', 'info_list': ['Jupyter Notebook', 'Updated Mar 6, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'HTML', 'Updated Mar 6, 2020', 'Python', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'JavaScript', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Mar 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['house_prices_kaggle\nNotebook for the ""House Prices: Advanced Regression Techniques"" competion in Kaggle\n'], 'url_profile': 'https://github.com/rodrigozmorelli', 'info_list': ['Jupyter Notebook', 'Updated Mar 6, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'HTML', 'Updated Mar 6, 2020', 'Python', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'JavaScript', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Mar 3, 2020']}","{'location': 'Ghaziabad', 'stats_list': [], 'contributions': '51 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/amitguptapc', 'info_list': ['Jupyter Notebook', 'Updated Mar 6, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'HTML', 'Updated Mar 6, 2020', 'Python', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'JavaScript', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Mar 3, 2020']}","{'location': 'The Netherlands ', 'stats_list': [], 'contributions': '1,244 contributions\n        in the last year', 'description': ['MachineLearningExample\nExample for Machine Learning including Kmeans clustering, logistic regression and decision trees using sklearn\n'], 'url_profile': 'https://github.com/Brainilio', 'info_list': ['Jupyter Notebook', 'Updated Mar 6, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'HTML', 'Updated Mar 6, 2020', 'Python', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'JavaScript', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Mar 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '121 contributions\n        in the last year', 'description': ['ML_Model_for_Predict_Big_Mart_Sale_Part2\nUse other algorithms to predict Big Mart Sale as follows: Lasso/Ridge Regression and Tree Based Model\nAuthor: Panuwat Ulis\nCompare simple linear regression performance to other algorithm\nData Set: train_bigmart_modified.csv\n'], 'url_profile': 'https://github.com/PanuwatUlis', 'info_list': ['Jupyter Notebook', 'Updated Mar 6, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'HTML', 'Updated Mar 6, 2020', 'Python', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'JavaScript', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Mar 3, 2020']}","{'location': 'San Francisco ', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': [""Automation Testing Suite with Visual Regression Image Diffing and Built-in Support for eyes.cypress and eyes.storybook.\nWrite Your First Test Tutorial:\nWriting Tests Tutorials\nCYPRESS.IO API DOCS:\nAPI DOCS\nStorybook React Samples\nThis repo includes support for React Storybook testing.\nIt also includes support for visually testing any other app, component, or page using\nApplitools' Storybook SDK and Cypress SDK.\nSome More References To Cook With Fire\nAdditional PDF crash course cheatsheet called Cypress E2E Test Automation PDF is in this repo, as well as another Readme to walk you through capturing screenshots locally without relying on the Eyes SDK for local testing purposes.\nThere is also an entire TEST-RECIPES-AND-EXAMPLES directory at the root of thsi project with even more tests and recipes.\nInstallation\nRun\nnpm install\nOpening the Interactive Test Runner\nThis is a great way to interactively work with Cypress and to get started fast the first time. The GUI test runner provides realtime feedback on the tests you write in your text editor in the cypress/integration directory. Write the automations and watch them excute!\nRun either the first version, or if you have npx, which is included in all npm versions > 5.2.0, use this command instead:\nnode_modules/.bin/cypress open\nPreferred Way:\nnpx cypress open\nSimplest Way To Run Tests Locally With Cypress-Visual-Regression Plugin:\nnpx cypress open\nThis opens the interactive test runner spec, you can add tests into the integration folder. You should find a few Gap tests to get started and a bunch more random tests to help with ideas.\nYou can than fire specific tests or the entire suite from the specRunner.\nYou can make realtime changes to the tests in the cypress/integration/*.js directory and you should see the new tests appear.\nThere are even more tests in the TEST-RECIPES-AND-EXAMPLES DIR located in the root of this project. Happy Testing!\nRunning the Test Spec To Generate Baselines\nRun\nnpm run get-base-image\nRunning the Test Spec To Generate Diffing Images\nThis will compare the bitmaps against the base images and store an image of the capturing differences.\nRun\nnpm run get-diff-image\nVisual Testing using Cypress and Exporting Applitools API Key\nCURRENTLY A WORK IN PROGRESS TO CONFIGURE PROPERLY. IT IS RECOMMENDED TO RUN SCREENSHOTS LOCALLY FOR NOW UNTIL UPDATED CODE IS PUSHED. I HAVE INCLUDED AN APPLITOOLS_EXAMPLE_SPEC.JS TO GET AN IDEA.\nCypress allows for plugins and custom commands and essentially is already built to support the Eyes SDK. Simply export the apis using the syntax below which can be found in the applitools.example.config.js. Alternatively, you can remove the 'example' and use the config file for those keys.\nSet the environment variable APPLITOOLS_API_KEY to your Applitools api key by running the command below in the command line.\nexport APPLITOOLS_API_KEY=<your-api-key> # Mac/Linux\n# or...\nset APPLITOOLS_API_KEY=<your-api-key> # Windows\nNow run Cypress.\nnpm run eyes-cypress\nThen click on the single test in the Cypress window to run it.\n\nOnce the tests passes,\ngoto Applitools Eyes Test manager to see the results.\nKey Commands To Use In Your Tests And Other Stuff To Know\nlocale screenshots:\ncy.screenshot() will take a local screenshot that usually gets added into screenshots directory which will be created if it does not exist..\nTo take a manual screenshot you can use the cy.screenshot() command.\nAdditionally, Cypress will automatically capture screenshots when a failure happens during runs outside of interactive mode.\nThis behavior can be turned off by setting screenshotOnRunFailure to false in the Cypress.Screenshot.defaults().\nScreenshots are stored in the screenshotsFolder which is set to cypress/screenshots by default.\nCypress clears any existing screenshots before cypress run. If you do not want to clear your screenshots folder before a run, you can set trashAssetsBeforeRuns to false.\nCapturing local video\nCypress records a video for each spec file when running tests.\nVideo recording can be turned off entirely by setting video to false from within your configuration.\nVideos are stored in the videosFolder which is set to cypress/videos by default.\nAfter cypress run completes, Cypress automatically compresses the video in order to save on file size. By default it compresses to a 32 CRF, but this is configurable with the videoCompression property.\nWhen using the --record flag while running your tests, videos are processed, compressed, and uploaded to the Dashboard Service after every spec file runs, successful or not. To change this behavior to only process videos in the case that tests fail, set the videoUploadOnPasses configuration option to false.\nCypress clears any existing videos before a cypress run. If you do not want to clear your videos folder before\nInvoking the diffing tool to either capture baselines or diffing image:\nWorks almost the same as using the native Cypress methods.  Essentially the Mocha/Chai testing structure never changes but what changes across local screencaptures, using the visual regression plugin, or leveraging the Applitools EYES Api is the method that gets called.\nHere is how to generate a local screenshot:\nit('should display the login page correctly', () => {\n  cy.visit('/03.html');\n  cy.get('H1').contains('Login');\n  cy.compareSnapshot('login', 0.0);\n  cy.compareSnapshot('login', 0.1);\n});\n\nUsing the Applitools EYES Integration to run the Visual Regression Tests\nEverything is configured to work out of the boxt. Simply run npx eyes-setup and then\nAdd cy.eyesOpen to start the test inside your it method:\nSteps to run Visual Regression using Applitools instead of the local plugin\nStep 1\nExport the API Key above if you have not done so already or include it in a file called applitools.config.js that should be in the root of the current working directory.\nConfigure Eyes plugin and commands by running the following: npx eyes-setup\nStep 2: Add cy.eyesOpen to start the test inside your it method\n//Start the test\ncy.eyesOpen({\n  appName: 'Hello World!',\n  testName: 'My first JavaScript test!',\n  browser: [\n    {width: 800, height: 600, name: 'firefox'},\n    {width: 1024, height: 768, name: 'chrome'}\n    //Add more variations\n  ],\n  batchName: 'My batch'\n  showLogs: true, // if you want verbose log messages to keep track of\n});\nother configurations: https://github.com/applitools/eyes-cypress#advanced-configuration\n\nStep 3: Add one or more eyesCheckWindow checkpoints for various test steps\n\n\nfor an entire page: cy.eyesCheckWindow('Some window name');\n\n\nFor a given CSS selector\n\n\ncy.eyesCheckWindow({\n  sizeMode: 'selector', //mode\n  selector: '.my-element' //CSS Selector\n});\n\n\nFor a specific region\n\ncy.eyesCheckWindow({\n  sizeMode: 'region',\n  region: {top: 100, left: 0, width: 1000, height: 200}\n});\n\n\nTo Ignore a region\n\ncy.eyesCheckWindow({\n  ignore: [\n    {top: 100, left: 0, width: 1000, height: 100},\n    {selector: '.some-div-to-ignore'}\n  ]\n});\n\nStep 4: End the test\ncy.eyesClose();\n\nStep 5: Running your tests\nInteractive Way:\n./node_modules/cypress/bin/cypress open\nCLI:\n./node_modules/cypress/bin/cypress run --spec cypress/integration/putNameOfYourTest.js\nVisual Testing using Storybook\nWIP\nThen wait for the test to pass, and goto Applitools Eyes Test manager to see the results.\n\nConfiguring more browsers to visually test\nEdit the file applitools.config.js and play around with the browser field\nto add or remove more browsers/browser widths. (Storybook & Cypress only)\n""], 'url_profile': 'https://github.com/mishafrenkel', 'info_list': ['Jupyter Notebook', 'Updated Mar 6, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'HTML', 'Updated Mar 6, 2020', 'Python', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'JavaScript', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Mar 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '97 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/TrizteX', 'info_list': ['Jupyter Notebook', 'Updated Mar 6, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'HTML', 'Updated Mar 6, 2020', 'Python', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'JavaScript', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Mar 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '73 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/AnahVeronica', 'info_list': ['Jupyter Notebook', 'Updated Mar 6, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'HTML', 'Updated Mar 6, 2020', 'Python', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'JavaScript', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Mar 3, 2020']}","{'location': 'Wilmington, NC', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['BAN-502-Course-Files\nPredictive Analytics with application of Linear Regression, Classification, Model Validation, Random Forests, Decision Trees, etc.\n'], 'url_profile': 'https://github.com/Christiane1987', 'info_list': ['Jupyter Notebook', 'Updated Mar 6, 2020', 'Python', 'GPL-3.0 license', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated May 2, 2020', 'HTML', 'Updated Mar 6, 2020', 'Python', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'JavaScript', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Mar 3, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['PySpark-Product-Classification-Kaggle\nA logistic regression model to classify data into 9 classes based on 93 features\n'], 'url_profile': 'https://github.com/Mohit1412', 'info_list': ['Jupyter Notebook', 'Updated Mar 5, 2020', 'Python', 'Updated Jul 7, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'R', 'Updated Mar 7, 2020', 'Updated Mar 4, 2020', 'Python', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Jun 13, 2020', 'Python', 'Updated Mar 7, 2020', 'Updated Jun 30, 2020', 'Python', 'Updated Mar 5, 2020']}","{'location': 'Philippines', 'stats_list': [], 'contributions': '81 contributions\n        in the last year', 'description': ['\nOpen-Contracting\nForecasting Contractors‚Äô Slippage for Infrastructure Projects in South Cotabato Using Gaussian Process Regression\n'], 'url_profile': 'https://github.com/cuburt', 'info_list': ['Jupyter Notebook', 'Updated Mar 5, 2020', 'Python', 'Updated Jul 7, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'R', 'Updated Mar 7, 2020', 'Updated Mar 4, 2020', 'Python', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Jun 13, 2020', 'Python', 'Updated Mar 7, 2020', 'Updated Jun 30, 2020', 'Python', 'Updated Mar 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': [""Project 2 - Ames Housing Data and Kaggle Challenge\nContents\n\nProblem Statement\nExecutive Summary\nConclusion\nSources\n\nProblem Statement:\nWe all have those significant moments in our lives such as getting our first car. However, as we get older more moments start to come where we actually become adults such as buying a house and starting a family. However, buying a house is a major financial commitment and there are many things to factor in when deciding on a property. Of course, money is the biggest factor as we have to have enough to buy a house, let alone our dream house. In an indeal world, we can all just buy a plot of land and build the house of our dreams with no worries. Sadly, we aren't in an ideal world. We live in an economy where buy houses in a supply and demand model. We need to give and take with the existing houses that are for sale, which means that we might have to sacrifice a pool for a garage. Ultimately, what determines the price of a house and whether we decide to sign? What should we be considering and prioritizing? Perhaps we can create something to help us predict the price of a house based on the features included.\nExecutive Summary:\nWe took two datasets that listed a myriad of features (about 80) regarding a house such as square footage of the first floor, pool quality, and lot area. However, the only difference was sale price. One dataset had the sale price and one didn't so we took our training dataset (the one with the sale price) and based three regression models (Linear, Lasso, and Ridge) on that data to find how we can best predict sale price.\nOur process began with importing and cleaning our datasets as null values can really pull our data one way or another so imputing values was our primary goal when cleaning the datasets. After cleaning our datasets, we went into exploratory data analysis where we could visualize relationships between variables and determine what features may have a bigger impact on sale price than others. In addition, we wanted to take care of outliers to prevent our data from being skewed because of them and try to normalize our data. However, we didn't discard all of the outliers as we did want our models to be able to handle any strays in the testing dataset. Then we explored some feature engineering where we created new features that encompasses multiple features together such as total property area by combining all individual areas like basement area, garage area, pool area, and first floor area.  Afterwards, we wanted to convert our categorical data to numerical so we could use all of our data when creating models (as they don't work well with categorical variables).\nOnce we have our master dataset that we can work with when fitting the data to our Linear Regression, Lasso Regression, and Ridge Regression. After fitting, predicting, and plotting our data for visualization, we can immediately see that all three models seem to do quite well in predicting sale prices with minimal variance. However, with any regression model, there is a major downfall of it being boundless; but, we fix this through winsorizing by creating an upper and lower boundary. With our metrics scores, we can proceed with a more in-depth analysis on which model does best. Finally, we finish with some outside research before making our conclusions as there may be some factors that are unique to Ames, Iowa that may affect sale price as well such as there being a state university.\nConclusion\nThrough our data analysis, all three regression models do quite well but our Ridge Regression Model is our best option. Based on our $R^2$ scores for our train and test datasets, the values tell us that the model does very well in predicting the sale price when looking at all the features possible on data that it has been exposed to and trained on; and the model does well in predicting the sale price when exposed to data that is new.\nHowever, our models aren't perfect because there are so many other factors involved that can't be easily written in as data. As discovered from outside research, there could be outside factors of the location that affect the housing price. Deciding on whether you want to buy a house buy the university or away from the university can impact the price. Furthermore, gender could come into play if you're looking to be the sole owner as there's approximately a 15,000 dollar difference in annual salary between men and women; thus, it could be harder for a woman to afford a new house than a man. In addition, personal preferences can affect how much money will be needed overall such as moving expenses and repair costs.\nOverall, our Ridge Regression model is quite accurate in predicting the sale price of a house in Ames, Iowa given numerous features of the house and then you can tack on a a couple hundred or thousand dollars to account for personal or outside factors. For the time being, when looking at buying a house in Ames look at the following features first because they have the highest impact on sale price:\n\nOverall Quality\nHouse Area\nAbove Grade Living Area\nYear Built\nExterior Quality\n\nHere we explain the coefficients for the top five features that have the highest impact on sale price based on the Ridge Regression Model. We can interpret these numbers as follows:\n\n\n\nFeature\nMeaning\n\n\n\n\nOverall Qual\nFor all else held equal, a one unit increase in overall quality yields a 8,339 increase in Sale Price\n\n\nHouse Area\nFor all else held equal, a one unit increase in total house area yields a 7,046 increase in Sale Price\n\n\nGr Liv Area\nFor all else held equal, a one unit increase in above grade living area yields a 6,717 increase in Sale Price\n\n\nYear Built\nFor all else held equal, a one unit increase in the year built yields a 4,588 increase in Sale Price\n\n\nExter Qual\nFor all else held equal, a one unit increase in the first floor square footage yields a 4,458 increase in Sale Price\n\n\n\nUntil the model can be refined with more features taken into consideration such as personal and outside factors, referring to the model and specific features when looking at the house are ideal. To calculate how much you would need in total, do a personal calculation of how much a moving truck would cost and the cost for parts of the house you would like to be repaired, rennovated, or installed.\nSources\n\nAmes, Iowa\nBuying a House\n\n""], 'url_profile': 'https://github.com/kellywu218', 'info_list': ['Jupyter Notebook', 'Updated Mar 5, 2020', 'Python', 'Updated Jul 7, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'R', 'Updated Mar 7, 2020', 'Updated Mar 4, 2020', 'Python', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Jun 13, 2020', 'Python', 'Updated Mar 7, 2020', 'Updated Jun 30, 2020', 'Python', 'Updated Mar 5, 2020']}","{'location': 'Islamabad', 'stats_list': [], 'contributions': '41 contributions\n        in the last year', 'description': ['Comparing-M-estimators\nComparing different M-estimators for robust regression and outlier detection based on efficiency factor.\n'], 'url_profile': 'https://github.com/AttiqUrRehmann', 'info_list': ['Jupyter Notebook', 'Updated Mar 5, 2020', 'Python', 'Updated Jul 7, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'R', 'Updated Mar 7, 2020', 'Updated Mar 4, 2020', 'Python', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Jun 13, 2020', 'Python', 'Updated Mar 7, 2020', 'Updated Jun 30, 2020', 'Python', 'Updated Mar 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '398 contributions\n        in the last year', 'description': ['SurvivalAnalysis\nOptimizing Survival Analysis: form of regression that specializes in time-dependent predictive variables\n'], 'url_profile': 'https://github.com/gbains8172', 'info_list': ['Jupyter Notebook', 'Updated Mar 5, 2020', 'Python', 'Updated Jul 7, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'R', 'Updated Mar 7, 2020', 'Updated Mar 4, 2020', 'Python', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Jun 13, 2020', 'Python', 'Updated Mar 7, 2020', 'Updated Jun 30, 2020', 'Python', 'Updated Mar 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['MarchMadnessBracket\nProgram that simulates n brackets for march madness based on logistic regression\n'], 'url_profile': 'https://github.com/sydneysingleton', 'info_list': ['Jupyter Notebook', 'Updated Mar 5, 2020', 'Python', 'Updated Jul 7, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'R', 'Updated Mar 7, 2020', 'Updated Mar 4, 2020', 'Python', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Jun 13, 2020', 'Python', 'Updated Mar 7, 2020', 'Updated Jun 30, 2020', 'Python', 'Updated Mar 5, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '40 contributions\n        in the last year', 'description': ['CarPricePrediction-LinearRegression\nProblem Statement\nA  automobile company XYZ Auto aspires to enter the US market by setting up their manufacturing unit there and producing cars locally to give competition to their US and European counterparts.\nThey have contracted an automobile consulting company to understand the factors on which the pricing of cars depends. Specifically, they want to understand the factors affecting the pricing of cars in the American market, since those may be very different from the Chinese market. The company wants to know:\nWhich variables are significant in predicting the price of a car\nHow well those variables describe the price of a car\nBased on various market surveys, the consulting firm has gathered a large dataset of different types of cars across the Americal market.\nBusiness Goal\nYou are required to model the price of cars with the available independent variables. It will be used by the management to understand how exactly the prices vary with the independent variables. They can accordingly manipulate the design of the cars, the business strategy etc. to meet certain price levels. Further, the model will be a good way for management to understand the pricing dynamics of a new market.\nDataset link:  https://drive.google.com/file/d/1W3js1bHQPczYJTU3vAdamicn2ByvBsHz/view?usp=sharing\n'], 'url_profile': 'https://github.com/harishkumar1111', 'info_list': ['Jupyter Notebook', 'Updated Mar 5, 2020', 'Python', 'Updated Jul 7, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'R', 'Updated Mar 7, 2020', 'Updated Mar 4, 2020', 'Python', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Jun 13, 2020', 'Python', 'Updated Mar 7, 2020', 'Updated Jun 30, 2020', 'Python', 'Updated Mar 5, 2020']}","{'location': 'Bangladesh', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Hridoyshafayet', 'info_list': ['Jupyter Notebook', 'Updated Mar 5, 2020', 'Python', 'Updated Jul 7, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'R', 'Updated Mar 7, 2020', 'Updated Mar 4, 2020', 'Python', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Jun 13, 2020', 'Python', 'Updated Mar 7, 2020', 'Updated Jun 30, 2020', 'Python', 'Updated Mar 5, 2020']}","{'location': 'Montpellier, France', 'stats_list': [], 'contributions': '82 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mcrimi', 'info_list': ['Jupyter Notebook', 'Updated Mar 5, 2020', 'Python', 'Updated Jul 7, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'R', 'Updated Mar 7, 2020', 'Updated Mar 4, 2020', 'Python', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Jun 13, 2020', 'Python', 'Updated Mar 7, 2020', 'Updated Jun 30, 2020', 'Python', 'Updated Mar 5, 2020']}","{'location': 'Canada', 'stats_list': [], 'contributions': '151 contributions\n        in the last year', 'description': ['Gradient-Descent-implementation\nThis repo contains implementation of gradient descent and stochastic gradient descent optimization techniques for logitic regression.\ndata.txt\n\nInput dataset for the implementation\n\ndata_utils.py\n\nBase code with implementations for drawing the hyperplane seperator and plotting it\n\nlogistic_regression.py\n\nPerforms gradient descent and plots negative log-likelihood curve for learning rate 0.5\n\nlogistic_regression_mod.py\n\nPerforms gradient descent and plots negative log-likelihood curve for different learning rates\n\nlogistic_regression_sgd.py\n\nPerforms stochastic gradient descent and plots negative log-likelihood curve for different learning rates\n\nlogistic_regression_sgd_2.py\n\nPerforms stochastic gradient descent using random sampling and plots negative log-likelihood curve for different learning rates\n\nFolders plots, plots-mod, plots-sgd contain visualizations.\nFor execution:\npython3 filename.py\n\n'], 'url_profile': 'https://github.com/sachinnpraburaj', 'info_list': ['Jupyter Notebook', 'Updated Mar 5, 2020', 'Python', 'Updated Jul 7, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'R', 'Updated Mar 7, 2020', 'Updated Mar 4, 2020', 'Python', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Jun 13, 2020', 'Python', 'Updated Mar 7, 2020', 'Updated Jun 30, 2020', 'Python', 'Updated Mar 5, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '54 contributions\n        in the last year', 'description': ['data-science-real-estate\nKaggle - Predict house prices with Random Forest, Decision Tree, Linear Regression and KNN\nData engineering:\ni.   Removing features with low correlation.\nii.  Drop numerical features which mostly have a single value.\niii. Drop categorical features which mainly have 1 feature.\niv.  Merge long tail categories.\nv.   Fill missing values.\nAlgorithms compare and analyze performance:\ni.   K nearest neighbors\nii.  K nearest neighbors with scaled values.\niii. Decision tree.\niv.\t Random Forest.\nv.\t Lasso regression with scaled values.\nHyperparameters:\nI looked for the hyper parameters with maximum performance.\nData preparation:\nI tried different versions of data normalization and imputation.\n'], 'url_profile': 'https://github.com/ohadzer', 'info_list': ['Jupyter Notebook', 'Updated Jul 8, 2020', 'Jupyter Notebook', 'Updated Oct 8, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'R', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'HTML', 'Updated Mar 2, 2020', '1', 'Python', 'Updated Mar 6, 2020', 'JavaScript', 'Updated Mar 3, 2020', 'R', 'Updated Apr 25, 2020']}","{'location': 'United States', 'stats_list': [], 'contributions': '343 contributions\n        in the last year', 'description': [""<<<<<<< HEAD\nDota Winrate Analysis\n=======\nA Regression and Deep Learning Approach to Predictive Dota-2 Winrate Analysis\nA full writeup can be found here.\n\nRequirements\nOverview\nFAQ\n\nRequirements \nThe project requires a handful of python packages. Install them using:\npip install -r requirements.txt\nIn addition this project utilizes JupyterLab which can be found at https://jupyter.org/.\nProject Overview \nEach of the following Notebook Files will walk you through step by step on\n\nData Gathering and Processing\nRegression Model\nDeep Learning Model\nModel Comparison\n\nFAQ \n1. Only 60% accuracy? That is not much better than predicting that radiant always wins.\n      * Yes, but due to the extreme number of variables and hero combinations that attribute to predicting the match outcome, much more data would be needed to achieve higher accuracy.\n3. Why don't you use only 6k+ games to train your model then get rich by betting on pro games?\n      * Sadly pro games and high mmr games are extremely different. The accuracy would be very low due to the differences and therefor not viable. \n4. Why did you not use other statistics, such as XPM, GPM or itemization?\n      * This project was an attempt to utilize only the drafting data to predict which team would win. While including XPM, GPM, and other related statistics would be extremely useful in predicting the winning team, it would be dependent on data not available at the start of a game.\n5. How many games do I need to achieve the best accuracy possible?\n      * In this project I utilized around 6k matches. To consistently achieve a ‚Äúhigh‚Äù accuracy up to a million matches would have to be analyzed. Organizations such as Dotabuff and OpenDota utilize this amount of data and are able to consistently predict match outcomes.\n\n\n\n\n\n\n\n\nf8ed1965bf97eea096741b2b60ceb433dfea29ce\n\n\n\n\n\n\n\n""], 'url_profile': 'https://github.com/choldener', 'info_list': ['Jupyter Notebook', 'Updated Jul 8, 2020', 'Jupyter Notebook', 'Updated Oct 8, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'R', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'HTML', 'Updated Mar 2, 2020', '1', 'Python', 'Updated Mar 6, 2020', 'JavaScript', 'Updated Mar 3, 2020', 'R', 'Updated Apr 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '50 contributions\n        in the last year', 'description': ['Sentiment-Analysis-with-Logistic-Regression\n'], 'url_profile': 'https://github.com/anishdulal', 'info_list': ['Jupyter Notebook', 'Updated Jul 8, 2020', 'Jupyter Notebook', 'Updated Oct 8, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'R', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'HTML', 'Updated Mar 2, 2020', '1', 'Python', 'Updated Mar 6, 2020', 'JavaScript', 'Updated Mar 3, 2020', 'R', 'Updated Apr 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['DecisionTree-and-LogisticRegression\n\ntitle: ""DecisionTree and LogisticRegression""\nauthor: ""Doris Ying-Yu Kuo""\ndate: ""12/7/2019""\noutput:\nhtml_document:\ntoc: true\ntoc_float: true\ntheme: darkly\nknitr::opts_chunk$set(echo = TRUE)\n\n‚óè PART I: Collecting the Data\nmushroom = read.csv(\'https://s3.amazonaws.com/notredame.analytics.data/mushrooms.csv\')\nhead(mushroom)\n\nlibrary(tidyverse)\n\nmushroom<-mushroom%>%\n  mutate(edible=ifelse(type==\'edible\',\'Yes\',\'No\'))%>%\n  mutate(edible=as.factor(edible))\n\n#check\nmushroom%>%\n  select(edible,type)%>%\n  head()\n\n#delete type\nmushroom<-mushroom%>%\n  select(-type)\n\n‚óè PARTII: Explore and Prepare the Data\n1. Explore Data\nGraphical Method\nsummary(mushroom)\nstr(mushroom)\n\nmushroom%>%\n  gather(key=\'key\',value=\'value\')%>%\n  ggplot()+\n  geom_histogram(mapping = aes(x=value,fill=key),stat=""count"")+\n  #scale=\'free\': they can have different scales on both x and y\n  facet_wrap(~key,scales = \'free\')+\n  theme_minimal()\n\nAfter exploring all variables through graphic method, we intend to drop five variables with low information value. They are gill_attachment,gill_spacing,ring_number,veil_color,veil_type. To confirm their exact distributions, we further use statistical method to confirm our decision.\nStatistical Method\nlibrary(Hmisc)\n\nmushroom%>%\n  select(gill_attachment,gill_spacing,ring_number,veil_color,veil_type)%>%\n  describe()\n\n\nSince these imbalanced features all include one level up to 80%, which we believe will lead to low information value problem, we are determined to drop these five variables.\n\nmushroom <- mushroom%>%\n  select(-gill_attachment,-gill_spacing,-ring_number,-veil_color,-veil_type)\n\nhead(mushroom)\n\n2. Split Data - Stratified Sampling\nlibrary(caTools)\nset.seed(1234)\nmushroom_set <- mushroom%>%\n  pull(edible)%>%\n  sample.split(SplitRatio = 0.60)\nmushroom_train <- subset(mushroom, mushroom_set==TRUE)\nmushroom_test <- subset(mushroom, mushroom_set==FALSE)\n\nCheck the distribution:\n#origianl dataset\nprop.table(table(mushroom$edible))\n\n#trainset\nprop.table(table(mushroom_train$edible))\n\n#testset\nprop.table(table(mushroom_test$edible))\n\n‚óè PART III: Train the Models\nSince all the features are nominal variables, we don\'t think it is good to use KNN model because it would be hard and meaningless to calculate the distances for nominal variables. Therefore, we will use decision tree model first and then use logistic regression.\nBuild model\nDecision Tree\nlibrary(rpart)\n\ntree_mod <-\n  rpart(\n    edible ~ .,\n    method = ""class"",\n    data = mushroom_train,\n    control = rpart.control(cp = 0.001)\n  )\n\nlibrary(rpart.plot)\nrpart.plot(tree_mod)\n\nLogistic Regression\n\nFor logistic regression:\\\n\n\nWe choose the odor feature in the first nodes of our decision tree as the main predictor in the logistic regression model, because the first nodes selected in the decision tree provide largest information gain with only one variable in the model.\\\nTo confirm the first argument, we put every feature in the logistic model. And check its p-value.\n\nrun the odor variable first:\nlogit_mod <- glm(edible ~ ., family=binomial(link=""logit"") , data = mushroom_train)\nsummary(logit_mod)\n\n\nAfter putting all variables as the predictor of edible in logistic regression model, we confirm that the variable - odor has the lowest AIC. Therefore, we are sure that if only one variable can be used in the logistic regression, the variable should be odor.\n\n‚óè PART IV: Evaluate the Performance of the Model\n1. Predict\nDecision Tree\n#probability prediction\ntree_pred_prob <- predict(tree_mod, mushroom_test)\nhead(tree_pred_prob)\n\n#classification prediction\ntree_pred <- predict(tree_mod, mushroom_test,  type = ""class"")\nhead(tree_pred)\n\nLogistic Regression\n#probability prediction\nlogit_pred <- predict(logit_mod,  mushroom_test, type = \'response\')\nhead(logit_pred)\n\n#classification prediction\nlogit_pred <- ifelse(logit_pred > 0.5, \'Yes\', \'No\')\nhead(logit_pred)\n\n2. Confusion Matrix\nDecision Tree\ntree_pred_table <- table(mushroom_test$edible, tree_pred)\ntree_pred_table\n\nLogistic Regression\nlogit_pred_table <- table(mushroom_test$edible, logit_pred)\nlogit_pred_table\n\n3. Compare two models\nAccuracy for Decision Tree\nsum(diag(tree_pred_table)) / nrow(mushroom_test)\n\nAccuracy for Logistic Regression\nsum(diag(logit_pred_table)) / nrow(mushroom_test)\n\n\nConclusion: in the cases above, we should choose decision tree for the two reasons listed below:\\\n\n\nDecision tree has a better accuracy. \\\nAfter looking at the confusion matrix, both of the two model has 0 number for mushroom which is actual edible but is predicted as not edible, but logistic regression model has a way higher number of mushrooms which is actual not edible but is predicted as edible (situation we want to prevent because of the terrible result).\n\n'], 'url_profile': 'https://github.com/yingyums01', 'info_list': ['Jupyter Notebook', 'Updated Jul 8, 2020', 'Jupyter Notebook', 'Updated Oct 8, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'R', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'HTML', 'Updated Mar 2, 2020', '1', 'Python', 'Updated Mar 6, 2020', 'JavaScript', 'Updated Mar 3, 2020', 'R', 'Updated Apr 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': [""IncomePerCapita-Regression-Project\nAnalysis on IncomePerCapita based on county in the United States\nProject Members\n\nAkshay Indusekar\nChuck Pryor\n\nProject Scope and Background\nIn the United States, many factors affect income per capita in counties which are political subdivisions used in many states. Some states such as Louisiana and Alaska do not have counties, but they have functional equivalents. In order to explore the relationship between various factors and income per capita, a regression model was developed. It's important to explore this relationship because higher income per capita in a county leads to a higher quality of life for a county's population as long as factors that negatively influence purchasing power such as inflation and goods & service prices are only marginally changing year after year.\nProject Goals\n\nTo show which professions are vital in determining income per capita based on county in the United States\nAssess salary as it relates to these factors\nExamine the effect of poverty on income per capita\nPredict the salary of someone in the United States as the rate of these profession changes\nPredict which jobs we should grow to increase income per capita in a county\n\nData\nacs2017_census_tract_data.csv\nThe data was collected by the US Census Bureau and is publically available to use. It includes variables that potentially relate to income per capita in counties or functional equivalents in the United States.\nMethodology\nA regression model was developed to primarily explore the relationships between the independent variables of poverty rate, production, office, service, construction and professional jobs and the dependent variable income per capita based on a county in the United States.\nThe assumptions of regression modeling were checked prior to model development and included development of Pearson's R correlation heatmap, residual plots, exploration of variance inflation factors, Box-Cox transformation to assist with normalizing the relationship beween the dependent and independent variables. Prior to this, null values, duplicate values and outliers were purged from the dataset.\nProject Links\nPresentation\nPresentation Slides\nTechnical Notebook\nTechnical Notebook\nLibrary Imports\nLibrary Imports Python File\n""], 'url_profile': 'https://github.com/aindusekar', 'info_list': ['Jupyter Notebook', 'Updated Jul 8, 2020', 'Jupyter Notebook', 'Updated Oct 8, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'R', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'HTML', 'Updated Mar 2, 2020', '1', 'Python', 'Updated Mar 6, 2020', 'JavaScript', 'Updated Mar 3, 2020', 'R', 'Updated Apr 25, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '31 contributions\n        in the last year', 'description': [""Linear-Regression-Predicting-Housing-Prices\nYour neighbor is a real estate agent and wants some help predicting housing prices for regions in the USA. It would be great if you could somehow create a model for her that allows her to put in a few features of a house and returns back an estimate of what the house would sell for.\nShe has asked you if you could help her out with your new data science skills. You say yes, and decide that Linear Regression might be a good path to solve this problem!\nYour neighbor then gives you some information about a bunch of houses in regions of the United States,it is all in the data set: USA_Housing.csv.\nThe data contains the following columns:\n\n'Avg. Area Income': Avg. Income of residents of the city house is located in.\n'Avg. Area House Age': Avg Age of Houses in same city\n'Avg. Area Number of Rooms': Avg Number of Rooms for Houses in same city\n'Avg. Area Number of Bedrooms': Avg Number of Bedrooms for Houses in same city\n'Area Population': Population of city house is located in\n'Price': Price that the house sold at\n'Address': Address for the house\n\n""], 'url_profile': 'https://github.com/rshankarsharma9', 'info_list': ['Jupyter Notebook', 'Updated Jul 8, 2020', 'Jupyter Notebook', 'Updated Oct 8, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'R', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'HTML', 'Updated Mar 2, 2020', '1', 'Python', 'Updated Mar 6, 2020', 'JavaScript', 'Updated Mar 3, 2020', 'R', 'Updated Apr 25, 2020']}","{'location': 'Atlanta, Georgia, USA', 'stats_list': [], 'contributions': '38 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/chinyemba', 'info_list': ['Jupyter Notebook', 'Updated Jul 8, 2020', 'Jupyter Notebook', 'Updated Oct 8, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'R', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'HTML', 'Updated Mar 2, 2020', '1', 'Python', 'Updated Mar 6, 2020', 'JavaScript', 'Updated Mar 3, 2020', 'R', 'Updated Apr 25, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '176 contributions\n        in the last year', 'description': ['Multiple Linear Regression and Visualization of the Inflation Factor\nHere I have taken a real insurance data to trace visualizations\n'], 'url_profile': 'https://github.com/agasheaditya', 'info_list': ['Jupyter Notebook', 'Updated Jul 8, 2020', 'Jupyter Notebook', 'Updated Oct 8, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'R', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'HTML', 'Updated Mar 2, 2020', '1', 'Python', 'Updated Mar 6, 2020', 'JavaScript', 'Updated Mar 3, 2020', 'R', 'Updated Apr 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '80 contributions\n        in the last year', 'description': ['Taller-6-Visual-Regression-Testing\n'], 'url_profile': 'https://github.com/naty1610', 'info_list': ['Jupyter Notebook', 'Updated Jul 8, 2020', 'Jupyter Notebook', 'Updated Oct 8, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'R', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'HTML', 'Updated Mar 2, 2020', '1', 'Python', 'Updated Mar 6, 2020', 'JavaScript', 'Updated Mar 3, 2020', 'R', 'Updated Apr 25, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/jseshadrireddy', 'info_list': ['Jupyter Notebook', 'Updated Jul 8, 2020', 'Jupyter Notebook', 'Updated Oct 8, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'R', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'HTML', 'Updated Mar 2, 2020', '1', 'Python', 'Updated Mar 6, 2020', 'JavaScript', 'Updated Mar 3, 2020', 'R', 'Updated Apr 25, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '41 contributions\n        in the last year', 'description': ['MNIST_log_regression_fisher_discriminant\nThe purpose of this project is to understand how machine and deep learning algorithms work. For that, the MNIST handwritten digit dataset will be used and two algorithms will be implemented: A Fisher linear discriminant and a Logistic regression with a neural network mindset.\n'], 'url_profile': 'https://github.com/andonirazusta', 'info_list': ['Jupyter Notebook', 'Updated Mar 6, 2020', 'Python', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'MATLAB', 'Updated Mar 6, 2020', 'HTML', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020']}","{'location': 'Cape Town South Africa ', 'stats_list': [], 'contributions': '74 contributions\n        in the last year', 'description': [""Polynomial-Regression-to-Predict-Happiness\nIn this project, data is read in from the Happiness Index report found on Kaggle here:\nhttps://www.kaggle.com/unsdsn/world-happiness#2019.csv\nWhilst the happiness index is controversial and not meant to be interpreted as a science, its results are still interesting.\nIn particular the relationship of social support (close friends and family) on one's overall levels of happiness. The relationship appears to be exponential, thus the dataset made for a good opportunity to use Polynomial Regression, in order to model the curve.\nAs per the results of the of the survey, countries that ranked higher in the social support category appeared to have exponentially\nincreased levels of happiness.\nIn technical terms:\nThis project is written in Python and makes use of Matplotlib for data visualization, and Scikit Learn, for Linear and Polynomial Regression\nas well as training and testing the data.\nBONUS\nWhilst exploring the dataset, I also found a data visulation package called Plotly:\nhttps://plot.ly/\nUsing plotly, I was able to easily and neatly graph all the datasets within the Happiness index.\nOne of Plotly's interesting features is the ability to label datapoints and have the label appear only when the user hovers their mouse over the point.\nThis allows one to interact with the graph and see exactly which countries represent each datapoint in the given graphs.\nThe program is named: plotlyGraphs.py\n""], 'url_profile': 'https://github.com/XcessivePupil', 'info_list': ['Jupyter Notebook', 'Updated Mar 6, 2020', 'Python', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'MATLAB', 'Updated Mar 6, 2020', 'HTML', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['House-Price-Prediction\n'], 'url_profile': 'https://github.com/SarmadBasaria', 'info_list': ['Jupyter Notebook', 'Updated Mar 6, 2020', 'Python', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'MATLAB', 'Updated Mar 6, 2020', 'HTML', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['Image Inpainting with RBF Regression\n\nTrainRBFRegression implements the least-squares estimation process for the optimal weights\nevalRBFModel evaluates the RBF model at a set of input points using the estimated weights\nrbf2d.m returns the values of a RBF at a set of image coordinates\nGrayScalePatchReconstruction allows us to pick a small image patch, fits an RBF model to the patch and displays the reconstructed patch using the learned model\nTrainRBFRegression_regularized is the regularized version of TrainRBFRegression\n\n'], 'url_profile': 'https://github.com/jessicahwlau', 'info_list': ['Jupyter Notebook', 'Updated Mar 6, 2020', 'Python', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'MATLAB', 'Updated Mar 6, 2020', 'HTML', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['sales-forcasting-using-regression-analysis\n'], 'url_profile': 'https://github.com/manishboyina', 'info_list': ['Jupyter Notebook', 'Updated Mar 6, 2020', 'Python', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'MATLAB', 'Updated Mar 6, 2020', 'HTML', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '94 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/marioeid', 'info_list': ['Jupyter Notebook', 'Updated Mar 6, 2020', 'Python', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'MATLAB', 'Updated Mar 6, 2020', 'HTML', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020']}","{'location': 'National Institute of Technology,Rourkela, Sector 1, Sundargarh, Odisha , PIN : 769008', 'stats_list': [], 'contributions': '1,001 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Mrutyunjay01', 'info_list': ['Jupyter Notebook', 'Updated Mar 6, 2020', 'Python', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'MATLAB', 'Updated Mar 6, 2020', 'HTML', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020']}","{'location': 'San Francisco, CA ', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['Machine-Learning-Regression-based-analysis\nApprentice Chef is a fictional company which was created for this assignment.\nI was tasked to analyse historical data such as demographics and customer preferences in order to build a predictive model to understand of the revenue is impacted by these variables.\nMachine learning techniques used:\n\nFeature Engineering\nDummy Variable Encoding\nTraining and Testing Set Building\nModel Instantiation, Fitting, Predicting, and Scoring\nHyperparameter Tuning\nAutomated Hyperparameter Selection\nSupervised learning models such as OLS Linear Regression, CART Models and K-Nearest Neighbors.\n\n'], 'url_profile': 'https://github.com/Maxlev3', 'info_list': ['Jupyter Notebook', 'Updated Mar 6, 2020', 'Python', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'MATLAB', 'Updated Mar 6, 2020', 'HTML', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020']}","{'location': 'California', 'stats_list': [], 'contributions': '702 contributions\n        in the last year', 'description': ['Digit-recognizer-using-logistic-regression\nUsing MNIST dataset as an example.\n'], 'url_profile': 'https://github.com/pau-lo', 'info_list': ['Jupyter Notebook', 'Updated Mar 6, 2020', 'Python', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'MATLAB', 'Updated Mar 6, 2020', 'HTML', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['E-commerece-Linear-Regression-Project\nUsing mock consumer data from an E-commerce company to determine whether the company should focus their efforts developing their mobile app experience or their website.\nFirstly we begin with some EDA (Explantory Data Analysis). Once some intution is gathered regarding the data. We deploy a linear regression as our chosen machine learning algorithm, since its application is well suited for this task.\nWe conclude the model with an evaluation on the models performance.\n'], 'url_profile': 'https://github.com/Muk18', 'info_list': ['Jupyter Notebook', 'Updated Mar 6, 2020', 'Python', 'Updated Apr 15, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'MATLAB', 'Updated Mar 6, 2020', 'HTML', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '94 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/marioeid', 'info_list': ['Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Mar 4, 2020', 'JavaScript', 'Updated Mar 15, 2020', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', '1', 'R', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Kaminibokefode', 'info_list': ['Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Mar 4, 2020', 'JavaScript', 'Updated Mar 15, 2020', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', '1', 'R', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '112 contributions\n        in the last year', 'description': ['La explicacion detallada de cada punto del taller se puede ver en el Home de la wiki de este proyecto\n'], 'url_profile': 'https://github.com/nlemarodriguez', 'info_list': ['Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Mar 4, 2020', 'JavaScript', 'Updated Mar 15, 2020', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', '1', 'R', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}","{'location': 'France', 'stats_list': [], 'contributions': '38 contributions\n        in the last year', 'description': ['Comparison between some regularization techniques-Least-Squares Regression and Logistic Regression\n'], 'url_profile': 'https://github.com/seyni-diop', 'info_list': ['Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Mar 4, 2020', 'JavaScript', 'Updated Mar 15, 2020', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', '1', 'R', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '58 contributions\n        in the last year', 'description': ['Song-popularity-prediciton\nPrediction using extreme gradient boost regressor\n'], 'url_profile': 'https://github.com/aryamonani', 'info_list': ['Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Mar 4, 2020', 'JavaScript', 'Updated Mar 15, 2020', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', '1', 'R', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '40 contributions\n        in the last year', 'description': ['MRR\nProjet de m√©thodes de r√©gressions r√©gularis√©es\nCe projet est port√© sur l‚Äô√©tude du trafic urbain √† Sao Paulo, au Br√©sil. Avec l‚Äôavancement technologique\ndes derni√®res ann√©es, de nombreuses probl√©matiques touchant les transports et la logistique √©mergent. Le\nbut est de trouver un mod√®le de r√©gression permettant de pr√©dire la lenteur du trafic en fonction\ndes param√®tres fournis par la base de donn√©es.\nLa variable cible est donc Slowness in traffic, nous allons √©tudier ses variations en fonction des\nparam√®tres les plus influents.\n'], 'url_profile': 'https://github.com/shi-de-m', 'info_list': ['Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Mar 4, 2020', 'JavaScript', 'Updated Mar 15, 2020', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', '1', 'R', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '65 contributions\n        in the last year', 'description': ['Univariate-and-Multivariate-Linear-Regression-from-scratch-in-Tensorflow\nUnivariate Linear regression is done on Toy Dataset while multivariate Linear Regression is done using https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data as the data. Here the MPG is the target.\nExercise 2 is on using Logistic Regression for the Olivetti faces dataset.\nThese are part of Distributed Data Analytics Lab assignment.\n'], 'url_profile': 'https://github.com/NipamNayan', 'info_list': ['Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Mar 4, 2020', 'JavaScript', 'Updated Mar 15, 2020', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', '1', 'R', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}","{'location': 'Vallejo, CA', 'stats_list': [], 'contributions': '218 contributions\n        in the last year', 'description': ['STA-108-Project-2\nA term project for my Linear Regression Analysis class in which I conduct multi-linear regression techniques on the same regional dataset that I had for my 1st project.\n'], 'url_profile': 'https://github.com/JeffTheAggie', 'info_list': ['Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Mar 4, 2020', 'JavaScript', 'Updated Mar 15, 2020', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', '1', 'R', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '58 contributions\n        in the last year', 'description': ['House-Price-Prediction-using-Randoom-Forest-Regression\n'], 'url_profile': 'https://github.com/aryamonani', 'info_list': ['Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Mar 4, 2020', 'JavaScript', 'Updated Mar 15, 2020', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', '1', 'R', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Model Validation\nIntroduction\nPreviously you\'ve briefly touched upon model evaluation when using a multiple linear regression model for prediction. In this lesson you\'ll learn why it\'s important to split your data in a train and a test set if you want to do proper performance evaluation.\nObjectives\nYou will:\n\nCompare training and testing errors to determine if model is over or underfitting\n\nThe need for train-test split\nMaking predictions and evaluation\nSo far we\'ve simply been fitting models to data, and evaluated our models calculating the errors between our $\\hat y$ and our actual targets $y$, while these targets $y$ contributed in fitting the model.\nThe reason why we built the model in the first place, however, is because we want to predict the outcome for observations that are not necessarily in our dataset now; e.g: we want to predict miles per gallon for a new car that isn\'t part of our dataset, or for a new house in Boston.\nIn order to get a good sense of how well your model will be doing on new instances, you\'ll have to perform a so-called ""train-test-split"". What you\'ll be doing here, is take a sample of the data that serves as input to ""train"" our model - fit a linear regression and compute the parameter estimates for our variables, and calculate how well our predictive performance is doing comparing the actual targets $y$ and the fitted $\\hat y$ obtained by our model.\nUnderfitting and overfitting\nAnother reason to use train-test-split is because of a common problem which doesn\'t only affect linear models, but nearly all (other) machine learning algorithms: overfitting and underfitting. An overfit model is not generalizable and will not hold to future cases. An underfit model does not make full use of the information available and produces weaker predictions than is feasible. The following image gives a nice, more general demonstration:\n\nMechanics of train-test split\nWhen performing a train-test-split, it is important that the data is randomly split. At some point, you will encounter datasets that have certain characteristics that are only present in certain segments of the data. For example, if you were looking at sales data for a website, you might expect the data to look different on days that promotional deals were held versus days that deals were not held. If we don\'t randomly split the data, there is a chance we might overfit to the characteristics of certain segments of data.\nAnother thing to consider is just how big each training and testing set should be. There is no hard and fast rule for deciding the correct size, but the range of training set is usually anywhere from 66% - 80% (and testing set between 33% and 20%). Some types of machine learning models need a substantial amount of data to train on, and as such, the training sets should be larger. Some models with many different tuning parameters will need to be validated with larger sets (the test size should be larger) to determine what the optimal parameters should be. When in doubt, just stick with training set sizes around 70% and test set sizes around 30%.\nHow to evaluate?\nIt is pretty straightforward that, to evaluate the model, you\'ll want to compare your predicted values, $\\hat y$ with the actual value, $y$. The difference between the two values is referred to as the residuals. When using a train-test split, you\'ll compare your residuals for both test set and training set:\n$r_{i,train} = y_{i,train} - \\hat y_{i,train}$\n$r_{i,test} = y_{i,test} - \\hat y_{i,test}$\nTo get a summarized measure over all the instances in the test set and training set, a popular metric is the (Root) Mean Squared Error:\nRMSE = $\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_{i} - \\hat y_{i})^2}$\nMSE = $\\frac{1}{n}\\sum_{i=1}^{n}(y_{i} - \\hat y_{i})^2$\nAgain, you can compute these for both the traing and the test set. A big difference in value between the test and training set (R)MSE is an indication of overfitting.\nApplying this to our auto-mpg data\nLet\'s copy our pre-processed auto-mpg data again\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(\'auto-mpg.csv\') \ndata[\'horsepower\'].astype(str).astype(int)\n\nacc = data[\'acceleration\']\nlogdisp = np.log(data[\'displacement\'])\nloghorse = np.log(data[\'horsepower\'])\nlogweight = np.log(data[\'weight\'])\n\nscaled_acc = (acc-min(acc))/(max(acc)-min(acc))\nscaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\nscaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\nscaled_weight = (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n\ndata_fin = pd.DataFrame([])\ndata_fin[\'acc\'] = scaled_acc\ndata_fin[\'disp\'] = scaled_disp\ndata_fin[\'horse\'] = scaled_horse\ndata_fin[\'weight\'] = scaled_weight\ncyl_dummies = pd.get_dummies(data[\'cylinders\'], prefix=\'cyl\', drop_first=True)\nyr_dummies = pd.get_dummies(data[\'model year\'], prefix=\'yr\', drop_first=True)\norig_dummies = pd.get_dummies(data[\'origin\'], prefix=\'orig\', drop_first=True)\nmpg = data[\'mpg\']\ndata_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)\ndata = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis=1)\ny = data[[\'mpg\']]\nX = data.drop([\'mpg\'], axis=1)\nScikit-learn has a very useful function, train_test_split(). The optional argument test_size makes it possible to choose the size of the test set and the training set. Since the observations are randomly assigned to the training and test splits each time you run train_test_split(), you can also pass an optional random_state argument to obtain reproducible results. This will ensure your training and test sets are always the same.\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint(len(X_train), len(X_test), len(y_train), len(y_test))\n313 79 313 79\n\nfrom sklearn.linear_model import LinearRegression\nlinreg = LinearRegression()\nlinreg.fit(X_train, y_train)\n\ny_hat_train = linreg.predict(X_train)\ny_hat_test = linreg.predict(X_test)\nLook at the residuals and calculate the MSE for training and test sets:\ntrain_residuals = y_hat_train - y_train\ntest_residuals = y_hat_test - y_test\nmse_train = np.sum((y_train-y_hat_train)**2)/len(y_train)\nmse_test = np.sum((y_test-y_hat_test)**2)/len(y_test)\nprint(\'Train Mean Squarred Error:\', mse_train)\nprint(\'Test Mean Squarred Error:\', mse_test)\nTrain Mean Squarred Error: mpg    16.790262\ndtype: float64\nTest Mean Squarred Error: mpg    16.500021\ndtype: float64\n\nYou can also do this directly using sklearn\'s mean_squared_error() function:\nfrom sklearn.metrics import mean_squared_error\n\ntrain_mse = mean_squared_error(y_train, y_hat_train)\ntest_mse = mean_squared_error(y_test, y_hat_test)\nprint(\'Train Mean Squarred Error:\', train_mse)\nprint(\'Test Mean Squarred Error:\', test_mse)\nTrain Mean Squarred Error: 16.79026189951861\nTest Mean Squarred Error: 16.50002062788149\n\nGreat, there does not seem to be a big difference between the train and test MSE! Interestingly, the test set error is smaller than the training set error. This is fairly rare but does occasionally happen.\nAdditional resources\nGreat job! You now have a lot of ingredients to build a pretty good (multiple) linear regression model. We\'ll add one more concept in the next lesson: the idea of cross-validation. But first, we strongly recommend you have a look at this blogpost to get a refresher on a lot of the concepts learned!\nSummary\nIn this lesson, you learned the importance of the train-test split approach and were introduced to one of the most popular metrics for evaluating regression models, (R)MSE. You also saw how to use the train_test_split() function from sklearn to split your data into training and test sets.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Mar 4, 2020', 'JavaScript', 'Updated Mar 15, 2020', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', '1', 'R', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Analysis using Linear Algebra and NumPy - Code Along\nIntroduction\nIn the previous sections, you learned that in statistical modeling, regression analysis is a set of statistical processes for estimating the relationships between data entities (variables). Linear regression is an important predictive analytical tool in the data scientist\'s toolbox. Here, you\'ll try and develop a basic intuition for regression from a linear algebra perspective using vectors and matrix operations. This lesson covers least-squares regression with matrix algebra without digging deep into the geometric dimensions.\nYou can find a deeper mathematical and geometric explanation of the topic here. In this lesson, we\'ll try to keep things more data-oriented.\nObjectives\nYou will be able to:\n\nApply linear algebra to fit a function to data, describing linear mappings between input and output variables\nIndicate how linear algebra is related to regression modeling\n\nRegression analysis\nBy now, you know that the purpose of the regression process is to fit a mathematical model to a set of observed points, in order to later use that model for predicting new values e.g. predicting sales, based on historical sales figures, predicting house prices based on different features of the house, etc.\nLet\'s use a very simple toy example to understand how this works with linear algebra. Say you are collecting data on total number of sales per day for some business. Imagine you\'ve got three data points in the format:\n(day, total number of sales(in hundreds))\n\n(1, 1) , (2, 2) , (3, 2)\n\nIf we plot these points on a scatter plot with day (x-axis) vs. sales figures (y-axis), this is what we get:\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.array([1,2,3])\ny = np.array([1,2,2])\n               \nplt.plot(x, y, \'o\')\nplt.xticks(x)\n\nplt.show()\n#\xa0Code here \nFitting a model to data - A quick refresher\nThe purpose of linear regression would be to fit a mathematical model (a straight line) in the parameter space  that best describes the relationship between day and sales. Simple linear regression attempts to fit a line (in a 2-dimensional space) to describe the relationship between two variables as shown in the example below:\n\nFollowing this, if you were to identify a relationship between the day and total number of sales, the goal would be to seek a function that describes this line and allows us to linearly map input data points (day) or independent variable to outcome values (sales) or dependent variable.  If you do this, you first assume that there is an underlying relationship that maps ‚Äúdays‚Äù uniquely to ‚Äúnumber of sales‚Äù, that can be written in the function form as an equation of the straight line i.e.\n$$y = mx+c$$\nwhere $c$ is the intercept of the line and $m$ denotes the slope, as shown below:\n\nWe can write the fitting function based on the above as sales being a function of days.\n$$ \\text{sales} = f(\\text{days})$$\nor, from $y= mx+c$\n$$\\text{sales} = \\text{days}*x + \\text{intercept} $$\n\n(where y is the number of sales per day and x represents the day. c (intercept) and m (slope) are the regression coefficients we are looking for hoping that these co-efficients will linearly map day to the number of sales).\n\nSo using this, we can show our three data points ((1, 1) , (2, 2) , (3, 2)) as:\n\n$c + m*1 = 1$\n\n\n$c + m*2 = 2$\n\n\n$c + m*3 = 2$\n\nWe can see that our data points do not lie on a line. The first two points make a perfect linear system. When $x = 1$, $y = 1$; and when $x = 2$, $y = 2$ i.e. we can draw a straight line passing through these points. When x = 3, b = 2, you know the three points do not lie on the same line as first two points, and our model will be an approximation i.e.\n\nthere will be some error between the straight line and the REAL relationship between these parameters.\n\nThis behavior can be simulated by using NumPy\'s polyfit() function (similar to statsmodels.ols) to draw a regression line to the data points as shown below. Here is the documentation for np.polyfit().\nfrom numpy.polynomial.polynomial import polyfit\n\n# Fit with polyfit function to get c(intercept) and m(slope)\n# the degree parameter = 1 to models this as a straight line\nc, m = polyfit(x, y, 1)\n\n# Plot the data points and line calculated from ployfit\nplt.plot(x, y, \'o\')\nplt.plot(x, c + (m * x), \'-\')\nplt.xticks(x)\n\nplt.show()\nprint(c, m)\n#\xa0Code here \nThe numbers obtained here reflect the slope (0.5) and intercept values (0.66).\nThe line drawn above using this built-in regression model clearly doesn\'t touch all the data points. As a result, this is an approximation of the function you\'re trying to find. Now let\'s see how to achieve the same functionality with matrix algebra instead of the polyfit() function.\nCreate matrices and vectors\nA linear system like the one above can be solved using linear algebra! You only need to deal with a few vectors and matrices to set this up.\nRecalling linear systems from the previous lessons, you have:\n$$\n\\left[ {\\begin{array}{cc}\n1 & 1 \\\n1 & 2 \\\n1 & 3 \\\n\\end{array} } \\right]\n\\left[ {\\begin{array}{c}\nc \\\nm \\\n\\end{array} } \\right] =\n\\left[ {\\begin{array}{c}\n1 \\\n2 \\\n2 \\\n\\end{array} } \\right]\n$$\nThe intercept and error terms\nThe column of ones in the first matrix refers to the intercept ($c$) from $mx+c$. If you don\'t include this constant, then the function is constrained to the origin (0,0), which would strongly limit the types of relationships the model could describe. You want to include an intercept to allow for linear models to intersect with the $y$-axis at values different from 0 (in the image shown earlier, $c$ was 2, because the straight line crossed the $y$-axis at $y$=2).\nIn above , we are hoping that there is some linear combination of the columns of the first matrix that gives us our vector of observed values (the vector with values 1,2,2).\nUnfortunately, we already know that this vector does not fit our model perfectly. That means it is outside the column space of A and we can\'t solve that equation for the vector $x$ directly. Every line we draw will have some value of error $e$ associated with it.\nThe goal is to choose the vector $x$ for unknown variables to make $e$ as small as possible.\nOrdinary least squares\nA common measure to find and minimize the value of this error is called Ordinary Least Squares.\nThis says that our dependent variable, is composed of a linear part and error. The linear part is composed of an intercept and independent variable(s), along with their associated raw score regression weights.\nIn matrix terms, the same equation can be written as:\n$ y = \\boldsymbol{X} b + e $\nThis says to get y (sales), multiply each $\\boldsymbol{X}$ by the appropriate vector b (unknown parameters, the vector version of $m$ and $c$), then add an error term. We create a matrix $\\boldsymbol{X}$ , which has an extra column of 1s in it for the intercept. For each day, the 1 is used to add the intercept in the first row of the column vector $b$.\nLet\'s assume that the error is equal to zero on average and drop it to sketch a proof:\n$ y = \\boldsymbol{X} b$\nNow let\'s solve for $b$, so we need to get rid of $\\boldsymbol{X}$. First we will make X into a nice square, symmetric matrix by multiplying both sides of the equation by $\\boldsymbol{X}^T$ :\n$\\boldsymbol{X}^T y = \\boldsymbol{X}^T \\boldsymbol{X}b $\nAnd now we have a square matrix that with any luck has an inverse, which we will call $(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}$. Multiply both sides by this inverse, and we have\n$(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y =(\\boldsymbol{X}^T\\boldsymbol{X})^{-1} \\boldsymbol{X}^T \\boldsymbol{X}b $\nIt turns out that a matrix multiplied by its inverse is the identity matrix $(\\boldsymbol{X}^{-1}\\boldsymbol{X})= I$:\n$(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y =I b $\nAnd you know that $Ib= b$ So if you want to solve for $b$ (that is, remember, equivalent to finding the values $m$ and $c$ in this case), you find that:\n$ b= (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y $\nHere, we\'ll focus on the matrix and vector algebra perspective. With least squares regression, in order to solve for the expected value of weights, referred to as $\\hat{X}$ (""$X$-hat""), you need to solve the above equation.\nRemember all above variables represent vectors. The elements of the vector X-hat are the estimated regression coefficients $c$ and $m$ that you\'re looking for. They minimize the error between the model and the observed data in an elegant way that uses no calculus or complicated algebraic sums.\nThe above description can be summarized as:\nUsing linear regression is just trying to solve $Xb = y$. But if any of the observed points deviate from the model, you can\'t find a direct solution. To find a solution, you can multiply both sides by the transpose of $X$. The transpose of $X$ times $X$ will always allow us to solve for unknown variables.\nCalculate an OLS regression line\nLet\'s use the above formula to calculate a solution for our toy problem:\n# Calculate the solution\n\nX = np.array([[1, 1],[1, 2],[1, 3]])\ny = np.array([1, 2, 2])\nXt = X.T\nXtX = Xt.dot(X)\nXtX_inv = np.linalg.inv(XtX)\nXty = Xt.dot(y)\nx_hat = XtX_inv.dot(Xty) #\xa0the value for b shown above\nx_hat\n# Code here \nThe solution gives an intercept of 0.6 and slope value 0.5. Let\'s see what you get if you draw a line with these values with given data:\n# Define data points\nx = np.array([1, 2, 3])\ny = np.array([1, 2, 2])\n\n# Plot the data points and line parameters calculated above\nplt.plot(x, y, \'o\')\nplt.plot(x, x_hat[0] + (x_hat[1] * x), \'-\')\nplt.xticks(x)\n\nplt.show()\n#\xa0Code here \nThere you have it, an approximated line function! Just like the one you saw with polyfit(), by using simple matrix algebra.\nRegression with multiple variables\nAbove, you saw how you can draw a line on a 2D space using simple regression. If you perform a similar function with multiple variables, you can have a parameter space that is not 2D. With 3 parameters, i.e. two input and one output feature, the fitting function would not be a line, but would look like a plane:\n\nWhen you have more than one input variable, each data point can be seen as a feature vector $x_i$, composed of $x_1, x_2, \\ldots , x_m$ , where $m$ is the total number of features (columns). For multiple regression, each data point can contain two or more features of the input. To represent all of the input data along with the vector of output values we set up a input matrix X and an output vector y.\nyou can write this in general terms, as you saw earlier:\n\n$\\boldsymbol{X} \\beta \\approx y$\n\nWhere X are the input feature values, $\\beta$ represents the coefficients and y is the output (value to be predicted). In a simple least-squares linear regression model you are looking for a vector $\\beta$ so that the product $X \\beta$ most closely approximates the outcome vector y.\nFor each value of input features $x_i$, we can compute a predicted outcome value as:\nobserved data $\\rightarrow$ $y = b_0+b_1x_1+b_2x_2+ \\ldots + b_px_p+ \\epsilon $\npredicted data $\\rightarrow$ $\\hat y = \\hat b_0+\\hat b_1x_1+\\hat b_2x_2+ \\ldots + \\hat b_px_p $\nerror $\\rightarrow$ $\\epsilon = y - \\hat y $\nJust like before,  the formula to compute the beta vector remains:\n$ \\large b= (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y $\nSo you see that the general solution involves taking a matrix transpose, the inverse, and dot multiplications on the lines of solving a linear system of equations.\nIn the next lab, you\'ll use a simple dataset and with the above formulation for multivariate regression, you\'ll try to fit a model to the data and see how well it performs.\nFurther reading\nYou\'re strongly advised to visit the following links to develop a strong mathematical and geometrical intuition around how least squares work. These documents will provide you with a visual intuition as well as an in-depth mathematical formulation for above equations along with their proofs.\n\n\nQuora: Why do we need an extra column of ones in regression\n\n\nAn excellent visual demonstration of oridnary least squares\n\n\nSimple Regression in Matrix format\n\n\nSummary\nIn this lesson, you had a gentle introduction to how we can use linear algebra to solve regression problems. You saw a toy example in the case of simple linear regression, relating days to number of sales and calculated a function that approximates the linear mapping.\nYou also learned about how linear regression works in the context of multiple input variables and linear algebra. In the next lab, you\'ll use these equations to solve a real world problem.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', '1', 'Python', 'Updated Mar 3, 2020', 'MATLAB', 'Updated Mar 2, 2020', 'MATLAB', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression with Linear Algebra - Lab\nIntroduction\nIn this lab, you\'ll apply regression analysis using simple matrix manipulations to fit a model to given data, and then predict new values for previously unseen data. You\'ll follow the approach highlighted in the previous lesson where you used NumPy to build the appropriate matrices and vectors and solve for the $\\beta$ (unknown variables) vector. The beta vector will be used with test data to make new predictions. You\'ll also evaluate the model fit.\nIn order to make this experiment interesting, you\'ll use NumPy at every single stage of this experiment, i.e., loading data, creating matrices, performing train-test split, model fitting, and evaluation.\nObjectives\nIn this lab you will:\n\nUse matrix algebra to calculate the parameter values of a linear regression\n\nFirst, let\'s import necessary libraries:\nimport csv # for reading csv file\nimport numpy as np\nDataset\nThe dataset you\'ll use for this experiment is ""Sales Prices in the City of Windsor, Canada"", something very similar to the Boston Housing dataset. This dataset contains a number of input (independent) variables, including area, number of bedrooms/bathrooms, facilities(AC/garage), etc. and an output (dependent) variable, price.  You\'ll formulate a linear algebra problem to find linear mappings from input features using the equation provided in the previous lesson.\nThis will allow you to find a relationship between house features and house price for the given data, allowing you to find unknown prices for houses, given the input features.\nA description of the dataset and included features is available here.\nIn your repository, the dataset is available as windsor_housing.csv. There are 11 input features (first 11 columns):\nlotsize\tbedrooms  bathrms  stories\tdriveway  recroom\tfullbase  gashw\t airco  garagepl   prefarea\n\nand 1 output feature i.e. price (12th column).\nThe focus of this lab is not really answering a preset analytical question, but to learn how you can perform a regression experiment, using mathematical manipulations - similar to the one you performed using statsmodels. So you won\'t be using any pandas or statsmodels goodness here. The key objectives here are to:\n\nUnderstand regression with matrix algebra and\nMastery in NumPy scientific computation\n\nStage 1: Prepare data for modeling\nLet\'s give you a head start by importing the dataset. You\'ll perform the following steps to get the data ready for analysis:\n\n\nInitialize an empty list data for loading data\n\n\nRead the csv file containing complete (raw) windsor_housing.csv. Use csv.reader() for loading data.. Store this in data one row at a time\n\n\nDrop the first row of csv file as it contains the names of variables (header) which won\'t be used during analysis (keeping this will cause errors as it contains text values)\n\n\nAppend a column of all 1s to the data (bias) as the first column\n\n\nConvert data to a NumPy array and inspect first few rows\n\n\n\nNOTE: read.csv() reads the csv as a text file, so you should convert the contents to float.\n\n# Your code here\n\n\n\n\n\n# First 5 rows of raw data \n\n# array([[1.00e+00, 5.85e+03, 3.00e+00, 1.00e+00, 2.00e+00, 1.00e+00,\n#         0.00e+00, 1.00e+00, 0.00e+00, 0.00e+00, 1.00e+00, 0.00e+00,\n#         4.20e+04],\n#        [1.00e+00, 4.00e+03, 2.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         3.85e+04],\n#        [1.00e+00, 3.06e+03, 3.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         4.95e+04],\n#        [1.00e+00, 6.65e+03, 3.00e+00, 1.00e+00, 2.00e+00, 1.00e+00,\n#         1.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         6.05e+04],\n#        [1.00e+00, 6.36e+03, 2.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         6.10e+04]])\nStep 2: Perform a 80/20 train-test split\nExplore NumPy\'s official documentation to manually split a dataset using a random sampling method of your choice. Some useful methods are located in the numpy.random library.\n\nPerform a random 80/20 split on data using a method of your choice in NumPy\nSplit the data to create x_train, y_train, x_test, and y_test arrays\nInspect the contents to see if the split performed as expected\n\n\nNote: When randomly splitting data, it\'s always recommended to set a seed in order to ensure reproducibility\n\n# Your code here \n\n\n\n# Split results\n# Raw data Shape:  (546, 13)\n# Train/Test Split: (437, 13) (109, 13)\n# x_train, y_train, x_test, y_test: (437, 12) (437,) (109, 12) (109,)\nStep 3: Calculate the beta\nWith $X$ and $y$ in place, you can now compute your beta values with $x_\\text{train}$ and $y_\\text{train}$ as:\n$\\beta = (x_\\text{train}^T. x_\\text{train})^{-1} . x_\\text{train}^T . y_\\text{train}$\n\nUsing NumPy operations (transpose, inverse) that we saw earlier, compute the above equation in steps\nPrint your beta values\n\n#\xa0Your code here \n\n\n\n\n# Beta values\n# Due to random split, your answers may vary \n# [-5.46637290e+03  3.62457767e+00  2.75100964e+03  1.47223649e+04\n#   5.97774591e+03  5.71916945e+03  5.73109882e+03  3.83586258e+03\n#   8.12674607e+03  1.33296437e+04  3.74995169e+03  1.01514699e+04]\nStep 4: Make predictions\nGreat, you now have a set of coefficients that describe the linear mappings between $X$ and $y$. You can now use the calculated beta values with the test datasets that we left out to calculate $y$ predictions. Next, use all features in turn and multiply it with this beta. The result will give a prediction for each row which you can append to a new array of predictions.\n$\\hat{y} = x\\beta = \\beta_0 + \\beta_1 x_1 +  \\beta_2 x_2 + \\ldots + \\beta_m x_m $\n\nCreate a new empty list (y_pred) for saving predictions\nFor each row of x_test, take the dot product of the row with beta to calculate the prediction for that row\nAppend the predictions to y_pred\nPrint the new set of predictions\n\n#\xa0Your code here \nStep 5: Evaluate model\nVisualize actual vs. predicted values\nThis is exciting, now your model can use the beta value to predict the price of houses given the input features. Let\'s plot these predictions against the actual values in y_test to see how much our model deviates.\n# Plot predicted and actual values as line plots\nThis doesn\'t look so bad, does it? Your model, although isn\'t perfect at this stage, is making a good attempt to predict house prices although a few prediction seem a bit out. There could be a number of reasons for this. Let\'s try to dig a bit deeper to check model\'s predictive abilities by comparing these prediction with actual values of y_test individually. That will help you calculate the RMSE value (root mean squared error) for your model.\nRoot Mean Squared Error\nHere is the formula for RMSE:\n$$ \\large RMSE = \\sqrt{\\sum^N_{i=1}\\dfrac{ (\\text{Predicted}_i-\\text{Actual}_i)^2}{N}}$$\n\nInitialize an empty array err\nFor each row in y_test and y_pred, take the squared difference and append error for each row in the err array\nCalculate $RMSE$ from err using the formula shown above\n\n# Calculate RMSE\n\n# Due to random split, your answers may vary \n# RMSE = 14868.172645765708\nNormalized root mean squared error\nThe above error is clearly in terms of the dependent variable, i.e., the final house price. You can also use a normalized mean squared error in case of multiple regression which can be calculated from RMSE using following the formula:\n$$ \\large NRMSE = \\dfrac{RMSE}{max_i y_i - min_i y_i} $$\n\nCalculate normalized RMSE\n\n#\xa0Calculate NRMSE\n\n# Due to random split, your answers may vary \n# 0.09011013724706489\nThere it is. A complete multiple regression analysis using nothing but NumPy. Having good programming skills in NumPy allows you to dig deeper into analytical algorithms in machine learning and deep learning. Using matrix multiplication techniques you saw here, you can easily build a whole neural network from scratch.\nLevel up (Optional)\n\nCalculate the R-squared and adjusted R-squared for the above model\nPlot the residuals (similar to statsmodels) and comment on the variance and heteroscedasticity\nRun the experiment in statsmodels and compare the performance of both approaches in terms of computational cost\n\nSummary\nIn this lab, you built a predictive model for predicting house prices. Remember this is a very naive implementation of regression modeling. The purpose here was to get an introduction to the applications of linear algebra into machine learning and predictive analysis. There are a number of shortcomings in this modeling approach and you can further apply a number of data modeling techniques to improve this model.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', '1', 'Python', 'Updated Mar 3, 2020', 'MATLAB', 'Updated Mar 2, 2020', 'MATLAB', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '669 contributions\n        in the last year', 'description': ['Company-Profit-Prediction-With-Multiple-Regression\nThis is the model that predict the profit of a company having many branches in various state on the basis of investment in R&D,ADMINSTRATION.,MARKETING and LOCATION\nALL CODE IS WRITTEN IN JUPYTER NOTEBOOK IN PYTHON\nCODE AND DATASET HAS BEEN PROVIDED\n'], 'url_profile': 'https://github.com/devil-cyber', 'info_list': ['Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', '1', 'Python', 'Updated Mar 3, 2020', 'MATLAB', 'Updated Mar 2, 2020', 'MATLAB', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/subagan', 'info_list': ['Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', '1', 'Python', 'Updated Mar 3, 2020', 'MATLAB', 'Updated Mar 2, 2020', 'MATLAB', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '27 contributions\n        in the last year', 'description': ['Universal Linear Regression Model(MATLAB)\nI have tried fitting the given data points to the polynomial of any given order without using object-oriented programming in MATLAB.\nThe inputs are going to be X and Y coordinates of the data points and the order of the polynomial to which the data is fiited.\nThe output is a plot along with the equation of the polynomial.\nPlease ignore the comments.\n'], 'url_profile': 'https://github.com/AmarthyaK', 'info_list': ['Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', '1', 'Python', 'Updated Mar 3, 2020', 'MATLAB', 'Updated Mar 2, 2020', 'MATLAB', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Polynomial Regression\nIntroduction\nIn the last lesson, you saw how you can account for interactions between two variables by including interaction effects in your model. In this lesson, you\'ll learn about another way to extend your regression model by including polynomial terms.\nObjectives\nYou will be able to:\n\nDefine polynomial variables in a regression context\nUse sklearn\'s built-in capabilities to create polynomial features\n\nAn example with one predictor\nThe dataset \'yields.csv\', with just 21 cases, contains measurements of the yields from an experiment done at six different levels of temperature in degrees Fahrenheit. Let\'s plot them.\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nyld = pd.read_csv(\'yield.csv\', sep=\'\\s+\', index_col=0)\nyld.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nTemp\nYield\n\n\ni\n\n\n\n\n\n\n1\n50\n3.3\n\n\n2\n50\n2.8\n\n\n3\n50\n2.9\n\n\n4\n50\n3.2\n\n\n5\n60\n2.7\n\n\n\n\nWe will now seperate the DataFrame into the predictor and outcome variables, X and y. We do so by:\n\nAssigning the \'Yield\' column to y\nDropping the \'Yield\' column from the yld DataFrame to create X\n\nNote: All scikit-learn classes assume the data to be in a certain shape, and by preparing data in this way, we ensure that these requirements are always met.\ny = yld[\'Yield\']\nX = yld.drop(columns=\'Yield\', axis=1)\nplt.scatter(X, y, color=\'green\')\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nIt\'s clear that there is no linear relationship between Yield and Temperature. Let\'s try and plot a line anyways and see how the model performs:\nfrom sklearn.linear_model import LinearRegression\nreg = LinearRegression().fit(X, y)\nplt.scatter(X, y, color=\'green\')\nplt.plot(X, reg.predict(X))\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nmean_squared_error(y, reg.predict(X))\n\nr2_score(y, reg.predict(X))\n0.08605718085106362\n\nA quadratic relationship\nWhen relationships between predictors and outcome are not linear and show some sort of curvature, polynomials can be used to generate better approximations. The idea is that you can transform your input variable by e.g, squaring it. The corresponding model would then be:\n$\\hat y = \\hat \\beta_0 + \\hat \\beta_1x + \\hat \\beta_2 x^2$\nThe idea is simple. You can square your predictor (here, ""Temp"") and include it in your model as if it were a new predictor.\nX[\'Temp_sq\'] = X[\'Temp\']**2\nX.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nTemp\nTemp_sq\n\n\ni\n\n\n\n\n\n\n1\n50\n2500\n\n\n2\n50\n2500\n\n\n3\n50\n2500\n\n\n4\n50\n2500\n\n\n5\n60\n3600\n\n\n\n\nreg_q = LinearRegression().fit(X, y)\nplt.scatter(X[\'Temp\'], y, color=\'green\')\nplt.plot(X[\'Temp\'], reg_q.predict(X))\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nThis is the resulting plot. Note that the fit is much better, and this is confirmed by the $R^2$score: where it was 0.086 before, it now is 0.6948!\nmean_squared_error(y, reg_q.predict(X))\n0.04650413890879158\n\nr2_score(y, reg_q.predict(X))\n0.6948165884110553\n\nNote that you get a seemingly ""piecewise linear"" function here,  because the yields were only measured at 50, 60, 70, 80, 90 and 100. In reality, this model generates a smooth curve, as denoted below.\nimport numpy as np\nplt.scatter(X[\'Temp\'], y, color=\'green\')\n\nX_pred = pd.DataFrame(np.linspace(50, 100, 50), columns=[\'Temp\'])\nX_pred[\'Temp_sq\'] = X_pred**2 \ny_pred = reg_q.predict(X_pred)\n\nplt.plot(X_pred[\'Temp\'], y_pred)\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nHigher-order relationships\nThe use of polynomials is not restricted to quadratic relationships. You can explore cubic or higher order relationships as well! Imagine you want to go until the power of 10, it would be quite annoying to transform your variable 9 times. Of course, Scikit-Learn has a built-in polynomial option in the preprocessing module! Let\'s call it with a polynomial of 6!\nfrom sklearn.preprocessing import PolynomialFeatures\n\ny = yld[\'Yield\']\nX = yld.drop(columns=\'Yield\', axis=1)\n\npoly = PolynomialFeatures(6)\nX_fin = poly.fit_transform(X)\nTake a look at what these transformed features really look like. As you can see, Scikit-Learn transformed the X value of a single 50 into $50^1$ through $50^6$ ! The first value of 1 represents the intercept in the linear regression, which you can read more about in the PolynomialFeatures documentation.\nprint(\'The transformed feature names are: {}\'.format(poly.get_feature_names()))\nprint(\'------------------\')\nprint(\'The first row of transformed data is: {}\'.format(X_fin[0]))\nThe transformed feature names are: [\'1\', \'x0\', \'x0^2\', \'x0^3\', \'x0^4\', \'x0^5\', \'x0^6\']\n------------------\nThe first row of transformed data is: [1.0000e+00 5.0000e+01 2.5000e+03 1.2500e+05 6.2500e+06 3.1250e+08\n 1.5625e+10]\n\nNow you can fit a linear regression model with your transformed data.\nreg_poly = LinearRegression().fit(X_fin, y)\nX_linspace = pd.DataFrame(np.linspace(50, 100, 50), columns=[\'Temp\'])\n\nX_linspace_fin = poly.fit_transform(X_linspace)\ny_poly_pred = reg_poly.predict(X_linspace_fin)\nplt.scatter(X[\'Temp\'], y, color=\'green\')\nplt.plot(X_linspace, y_poly_pred)\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nmean_squared_error(y, reg_poly.predict(X_fin))\n0.03670634920635693\n\nr2_score(y, reg_poly.predict(X_fin))\n0.7591145833332826\n\nThis seems to be a pretty smooth fit! This good fit is also confirmed with an even better $R^2$. Do note that by adding polynomials, you make your model more complex. Instead of just having 2 parameters ($\\beta_0$ and $\\beta_1$) for a linear model, you now have 7 (one for the intercept, and 6 for the terms when going until a polynomial with degree 6). More on that later!\nSummary\nGreat! You now know how to include polynomials in your linear models. Let\'s go ahead and practice this knowledge!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', '1', 'Python', 'Updated Mar 3, 2020', 'MATLAB', 'Updated Mar 2, 2020', 'MATLAB', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Polynomial Regression - Lab\nIntroduction\nIn this lab, you\'ll practice your knowledge on adding polynomial terms to your regression model!\nObjectives\nYou will be able to:\n\nUse sklearn\'s built in capabilities to create polynomial features\n\nDataset\nHere is the dataset you will be working with in this lab:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndf = pd.read_csv(\'sample_data.csv\')\n\ndf.head()\nRun the following line of code. You will notice that the data is clearly of non-linear shape. Begin to think about what degree polynomial you believe will fit it best.\nplt.scatter(df[\'x\'], df[\'y\'], color=\'green\', s=50, marker=\'.\');\nTrain-test split\nThe next step is to split the data into training and test sets. Set the random_state to 42 and assign 75% of the data in the training set.\n# Split data into 75-25 train-test split \nfrom sklearn.model_selection import train_test_split\ny = df[\'y\']\nX = df.drop(columns=\'y\', axis=1)\nX_train, X_test, y_train, y_test = None\nBuild polynomial models\nNow it\'s time to determine the optimal degree of polynomial features for a model that is fit to this data. For each of second, third and fourth degrees:\n\nInstantiate PolynomialFeatures() with the number of degrees\nFit and transform the X_train features\nInstantiate and fit a linear regression model on the training data\nTransform the test data into polynomial features\nUse the model you built above to make predictions using the transformed test data\nEvaluate model performance on the test data using r2_score()\nIn order to plot how well the model performs on the full dataset, transform X using poly\nUse the same model (reg_poly) to make predictions using X_poly\n\n# Import relevant modules and functions\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\ncolors = [\'yellow\', \'lightgreen\', \'blue\']\nplt.figure(figsize=(10, 6))\nplt.scatter(df[\'x\'], df[\'y\'], color=\'green\', s=50, marker=\'.\', label=\'plot points\')\n\n# We\'ll fit 3 different polynomial regression models from degree 2 to degree 4\nfor index, degree in enumerate([2, 3, 4]):\n    \n    # Instantiate PolynomialFeatures\n    poly = None\n    \n    # Fit and transform X_train\n    X_poly_train = None\n    \n    # Instantiate and fit a linear regression model to the polynomial transformed train features\n    reg_poly = None\n    \n    # Transform the test data into polynomial features\n    X_poly_test = None\n    \n    # Get predicted values for transformed polynomial test data  \n    y_pred = None\n    \n    # Evaluate model performance on test data\n    print(""degree %d"" % degree, r2_score(y_test, y_pred))\n    \n    # Transform the full data\n    X_poly = None\n    \n    # Now, we want to see what the model predicts for the entire data \n    y_poly = None\n    \n    # Create plot of predicted values\n    plt.plot(X, y_poly, color = colors[index], linewidth=2, label=\'degree %d\' % degree)\n    plt.legend(loc=\'lower left\')\nSummary\nGreat job! You now know how to include polynomials in your linear models.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', '1', 'Python', 'Updated Mar 3, 2020', 'MATLAB', 'Updated Mar 2, 2020', 'MATLAB', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020']}","{'location': 'Chicago, IL', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['U-of-C-Decision-Tree---Regression\nWill use a decision tree to predict gas consumption in 48 states based on various demographic attributes.\n'], 'url_profile': 'https://github.com/Livergood', 'info_list': ['Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', '1', 'Python', 'Updated Mar 3, 2020', 'MATLAB', 'Updated Mar 2, 2020', 'MATLAB', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""MLE and Logistic Regression\nIntroduction\nIn this lesson, you'll further investigate the connections between maximum likelihood estimation and logistic regression. This is a common perspective for logistic regression and will be the underlying intuition for upcoming lessons where you'll code the algorithm from the ground up using NumPy.\nObjectives\nYou will be able to:\n\nDetermine how MLE is tied into logistic regression\n\nMLE formulation\nAs discussed, maximum likelihood estimation finds the underlying parameters of an assumed distribution to maximize the likelihood of the observations. Logistic regression expands upon this by investigating the conditional probabilities associated with the various features, treating them as independent probabilities and calculating the respective total probability.\nFor example, when predicting an individual's risk for heart disease, you might consider various factors such as their family history, weight, diet, exercise routines, blood pressure, and cholesterol. When looked at individually, each of these has an associated conditional probability that the individual has heart disease based on each of these factors. Mathematically, you can write each of these probabilities for each factor $X$ as:\n$\\pi_i = Pr(Y_i = 1|X_i = x_i)=\\dfrac{\\text{exp}(\\beta_0 + \\beta_1 x_i)}{1 + \\text{exp}(\\beta_0 + \\beta_1 x_i)}$\nThis is the standard linear regression model ($\\beta_0+\\beta_1 x_i$) you have seen previously, modified to have a range of 0 to 1. The range is modified and constrained by applying the sigmoid function since you're predicting probabilities.\nThen, combining these conditional probabilities from multiple features, you maximize the likelihood function of each of those independent conditional probabilities, giving you:\n$ L(\\beta_0,\\beta_1)=\\prod\\limits_{i=1}^N \\pi_i^{y_i}(1-\\pi_i)^{n_i-y_i}=\\prod\\limits_{i=1}^N \\dfrac{\\text{exp}{y_i(\\beta_0+\\beta_1 x_i)}}{1+\\text{exp}(\\beta_0+\\beta_1 x_i)}$\nNotes on mathematical symbols\nRecall that the $\\prod$ sign stands for a product of each of these individual probabilities. (Similar to how $\\sum$ stands for the sum of a series.) Since this is a monotonically increasing function, its maximum will be the same as the logarithm of the function, which is typically used in practice in order to decompose this product of probabilities into a sum of log probabilities for easier calculation of the derivative. In future sections, you'll investigate the derivative of this function and then use that in order to code up our own function for logistic regression.\nAlgorithm bias and ethical concerns\nIt should also be noted that while this is mathematically sound and a powerful tool, the model will simply reflect the data that is fed in. For example, logistic regression and other algorithms are used to inform a wide range of decisions including whether to provide someone with a loan, the degree of criminal sentencing, or whether to hire an individual for a job. In all of these scenarios, it is again important to remember that the algorithm is simply reflective of the underlying data itself. If an algorithm is trained on a dataset where African Americans have had disproportionate criminal prosecution, the algorithm will continue to perpetuate these racial injustices. Similarly, algorithms trained on data that reflect a gender pay-gap will also continue to promote this bias unless adequately accounted for through careful preprocessing and normalization. With this, substantial thought and analysis regarding problem set up and the resulting model is incredibly important. While future lessons and labs in this section return to underlying mathematical theory and how to implement logistic regression on your own, it is worthwhile to investigate some of the current problems regarding some of these algorithms, and how naive implementations can perpetuate unjust biases.\nAdditional resources\nBelow are a handful of resources providing further information regarding some of the topics discussed here. Be sure to check out some of the news articles describing how poor safeguards and problem formulation surrounding algorithms such as logistic regression can lead to unjust biases:\nAlgorithm bias and ethical concerns\n\n\nMachine Bias\n\n\nAmazon‚Äôs Gender-Biased Algorithm Is Not Alone\n\n\nThe software that runs our lives can be bigoted and unfair. But we can fix it\n\n\nWhy artificial intelligence is far too human\n\n\nCan Computers Be Racist? The Human-Like Bias Of Algorithms\n\n\nAdditional mathematical resources\nFor a more in-depth discussion of the mathematical ideas, check out Penn State's lecture here\nIf you want to really go down the math rabbit-hole, check out section 4.4 on Logistic Regression from the Elements of Statistical Learning which can be found here: https://web.stanford.edu/~hastie/ElemStatLearn//.\nSummary\nIn this lesson, you further analyzed logistic regression from the perspective of maximum likelihood estimation. Additionally, there was a brief pause to consider the setup and interpretation of algorithms such as logistic regression. In particular, remember that issues regarding racial and gender bias that can be perpetuated by these algorithms. Always try to ensure your models are ethically sound. In the proceeding labs and lessons, you will continue to formalize your knowledge of logistic regression, implementing gradient descent and then a full logistic regression algorithm using Python packages in order to give you a deeper understanding of how logistic regression works.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', '1', 'Python', 'Updated Mar 3, 2020', 'MATLAB', 'Updated Mar 2, 2020', 'MATLAB', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Model Validation\nIntroduction\nPreviously you\'ve briefly touched upon model evaluation when using a multiple linear regression model for prediction. In this lesson you\'ll learn why it\'s important to split your data in a train and a test set if you want to do proper performance evaluation.\nObjectives\nYou will:\n\nCompare training and testing errors to determine if model is over or underfitting\n\nThe need for train-test split\nMaking predictions and evaluation\nSo far we\'ve simply been fitting models to data, and evaluated our models calculating the errors between our $\\hat y$ and our actual targets $y$, while these targets $y$ contributed in fitting the model.\nThe reason why we built the model in the first place, however, is because we want to predict the outcome for observations that are not necessarily in our dataset now; e.g: we want to predict miles per gallon for a new car that isn\'t part of our dataset, or for a new house in Boston.\nIn order to get a good sense of how well your model will be doing on new instances, you\'ll have to perform a so-called ""train-test-split"". What you\'ll be doing here, is take a sample of the data that serves as input to ""train"" our model - fit a linear regression and compute the parameter estimates for our variables, and calculate how well our predictive performance is doing comparing the actual targets $y$ and the fitted $\\hat y$ obtained by our model.\nUnderfitting and overfitting\nAnother reason to use train-test-split is because of a common problem which doesn\'t only affect linear models, but nearly all (other) machine learning algorithms: overfitting and underfitting. An overfit model is not generalizable and will not hold to future cases. An underfit model does not make full use of the information available and produces weaker predictions than is feasible. The following image gives a nice, more general demonstration:\n\nMechanics of train-test split\nWhen performing a train-test-split, it is important that the data is randomly split. At some point, you will encounter datasets that have certain characteristics that are only present in certain segments of the data. For example, if you were looking at sales data for a website, you might expect the data to look different on days that promotional deals were held versus days that deals were not held. If we don\'t randomly split the data, there is a chance we might overfit to the characteristics of certain segments of data.\nAnother thing to consider is just how big each training and testing set should be. There is no hard and fast rule for deciding the correct size, but the range of training set is usually anywhere from 66% - 80% (and testing set between 33% and 20%). Some types of machine learning models need a substantial amount of data to train on, and as such, the training sets should be larger. Some models with many different tuning parameters will need to be validated with larger sets (the test size should be larger) to determine what the optimal parameters should be. When in doubt, just stick with training set sizes around 70% and test set sizes around 30%.\nHow to evaluate?\nIt is pretty straightforward that, to evaluate the model, you\'ll want to compare your predicted values, $\\hat y$ with the actual value, $y$. The difference between the two values is referred to as the residuals. When using a train-test split, you\'ll compare your residuals for both test set and training set:\n$r_{i,train} = y_{i,train} - \\hat y_{i,train}$\n$r_{i,test} = y_{i,test} - \\hat y_{i,test}$\nTo get a summarized measure over all the instances in the test set and training set, a popular metric is the (Root) Mean Squared Error:\nRMSE = $\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_{i} - \\hat y_{i})^2}$\nMSE = $\\frac{1}{n}\\sum_{i=1}^{n}(y_{i} - \\hat y_{i})^2$\nAgain, you can compute these for both the traing and the test set. A big difference in value between the test and training set (R)MSE is an indication of overfitting.\nApplying this to our auto-mpg data\nLet\'s copy our pre-processed auto-mpg data again\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(\'auto-mpg.csv\') \ndata[\'horsepower\'].astype(str).astype(int)\n\nacc = data[\'acceleration\']\nlogdisp = np.log(data[\'displacement\'])\nloghorse = np.log(data[\'horsepower\'])\nlogweight = np.log(data[\'weight\'])\n\nscaled_acc = (acc-min(acc))/(max(acc)-min(acc))\nscaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\nscaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\nscaled_weight = (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n\ndata_fin = pd.DataFrame([])\ndata_fin[\'acc\'] = scaled_acc\ndata_fin[\'disp\'] = scaled_disp\ndata_fin[\'horse\'] = scaled_horse\ndata_fin[\'weight\'] = scaled_weight\ncyl_dummies = pd.get_dummies(data[\'cylinders\'], prefix=\'cyl\', drop_first=True)\nyr_dummies = pd.get_dummies(data[\'model year\'], prefix=\'yr\', drop_first=True)\norig_dummies = pd.get_dummies(data[\'origin\'], prefix=\'orig\', drop_first=True)\nmpg = data[\'mpg\']\ndata_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)\ndata = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis=1)\ny = data[[\'mpg\']]\nX = data.drop([\'mpg\'], axis=1)\nScikit-learn has a very useful function, train_test_split(). The optional argument test_size makes it possible to choose the size of the test set and the training set. Since the observations are randomly assigned to the training and test splits each time you run train_test_split(), you can also pass an optional random_state argument to obtain reproducible results. This will ensure your training and test sets are always the same.\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint(len(X_train), len(X_test), len(y_train), len(y_test))\n313 79 313 79\n\nfrom sklearn.linear_model import LinearRegression\nlinreg = LinearRegression()\nlinreg.fit(X_train, y_train)\n\ny_hat_train = linreg.predict(X_train)\ny_hat_test = linreg.predict(X_test)\nLook at the residuals and calculate the MSE for training and test sets:\ntrain_residuals = y_hat_train - y_train\ntest_residuals = y_hat_test - y_test\nmse_train = np.sum((y_train-y_hat_train)**2)/len(y_train)\nmse_test = np.sum((y_test-y_hat_test)**2)/len(y_test)\nprint(\'Train Mean Squarred Error:\', mse_train)\nprint(\'Test Mean Squarred Error:\', mse_test)\nTrain Mean Squarred Error: mpg    16.790262\ndtype: float64\nTest Mean Squarred Error: mpg    16.500021\ndtype: float64\n\nYou can also do this directly using sklearn\'s mean_squared_error() function:\nfrom sklearn.metrics import mean_squared_error\n\ntrain_mse = mean_squared_error(y_train, y_hat_train)\ntest_mse = mean_squared_error(y_test, y_hat_test)\nprint(\'Train Mean Squarred Error:\', train_mse)\nprint(\'Test Mean Squarred Error:\', test_mse)\nTrain Mean Squarred Error: 16.79026189951861\nTest Mean Squarred Error: 16.50002062788149\n\nGreat, there does not seem to be a big difference between the train and test MSE! Interestingly, the test set error is smaller than the training set error. This is fairly rare but does occasionally happen.\nAdditional resources\nGreat job! You now have a lot of ingredients to build a pretty good (multiple) linear regression model. We\'ll add one more concept in the next lesson: the idea of cross-validation. But first, we strongly recommend you have a look at this blogpost to get a refresher on a lot of the concepts learned!\nSummary\nIn this lesson, you learned the importance of the train-test split approach and were introduced to one of the most popular metrics for evaluating regression models, (R)MSE. You also saw how to use the train_test_split() function from sklearn to split your data into training and test sets.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', '1', 'Python', 'Updated Mar 3, 2020', 'MATLAB', 'Updated Mar 2, 2020', 'MATLAB', 'Updated Mar 18, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020']}"
"{'location': 'Delhi, India', 'stats_list': [], 'contributions': '40 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/vm-iiit', 'info_list': ['Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Regression and Model Validation - Introduction\nIntroduction\nIn this section, you\'ll start to see more advanced regression models which employ multiple predictors. With that, you\'ll also start to investigate how to validate your models to ensure they are neither overfit nor underfit, and will generalize well to future cases.\nMultiple Linear Regression\nIn the last section, you learned how to perform a basic linear regression with a single predictor variable. Here, you\'ll explore how to perform linear regressions using multiple independent variables to better predict a target variable.\nImproving a Baseline Model\nOver the next few lessons, you\'ll see some ways to improve basic regression models using different combinations of features as well as some feature engineering.  Often, a simple linear regression can be used as a ""baseline model"" upon which new features can be added to improve the predictions.  Any decisions on how to change features should be compared against a simpler model to see if the changes have improved the model or not. This section will give an introduction to many of the techniques which can improve a regression model.\nDealing with Categorical Variables\nUp to this point you\'ve only seen continuous predictor variables. Here, you\'ll get a further look at how to identify and then transform categorical variables to utilize them as predictors of our target variable.\nMulticollinearity of Features\nWhile multiple predictors will ultimately increase model performance and yield better predictions, there are also possible negative effects when using multiple predictors that have a high correlation with each other. This is known as multicollinearity and can muddy model interpretation.\nFeature Scaling and Normalization\nAnother consideration when using multiple predictors in any model is the scale of those features. For example, a dataset surrounding households might have a ""number of children"" feature and an ""income"" feature. These two variables are of vastly different scales and as such, simply having a feature like income which is on a much larger scale can impact its influence over the model. To avoid this, it is best practice to normalize the scale of all features before feeding the data to a machine learning algorithm.\nMultiple Linear Regression in Statsmodels\nAfter covering a lot of the key theory, you\'ll then get some hands-on practice in performing multiple linear regressions using the Statsmodels and Scikit-Learn libraries.\nModel Fit and Validation\nYou\'ll continue the section by looking at how we can analyze the results of a regression, and learn the importance of splitting data into training and test sets to determine how well a model predicts ""unknown"" values (the test dataset). Finally, you\'ll wrap up the section by looking at how k-fold cross-validation can be used to get additional model validations on a limited data set by taking multiple splits of training and testing data.\nSummary\nIn this section, you\'ll continue to bolster your understanding of linear regression so that you can solve a wider range of problems more accurately. Many of the principles covered in this section will also apply in later modules as you move onto working with other machine learning models.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Simple Linear Regression\nIntroduction\nRegression analysis is often the first real learning application that aspiring data scientists will come across. It is one of the simplest techniques to master, but it still requires some mathematical and statistical understanding of the underlying process. This lesson will introduce you to the regression process based on the statistical ideas we have discovered so far.\nObjectives\nYou will be able to:\n\nPerform a linear regression using self-constructed functions\nInterpret the parameters of a simple linear regression model in relation to what they signify for specific data\n\nLinear Regression\nRegression analysis is one of the most important statistical techniques for business applications. It‚Äôs a statistical methodology that helps estimate the strength and direction of the relationship between two (or more) variables. Regression results show whether the relationship is valid or not. It also helps to predict an unknown value based on the derived relationship.\n\nRegression Analysis is a parametric technique meaning a set of parameters are used to predict the value of an unknown target variable (or dependent variable) $y$ based on one or more of known input features (or independent variables, predictors), often denoted by $x$.\n\nLet\'s consider another example. Someone\'s height and foot size are generally considered to be related. Generally speaking, taller people tend to have bigger feet (and, obviously, shoe size).\n\nWe can use a linear regression analysis here to predict foot size (dependent variable), given height (independent variable) of an individual. Regression is proven to give credible results if the data follows some assumptions which will be covered in upcoming lessons in detail. In general, regression analysis helps us in the following ways:\n\nFinding an association or relationship between certain phenomena or variables\nIdentifying which variables contribute more towards the outcomes\nPrediction of future observations\n\nWhy ""linear"" regression?\nThe term linear implies that the model functions along with a straight (or nearly straight) line. Linearity, one of the assumptions of this approach, suggests that the relationship between dependent and independent variables can be expressed as a straight line.\nSimple Linear Regression uses a single feature (one independent variable) to model a linear relationship with a target (the dependent variable) by fitting an optimal model (i.e. the best straight line) to describe this relationship.\nMultiple Linear Regression uses more than one feature to predict a target variable by fitting the best linear relationship.\nIn this section, we will mainly focus on simple regression to build a sound understanding. For the example shown above i.e. height vs foot size, a simple linear regression model would fit a line to the data points as follows:\n\nThis line can then be used to describe the data and conduct further experiments using this fitted model. So let\'s move on and see how to calculate this ""best-fit line"" in a simple linear regression context.\nCalculating Regression Coefficients: Slope and Intercepts\nA straight line can be written as :\n$$y=mx+c$$\nor, alternatively\n$$y =  \\beta_0+ \\beta_1 x $$\nYou may come across other ways of expressing this straight line equation for simple linear regression. Yet there are four key components you\'ll want to keep in mind:\n\n\nA dependent variable that needs to estimated and predicted (here: $y$)\nAn independent variable, the input variable (here: $x$)\nThe slope which determines the angle of the line. Here, the slope is denoted as $m$, or $\\beta_1$.\nThe intercept which is the constant determining the value of $y$ when $x$ is 0. We denoted the intercept here as $c$ or $\\beta_0$.\n\n\nSlope and Intercept are the coefficients or the parameters of a linear regression model. Calculating the regression model simply involves the calculation of these two values.\n\nLinear regression is simply a manifestation of this simple equation! So this is as complicated as our linear regression model gets. The equation here is the same one used to find a line in algebra, but in statistics, the actual data points don\'t necessarily lie on a line!\n\nThe real challenge for regression analysis is to fit a line, out of an infinite number of lines that best describes the data.\n\nConsider the line below to see how we calculate slope and intercept.\n\nIn our example:\n$c$ is equal to 15, which is where our line intersects with the y-axis.\n$m$ is equal to 3, which is our slope.\nYou can find a slope by taking an arbitrary part of the line, looking at the\ndifferences for the x-value and the y-value for that part of the line, and dividing $\\Delta y$ by $\\Delta x$. In other words, you can look at the change in y over the change in x to find the slope!\nImportant note on notation\nNow that you know how the slope and intercept define the line, it\'s time for some more notation.\nLooking at the above plots, you know that you have the green dots that are our observations associated with x- and y-values.\nNow, when we draw our regression line based on these few green dots, we use the following notations:\n$$\\hat{y}=\\hat m x+ \\hat{c}$$ or\n$$\\hat y =  \\hat \\beta_0+ \\hat \\beta_1 x $$\nAs you can see, you\'re using a ""hat"" notation which stands for the fact that we are working with estimations.\n\nWhen trying to draw a ""best fit line"", you\'re estimating the most appropriate value possible for your intercept and your slope, hence $\\hat{c}$ /$ \\hat \\beta_0 $ and  $\\hat{m}$ /$ \\hat \\beta_1 $.\nNext, when we use our line to predict new values $y$ given $x$, your estimate is an approximation based on our estimated parameter values. Hence we use $\\hat y $ instead of $y$. $\\hat y$ lies ON your regression line, $y$ is the associated y-value for each of the green dots in the plot above. The error or the vertical offset between the line and the actual observation values is denoted by the red vertical lines in the plot above. Mathematically, the vertical offset can be written as $\\mid \\hat y - y\\mid$.\n\nSo how do you find the line with the best fit? You may think that you have to try lots and lots of different lines to see which one fits best. Fortunately, this task is not as complicated as in may seem. Given some data points, the best-fit line always has a distinct slope and y-intercept that can be calculated using simple linear algebraic approaches. Let\'s quickly visit the required formulas.\nBest-Fit Line Ingredients\nBefore we calculate the best-fit line, we have to make sure that we have calculated the following measures for variables X and Y:\n\n\nThe mean of the X $(\\bar{X})$\n\n\nThe mean of the Y $(\\bar{Y})$\n\n\nThe standard deviation of the X values $(S_X)$\n\n\nThe standard deviation of the y values $(S_Y)$\n\n\nThe correlation between X and Y ( often denoted by the Greek letter ""Rho"" or $\\rho$ - Pearson Correlation)\n\n\nCalculating Slope\nWith the above ingredients in hand, we can calculate the slope (shown as $b$ below) of the best-fit line, using the formula:\n$$\\hat m = \\rho \\frac{S_Y}{S_X}$$\nThis formula is also known as the least-squares method.\nYou can visit this Wikipedia link to get take a look into the math behind the derivation of this formula.\nThe slope of the best-fit line can be a negative number following a negative correlation.  For example, if an increase in police officers is related to a decrease in the number of crimes in a linear fashion, the correlation and hence the slope of the best-fitting line in this particular setting is negative.\nCalculating Intercept\nSo now that we have the slope value (\\hat m), we can put it back into our formula $(\\hat y = \\hat m x+ \\hat c)$ to calculate intercept. The idea is that\n$$\\bar{Y} = \\hat c + \\hat m \\bar{X}$$\n$$ \\hat c = \\bar{Y} - \\hat m\\bar{X}$$\nRecall that $\\bar{X}$ and $\\bar{Y}$ are the mean values for variables X and Y.  So, in order to calculate the $\\hat y$-intercept of the best-fit line, we start by finding the slope of the best-fit line using the above formula. Then to find the $\\hat y$-intercept, we multiply the slope value by the mean of x and subtract the result from the mean of y.\nPredicting from the model\nAs mentioned before, when you have a regression line with defined parameters for slope and intercept as calculated above, you can easily predict the $\\hat{y}$ (target) value for a new $x$ (feature) value using the estimated parameter values:\n$$\\hat{y} = \\hat mx + \\hat c$$\nRemember that the difference between y and $\\hat{y}$ is that $\\hat{y}$ is the value predicted by the fitted model, whereas $y$ carries actual values of the variable (called the truth values) that were used to calculate the best fit.\nNext, let\'s move on and try to code these equations to fit a regression line to a simple dataset to see all of this in action.\nAdditional Reading\nVisit the following series of blogs by Bernadette Low for details on topics covered in this lesson.\n\nSuper Simple Machine Learning\u200a‚Äî\u200aSimple Linear Regression Part 1\nSuper Simple Machine Learning\u200a‚Äî\u200aSimple Linear Regression Part 2\n\nSummary\nIn this lesson, you learned the basics of a simple linear regression. Specifically, you learned some details about performing the actual technique and got some practice interpreting regression parameters. Finally, you saw how the parameters can be used to make predictions!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kishoremanamala', 'info_list': ['Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020']}","{'location': 'Chicago, IL', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['U-of-C-Logistic-Regression-Model\nLooking at the Titanic tragedy (which led to the death of over 1,500 passengers and crew, more than half the people on board) I wanted to construct a Logistic Regression model that uses available data about the passengers to predict their survival.\n'], 'url_profile': 'https://github.com/Livergood', 'info_list': ['Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020']}","{'location': 'Andheri,Mumbai', 'stats_list': [], 'contributions': '26 contributions\n        in the last year', 'description': ['Machine-Learning-with-python-Regression-Techniques\n                                      Big Mart Sales III\n\nThis is a Hackathon held on Analytics Vidya,here the data scientists at BigMart have collected 2013 sales data for 1559 products across 10 stores in different cities. Also, certain attributes of each product and store have been defined. The aim is to build a predictive model and find out the sales of each product at a particular store.\nUsing this model, BigMart will try to understand the properties of products and stores which play a key role in increasing sales.\nPlease note that the data may have missing values as some stores might not report all the data due to technical glitches. Hence, it will be required to treat them accordingly.\nData\nWe have train (8523) and test (5681) data set, train data set has both input and output variable(s). You need to predict the sales for test data set.\nVariable            Description\nItem_Identifier :- Unique product ID\nItem_Weight     :-  Weight of product\nItem_Fat_Content  :- Whether the product is low fat or not\nItem_Visibility  :- The % of total display area of all products in a store allocated to the particular product\nItem_Type :-  The category to which the product belongs\nItem_MRP  :-  Maximum Retail Price (list price) of the product\nOutlet_Identifier  v Unique store ID\nOutlet_Establishment_Year  :- The year in which store was established\nOutlet_Size :-  The size of the store in terms of ground area covered\nOutlet_Location_Type  :-  The type of city in which the store is located\nOutlet_Type :-  Whether the outlet is just a grocery store or some sort of supermarket\nItem_Outlet_Sales   :-    Sales of the product in the particulat store. This is the outcome variable to be predicted.\nEvaluation Metric:\nYour model performance will be evaluated on the basis of your prediction of the sales for the test data (test.csv), which contains similar data-points as train except for the sales to be predicted. Your submission needs to be in the format as shown in ""SampleSubmission.csv"".\nWe at our end, have the actual sales for the test dataset, against which your predictions will be evaluated. We will use the Root Mean Square Error value to judge your response.\n'], 'url_profile': 'https://github.com/JEEVANARANDE', 'info_list': ['Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Assumptions for Linear Regression\nIntroduction\nLeast Squares is one of the most common regression techniques for linear models. As long as our model satisfies the least squares regression assumptions, we can get the best possible estimates. In this lesson, you will learn about these assumptions.\nObjectives\nYou will be able to:\n\nList the assumptions of linear regression\nDetermine if a particular set of data exhibits the assumptions of linear regression\n\nAbout Regression Assumptions\nRegression is a powerful analysis technique that is routinely used to answer complex analytical questions. However, if some of the necessary assumptions are not satisfied, you may not be able to get good and trustworthy results!\nIn this lesson, you\'ll dig deeper into the topic of ordinary least squares (OLS) regression assumptions. Additionally, you\'ll learn about their importance as well as some techniques to help us determine whether your model satisfies the assumptions.\nRegression is ""Parametric""\nRegression is a parametric technique, which means that it uses parameters learned from the data. Because of that, certain assumptions must be made. These assumptions define the complete scope of regression analysis and it is mandatory that the underlying data fulfills these assumptions. If violated, regression makes biased and unreliable predictions. Luckily, we have measures to check for these assumptions.\n1. Linearity\n\nThe linearity assumptions requires that there is a linear relationship between the response variable (Y) and predictor (X). Linear means that the change in Y by 1-unit change in X, is constant.\n\n\nAs shown above, If we try to fit a linear model to a non-linear data set, OLS will fail to capture the trend mathematically, resulting in an inaccurate relationship. This will also result in erroneous predictions on an unseen data set.\n\nThe linearity assumption can best be tested with scatter plots\n\nFor non-linear relationships, you can use non-linear mathematical functions to fit the data e.g. polynomial and exponential functions. You\'ll come across these later.\nNote: As an extra measure, it is also important to check for outliers as the presence of outliers in the data can have a major impact on the model.\n\nIn the above example, we can see that an outlier prohibits the model to estimate the true relationship between variables by introducing bias.\n2. Normality\n\nThe normality assumption states that the model residuals should follow a normal distribution\n\nNote that the normality assumption talks about the model residuals and not about the distributions of the variables! In general, data scientists will often check the distributions of the variables as well. Keep in mind that the normality assumption is mandatory for the residuals, and it is useful to check normality of your variables to check for weirdness (more on data distributions later), but OLS works fine for non-normal data distributions in the context of prediction.\nThe easiest way to check for the normality assumption is with histograms or a Q-Q-Plots.\nHistograms\nWe have already seen quite a few histograms and also know how to build them. You can use histograms to check the errors generated by the model and see if the plot shows a so-called ""normal distribution"" (bell curve shape). As the error term follows a normal distribution, we can develop better confidence in the results and calculate the statistical significance. An example of a regression error histogram is shown below:\n\nQ-Q Plots\n\nIn statistics, a Q‚ÄìQ (quantile-quantile) plot is a probability plot, which is a graphical method for comparing two probability distributions by plotting their quantiles against each other.\n\nThe Q-Q plot (quantile-quantile plot) is used to help assess if a sample comes from a known distribution such as a normal distribution. For regression, when checking if the data in this sample is normally distributed, we can use a Normal Q-Q plot to test that assumption. Remember that this is just a visual check, so the interpretation remains subjective. However, it is a good first check to see the overall shape of your data against the required distribution. If you can reject normality through Q-Q plots, you have saved yourself from a lot of statistical testing. You have to be careful, however, when deciding that data is totally normal just by looking at a Q-Q plot.\nBelow, you can find a few examples of comparing histograms and corresponding plots. You can see how the quantiles of normal data appear as a straight line along the diagonal when plotted against a standard normal distribution\'s quantiles. The skewness and kurtosis of data can also be inspected this way\n\nIn the context of normality of residuals, Q-Q plots can help you validate the assumption of normally distributed residuals. It uses standardized values of residuals to determine the normal distribution of errors. Ideally, this plot should show a straight line. A curved, distorted line suggests residuals have a non-normal distribution.Here is a good article explaining the interpretation of Q-Q plots in detail.\nNormality can also be checked with goodness of fit tests such as the Kolmogorov-Smirnov test.  When the data is not normally distributed, there are some ways to fix that, such as a non-linear transformation (e.g., log-transformation).\n3. Homoscedasticity\n\nHeteroscedasticity (also spelled heteroskedasticity) refers to the circumstance in which the dependent variable is unequal across the range of values of the predictor(s).\n\nWhen there is heteroscedasticity in the data, a scatterplot of these variables will often create a cone-like shape. The scatter of the dependent variable widens or narrows as the value of the independent variable increases.\nThe inverse of heteroscedasticity is homoscedasticity, which indicates that a dependent variable\'s variability is equal across values of the independent variable. Homoscedasticity is the third assumption necessary when creating a linear regression model.\n\nA scatter plot is good way to check whether the data are homoscedastic (meaning the residuals are equal across the regression line).  The scatter plots shown here are examples of data that are heteroscedastic (except the plot far right). You can also use significance tests like Breusch-Pagan / Cook-Weisberg test or White general test to detect this phenomenon. Remember that, if these tests give you a p-value < 0.05, the null hypothesis can rejected, and you can assume the data is heteroscedastic.\nWhat Else?\nThere are other assumptions for linear regression that apply to more complicated cases, but for now these three assumptions are sufficient.\nAs a first check, always looks at plots of the residuals. If you see anything similar to what is shown below, you are violating one or more assumptions and the results will not be reliable.\n\nSummary\nIn this lesson, you learned about some assumptions for a simple linear regression that must be held in order to interpret the results reliably. As mentioned earlier, once these assumptions are confirmed, you can run your regression model. Next, you\'ll be exposed to some examples!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Logistic Regression - Introduction\nIntroduction\nIn this you'll be introduced to a new type of machine learning technique: classification! You'll learn about an algorithm called logistic regression as well as different ways that data scientists can evaluate the performance of classification models.\nLogistic Regression\nYou're familiar with linear regression to predict continuous values. You're now going to return to regression to look at how it can be used as a classifier instead to determine the likelihood of a given data point being associated with one of two categories.\nWe'll start by introducing the sigmoid function and showing how it can be used to fit a curve that matches a binary classifier (e.g. does someone make over or under $40k a year or are they a good or bad credit risk).\nEvaluating Classifiers\nWe'll then look at the practicalities of evaluating logistic regression models based on precision, recall, and accuracy to evaluate other classifiers.\nWe also take a little time to look at how to plot a confusion matrix for a logistic regression classifier and introduce a couple of key concepts for determining the optimal precision-recall trade-off for a given classifier - Receiver Operating Characteristic (ROC) curves and AUC (the Area Under the Curve).\nClass Imbalance Problems\nWe then introduce the concept of class imbalance. Imagine a classifier for cancer where only 1 screened individual in 1000 is sick. You could obtain over 99 percent accuracy by just saying everyone is fine, but that wouldn't be a very useful approach. We look at the ideas of class weights and over/undersampling and how they can be used to work with highly imbalanced classes.\nSummary\nIt's important to be aware of logistic regression as one of the most basic classifiers that you can use, and many of the concepts around model evaluation will be useful whenever you're trying to solve a classification problem.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020']}","{'location': 'Raleigh, North Carolina', 'stats_list': [], 'contributions': '179 contributions\n        in the last year', 'description': ['Predicting-Bitcoin-Price-Variations-using-Bayesian-Regression\nTo implement the Bayesian Regression model to predict the future price variation of bitcoin as described in the reference paper.\nReference paper (Bayesian Regression and Bitcoin.pdf) is provided in the folder.\n'], 'url_profile': 'https://github.com/binpatel31', 'info_list': ['Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Diagnostics in Statsmodels\nIntroduction\nSo far, you have looked mainly at R-Squared values along with some visualization techniques to confirm that regression assumptions are met. Now, you\'ll look at some statistical procedures to further understand your model and results. You\'ll be looking at the results obtained in the regression analysis outcomes for the advertising dataset in the previous lab.\nNote: Some of the terms in this lesson highlighting underlying statistical testing concepts will be new to you. These terms will be covered in detail in later sections. Here, the focus will be on running and interpreting the results of these tests in a regression context.\nObjectives\nYou will be able to:\n\nDetermine if a particular set of data exhibits the assumptions of linear regression\n\nLet\'s get started\nRegression diagnostics is a set of procedures available for regression analysis that assess the validity of a model in a number of different ways.\nThis could be:\n\nAn exploration of the model\'s underlying statistical assumptions\nAn examination of the structure of the model by considering formulations that have less, more or different explanatory variables\nA study of subgroups of observations, looking for those that are either poorly represented by the model (outliers) or that have a relatively large effect on the regression model\'s predictions\n\nFor a thorough overview, you can go and have a look at the wikipedia page on regression diagnostics.\nHere we\'ll revisit some of the methods you\'ve already seen, along with some new tests and how to interpret them.\nNormality Check (Q-Q plots)\nYou\'ve already seen Q-Q Plots as a measure to check for normality (or, by extension, any other distribution).\nQ-Q plots are also referred to as normal density plots when used with standard normal quantiles. These plots are a good way to inspect the distribution of model errors. You saw this earlier with the small height-weight data set. Let\'s quickly generate a Q-Q plot for the residuals in the sales ~ TV and the sales ~ radio models again!\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport statsmodels.api as sm\nimport statsmodels.stats.api as sms\nimport statsmodels.formula.api as smf\nimport scipy.stats as stats\nplt.style.use(\'ggplot\')\n\ndata = pd.read_csv(\'advertising.csv\', index_col=0)\nf = \'sales~TV\'\nf2 = \'sales~radio\'\nmodel = smf.ols(formula=f, data=data).fit()\nmodel2 = smf.ols(formula=f2, data=data).fit()\n\nresid1 = model.resid\nresid2 = model2.resid\nfig = sm.graphics.qqplot(resid1, dist=stats.norm, line=\'45\', fit=True)\nfig = sm.graphics.qqplot(resid2, dist=stats.norm, line=\'45\', fit=True)\n\n\nNormal Q-Q Plots are a direct visual assessment of how well our residuals match what we would expect from a normal distribution.\nIn the Q-Q plots above, you can see that residuals are better normally distributed for TV than for radio.\nYou can also spot an outlier in the left tail of radio residuals. Dealing with this might help improve the fit of the model. Outliers, skew, heavy and light-tailed aspects of distributions (all violations of normality) can be assessed from Q-Q plots. It might require a bit of practice before you can truly start to interpret them.\nThe images below show you how to relate a histogram to their respective Q-Q Plots.\n\nNormality Check (Jarque-Bera Test)\nThe Jarque-Bera (JB) test is a test for normality. This test is usually used for large data sets, because other tests like Q-Q Plots can become unreliable when your sample size is large.\n\nThe Jarque-Bera test inspects the skewness and kurtosis of data to see if it matches a normal distribution. It is a common method for inspecting errors distribution in regression as shown below.\n\n$$JB = n *\\Bigl(\\dfrac{S^2}{6} + \\dfrac{(K ‚Äì 3)^2}{24}\\Bigr)$$\nHere, $n$ is the sample size, $S$ is the sample skewness coefficient and $K$ is the sample kurtosis.\nHere is how you use JB in statsmodels. A JB value of roughly 6 or higher indicates that errors are not normally distributed. In other words, this means that the normality null hypothesis has been rejected at the $5%$ significance level. A value close to 0 on the contrary, indicates the data $is$ normally distributed. We have already seen the JB test using model.summary(). The code below shows you how to run this test on its own.\n# JB test for TV\nname = [\'Jarque-Bera\',\'Prob\',\'Skew\', \'Kurtosis\']\ntest = sms.jarque_bera(model.resid)\nlist(zip(name, test))\n[(\'Jarque-Bera\', 0.6688077048615619),\n (\'Prob\', 0.7157646605518615),\n (\'Skew\', -0.08863202396577206),\n (\'Kurtosis\', 2.779014973597054)]\n\nWe have a JB value = 0.67, which is pretty low (and in favor of normality), and the p-value of 0.71 is quite high to reject the null hypothesis for normality. Additionally, the kurtosis is below 3, where a kurtosis higher than 3 indicates heavier tails than a normal distribution. The skewness values however show that underlying data is moderately skewed. Let\'s see what happens if we look at the radio residuals.\n# JB test for radio\nname = [\'Jarque-Bera\',\'Prob\',\'Skew\', \'Kurtosis\']\ntest2 = sms.jarque_bera(model2.resid)\nlist(zip(name, test2))\n[(\'Jarque-Bera\', 21.90969546280269),\n (\'Prob\', 1.74731047370758e-05),\n (\'Skew\', -0.7636952540480038),\n (\'Kurtosis\', 3.5442808937621666)]\n\nWhere The TV residuals showed to be close to normality, the JB results for radio are considerably worse. More-over, a JB p-value much smaller than 0.05 indicates that the normality assumption should definitely be rejected.\nThese results show that even when in the Q-Q plots the results seemed moderately different, the JB test could shed new light on the normality assumption.\nChecking Heteroscadasticity (Goldfeld-Quandt test)\nThe Goldfeld Quandt (GQ) test is used in regression analysis to check for homoscedasticity in the error terms. The GQ test checks if you can define a point that can be used to differentiate the variance of the error term. It is a parametric test and uses the assumption that the data is normally distributed. So it is general practice to check for normality before going over to the GQ test!\nIn the image below, you can see how observations are split into two groups. Next, a test statistic is run through taking the ratio of mean square residual errors for the regressions on the two subsets. Evidence of heteroscedasticity is based on performing a hypothesis test (more on this later) as shown in the image.\nlwr_thresh = data.TV.quantile(q=.45)\nupr_thresh = data.TV.quantile(q=.55)\nmiddle_10percent_indices = data[(data.TV >= lwr_thresh) & (data.TV<=upr_thresh)].index\n# len(middle_10percent_indices)\n\nindices = [x-1 for x in data.index if x not in middle_10percent_indices]\nplt.scatter(data.TV.iloc[indices], model.resid.iloc[indices])\nplt.xlabel(\'TV\')\nplt.ylabel(\'Model Residuals\')\nplt.title(""Residuals versus TV Feature"")\nplt.vlines(lwr_thresh, ymax=8, ymin=-8, linestyles=\'dashed\',linewidth=2)\nplt.vlines(upr_thresh, ymax=8, ymin=-8, linestyles=\'dashed\',linewidth=2);\n\nHere is a brief description of the steps involved:\n\nOrder the data in ascending order\nSplit your data into three parts and drop values in the middle part.\nRun separate regression analyses on two parts. After each regression, find the Residual Sum of Squares.\nCalculate the ratio of the Residual sum of squares of two parts.\nApply the F-test.\n\n(F-test will be covered later in the syllabus. Here is a quick introduction)\nFor now, you should just remember that high F values typically indicate that the variances are different. If the error term is homoscedastic, there should be no systematic difference between residuals and F values will be small.\nHowever, if the standard deviation of the distribution of the error term is proportional to the x variable, one part will generate a higher sum of square values than the other.\nHere is how you can run this test in statsmodels.\n# Run Goldfeld Quandt test\nname = [\'F statistic\', \'p-value\']\ntest = sms.het_goldfeldquandt(model.resid.iloc[indices], model.model.exog[indices])\nlist(zip(name, test))\n[(\'F statistic\', 1.1993147096678916), (\'p-value\', 0.19780602597731686)]\n\n# Run Goldfeld Quandt test\nimport statsmodels.stats.api as sms\nname = [\'F statistic\', \'p-value\']\ntest = sms.het_goldfeldquandt(model2.resid.iloc[indices], model2.model.exog[indices])\nlist(zip(name, test))\n[(\'F statistic\', 1.2189878283402957), (\'p-value\', 0.1773756718718901)]\n\nThe null hypothesis for the GQ test is homoscedasticity. The larger the F-statistic, the more evidence we will have against the homoscedasticity assumption and the more likely we have heteroscedasticity (different variance for the two groups).\nThe p-value for our tests above tells us whether or not to reject the null-hypothesis of homoscedasticity. Taking a confidence level of alpha = 0.05, we cannot reject the null hypothesis because for both TV and radio, p-values are larger than 0.05. So even though we visually inspected some heteroscedasticity previously, this cannot be confirmed by the GQ test.\nStatsmodel also offers newer tests for heteroscadasticity including the Breush-Pagan Test and White‚Äôs Test which may be advantageous in certain cases.\nSummary\nIn this lesson, you learned a few methods to check for regression assumptions in addition to the visual methods learned earlier. An understanding and hands-on experience with visual as well as statistical techniques to check your regression analysis will provide you with a good set of tools to run more detailed regression experiments later.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Sep 22, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '104 contributions\n        in the last year', 'description': ['Cancer-dignostic-prediction-through-Logistic-Regression\nThe data set is being taken from  here .\n'], 'url_profile': 'https://github.com/MaclaurinYudhisthira', 'info_list': ['Jupyter Notebook', 'Updated Mar 8, 2020', '1', 'Jupyter Notebook', 'Updated Oct 2, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Python', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020']}","{'location': 'United States', 'stats_list': [], 'contributions': '63 contributions\n        in the last year', 'description': ['Kaggle-House-Price-Prediction.Advanced-Regression-Techniques\nHouse Prices: Advanced Regression: Built a predictive model in Python with 90% accuracy using boosting techniques to predict the target based on 183 features. Designed new features, used imputers, encoding techniques and followed Log normal distribution to clean and pre-process the data. Kaggle Link: https://www.kaggle.com/harithathiagukumar\n'], 'url_profile': 'https://github.com/HarithaTK', 'info_list': ['Jupyter Notebook', 'Updated Mar 8, 2020', '1', 'Jupyter Notebook', 'Updated Oct 2, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Python', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Diagnostics in Statsmodels\nIntroduction\nSo far, you have looked mainly at R-Squared values along with some visualization techniques to confirm that regression assumptions are met. Now, you\'ll look at some statistical procedures to further understand your model and results. You\'ll be looking at the results obtained in the regression analysis outcomes for the advertising dataset in the previous lab.\nNote: Some of the terms in this lesson highlighting underlying statistical testing concepts will be new to you. These terms will be covered in detail in later sections. Here, the focus will be on running and interpreting the results of these tests in a regression context.\nObjectives\nYou will be able to:\n\nDetermine if a particular set of data exhibits the assumptions of linear regression\n\nLet\'s get started\nRegression diagnostics is a set of procedures available for regression analysis that assess the validity of a model in a number of different ways.\nThis could be:\n\nAn exploration of the model\'s underlying statistical assumptions\nAn examination of the structure of the model by considering formulations that have less, more or different explanatory variables\nA study of subgroups of observations, looking for those that are either poorly represented by the model (outliers) or that have a relatively large effect on the regression model\'s predictions\n\nFor a thorough overview, you can go and have a look at the wikipedia page on regression diagnostics.\nHere we\'ll revisit some of the methods you\'ve already seen, along with some new tests and how to interpret them.\nNormality Check (Q-Q plots)\nYou\'ve already seen Q-Q Plots as a measure to check for normality (or, by extension, any other distribution).\nQ-Q plots are also referred to as normal density plots when used with standard normal quantiles. These plots are a good way to inspect the distribution of model errors. You saw this earlier with the small height-weight data set. Let\'s quickly generate a Q-Q plot for the residuals in the sales ~ TV and the sales ~ radio models again!\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport statsmodels.api as sm\nimport statsmodels.stats.api as sms\nimport statsmodels.formula.api as smf\nimport scipy.stats as stats\nplt.style.use(\'ggplot\')\n\ndata = pd.read_csv(\'advertising.csv\', index_col=0)\nf = \'sales~TV\'\nf2 = \'sales~radio\'\nmodel = smf.ols(formula=f, data=data).fit()\nmodel2 = smf.ols(formula=f2, data=data).fit()\n\nresid1 = model.resid\nresid2 = model2.resid\nfig = sm.graphics.qqplot(resid1, dist=stats.norm, line=\'45\', fit=True)\nfig = sm.graphics.qqplot(resid2, dist=stats.norm, line=\'45\', fit=True)\n\n\nNormal Q-Q Plots are a direct visual assessment of how well our residuals match what we would expect from a normal distribution.\nIn the Q-Q plots above, you can see that residuals are better normally distributed for TV than for radio.\nYou can also spot an outlier in the left tail of radio residuals. Dealing with this might help improve the fit of the model. Outliers, skew, heavy and light-tailed aspects of distributions (all violations of normality) can be assessed from Q-Q plots. It might require a bit of practice before you can truly start to interpret them.\nThe images below show you how to relate a histogram to their respective Q-Q Plots.\n\nNormality Check (Jarque-Bera Test)\nThe Jarque-Bera (JB) test is a test for normality. This test is usually used for large data sets, because other tests like Q-Q Plots can become unreliable when your sample size is large.\n\nThe Jarque-Bera test inspects the skewness and kurtosis of data to see if it matches a normal distribution. It is a common method for inspecting errors distribution in regression as shown below.\n\n$$JB = n *\\Bigl(\\dfrac{S^2}{6} + \\dfrac{(K ‚Äì 3)^2}{24}\\Bigr)$$\nHere, $n$ is the sample size, $S$ is the sample skewness coefficient and $K$ is the sample kurtosis.\nHere is how you use JB in statsmodels. A JB value of roughly 6 or higher indicates that errors are not normally distributed. In other words, this means that the normality null hypothesis has been rejected at the $5%$ significance level. A value close to 0 on the contrary, indicates the data $is$ normally distributed. We have already seen the JB test using model.summary(). The code below shows you how to run this test on its own.\n# JB test for TV\nname = [\'Jarque-Bera\',\'Prob\',\'Skew\', \'Kurtosis\']\ntest = sms.jarque_bera(model.resid)\nlist(zip(name, test))\n[(\'Jarque-Bera\', 0.6688077048615619),\n (\'Prob\', 0.7157646605518615),\n (\'Skew\', -0.08863202396577206),\n (\'Kurtosis\', 2.779014973597054)]\n\nWe have a JB value = 0.67, which is pretty low (and in favor of normality), and the p-value of 0.71 is quite high to reject the null hypothesis for normality. Additionally, the kurtosis is below 3, where a kurtosis higher than 3 indicates heavier tails than a normal distribution. The skewness values however show that underlying data is moderately skewed. Let\'s see what happens if we look at the radio residuals.\n# JB test for radio\nname = [\'Jarque-Bera\',\'Prob\',\'Skew\', \'Kurtosis\']\ntest2 = sms.jarque_bera(model2.resid)\nlist(zip(name, test2))\n[(\'Jarque-Bera\', 21.90969546280269),\n (\'Prob\', 1.74731047370758e-05),\n (\'Skew\', -0.7636952540480038),\n (\'Kurtosis\', 3.5442808937621666)]\n\nWhere The TV residuals showed to be close to normality, the JB results for radio are considerably worse. More-over, a JB p-value much smaller than 0.05 indicates that the normality assumption should definitely be rejected.\nThese results show that even when in the Q-Q plots the results seemed moderately different, the JB test could shed new light on the normality assumption.\nChecking Heteroscadasticity (Goldfeld-Quandt test)\nThe Goldfeld Quandt (GQ) test is used in regression analysis to check for homoscedasticity in the error terms. The GQ test checks if you can define a point that can be used to differentiate the variance of the error term. It is a parametric test and uses the assumption that the data is normally distributed. So it is general practice to check for normality before going over to the GQ test!\nIn the image below, you can see how observations are split into two groups. Next, a test statistic is run through taking the ratio of mean square residual errors for the regressions on the two subsets. Evidence of heteroscedasticity is based on performing a hypothesis test (more on this later) as shown in the image.\nlwr_thresh = data.TV.quantile(q=.45)\nupr_thresh = data.TV.quantile(q=.55)\nmiddle_10percent_indices = data[(data.TV >= lwr_thresh) & (data.TV<=upr_thresh)].index\n# len(middle_10percent_indices)\n\nindices = [x-1 for x in data.index if x not in middle_10percent_indices]\nplt.scatter(data.TV.iloc[indices], model.resid.iloc[indices])\nplt.xlabel(\'TV\')\nplt.ylabel(\'Model Residuals\')\nplt.title(""Residuals versus TV Feature"")\nplt.vlines(lwr_thresh, ymax=8, ymin=-8, linestyles=\'dashed\',linewidth=2)\nplt.vlines(upr_thresh, ymax=8, ymin=-8, linestyles=\'dashed\',linewidth=2);\n\nHere is a brief description of the steps involved:\n\nOrder the data in ascending order\nSplit your data into three parts and drop values in the middle part.\nRun separate regression analyses on two parts. After each regression, find the Residual Sum of Squares.\nCalculate the ratio of the Residual sum of squares of two parts.\nApply the F-test.\n\n(F-test will be covered later in the syllabus. Here is a quick introduction)\nFor now, you should just remember that high F values typically indicate that the variances are different. If the error term is homoscedastic, there should be no systematic difference between residuals and F values will be small.\nHowever, if the standard deviation of the distribution of the error term is proportional to the x variable, one part will generate a higher sum of square values than the other.\nHere is how you can run this test in statsmodels.\n# Run Goldfeld Quandt test\nname = [\'F statistic\', \'p-value\']\ntest = sms.het_goldfeldquandt(model.resid.iloc[indices], model.model.exog[indices])\nlist(zip(name, test))\n[(\'F statistic\', 1.1993147096678916), (\'p-value\', 0.19780602597731686)]\n\n# Run Goldfeld Quandt test\nimport statsmodels.stats.api as sms\nname = [\'F statistic\', \'p-value\']\ntest = sms.het_goldfeldquandt(model2.resid.iloc[indices], model2.model.exog[indices])\nlist(zip(name, test))\n[(\'F statistic\', 1.2189878283402957), (\'p-value\', 0.1773756718718901)]\n\nThe null hypothesis for the GQ test is homoscedasticity. The larger the F-statistic, the more evidence we will have against the homoscedasticity assumption and the more likely we have heteroscedasticity (different variance for the two groups).\nThe p-value for our tests above tells us whether or not to reject the null-hypothesis of homoscedasticity. Taking a confidence level of alpha = 0.05, we cannot reject the null hypothesis because for both TV and radio, p-values are larger than 0.05. So even though we visually inspected some heteroscedasticity previously, this cannot be confirmed by the GQ test.\nStatsmodel also offers newer tests for heteroscadasticity including the Breush-Pagan Test and White‚Äôs Test which may be advantageous in certain cases.\nSummary\nIn this lesson, you learned a few methods to check for regression assumptions in addition to the visual methods learned earlier. An understanding and hands-on experience with visual as well as statistical techniques to check your regression analysis will provide you with a good set of tools to run more detailed regression experiments later.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 8, 2020', '1', 'Jupyter Notebook', 'Updated Oct 2, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Python', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression with Linear Algebra - Lab\nIntroduction\nIn this lab, you\'ll apply regression analysis using simple matrix manipulations to fit a model to given data, and then predict new values for previously unseen data. You\'ll follow the approach highlighted in the previous lesson where you used NumPy to build the appropriate matrices and vectors and solve for the $\\beta$ (unknown variables) vector. The beta vector will be used with test data to make new predictions. You\'ll also evaluate the model fit.\nIn order to make this experiment interesting, you\'ll use NumPy at every single stage of this experiment, i.e., loading data, creating matrices, performing train-test split, model fitting, and evaluation.\nObjectives\nIn this lab you will:\n\nUse matrix algebra to calculate the parameter values of a linear regression\n\nFirst, let\'s import necessary libraries:\nimport csv # for reading csv file\nimport numpy as np\nDataset\nThe dataset you\'ll use for this experiment is ""Sales Prices in the City of Windsor, Canada"", something very similar to the Boston Housing dataset. This dataset contains a number of input (independent) variables, including area, number of bedrooms/bathrooms, facilities(AC/garage), etc. and an output (dependent) variable, price.  You\'ll formulate a linear algebra problem to find linear mappings from input features using the equation provided in the previous lesson.\nThis will allow you to find a relationship between house features and house price for the given data, allowing you to find unknown prices for houses, given the input features.\nA description of the dataset and included features is available here.\nIn your repository, the dataset is available as windsor_housing.csv. There are 11 input features (first 11 columns):\nlotsize\tbedrooms  bathrms  stories\tdriveway  recroom\tfullbase  gashw\t airco  garagepl   prefarea\n\nand 1 output feature i.e. price (12th column).\nThe focus of this lab is not really answering a preset analytical question, but to learn how you can perform a regression experiment, using mathematical manipulations - similar to the one you performed using statsmodels. So you won\'t be using any pandas or statsmodels goodness here. The key objectives here are to:\n\nUnderstand regression with matrix algebra and\nMastery in NumPy scientific computation\n\nStage 1: Prepare data for modeling\nLet\'s give you a head start by importing the dataset. You\'ll perform the following steps to get the data ready for analysis:\n\n\nInitialize an empty list data for loading data\n\n\nRead the csv file containing complete (raw) windsor_housing.csv. Use csv.reader() for loading data.. Store this in data one row at a time\n\n\nDrop the first row of csv file as it contains the names of variables (header) which won\'t be used during analysis (keeping this will cause errors as it contains text values)\n\n\nAppend a column of all 1s to the data (bias) as the first column\n\n\nConvert data to a NumPy array and inspect first few rows\n\n\n\nNOTE: read.csv() reads the csv as a text file, so you should convert the contents to float.\n\n# Your code here\n\n\n\n\n\n# First 5 rows of raw data \n\n# array([[1.00e+00, 5.85e+03, 3.00e+00, 1.00e+00, 2.00e+00, 1.00e+00,\n#         0.00e+00, 1.00e+00, 0.00e+00, 0.00e+00, 1.00e+00, 0.00e+00,\n#         4.20e+04],\n#        [1.00e+00, 4.00e+03, 2.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         3.85e+04],\n#        [1.00e+00, 3.06e+03, 3.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         4.95e+04],\n#        [1.00e+00, 6.65e+03, 3.00e+00, 1.00e+00, 2.00e+00, 1.00e+00,\n#         1.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         6.05e+04],\n#        [1.00e+00, 6.36e+03, 2.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n#         6.10e+04]])\nStep 2: Perform a 80/20 train-test split\nExplore NumPy\'s official documentation to manually split a dataset using a random sampling method of your choice. Some useful methods are located in the numpy.random library.\n\nPerform a random 80/20 split on data using a method of your choice in NumPy\nSplit the data to create x_train, y_train, x_test, and y_test arrays\nInspect the contents to see if the split performed as expected\n\n\nNote: When randomly splitting data, it\'s always recommended to set a seed in order to ensure reproducibility\n\n# Your code here \n\n\n\n# Split results\n# Raw data Shape:  (546, 13)\n# Train/Test Split: (437, 13) (109, 13)\n# x_train, y_train, x_test, y_test: (437, 12) (437,) (109, 12) (109,)\nStep 3: Calculate the beta\nWith $X$ and $y$ in place, you can now compute your beta values with $x_\\text{train}$ and $y_\\text{train}$ as:\n$\\beta = (x_\\text{train}^T. x_\\text{train})^{-1} . x_\\text{train}^T . y_\\text{train}$\n\nUsing NumPy operations (transpose, inverse) that we saw earlier, compute the above equation in steps\nPrint your beta values\n\n#\xa0Your code here \n\n\n\n\n# Beta values\n# Due to random split, your answers may vary \n# [-5.46637290e+03  3.62457767e+00  2.75100964e+03  1.47223649e+04\n#   5.97774591e+03  5.71916945e+03  5.73109882e+03  3.83586258e+03\n#   8.12674607e+03  1.33296437e+04  3.74995169e+03  1.01514699e+04]\nStep 4: Make predictions\nGreat, you now have a set of coefficients that describe the linear mappings between $X$ and $y$. You can now use the calculated beta values with the test datasets that we left out to calculate $y$ predictions. Next, use all features in turn and multiply it with this beta. The result will give a prediction for each row which you can append to a new array of predictions.\n$\\hat{y} = x\\beta = \\beta_0 + \\beta_1 x_1 +  \\beta_2 x_2 + \\ldots + \\beta_m x_m $\n\nCreate a new empty list (y_pred) for saving predictions\nFor each row of x_test, take the dot product of the row with beta to calculate the prediction for that row\nAppend the predictions to y_pred\nPrint the new set of predictions\n\n#\xa0Your code here \nStep 5: Evaluate model\nVisualize actual vs. predicted values\nThis is exciting, now your model can use the beta value to predict the price of houses given the input features. Let\'s plot these predictions against the actual values in y_test to see how much our model deviates.\n# Plot predicted and actual values as line plots\nThis doesn\'t look so bad, does it? Your model, although isn\'t perfect at this stage, is making a good attempt to predict house prices although a few prediction seem a bit out. There could be a number of reasons for this. Let\'s try to dig a bit deeper to check model\'s predictive abilities by comparing these prediction with actual values of y_test individually. That will help you calculate the RMSE value (root mean squared error) for your model.\nRoot Mean Squared Error\nHere is the formula for RMSE:\n$$ \\large RMSE = \\sqrt{\\sum^N_{i=1}\\dfrac{ (\\text{Predicted}_i-\\text{Actual}_i)^2}{N}}$$\n\nInitialize an empty array err\nFor each row in y_test and y_pred, take the squared difference and append error for each row in the err array\nCalculate $RMSE$ from err using the formula shown above\n\n# Calculate RMSE\n\n# Due to random split, your answers may vary \n# RMSE = 14868.172645765708\nNormalized root mean squared error\nThe above error is clearly in terms of the dependent variable, i.e., the final house price. You can also use a normalized mean squared error in case of multiple regression which can be calculated from RMSE using following the formula:\n$$ \\large NRMSE = \\dfrac{RMSE}{max_i y_i - min_i y_i} $$\n\nCalculate normalized RMSE\n\n#\xa0Calculate NRMSE\n\n# Due to random split, your answers may vary \n# 0.09011013724706489\nThere it is. A complete multiple regression analysis using nothing but NumPy. Having good programming skills in NumPy allows you to dig deeper into analytical algorithms in machine learning and deep learning. Using matrix multiplication techniques you saw here, you can easily build a whole neural network from scratch.\nLevel up (Optional)\n\nCalculate the R-squared and adjusted R-squared for the above model\nPlot the residuals (similar to statsmodels) and comment on the variance and heteroscedasticity\nRun the experiment in statsmodels and compare the performance of both approaches in terms of computational cost\n\nSummary\nIn this lab, you built a predictive model for predicting house prices. Remember this is a very naive implementation of regression modeling. The purpose here was to get an introduction to the applications of linear algebra into machine learning and predictive analysis. There are a number of shortcomings in this modeling approach and you can further apply a number of data modeling techniques to improve this model.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 8, 2020', '1', 'Jupyter Notebook', 'Updated Oct 2, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Python', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020']}","{'location': 'Hyderabad, India', 'stats_list': [], 'contributions': '90 contributions\n        in the last year', 'description': ['Logestic-Regression-Example-in-Machine-Learning\nSpecification\n\n\n\nIDE used --> Spyder (3.3.6)\n\n\nPython Version --> Python 3.7\n\n\npip version --> 19.1.1\n\n\nData Set Name --> Social_Network_Ads\n\n\nAnaconda --> 4.7.10\n\n\nOUTPUT\n\nOutput will be in form of a graph divided into two parts\n\n\nGreen\n\n\nRed\n\n\nThere will be red dots as well as green dots which will help in understanding thew set easily\n'], 'url_profile': 'https://github.com/ks1912', 'info_list': ['Jupyter Notebook', 'Updated Mar 8, 2020', '1', 'Jupyter Notebook', 'Updated Oct 2, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Python', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020']}","{'location': 'Gandhinagar', 'stats_list': [], 'contributions': '65 contributions\n        in the last year', 'description': ['Logistic-Regression-using-Encoder-Decoder-Dimensionality-Reduction\n\nI try to compare the performance of simple Logistic Regression on the text classification using tf-idf vectorizer vs when I try to reduce the dimension of the tf-idf vectorizer using encoder-decoder.\nBoth the models are giving almost equal results.\nIn case of encoder-decoder, I have not done hyperparameter tunning, it may increase the performance of the encoder-decoder based dimensionality reduction model.\nIt was really fun to see the results, when we are using encoder as dimensionality reduction tools.\n\nSteps for encoder-decoder based model:\n\nData Cleaning\nVectorize the dataset using tfidf\nReduce the dimensions of the dataset using encoder\nApply Logistic Regression\n\nSteps for Simple Logistic Regression based model:\n\nData Cleaning\nVectorize the dataset using tfidf\nApply Logistic Regression\n\n'], 'url_profile': 'https://github.com/kkaran0908', 'info_list': ['Jupyter Notebook', 'Updated Mar 8, 2020', '1', 'Jupyter Notebook', 'Updated Oct 2, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Python', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020']}","{'location': 'CHENNAI', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/vkaravindraman', 'info_list': ['Jupyter Notebook', 'Updated Mar 8, 2020', '1', 'Jupyter Notebook', 'Updated Oct 2, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Python', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""MLE and Logistic Regression\nIntroduction\nIn this lesson, you'll further investigate the connections between maximum likelihood estimation and logistic regression. This is a common perspective for logistic regression and will be the underlying intuition for upcoming lessons where you'll code the algorithm from the ground up using NumPy.\nObjectives\nYou will be able to:\n\nDetermine how MLE is tied into logistic regression\n\nMLE formulation\nAs discussed, maximum likelihood estimation finds the underlying parameters of an assumed distribution to maximize the likelihood of the observations. Logistic regression expands upon this by investigating the conditional probabilities associated with the various features, treating them as independent probabilities and calculating the respective total probability.\nFor example, when predicting an individual's risk for heart disease, you might consider various factors such as their family history, weight, diet, exercise routines, blood pressure, and cholesterol. When looked at individually, each of these has an associated conditional probability that the individual has heart disease based on each of these factors. Mathematically, you can write each of these probabilities for each factor $X$ as:\n$\\pi_i = Pr(Y_i = 1|X_i = x_i)=\\dfrac{\\text{exp}(\\beta_0 + \\beta_1 x_i)}{1 + \\text{exp}(\\beta_0 + \\beta_1 x_i)}$\nThis is the standard linear regression model ($\\beta_0+\\beta_1 x_i$) you have seen previously, modified to have a range of 0 to 1. The range is modified and constrained by applying the sigmoid function since you're predicting probabilities.\nThen, combining these conditional probabilities from multiple features, you maximize the likelihood function of each of those independent conditional probabilities, giving you:\n$ L(\\beta_0,\\beta_1)=\\prod\\limits_{i=1}^N \\pi_i^{y_i}(1-\\pi_i)^{n_i-y_i}=\\prod\\limits_{i=1}^N \\dfrac{\\text{exp}{y_i(\\beta_0+\\beta_1 x_i)}}{1+\\text{exp}(\\beta_0+\\beta_1 x_i)}$\nNotes on mathematical symbols\nRecall that the $\\prod$ sign stands for a product of each of these individual probabilities. (Similar to how $\\sum$ stands for the sum of a series.) Since this is a monotonically increasing function, its maximum will be the same as the logarithm of the function, which is typically used in practice in order to decompose this product of probabilities into a sum of log probabilities for easier calculation of the derivative. In future sections, you'll investigate the derivative of this function and then use that in order to code up our own function for logistic regression.\nAlgorithm bias and ethical concerns\nIt should also be noted that while this is mathematically sound and a powerful tool, the model will simply reflect the data that is fed in. For example, logistic regression and other algorithms are used to inform a wide range of decisions including whether to provide someone with a loan, the degree of criminal sentencing, or whether to hire an individual for a job. In all of these scenarios, it is again important to remember that the algorithm is simply reflective of the underlying data itself. If an algorithm is trained on a dataset where African Americans have had disproportionate criminal prosecution, the algorithm will continue to perpetuate these racial injustices. Similarly, algorithms trained on data that reflect a gender pay-gap will also continue to promote this bias unless adequately accounted for through careful preprocessing and normalization. With this, substantial thought and analysis regarding problem set up and the resulting model is incredibly important. While future lessons and labs in this section return to underlying mathematical theory and how to implement logistic regression on your own, it is worthwhile to investigate some of the current problems regarding some of these algorithms, and how naive implementations can perpetuate unjust biases.\nAdditional resources\nBelow are a handful of resources providing further information regarding some of the topics discussed here. Be sure to check out some of the news articles describing how poor safeguards and problem formulation surrounding algorithms such as logistic regression can lead to unjust biases:\nAlgorithm bias and ethical concerns\n\n\nMachine Bias\n\n\nAmazon‚Äôs Gender-Biased Algorithm Is Not Alone\n\n\nThe software that runs our lives can be bigoted and unfair. But we can fix it\n\n\nWhy artificial intelligence is far too human\n\n\nCan Computers Be Racist? The Human-Like Bias Of Algorithms\n\n\nAdditional mathematical resources\nFor a more in-depth discussion of the mathematical ideas, check out Penn State's lecture here\nIf you want to really go down the math rabbit-hole, check out section 4.4 on Logistic Regression from the Elements of Statistical Learning which can be found here: https://web.stanford.edu/~hastie/ElemStatLearn//.\nSummary\nIn this lesson, you further analyzed logistic regression from the perspective of maximum likelihood estimation. Additionally, there was a brief pause to consider the setup and interpretation of algorithms such as logistic regression. In particular, remember that issues regarding racial and gender bias that can be perpetuated by these algorithms. Always try to ensure your models are ethically sound. In the proceeding labs and lessons, you will continue to formalize your knowledge of logistic regression, implementing gradient descent and then a full logistic regression algorithm using Python packages in order to give you a deeper understanding of how logistic regression works.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 8, 2020', '1', 'Jupyter Notebook', 'Updated Oct 2, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Python', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['Real-time-Object-Recognition-Using-Cascaded-Regression\n\nIf you want to test clone and follow these steps:\n\n\nCreat a virtual environment : conda install -n name\nActivate the environment: activate name\nInstall necessary packages in requirements.txt : pip install -r requirements.txt\nImplement web interface and test : python app.py\n\n'], 'url_profile': 'https://github.com/thanhvinhle26', 'info_list': ['Jupyter Notebook', 'Updated Mar 8, 2020', '1', 'Jupyter Notebook', 'Updated Oct 2, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Python', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Complete Regression - Lab\nIntroduction\nBy now, you have created all the necessary functions to calculate the slope, intercept, best-fit line, prediction, and visualizations. In this lab you will put them all together to run a regression experiment and calculate the model loss.\nObjectives\nYou will be able to:\n\nPerform a linear regression using self-constructed functions\nCalculate the coefficient of determination using self-constructed functions\nUse the coefficient of determination to determine model performance\n\nThe formulas\nSlope:\n$\\hat m = \\dfrac{\\overline{x}*\\overline{y}-\\overline{xy}}{(\\overline{x})^2-\\overline{x^2}}$\nIntercept: $ \\hat c = \\bar{y} - \\hat m\\bar{x}$\nPrediction: $\\hat{y} = \\hat mx + \\hat c$\nR-Squared:\n$ R^2 = 1- \\dfrac{SS_{RES}}{SS_{TOT}} = 1 - \\dfrac{\\sum_i(y_i - \\hat y_i)^2}{\\sum_i(y_i - \\overline y_i)^2} $\nUse the Python functions created earlier to implement these formulas to run a regression analysis using x and y as input variables.\n# Combine all the functions created so far to run a complete regression experiment. \n# Produce an output similar to the one shown below. \n\nX = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=np.float64)\nY = np.array([7, 7, 8, 9, 9, 10, 10, 11, 11, 12], dtype=np.float64)\n# Basic Regression Diagnostics\n# ----------------------------\n# Slope: 0.56\n# Y-Intercept: 6.33\n# R-Squared: 0.97\n# ----------------------------\n# Model: Y = 0.56 * X + 6.33\nBasic Regression Diagnostics\n----------------------------\nSlope: 0.56\nY-Intercept: 6.33\nR-Squared: 0.97\n----------------------------\nModel: Y = 0.56 * X + 6.33\n\n\nMake Predictions\nPredict and plot the value of y using regression line above for a new value of $x = 4.5$.\n# Make prediction for x = 4.5 and visualize on the scatter plot\n\nLevel up - Optional\nLoad the ""heightweight.csv"" dataset. Use the height as an independent and weight as a dependent variable and draw a regression line to data using your code above. Calculate your R-Squared value for the model and try to predict new values of y.\nSummary\nIn this lab, we ran a complete simple regression analysis experiment using functions created so far. Next up, you\'ll learn how you can use Python\'s built-in modules to perform similar analyses with a much higher level of sophistication.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 8, 2020', '1', 'Jupyter Notebook', 'Updated Oct 2, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Jul 16, 2020', 'Python', 'Updated Mar 15, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '84 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/harsha-bsm', 'info_list': ['Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kishoremanamala', 'info_list': ['Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['Logistic-Regression-with-Python-in-Titanic-Dataset\n'], 'url_profile': 'https://github.com/SauravAdhikari', 'info_list': ['Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020']}","{'location': 'iran', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['Prediction-Of-Coronavirus-Confirmed-White-Linear-Regression\nwe use GRADIENT DESCENT to find the best fitting line\n'], 'url_profile': 'https://github.com/alinowshad', 'info_list': ['Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Analysis using Linear Algebra and NumPy - Code Along\nIntroduction\nIn the previous sections, you learned that in statistical modeling, regression analysis is a set of statistical processes for estimating the relationships between data entities (variables). Linear regression is an important predictive analytical tool in the data scientist\'s toolbox. Here, you\'ll try and develop a basic intuition for regression from a linear algebra perspective using vectors and matrix operations. This lesson covers least-squares regression with matrix algebra without digging deep into the geometric dimensions.\nYou can find a deeper mathematical and geometric explanation of the topic here. In this lesson, we\'ll try to keep things more data-oriented.\nObjectives\nYou will be able to:\n\nApply linear algebra to fit a function to data, describing linear mappings between input and output variables\nIndicate how linear algebra is related to regression modeling\n\nRegression analysis\nBy now, you know that the purpose of the regression process is to fit a mathematical model to a set of observed points, in order to later use that model for predicting new values e.g. predicting sales, based on historical sales figures, predicting house prices based on different features of the house, etc.\nLet\'s use a very simple toy example to understand how this works with linear algebra. Say you are collecting data on total number of sales per day for some business. Imagine you\'ve got three data points in the format:\n(day, total number of sales(in hundreds))\n\n(1, 1) , (2, 2) , (3, 2)\n\nIf we plot these points on a scatter plot with day (x-axis) vs. sales figures (y-axis), this is what we get:\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.array([1,2,3])\ny = np.array([1,2,2])\n               \nplt.plot(x, y, \'o\')\nplt.xticks(x)\n\nplt.show()\n#\xa0Code here \nFitting a model to data - A quick refresher\nThe purpose of linear regression would be to fit a mathematical model (a straight line) in the parameter space  that best describes the relationship between day and sales. Simple linear regression attempts to fit a line (in a 2-dimensional space) to describe the relationship between two variables as shown in the example below:\n\nFollowing this, if you were to identify a relationship between the day and total number of sales, the goal would be to seek a function that describes this line and allows us to linearly map input data points (day) or independent variable to outcome values (sales) or dependent variable.  If you do this, you first assume that there is an underlying relationship that maps ‚Äúdays‚Äù uniquely to ‚Äúnumber of sales‚Äù, that can be written in the function form as an equation of the straight line i.e.\n$$y = mx+c$$\nwhere $c$ is the intercept of the line and $m$ denotes the slope, as shown below:\n\nWe can write the fitting function based on the above as sales being a function of days.\n$$ \\text{sales} = f(\\text{days})$$\nor, from $y= mx+c$\n$$\\text{sales} = \\text{days}*x + \\text{intercept} $$\n\n(where y is the number of sales per day and x represents the day. c (intercept) and m (slope) are the regression coefficients we are looking for hoping that these co-efficients will linearly map day to the number of sales).\n\nSo using this, we can show our three data points ((1, 1) , (2, 2) , (3, 2)) as:\n\n$c + m*1 = 1$\n\n\n$c + m*2 = 2$\n\n\n$c + m*3 = 2$\n\nWe can see that our data points do not lie on a line. The first two points make a perfect linear system. When $x = 1$, $y = 1$; and when $x = 2$, $y = 2$ i.e. we can draw a straight line passing through these points. When x = 3, b = 2, you know the three points do not lie on the same line as first two points, and our model will be an approximation i.e.\n\nthere will be some error between the straight line and the REAL relationship between these parameters.\n\nThis behavior can be simulated by using NumPy\'s polyfit() function (similar to statsmodels.ols) to draw a regression line to the data points as shown below. Here is the documentation for np.polyfit().\nfrom numpy.polynomial.polynomial import polyfit\n\n# Fit with polyfit function to get c(intercept) and m(slope)\n# the degree parameter = 1 to models this as a straight line\nc, m = polyfit(x, y, 1)\n\n# Plot the data points and line calculated from ployfit\nplt.plot(x, y, \'o\')\nplt.plot(x, c + (m * x), \'-\')\nplt.xticks(x)\n\nplt.show()\nprint(c, m)\n#\xa0Code here \nThe numbers obtained here reflect the slope (0.5) and intercept values (0.66).\nThe line drawn above using this built-in regression model clearly doesn\'t touch all the data points. As a result, this is an approximation of the function you\'re trying to find. Now let\'s see how to achieve the same functionality with matrix algebra instead of the polyfit() function.\nCreate matrices and vectors\nA linear system like the one above can be solved using linear algebra! You only need to deal with a few vectors and matrices to set this up.\nRecalling linear systems from the previous lessons, you have:\n$$\n\\left[ {\\begin{array}{cc}\n1 & 1 \\\n1 & 2 \\\n1 & 3 \\\n\\end{array} } \\right]\n\\left[ {\\begin{array}{c}\nc \\\nm \\\n\\end{array} } \\right] =\n\\left[ {\\begin{array}{c}\n1 \\\n2 \\\n2 \\\n\\end{array} } \\right]\n$$\nThe intercept and error terms\nThe column of ones in the first matrix refers to the intercept ($c$) from $mx+c$. If you don\'t include this constant, then the function is constrained to the origin (0,0), which would strongly limit the types of relationships the model could describe. You want to include an intercept to allow for linear models to intersect with the $y$-axis at values different from 0 (in the image shown earlier, $c$ was 2, because the straight line crossed the $y$-axis at $y$=2).\nIn above , we are hoping that there is some linear combination of the columns of the first matrix that gives us our vector of observed values (the vector with values 1,2,2).\nUnfortunately, we already know that this vector does not fit our model perfectly. That means it is outside the column space of A and we can\'t solve that equation for the vector $x$ directly. Every line we draw will have some value of error $e$ associated with it.\nThe goal is to choose the vector $x$ for unknown variables to make $e$ as small as possible.\nOrdinary least squares\nA common measure to find and minimize the value of this error is called Ordinary Least Squares.\nThis says that our dependent variable, is composed of a linear part and error. The linear part is composed of an intercept and independent variable(s), along with their associated raw score regression weights.\nIn matrix terms, the same equation can be written as:\n$ y = \\boldsymbol{X} b + e $\nThis says to get y (sales), multiply each $\\boldsymbol{X}$ by the appropriate vector b (unknown parameters, the vector version of $m$ and $c$), then add an error term. We create a matrix $\\boldsymbol{X}$ , which has an extra column of 1s in it for the intercept. For each day, the 1 is used to add the intercept in the first row of the column vector $b$.\nLet\'s assume that the error is equal to zero on average and drop it to sketch a proof:\n$ y = \\boldsymbol{X} b$\nNow let\'s solve for $b$, so we need to get rid of $\\boldsymbol{X}$. First we will make X into a nice square, symmetric matrix by multiplying both sides of the equation by $\\boldsymbol{X}^T$ :\n$\\boldsymbol{X}^T y = \\boldsymbol{X}^T \\boldsymbol{X}b $\nAnd now we have a square matrix that with any luck has an inverse, which we will call $(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}$. Multiply both sides by this inverse, and we have\n$(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y =(\\boldsymbol{X}^T\\boldsymbol{X})^{-1} \\boldsymbol{X}^T \\boldsymbol{X}b $\nIt turns out that a matrix multiplied by its inverse is the identity matrix $(\\boldsymbol{X}^{-1}\\boldsymbol{X})= I$:\n$(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y =I b $\nAnd you know that $Ib= b$ So if you want to solve for $b$ (that is, remember, equivalent to finding the values $m$ and $c$ in this case), you find that:\n$ b= (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y $\nHere, we\'ll focus on the matrix and vector algebra perspective. With least squares regression, in order to solve for the expected value of weights, referred to as $\\hat{X}$ (""$X$-hat""), you need to solve the above equation.\nRemember all above variables represent vectors. The elements of the vector X-hat are the estimated regression coefficients $c$ and $m$ that you\'re looking for. They minimize the error between the model and the observed data in an elegant way that uses no calculus or complicated algebraic sums.\nThe above description can be summarized as:\nUsing linear regression is just trying to solve $Xb = y$. But if any of the observed points deviate from the model, you can\'t find a direct solution. To find a solution, you can multiply both sides by the transpose of $X$. The transpose of $X$ times $X$ will always allow us to solve for unknown variables.\nCalculate an OLS regression line\nLet\'s use the above formula to calculate a solution for our toy problem:\n# Calculate the solution\n\nX = np.array([[1, 1],[1, 2],[1, 3]])\ny = np.array([1, 2, 2])\nXt = X.T\nXtX = Xt.dot(X)\nXtX_inv = np.linalg.inv(XtX)\nXty = Xt.dot(y)\nx_hat = XtX_inv.dot(Xty) #\xa0the value for b shown above\nx_hat\n# Code here \nThe solution gives an intercept of 0.6 and slope value 0.5. Let\'s see what you get if you draw a line with these values with given data:\n# Define data points\nx = np.array([1, 2, 3])\ny = np.array([1, 2, 2])\n\n# Plot the data points and line parameters calculated above\nplt.plot(x, y, \'o\')\nplt.plot(x, x_hat[0] + (x_hat[1] * x), \'-\')\nplt.xticks(x)\n\nplt.show()\n#\xa0Code here \nThere you have it, an approximated line function! Just like the one you saw with polyfit(), by using simple matrix algebra.\nRegression with multiple variables\nAbove, you saw how you can draw a line on a 2D space using simple regression. If you perform a similar function with multiple variables, you can have a parameter space that is not 2D. With 3 parameters, i.e. two input and one output feature, the fitting function would not be a line, but would look like a plane:\n\nWhen you have more than one input variable, each data point can be seen as a feature vector $x_i$, composed of $x_1, x_2, \\ldots , x_m$ , where $m$ is the total number of features (columns). For multiple regression, each data point can contain two or more features of the input. To represent all of the input data along with the vector of output values we set up a input matrix X and an output vector y.\nyou can write this in general terms, as you saw earlier:\n\n$\\boldsymbol{X} \\beta \\approx y$\n\nWhere X are the input feature values, $\\beta$ represents the coefficients and y is the output (value to be predicted). In a simple least-squares linear regression model you are looking for a vector $\\beta$ so that the product $X \\beta$ most closely approximates the outcome vector y.\nFor each value of input features $x_i$, we can compute a predicted outcome value as:\nobserved data $\\rightarrow$ $y = b_0+b_1x_1+b_2x_2+ \\ldots + b_px_p+ \\epsilon $\npredicted data $\\rightarrow$ $\\hat y = \\hat b_0+\\hat b_1x_1+\\hat b_2x_2+ \\ldots + \\hat b_px_p $\nerror $\\rightarrow$ $\\epsilon = y - \\hat y $\nJust like before,  the formula to compute the beta vector remains:\n$ \\large b= (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y $\nSo you see that the general solution involves taking a matrix transpose, the inverse, and dot multiplications on the lines of solving a linear system of equations.\nIn the next lab, you\'ll use a simple dataset and with the above formulation for multivariate regression, you\'ll try to fit a model to the data and see how well it performs.\nFurther reading\nYou\'re strongly advised to visit the following links to develop a strong mathematical and geometrical intuition around how least squares work. These documents will provide you with a visual intuition as well as an in-depth mathematical formulation for above equations along with their proofs.\n\n\nQuora: Why do we need an extra column of ones in regression\n\n\nAn excellent visual demonstration of oridnary least squares\n\n\nSimple Regression in Matrix format\n\n\nSummary\nIn this lesson, you had a gentle introduction to how we can use linear algebra to solve regression problems. You saw a toy example in the case of simple linear regression, relating days to number of sales and calculated a function that approximates the linear mapping.\nYou also learned about how linear regression works in the context of multiple input variables and linear algebra. In the next lab, you\'ll use these equations to solve a real world problem.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Polynomial Regression - Lab\nIntroduction\nIn this lab, you\'ll practice your knowledge on adding polynomial terms to your regression model!\nObjectives\nYou will be able to:\n\nUse sklearn\'s built in capabilities to create polynomial features\n\nDataset\nHere is the dataset you will be working with in this lab:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndf = pd.read_csv(\'sample_data.csv\')\n\ndf.head()\nRun the following line of code. You will notice that the data is clearly of non-linear shape. Begin to think about what degree polynomial you believe will fit it best.\nplt.scatter(df[\'x\'], df[\'y\'], color=\'green\', s=50, marker=\'.\');\nTrain-test split\nThe next step is to split the data into training and test sets. Set the random_state to 42 and assign 75% of the data in the training set.\n# Split data into 75-25 train-test split \nfrom sklearn.model_selection import train_test_split\ny = df[\'y\']\nX = df.drop(columns=\'y\', axis=1)\nX_train, X_test, y_train, y_test = None\nBuild polynomial models\nNow it\'s time to determine the optimal degree of polynomial features for a model that is fit to this data. For each of second, third and fourth degrees:\n\nInstantiate PolynomialFeatures() with the number of degrees\nFit and transform the X_train features\nInstantiate and fit a linear regression model on the training data\nTransform the test data into polynomial features\nUse the model you built above to make predictions using the transformed test data\nEvaluate model performance on the test data using r2_score()\nIn order to plot how well the model performs on the full dataset, transform X using poly\nUse the same model (reg_poly) to make predictions using X_poly\n\n# Import relevant modules and functions\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\ncolors = [\'yellow\', \'lightgreen\', \'blue\']\nplt.figure(figsize=(10, 6))\nplt.scatter(df[\'x\'], df[\'y\'], color=\'green\', s=50, marker=\'.\', label=\'plot points\')\n\n# We\'ll fit 3 different polynomial regression models from degree 2 to degree 4\nfor index, degree in enumerate([2, 3, 4]):\n    \n    # Instantiate PolynomialFeatures\n    poly = None\n    \n    # Fit and transform X_train\n    X_poly_train = None\n    \n    # Instantiate and fit a linear regression model to the polynomial transformed train features\n    reg_poly = None\n    \n    # Transform the test data into polynomial features\n    X_poly_test = None\n    \n    # Get predicted values for transformed polynomial test data  \n    y_pred = None\n    \n    # Evaluate model performance on test data\n    print(""degree %d"" % degree, r2_score(y_test, y_pred))\n    \n    # Transform the full data\n    X_poly = None\n    \n    # Now, we want to see what the model predicts for the entire data \n    y_poly = None\n    \n    # Create plot of predicted values\n    plt.plot(X, y_poly, color = colors[index], linewidth=2, label=\'degree %d\' % degree)\n    plt.legend(loc=\'lower left\')\nSummary\nGreat job! You now know how to include polynomials in your linear models.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020']}","{'location': 'Kharagpur, West Bengal, India', 'stats_list': [], 'contributions': '246 contributions\n        in the last year', 'description': ['Regression-and-svm-very-basic-with-scikitlearn\n1.Took Stock Data for this project from Quandl \n2.Added Features accordingly\n3.Added label accordingly\n4.Removed the NaN values\n5.Trained the model using Linear Regression model of scikit-learn after splitting the data into Test Set and Training Set\n6.Tested and I got the score 84.79% (This is low as the training set that I took a small datasize(which I realized later)\n7.Then I trained the model with svm and got 77.9% accuracy on testing\n'], 'url_profile': 'https://github.com/gawd-coder', 'info_list': ['Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Polynomial Regression\nIntroduction\nIn the last lesson, you saw how you can account for interactions between two variables by including interaction effects in your model. In this lesson, you\'ll learn about another way to extend your regression model by including polynomial terms.\nObjectives\nYou will be able to:\n\nDefine polynomial variables in a regression context\nUse sklearn\'s built-in capabilities to create polynomial features\n\nAn example with one predictor\nThe dataset \'yields.csv\', with just 21 cases, contains measurements of the yields from an experiment done at six different levels of temperature in degrees Fahrenheit. Let\'s plot them.\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nyld = pd.read_csv(\'yield.csv\', sep=\'\\s+\', index_col=0)\nyld.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nTemp\nYield\n\n\ni\n\n\n\n\n\n\n1\n50\n3.3\n\n\n2\n50\n2.8\n\n\n3\n50\n2.9\n\n\n4\n50\n3.2\n\n\n5\n60\n2.7\n\n\n\n\nWe will now seperate the DataFrame into the predictor and outcome variables, X and y. We do so by:\n\nAssigning the \'Yield\' column to y\nDropping the \'Yield\' column from the yld DataFrame to create X\n\nNote: All scikit-learn classes assume the data to be in a certain shape, and by preparing data in this way, we ensure that these requirements are always met.\ny = yld[\'Yield\']\nX = yld.drop(columns=\'Yield\', axis=1)\nplt.scatter(X, y, color=\'green\')\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nIt\'s clear that there is no linear relationship between Yield and Temperature. Let\'s try and plot a line anyways and see how the model performs:\nfrom sklearn.linear_model import LinearRegression\nreg = LinearRegression().fit(X, y)\nplt.scatter(X, y, color=\'green\')\nplt.plot(X, reg.predict(X))\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nmean_squared_error(y, reg.predict(X))\n\nr2_score(y, reg.predict(X))\n0.08605718085106362\n\nA quadratic relationship\nWhen relationships between predictors and outcome are not linear and show some sort of curvature, polynomials can be used to generate better approximations. The idea is that you can transform your input variable by e.g, squaring it. The corresponding model would then be:\n$\\hat y = \\hat \\beta_0 + \\hat \\beta_1x + \\hat \\beta_2 x^2$\nThe idea is simple. You can square your predictor (here, ""Temp"") and include it in your model as if it were a new predictor.\nX[\'Temp_sq\'] = X[\'Temp\']**2\nX.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nTemp\nTemp_sq\n\n\ni\n\n\n\n\n\n\n1\n50\n2500\n\n\n2\n50\n2500\n\n\n3\n50\n2500\n\n\n4\n50\n2500\n\n\n5\n60\n3600\n\n\n\n\nreg_q = LinearRegression().fit(X, y)\nplt.scatter(X[\'Temp\'], y, color=\'green\')\nplt.plot(X[\'Temp\'], reg_q.predict(X))\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nThis is the resulting plot. Note that the fit is much better, and this is confirmed by the $R^2$score: where it was 0.086 before, it now is 0.6948!\nmean_squared_error(y, reg_q.predict(X))\n0.04650413890879158\n\nr2_score(y, reg_q.predict(X))\n0.6948165884110553\n\nNote that you get a seemingly ""piecewise linear"" function here,  because the yields were only measured at 50, 60, 70, 80, 90 and 100. In reality, this model generates a smooth curve, as denoted below.\nimport numpy as np\nplt.scatter(X[\'Temp\'], y, color=\'green\')\n\nX_pred = pd.DataFrame(np.linspace(50, 100, 50), columns=[\'Temp\'])\nX_pred[\'Temp_sq\'] = X_pred**2 \ny_pred = reg_q.predict(X_pred)\n\nplt.plot(X_pred[\'Temp\'], y_pred)\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nHigher-order relationships\nThe use of polynomials is not restricted to quadratic relationships. You can explore cubic or higher order relationships as well! Imagine you want to go until the power of 10, it would be quite annoying to transform your variable 9 times. Of course, Scikit-Learn has a built-in polynomial option in the preprocessing module! Let\'s call it with a polynomial of 6!\nfrom sklearn.preprocessing import PolynomialFeatures\n\ny = yld[\'Yield\']\nX = yld.drop(columns=\'Yield\', axis=1)\n\npoly = PolynomialFeatures(6)\nX_fin = poly.fit_transform(X)\nTake a look at what these transformed features really look like. As you can see, Scikit-Learn transformed the X value of a single 50 into $50^1$ through $50^6$ ! The first value of 1 represents the intercept in the linear regression, which you can read more about in the PolynomialFeatures documentation.\nprint(\'The transformed feature names are: {}\'.format(poly.get_feature_names()))\nprint(\'------------------\')\nprint(\'The first row of transformed data is: {}\'.format(X_fin[0]))\nThe transformed feature names are: [\'1\', \'x0\', \'x0^2\', \'x0^3\', \'x0^4\', \'x0^5\', \'x0^6\']\n------------------\nThe first row of transformed data is: [1.0000e+00 5.0000e+01 2.5000e+03 1.2500e+05 6.2500e+06 3.1250e+08\n 1.5625e+10]\n\nNow you can fit a linear regression model with your transformed data.\nreg_poly = LinearRegression().fit(X_fin, y)\nX_linspace = pd.DataFrame(np.linspace(50, 100, 50), columns=[\'Temp\'])\n\nX_linspace_fin = poly.fit_transform(X_linspace)\ny_poly_pred = reg_poly.predict(X_linspace_fin)\nplt.scatter(X[\'Temp\'], y, color=\'green\')\nplt.plot(X_linspace, y_poly_pred)\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nmean_squared_error(y, reg_poly.predict(X_fin))\n0.03670634920635693\n\nr2_score(y, reg_poly.predict(X_fin))\n0.7591145833332826\n\nThis seems to be a pretty smooth fit! This good fit is also confirmed with an even better $R^2$. Do note that by adding polynomials, you make your model more complex. Instead of just having 2 parameters ($\\beta_0$ and $\\beta_1$) for a linear model, you now have 7 (one for the intercept, and 6 for the terms when going until a polynomial with degree 6). More on that later!\nSummary\nGreat! You now know how to include polynomials in your linear models. Let\'s go ahead and practice this knowledge!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Linear Regression\nIntroduction\nIn this lecture, you\'ll be introduced to the multiple linear regression model. We\'ll start with an introductory example using linear regression, which you\'ve seen before, to act as a segue into multiple linear regression. We\'ll conclude this lecture by looking at a real data example.\nObjectives\nYou will be able to:\n\nInterpret the parameters of a multiple regression\n\nFrom simple to multiple linear regression\nYou have previously learned about linear regression models. In these models, what you try to do is fit a linear relationship between two variables. Let\'s refresh our memory with the example below. Here, we are trying to find a relationship between seniority and monthly income. It is definitely reasonable to assume that, on average, people with more seniority have a higher income than people who are newer to the job market and have less experience. A potential relationship could look like the plot below. The monthly income is shown in 1000 USD.\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nnp.random.seed(1234)\nsen = np.random.uniform(18, 65, 100)\nincome = np.random.normal((sen/10), 0.5)\nsen = sen.reshape(-1, 1)\n\nfig = plt.figure(figsize=(7, 5))\nfig.suptitle(\'seniority vs. income\', fontsize=16)\nplt.scatter(sen, income)\nplt.plot(sen, sen/10, c=\'black\')\nplt.xlabel(\'seniority\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.show()\n\nHere, seniority is the independent variable, and monthly income is the target variable.\nOf course, you know that seniority is not the only factor that drives income. Other factors that can play a role are, among others, the number of hours worked, years of education, and the city of employment. Now, you could create simple linear regression models for each of these factors and monthly outcome as a target, but more interestingly, you can also create a model where all these potential predictors serve as independent variables at once. How does this work? Let\'s start with an example with two predictors. As a general expression for our linear model, we had that $\\hat Y = bX + a$. Applied to this example, this would boil down to:\n$\\text{estimated monthly income} = slope * seniority + intercept $\nadding in years of education as a predictor, you can extend this model to:\n$\\text{estimated monthly income} = slope_sen * seniority + slope_ed * years_of_education  + intercept $\nWhat exactly does that look like?\n\nAs we have two predictors here, the simple line is replaced by a plane. Our $slope_sen$ represents the slope in the direction of the axis associated with seniority, our $slope_ed$ represents the slope in the direction of the axis associated with years of education.\nAnd it obviously doesn\'t stop here! We can add as many predictors as we like. What is important to note, however, is that for models with more than two predictors visualizing a multiple linear model becomes very difficult and even impossible! Still, they are used all the time, as linear models of all sorts are extensively helpful in many fields!\nWhen thinking of lines and slopes statistically, slope parameters associated with a particular predictor $x_i$ are often denoted by $\\beta_i$. Extending this example mathematically, you would write a multiple linear regression model as follows:\n$$ \\hat y = \\hat\\beta_0 + \\hat\\beta_1 x_1 + \\hat\\beta_2 x_2 +\\ldots + \\hat\\beta_n x_n $$\nwhere $n$ is the number of predictors, $\\beta_0$ is the intercept, and $\\hat y$ is the so-called ""fitted line"" or the predicted value associated with the dependent variable.\nSummary\nCongratulations! You have gained an initial understanding of a multiple linear regression model. To illustrate the usage of the multiple linear regression model, we\'ll be using the auto-mpg dataset which contains information on the technical specifications of a variety of cars. In the labs, you\'ll be practicing your newly gained knowledge using the Boston Housing data again.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020']}","{'location': 'Chicago, IL', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['U-of-C-Multiple-Linear-Regression-Model\nWill build a model that will predict housing values in Boston suburbs using various predictor variables available.\n'], 'url_profile': 'https://github.com/Livergood', 'info_list': ['Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Polynomial Regression\nIntroduction\nIn the last lesson, you saw how you can account for interactions between two variables by including interaction effects in your model. In this lesson, you\'ll learn about another way to extend your regression model by including polynomial terms.\nObjectives\nYou will be able to:\n\nDefine polynomial variables in a regression context\nUse sklearn\'s built-in capabilities to create polynomial features\n\nAn example with one predictor\nThe dataset \'yields.csv\', with just 21 cases, contains measurements of the yields from an experiment done at six different levels of temperature in degrees Fahrenheit. Let\'s plot them.\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nyld = pd.read_csv(\'yield.csv\', sep=\'\\s+\', index_col=0)\nyld.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nTemp\nYield\n\n\ni\n\n\n\n\n\n\n1\n50\n3.3\n\n\n2\n50\n2.8\n\n\n3\n50\n2.9\n\n\n4\n50\n3.2\n\n\n5\n60\n2.7\n\n\n\n\nWe will now seperate the DataFrame into the predictor and outcome variables, X and y. We do so by:\n\nAssigning the \'Yield\' column to y\nDropping the \'Yield\' column from the yld DataFrame to create X\n\nNote: All scikit-learn classes assume the data to be in a certain shape, and by preparing data in this way, we ensure that these requirements are always met.\ny = yld[\'Yield\']\nX = yld.drop(columns=\'Yield\', axis=1)\nplt.scatter(X, y, color=\'green\')\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nIt\'s clear that there is no linear relationship between Yield and Temperature. Let\'s try and plot a line anyways and see how the model performs:\nfrom sklearn.linear_model import LinearRegression\nreg = LinearRegression().fit(X, y)\nplt.scatter(X, y, color=\'green\')\nplt.plot(X, reg.predict(X))\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nmean_squared_error(y, reg.predict(X))\n\nr2_score(y, reg.predict(X))\n0.08605718085106362\n\nA quadratic relationship\nWhen relationships between predictors and outcome are not linear and show some sort of curvature, polynomials can be used to generate better approximations. The idea is that you can transform your input variable by e.g, squaring it. The corresponding model would then be:\n$\\hat y = \\hat \\beta_0 + \\hat \\beta_1x + \\hat \\beta_2 x^2$\nThe idea is simple. You can square your predictor (here, ""Temp"") and include it in your model as if it were a new predictor.\nX[\'Temp_sq\'] = X[\'Temp\']**2\nX.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nTemp\nTemp_sq\n\n\ni\n\n\n\n\n\n\n1\n50\n2500\n\n\n2\n50\n2500\n\n\n3\n50\n2500\n\n\n4\n50\n2500\n\n\n5\n60\n3600\n\n\n\n\nreg_q = LinearRegression().fit(X, y)\nplt.scatter(X[\'Temp\'], y, color=\'green\')\nplt.plot(X[\'Temp\'], reg_q.predict(X))\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nThis is the resulting plot. Note that the fit is much better, and this is confirmed by the $R^2$score: where it was 0.086 before, it now is 0.6948!\nmean_squared_error(y, reg_q.predict(X))\n0.04650413890879158\n\nr2_score(y, reg_q.predict(X))\n0.6948165884110553\n\nNote that you get a seemingly ""piecewise linear"" function here,  because the yields were only measured at 50, 60, 70, 80, 90 and 100. In reality, this model generates a smooth curve, as denoted below.\nimport numpy as np\nplt.scatter(X[\'Temp\'], y, color=\'green\')\n\nX_pred = pd.DataFrame(np.linspace(50, 100, 50), columns=[\'Temp\'])\nX_pred[\'Temp_sq\'] = X_pred**2 \ny_pred = reg_q.predict(X_pred)\n\nplt.plot(X_pred[\'Temp\'], y_pred)\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nHigher-order relationships\nThe use of polynomials is not restricted to quadratic relationships. You can explore cubic or higher order relationships as well! Imagine you want to go until the power of 10, it would be quite annoying to transform your variable 9 times. Of course, Scikit-Learn has a built-in polynomial option in the preprocessing module! Let\'s call it with a polynomial of 6!\nfrom sklearn.preprocessing import PolynomialFeatures\n\ny = yld[\'Yield\']\nX = yld.drop(columns=\'Yield\', axis=1)\n\npoly = PolynomialFeatures(6)\nX_fin = poly.fit_transform(X)\nTake a look at what these transformed features really look like. As you can see, Scikit-Learn transformed the X value of a single 50 into $50^1$ through $50^6$ ! The first value of 1 represents the intercept in the linear regression, which you can read more about in the PolynomialFeatures documentation.\nprint(\'The transformed feature names are: {}\'.format(poly.get_feature_names()))\nprint(\'------------------\')\nprint(\'The first row of transformed data is: {}\'.format(X_fin[0]))\nThe transformed feature names are: [\'1\', \'x0\', \'x0^2\', \'x0^3\', \'x0^4\', \'x0^5\', \'x0^6\']\n------------------\nThe first row of transformed data is: [1.0000e+00 5.0000e+01 2.5000e+03 1.2500e+05 6.2500e+06 3.1250e+08\n 1.5625e+10]\n\nNow you can fit a linear regression model with your transformed data.\nreg_poly = LinearRegression().fit(X_fin, y)\nX_linspace = pd.DataFrame(np.linspace(50, 100, 50), columns=[\'Temp\'])\n\nX_linspace_fin = poly.fit_transform(X_linspace)\ny_poly_pred = reg_poly.predict(X_linspace_fin)\nplt.scatter(X[\'Temp\'], y, color=\'green\')\nplt.plot(X_linspace, y_poly_pred)\nplt.xlabel(\'Temperature\')\nplt.ylabel(\'Yield\');\n\nmean_squared_error(y, reg_poly.predict(X_fin))\n0.03670634920635693\n\nr2_score(y, reg_poly.predict(X_fin))\n0.7591145833332826\n\nThis seems to be a pretty smooth fit! This good fit is also confirmed with an even better $R^2$. Do note that by adding polynomials, you make your model more complex. Instead of just having 2 parameters ($\\beta_0$ and $\\beta_1$) for a linear model, you now have 7 (one for the intercept, and 6 for the terms when going until a polynomial with degree 6). More on that later!\nSummary\nGreat! You now know how to include polynomials in your linear models. Let\'s go ahead and practice this knowledge!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Aug 27, 2020', 'MATLAB', 'Updated Mar 2, 2020', '1', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 5, 2020', '1', 'HTML', 'Updated Mar 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Linear Regression\nIntroduction\nIn this lecture, you\'ll be introduced to the multiple linear regression model. We\'ll start with an introductory example using linear regression, which you\'ve seen before, to act as a segue into multiple linear regression. We\'ll conclude this lecture by looking at a real data example.\nObjectives\nYou will be able to:\n\nInterpret the parameters of a multiple regression\n\nFrom simple to multiple linear regression\nYou have previously learned about linear regression models. In these models, what you try to do is fit a linear relationship between two variables. Let\'s refresh our memory with the example below. Here, we are trying to find a relationship between seniority and monthly income. It is definitely reasonable to assume that, on average, people with more seniority have a higher income than people who are newer to the job market and have less experience. A potential relationship could look like the plot below. The monthly income is shown in 1000 USD.\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nnp.random.seed(1234)\nsen = np.random.uniform(18, 65, 100)\nincome = np.random.normal((sen/10), 0.5)\nsen = sen.reshape(-1, 1)\n\nfig = plt.figure(figsize=(7, 5))\nfig.suptitle(\'seniority vs. income\', fontsize=16)\nplt.scatter(sen, income)\nplt.plot(sen, sen/10, c=\'black\')\nplt.xlabel(\'seniority\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.show()\n\nHere, seniority is the independent variable, and monthly income is the target variable.\nOf course, you know that seniority is not the only factor that drives income. Other factors that can play a role are, among others, the number of hours worked, years of education, and the city of employment. Now, you could create simple linear regression models for each of these factors and monthly outcome as a target, but more interestingly, you can also create a model where all these potential predictors serve as independent variables at once. How does this work? Let\'s start with an example with two predictors. As a general expression for our linear model, we had that $\\hat Y = bX + a$. Applied to this example, this would boil down to:\n$\\text{estimated monthly income} = slope * seniority + intercept $\nadding in years of education as a predictor, you can extend this model to:\n$\\text{estimated monthly income} = slope_sen * seniority + slope_ed * years_of_education  + intercept $\nWhat exactly does that look like?\n\nAs we have two predictors here, the simple line is replaced by a plane. Our $slope_sen$ represents the slope in the direction of the axis associated with seniority, our $slope_ed$ represents the slope in the direction of the axis associated with years of education.\nAnd it obviously doesn\'t stop here! We can add as many predictors as we like. What is important to note, however, is that for models with more than two predictors visualizing a multiple linear model becomes very difficult and even impossible! Still, they are used all the time, as linear models of all sorts are extensively helpful in many fields!\nWhen thinking of lines and slopes statistically, slope parameters associated with a particular predictor $x_i$ are often denoted by $\\beta_i$. Extending this example mathematically, you would write a multiple linear regression model as follows:\n$$ \\hat y = \\hat\\beta_0 + \\hat\\beta_1 x_1 + \\hat\\beta_2 x_2 +\\ldots + \\hat\\beta_n x_n $$\nwhere $n$ is the number of predictors, $\\beta_0$ is the intercept, and $\\hat y$ is the so-called ""fitted line"" or the predicted value associated with the dependent variable.\nSummary\nCongratulations! You have gained an initial understanding of a multiple linear regression model. To illustrate the usage of the multiple linear regression model, we\'ll be using the auto-mpg dataset which contains information on the technical specifications of a variety of cars. In the labs, you\'ll be practicing your newly gained knowledge using the Boston Housing data again.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Aug 27, 2020', 'MATLAB', 'Updated Mar 2, 2020', '1', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 5, 2020', '1', 'HTML', 'Updated Mar 31, 2020']}","{'location': 'Chicago, IL', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['U-of-C-Multiple-Linear-Regression-Model\nWill build a model that will predict housing values in Boston suburbs using various predictor variables available.\n'], 'url_profile': 'https://github.com/Livergood', 'info_list': ['Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Aug 27, 2020', 'MATLAB', 'Updated Mar 2, 2020', '1', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 5, 2020', '1', 'HTML', 'Updated Mar 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['RiboViz regression test data 04/03/2020\nProvenance:\n\nURL: https://github.com/riboviz/riboviz\nDate: Wed Mar 4 05:45:40 2020 -0800\nBranch: master\nCommit: 0ecd69ec0786eaeee78c4ce0999f4a181c775044\nEnvironment: see environment.txt\n\nUsage:\n$ git clone https://github.com/riboviz/regression-test-data-2.0.beta\n$ cd riboviz\n$ pytest riboviz/test/regression/test_vignette.py --expected=$HOME/regression-test-data-2.0.beta\nFor full instructions see the RiboViz developer guide in the RiboViz repository.\n'], 'url_profile': 'https://github.com/riboviz', 'info_list': ['Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Aug 27, 2020', 'MATLAB', 'Updated Mar 2, 2020', '1', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 5, 2020', '1', 'HTML', 'Updated Mar 31, 2020']}","{'location': 'Paris', 'stats_list': [], 'contributions': '222 contributions\n        in the last year', 'description': [""Projet-apprentissage-automatique-M1\nEffectuer de l'analyse de donn√©es, feature engineering ainsi que de l'apprentissage automatique sur un probl√®me de regression classique.\n""], 'url_profile': 'https://github.com/raysr', 'info_list': ['Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Aug 27, 2020', 'MATLAB', 'Updated Mar 2, 2020', '1', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 5, 2020', '1', 'HTML', 'Updated Mar 31, 2020']}","{'location': 'Canada', 'stats_list': [], 'contributions': '176 contributions\n        in the last year', 'description': [""What's weather like?\nObjective:\nTo visualize the weather of 500+ cities across the world of varying distance from the equator. Also, plan the vacations using the analysis obtained from weather data.\nData Source:\nOpenWeatherMap API\nOverview:\nCreated a Python script to pull in the weather data using OpenWeather API of 500+ cities randomly selected on the basis of latitude and longitude from citipy. \nUsing Matplotlib created a series of scatter plots exploring the following relationships:\n\nTemperature (F) vs. Latitude\n\n\n\nHumidity (%) vs. Latitude\n\n\n\nCloudiness (%) vs. Latitude\n\n\n\nWind Speed (mph) vs. Latitude\n\n\nFurthermore the linear regression aspect was analysed for the previously generated scatter plots only this time separating them into Northern and Southern hemisphere.\nThe weather analysis performed was then used for planning an ideal vacation. Using jupyter gmaps and Google Places API, a heatmap was generated using gmaps displaying the humidity level of each city selected for weather analysis.\n\nAfter narrowing down the dataframe for an ideal weather condition Google Places API was used to search for the first hotel for each city located within 5000 meters of the coordinates and the hotels were pinned on top of the humidity heatmap with each pin containing the Hotel Name, City, and Country.\n\n""], 'url_profile': 'https://github.com/Vaishali91tanwar', 'info_list': ['Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Aug 27, 2020', 'MATLAB', 'Updated Mar 2, 2020', '1', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 5, 2020', '1', 'HTML', 'Updated Mar 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/subagan', 'info_list': ['Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Aug 27, 2020', 'MATLAB', 'Updated Mar 2, 2020', '1', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 5, 2020', '1', 'HTML', 'Updated Mar 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['AirQualityIndex-Prediction\nPredicting the Air quality index of bangalore (india) using state of the art machine learning regression techniques\n'], 'url_profile': 'https://github.com/srivatsavaappalla', 'info_list': ['Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Aug 27, 2020', 'MATLAB', 'Updated Mar 2, 2020', '1', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 5, 2020', '1', 'HTML', 'Updated Mar 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '65 contributions\n        in the last year', 'description': ['model_implementations\nthis folder contains following model implementations: bigram, HMM, FFN, FST, Naive Bayes, Logistic Regression, Perceptron, Distributional Semantics\n'], 'url_profile': 'https://github.com/LizLian', 'info_list': ['Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Aug 27, 2020', 'MATLAB', 'Updated Mar 2, 2020', '1', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 5, 2020', '1', 'HTML', 'Updated Mar 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '58 contributions\n        in the last year', 'description': [""Bitcoin Price Prediction\nPredict bitcoin price using gold and S&P 500 data implementing LSTM, Gradient Boosting Regression, and Random Forest\ndata_engineering.py\nIncludes functions for downloading and modelling the price of BTC in USD\nSource Data:\n\nAPI: cryptocompare.com for BTC and associated metrics\nSPDR GoldShares (GLD) to represent gold prices\nBusiness Insider: SPDR Gold Shares for latest price\nS&P Dow Jones Indices LLC, S&P 500, retrieved from FRED, Federal Reserve Bank of St. Louis\n\nReferences:\n\nSPDR Gold Shares Fact Sheet\nSelecting Features for RandomForrest, datadive.net\nLMBA Gold Price\nDiving Into Data: Selecting Good Features\nColah's Blog: Understanding LSTM Networks\nMachine Learning Mastery: Multivariate Time Series Forecasting with LSTMs in Keras\nDavid Sheehan Blog: Predicting Cryptocurrency Prices With Deep Learning\nsklearn.ensemble\nWikipedia: SPDR ETFs\nWikipedia: Gradient Boosting\nWikipedia: Random Forest\nWikipedia: Ensemble Learning\nWikipedia: Long short-term memory (LSTM)\n\n""], 'url_profile': 'https://github.com/silverrainb', 'info_list': ['Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Aug 27, 2020', 'MATLAB', 'Updated Mar 2, 2020', '1', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 5, 2020', '1', 'HTML', 'Updated Mar 31, 2020']}"
"{'location': 'Hamburg, Germany', 'stats_list': [], 'contributions': '215 contributions\n        in the last year', 'description': [""Project 1 Data Analysis of Housing Market in King County\nIn General\nThis project is centered around exploratory data\nanalysis (EDA) techniques and statistical analysis,\nas well as modeling data using linear regression.\nIn the jupyter notebook called 'Instructions' you will\nfind all information you need concerning the tasks\nof the project.\nThis is week 3, the first Data Analysis project for the Data Science bootcamp at neuefische. All credits about the Instructions to them!\nneue fische\nFictional business case: The prefabricator\nI will look at the King County Housing dataset from the point of view of an analyst at a Housing Company which is focused on prefabrication. They have a set of 4 Housing types and are thinking whether to expand into King County local business or not. My task is to take a look at the local market and estimate housing prices.\nFor further Info take a look\n\nin the presentation: link here\nor in the notebook here\n\n""], 'url_profile': 'https://github.com/HssDix', 'info_list': ['Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Nov 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 8, 2020', '1', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Java', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Dec 6, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}","{'location': 'Africa', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': [""A SCHEMA FOR PREDICTING STUDENTS' GRADUATION LIKELYHOOD  USING DATA MINING MODELS\n\nThe purpose of this work is to use data mining model to predict students performance and dropout. The Project uses both supervised and unsupervised models to make prediction.*\n\n\nIntroduction\nThis is a data science project that uses both regression and classification model to predict students' graduation. This was done using KMeans algorithm in order see into how many classes the students are classified and then Support Vector Machine was used to predicting students' graduation and finally used Linear regression to predict students' Performance.\nDataset Source and Rows\ntwo sets of different datasets were used one to predict students performance and the other one to predict students graduation. The data used were both from KAGGLE and UCI Repository, both are free sources for datasets that are recognised by so many people in both academia and industry.\nThe performance datasets consists of 32 while for the students graduation/dropout data it includes more.\nAlgorithms and Libraries\nFor the prediction part two algorithms were used which are Support Vector Machine and Linear Regression. These models were implemented using Scikit learning. Other libraries used mostly during data analysis and data analysis and data cleaning includes:\n-Pandas: for importing data and cleaning.\n-Numpy: for mathematical calculations.\n-Matplotlib: for data visualization.\n-Seaborn: for data visualizations also.\nand many others...\nActivities.\nthe entire project was classified into three activities namely:\n-importing and cleaning data.\n-data analysis.\n-pipeline creation and evaluation.\n""], 'url_profile': 'https://github.com/muhammadbashir87', 'info_list': ['Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Nov 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 8, 2020', '1', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Java', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Dec 6, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}","{'location': 'Pune', 'stats_list': [], 'contributions': '38 contributions\n        in the last year', 'description': ['Future-Loan-Prediction\nTo build a data model and predict the probability of loan status by using Logistic Regression Algorithm.\n'], 'url_profile': 'https://github.com/AdityaSatdive', 'info_list': ['Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Nov 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 8, 2020', '1', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Java', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Dec 6, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '669 contributions\n        in the last year', 'description': ['Salary-Prediction\nThis project will give you an idea to predict salary for new person in company by Simple Linear Regression\nJupyter Notebook has been used for this project\n....................................MACHINE LEARNING IS FUN...............................................\n'], 'url_profile': 'https://github.com/devil-cyber', 'info_list': ['Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Nov 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 8, 2020', '1', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Java', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Dec 6, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}","{'location': 'banglore', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['Advertisement-Data-Set\nIndicating Whether or not a particular internet user clicked on a advertisement and get best accuracy using Logistic Regression\n'], 'url_profile': 'https://github.com/yasinthree', 'info_list': ['Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Nov 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 8, 2020', '1', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Java', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Dec 6, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}","{'location': 'Krak√≥w, Poland', 'stats_list': [], 'contributions': '104 contributions\n        in the last year', 'description': ['Demonstrates a regression between Spring Boot 2.2.4 and 2.2.5 in how bean validation works.\nTo reproduce run ./gradlew test.\n'], 'url_profile': 'https://github.com/pkubowicz', 'info_list': ['Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Nov 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 8, 2020', '1', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Java', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Dec 6, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '142 contributions\n        in the last year', 'description': ['Sentiment analysis on IMDb movie review using a logistic regression model. Deployed app using Plotly and Heroku\n'], 'url_profile': 'https://github.com/cdixson-ds', 'info_list': ['Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Nov 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 8, 2020', '1', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Java', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Dec 6, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}","{'location': 'Kathmandu, Nepal', 'stats_list': [], 'contributions': '83 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/kshitijchaudhary', 'info_list': ['Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Nov 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 8, 2020', '1', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Java', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Dec 6, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}","{'location': 'San Antonio', 'stats_list': [], 'contributions': '35 contributions\n        in the last year', 'description': ['Predicting-AQI-level of Delhi\nThe aim of this project is to predict the AQI levels of Delhi ,India which is one of the most polluted cities in the World.\nSeveral Machine Learning models such as Linear Regression, Decision trees, Random Forests, Ridge Regression, Lasso Regression, Elastci net models were trained and hyperparemter tuning was performed using Gridsearch.\nReproducibility\nThe Jupyter notebook contains the exploratory data analysis, data cleaning, and algorithm testing. Notebook is written in Python 3.6. Requirements.txt lists all dependencies to run the code. This is a combination of weather data and pollution data.The weather data is included in weather_data file and the AQI data for three years are present in separate files which are merged with the weather data to get the complete dataset.\nTraining and Evaluation\nThe Data was split into training and testing and different models were trained and RMSE scores were used to evaluate the performance of the models. Random forest produced the least RMSE scores of 4.93 .\n'], 'url_profile': 'https://github.com/Alinahuda', 'info_list': ['Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Nov 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 8, 2020', '1', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Java', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Dec 6, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Coding Logistic Regression From Scratch - Lab\nIntroduction\nIn this lab, you\'ll practice your ability to translate mathematical algorithms into Python functions. This will deepen and solidify your understanding of logistic regression!\nObjectives\nIn this lab you will:\n\nBuild a logistic regression model from scratch using gradient descent\n\nOverview\nRecall that the logistic regression algorithm builds upon the intuition from linear regression. In logistic regression, you start by taking the input data, X, and multiplying it by a vector of weights for each of the individual features, which produces an output, y. Afterward, you\'ll work on using an iterative approach via gradient descent to tune these weights.\nLinear regression setup\nWrite a simple function predict_y() that takes in a matrix X of observations and a vector of feature weights w and outputs a vector of predictions for the various observations.\nRecall that this is the sum of the product of each of the feature observations and their corresponding feature weights:\n$\\large \\hat{y}i = X{i1} \\cdot w_1 + X_{i2} \\cdot w_2 + X_{i3} \\cdot w_3 + ... + X_{in} \\cdot w_n$\n\nHint: Think about which mathematical operation you\'ve seen previously that will take a matrix (X) and multiply it by a vector of weights (w). Use NumPy!\n\n# Your code here\nimport numpy as np\n\ndef predict_y(X, w): \n    pass\nThe sigmoid function\nRecall that the sigmoid function is used to map the linear regression model output to a range of 0 to 1, satisfying basic premises of probability. As a reminder, the sigmoid function is defined by:\n$S(x) = \\dfrac{1}{1+e^(-x)}$\nWrite this as a Python function where x is the input and the function outputs the result of the sigmoid function.\n\nHint: Use NumPy!\n\n# Your code here\nPlot the sigmoid\nFor good measure, let\'s do a brief investigation of your new function. Plot the output of your sigmoid() function using 10,000 values evenly spaced from -20 to 20.\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Plot sigmoid\nGradient descent with the sigmoid function\nRecall that gradient descent is a numerical method for finding a minimum to a cost function. In the case of logistic regression, you are looking to minimize the error between the model\'s predictions and the actual data labels. To do this, you first calculate an error vector based on the current model\'s feature weights. You then multiply the transpose of the training matrix itself by this error vector in order to obtain the gradient. Finally, you take the gradient, multiply it by the step size and add this to our current weight vector to update it. Below, write such a function. It will take 5 inputs:\n\nX\ny\nmax_iterations\nalpha (the step size)\ninitial_weights\n\nBy default, have your function set the initial_weights parameter to a vector where all feature weights are set to 1.\n# Your code here\ndef grad_desc(X, y, max_iterations, alpha, initial_weights=None):\n    """"""Be sure to set default behavior for the initial_weights parameter.""""""\n    # Create a for loop of iterations\n        # Generate predictions using the current feature weights\n        # Calculate an error vector based on these initial predictions and the correct labels\n        # Calculate the gradient \n        # As we saw in the previous lab, calculating the gradient is often the most difficult task.\n        # Here, your are provided with the closed form solution for the gradient of the log-loss function derived from MLE\n        # For more details on the derivation, see the additional resources section below.\n        gradient = np.dot(X.transpose(), error_vector) \n        # Update the weight vector take a step of alpha in direction of gradient \n    # Return finalized weights\n    \nRunning your algorithm\nNow that you\'ve coded everything from the ground up, you can further investigate the convergence behavior of the gradient descent algorithm. Remember that gradient descent does not guarantee a global minimum, only a local minimum, and that small deviations in the starting point or step size can lead to different outputs.\nFirst, run the following cell to import the data and create the predictor and target variables:\n# Import data\nimport pandas as pd\ndf = pd.read_csv(\'heart.csv\')\n\n# Create the predictor and target variables\ny = df[\'target\']\nX = df.drop(columns=[\'target\'], axis=1)\n\nprint(y.value_counts())\nX.head()\nRun your algorithm and plot the successive weights of the features through iterations. Below is a dataset, with X and y predefined for you. Use your logistic regression function to train a model. As the model trains, record the iteration cycle of the gradient descent algorithm and the weights of the various features. Then, plot this data on subplots for each of the individual features. Each graph should have the iteration number on the x-axis and the value of that feature weight for that iteration cycle on the y-axis. This will visually display how the algorithm is adjusting the weights over successive iterations, and hopefully show convergence to stable weights.\n# Your code here\nScikit-learn\nFor comparison, import scikit-learn\'s standard LogisticRegression() function. Initialize it with no intercept and C=1e16 or another very high number. The reason is as follows: our implementation has not used an intercept, and you have not performed any regularization such as Lasso or Ridge (scikit-learn uses l2 by default). The high value of C will essentially negate this. Also, set the random_state to 2 and use the \'liblinear\' solver.\nAfter initializing a regression object, fit it to X and y.\n# Your code here\nCompare the models\nCompare the coefficient weights of your model to that generated by scikit-learn.\n# Your code here\nLevel up (Optional)\nUpdate the gradient descent algorithm to also return the cost after each iteration. Then rerun the algorithm and create a graph displaying the cost versus the iteration number.\n# Your code here\nAdditional Resources\nIf you want to see more of the mathematics behind the gradient derivation above, check out section 4.4.1 from the Elements of Statistical Learning which can be found here: https://web.stanford.edu/~hastie/ElemStatLearn//.\nSummary\nCongratulations! You just coded logistic regression from the ground up using NumPy! With this, you should have a fairly deep understanding of logistic regression and how the algorithm works! In the upcoming labs, you\'ll continue to explore this from a few more angles, plotting your data along with the decision boundary for our predictions.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Nov 7, 2020', '1', 'Jupyter Notebook', 'Updated Mar 8, 2020', '1', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Java', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Dec 6, 2020', 'Jupyter Notebook', 'MIT license', 'Updated May 8, 2020', '1', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""A Complete Data Science Project Using Multiple Regression -  Recap\nKey Takeaways\nCongratulations! You're really coming along! This section gave you an opportunity to review some of the wide ranging skills you've acquired and conduct a full Data Science project! With that, you should have seen that a good Data Science process requires careful thought and not just about the technical details, but also about the general structure and story behind the data itself. Indeed, substantial business value comes from asking the right questions and persuasively communicating the results. Similarly, even when modeling, much of the predictive value comes from thoughtful selection, and creative feature engineering through exploration of the data.\nTo further summarize:\n\nThe most common Data Science frameworks are CRISP-DM, KDD, and OSEMiN.\nThe process of finding, filtering, and loading the appropriate data to answer a question is non-trivial.\nDecisions made in the data munging/scrubbing phase can have a huge impact on the accuracy of your predictions.\nVisualization is a key phase in EDA.\nAnalyzing regression models:\n\nCheck p-values to determine whether features are significant\nUse Q-Q plots to check for normality\nPlot residuals against the target variable to check for homoscedasticity (and rule out heteroscedasticity)\nUse the Variance Inflation Factor to assess Multicollinearity among independent variables\n\n\n\nSummary\nAgain, well done! You put a lot of your skills to work and went through a full process of collecting data, cleaning it, analyzing and using it to answer questions.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', '1', 'Jupyter Notebook', 'Updated Mar 6, 2020', '1', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Model Fit in Linear Regression\nIntroduction\nIn this lesson, you\'ll learn how to evaluate your model results and you\'ll learn methods to select the appropriate features.\nObjectives\nYou will be able to:\n\nUse stepwise selection methods to determine the most important features for a model\nUse recursive feature elimination to determine the most important features for a model\n\nR-squared and adjusted R-squared\nTake another look at the model summary output for the auto-mpg dataset.\nThe code below reiterates the steps we\'ve taken before: we\'ve created dummies for our categorical variables and have log-transformed some of our continuous predictors.\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(\'auto-mpg.csv\')\n\nacc = data[\'acceleration\']\nlogdisp = np.log(data[\'displacement\'])\nloghorse = np.log(data[\'horsepower\'])\nlogweight = np.log(data[\'weight\'])\n\nscaled_acc = (acc-min(acc))/(max(acc)-min(acc))\t\nscaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\nscaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\nscaled_weight = (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n\ndata_fin = pd.DataFrame([])\ndata_fin[\'acc\'] = scaled_acc\ndata_fin[\'disp\'] = scaled_disp\ndata_fin[\'horse\'] = scaled_horse\ndata_fin[\'weight\'] = scaled_weight\ncyl_dummies = pd.get_dummies(data[\'cylinders\'], prefix=\'cyl\', drop_first=True)\nyr_dummies = pd.get_dummies(data[\'model year\'], prefix=\'yr\', drop_first=True)\norig_dummies = pd.get_dummies(data[\'origin\'], prefix=\'orig\', drop_first=True)\nmpg = data[\'mpg\']\ndata_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)\ndata_ols = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis=1)\ndata_ols.head(3)\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nmpg\nacceleration\nweight\norig_2\norig_3\n\n\n\n\n0\n18.0\n0.238095\n0.720986\n0\n0\n\n\n1\n15.0\n0.208333\n0.908047\n0\n0\n\n\n2\n18.0\n0.178571\n0.651205\n0\n0\n\n\n\n\n# Import statsmodels.api as sm\nfrom statsmodels.formula.api import ols\noutcome = \'mpg\'\npredictors = data_ols.drop(\'mpg\', axis=1)\npred_sum = \'+\'.join(predictors.columns)\nformula = outcome + \'~\' + pred_sum\nmodel = ols(formula=formula, data=data_ols).fit()\nmodel.summary()\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 26 Sep 2019   Prob (F-statistic): 1.86e-107\n\n\nTime: 13:01:06   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nIntercept    20.7608     0.688    30.181  0.000    19.408    22.113\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_2     0.4124     0.639     0.645  0.519    -0.844     1.669\n\n\norig_3     1.7218     0.653     2.638  0.009     0.438     3.005\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.               8.47\n\nWarnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nLet\'s discuss some key metrics in light of our output:\n\nR-squared uses a baseline model which is a naive model. This baseline model does not make use of any independent variables to predict the value of dependent variable Y. Instead it uses the mean of the observed responses of the dependent variable Y and always predicts this mean as the value of Y. The mathematical formula to calculate R-squared for a linear regression line is in terms of squared errors for the fitted model and the baseline model. In the formula below, $SS_{RES}$ is the residual sum of squared errors or our model, also known as $SSE$, which is the error between the real and predicted values. $SS_{TOT}$ is the difference between real and the mean $y$ value.\n\n$$ R^2 = 1-\\dfrac{SS_{RES}}{SS_{TOT}}=1 - \\dfrac{\\sum_i (y_i-\\hat{y_i})^2}{\\sum_i {(y_i-\\bar{y})^2}}$$\n\nHowever, the value of $R^2$ increases each time we add a new predictor -- even if this new predictor doesn\'t add any new information to the model. That is, the model tends to overfit if we only use $R^2$ as our model fitting criterion. This is why train-test split is essential and why regularization techniques are used to refine more advanced regression models. Make sure to read this blogpost on the difference between the two to get a better sense to why use $R^2_{adj}$ !\n\nThe parameter estimates and p-values\nJust like with simple linear regression, the parameters or coefficients we\'re calculating have a p-value or significance attached to them. The interpretation of the p-value for each parameter is exactly the same as for multiple regression:\n\nThe p-value represents the probability that the coefficient is actually zero.\n\nIn the Statsmodels output, the p-value can be found in the column with name $P>|t|$. A popular threshold for the p-value is 0.05, where we $p<0.05$ denotes that a certain parameter is significant, and $p>0.05$ means that the parameter isn\'t significant.\nThe two columns right to the p-value column represent the bounds associated with the 95% confidence interval. What this means is that, after having run the model, we are 95% certain that our parameter value is within the bounds of this interval. When you chose a p-value cut-off of 0.05, there is an interesting relationship between the 95% confidence interval and the p-value: If the 95% confidence does not include 0, the p-value will be smaller than 0.05, and the parameter estimate will be significant.\nWhich variables are most important when predicting the target?\nNow that you know how predictors influence the target, let\'s talk about selecting the right predictors for your model. It is reasonable to think that when having many potential predictors (sometimes 100s or 1000s of predictors can be available!) not only will adding all of them largely increase computation time, but it might even lead to inferior $R^2_{adj}$ or inferior predictions in general. There are several ways to approach variable selection, and we\'ll only touch upon this in an ad-hoc manner in this lesson. The most straightforward way is to run a model with each possible combination of variables and see which one results in the best metric of your choice, let\'s say, $R^2_{adj}$.\n\nThis is where your combinatorics knowledge comes in handy!\n\nNow, when you know about combinations, you know that the number of combinations can add up really quickly.\n\nImagine you have 6 variables. How many models can you create with 6 variables and all subsets of variables?\n\nYou can create:\n\n1 model with all 6 variables\n6 models with just 5 variables: $\\dbinom{6}{5}$\n15 models with just 4 variables: $\\dbinom{6}{4}= \\dfrac{6!}{2!*4!}=15$\n20 models with 3 variables: $\\dbinom{6}{3}= \\dfrac{6!}{3!*3!} = 20$\n15 models with 2 variables: $\\dbinom{6}{2}= \\dfrac{6!}{4!*2!} = 15$\n6 models with 1 variable: $\\dbinom{6}{1}$\nAnd, technically, 1 model with no predictors: this will return a model that simply returns the mean of $Y$.\n\nThis means that with just 6 variables, a so-called exhaustive search of all submodels results in having to evaluate 64 models. Below, we\'ll describe two methods that can be used to select a submodel with the most appropriate features: stepwise selection on the one hand, and feature ranking with recursive feature elimination on the other hand.\nStepwise selection with p-values\nIn stepwise selection, you start with an empty model (which only includes the intercept), and each time, the variable that has an associated parameter estimate with the lowest p-value is added to the model (forward step). After adding each new variable in the model, the algorithm will look at the p-values of all the other parameter estimates which were added to the model previously, and remove them if the p-value exceeds a certain value (backward step). The algorithm stops when no variables can be added or removed given the threshold values.\nFor more information, it\'s worth having a look at the wikipedia page on stepwise regression.\nUnfortunately, stepwise selection is not readily available a Python library just yet. This stackexchange post, however, presents the code to do a stepwise selection in statsmodels.\nimport statsmodels.api as sm\n\ndef stepwise_selection(X, y, \n                       initial_list=[], \n                       threshold_in=0.01, \n                       threshold_out = 0.05, \n                       verbose=True):\n    """""" Perform a forward-backward feature selection \n    based on p-value from statsmodels.api.OLS\n    Arguments:\n        X - pandas.DataFrame with candidate features\n        y - list-like with the target\n        initial_list - list of features to start with (column names of X)\n        threshold_in - include a feature if its p-value < threshold_in\n        threshold_out - exclude a feature if its p-value > threshold_out\n        verbose - whether to print the sequence of inclusions and exclusions\n    Returns: list of selected features \n    Always set threshold_in < threshold_out to avoid infinite looping.\n    See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n    """"""\n    included = list(initial_list)\n    while True:\n        changed=False\n        # forward step\n        excluded = list(set(X.columns)-set(included))\n        new_pval = pd.Series(index=excluded)\n        for new_column in excluded:\n            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n            new_pval[new_column] = model.pvalues[new_column]\n        best_pval = new_pval.min()\n        if best_pval < threshold_in:\n            best_feature = new_pval.idxmin()\n            included.append(best_feature)\n            changed=True\n            if verbose:\n                print(\'Add  {:30} with p-value {:.6}\'.format(best_feature, best_pval))\n\n        # backward step\n        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n        # use all coefs except intercept\n        pvalues = model.pvalues.iloc[1:]\n        worst_pval = pvalues.max() # null if pvalues is empty\n        if worst_pval > threshold_out:\n            changed=True\n            worst_feature = pvalues.argmax()\n            included.remove(worst_feature)\n            if verbose:\n                print(\'Drop {:30} with p-value {:.6}\'.format(worst_feature, worst_pval))\n        if not changed:\n            break\n    return included\nresult = stepwise_selection(predictors, data_fin[\'mpg\'], verbose=True)\nprint(\'resulting features:\')\nprint(result)\nAdd  weight                         with p-value 1.16293e-107\nAdd  acceleration                   with p-value 0.000646572\nAdd  orig_3                         with p-value 0.0091813\nresulting features:\n[\'weight\', \'acceleration\', \'orig_3\']\n\n\n//anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n  return ptp(axis=axis, out=out, **kwargs)\n\nApplying the stepwise selection on our small auto-mpg example (just starting with the predictors weight, acceleration and the origin categorical levels), we end up with a model that just keeps weight, acceleration, and orig_3.\nFeature ranking with recursive feature elimination\nScikit-learn also provides a few functionalities for feature selection. Their Feature Ranking with Recursive Feature Elimination selects the pre-specified $n$ most important features. This means you must specify the number of features to retail. If this number is not provided, half are used by default. See here for more information on how the algorithm works.\n#from sklearn.datasets import make_friedman1\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\n\nlinreg = LinearRegression()\nselector = RFE(linreg, n_features_to_select=3)\nselector = selector.fit(predictors, data_fin[\'mpg\'])\nCalling the .support_ attribute tells you which variables are selected\nselector.support_ \narray([ True,  True, False,  True])\n\nCalling .ranking_ shows the ranking of the features, selected features are assigned rank 1\nselector.ranking_\narray([1, 1, 2, 1])\n\nBy calling .estimator_ on the RFE object, you can get access to the parameter estimates through .coef_ and .intercept.\nestimators = selector.estimator_\nprint(estimators.coef_)\nprint(estimators.intercept_)\n[ 5.11183657 -5.95285464  1.53951788]\n20.841013816401656\n\nNote that the regression coefficients and intercept are slightly different. This is because only the three most important features were used in the model instead of the original four features. If you pass n_features_to_select=4, you should get the original coefficients.\nForward selection using adjusted R-squared\nThis resource provides code for a forward selection procedure (much like stepwise selection, but without a backward pass), but this time looking at the adjusted R-squared to make decisions on which variable to add to the model.\nSummary\nCongrats! In this lesson, you learned about how you can perform feature selection using stepwise selection methods and recursive feature elimination.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', '1', 'Jupyter Notebook', 'Updated Mar 6, 2020', '1', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Linear Regression in Statsmodels\nIntroduction\nIn this lecture, you\'ll learn how to run your first multiple linear regression model.\nObjectives\nYou will be able to:\n\nUse statsmodels to fit a multiple linear regression model\nEvaluate a linear regression model by using statistical performance metrics pertaining to overall model and specific parameters\n\nStatsmodels for multiple linear regression\nThis lesson will be more of a code-along, where you\'ll walk through a multiple linear regression model using both statsmodels and scikit-learn.\nRecall the initial regression model presented. It determines a line of best fit by minimizing the sum of squares of the errors between the models predictions and the actual data. In algebra and statistics classes, this is often limited to the simple 2 variable case of $y=mx+b$, but this process can be generalized to use multiple predictive variables.\nAuto-mpg data\nThe code below reiterates the steps you\'ve seen before:\n\nCreating dummy variables for each categorical feature\nLog-transforming select continuous predictors\n\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(\'auto-mpg.csv\') \ndata[\'horsepower\'].astype(str).astype(int)\n\nacc = data[\'acceleration\']\nlogdisp = np.log(data[\'displacement\'])\nloghorse = np.log(data[\'horsepower\'])\nlogweight= np.log(data[\'weight\'])\n\nscaled_acc = (acc-min(acc))/(max(acc)-min(acc))\t\nscaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\nscaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\nscaled_weight= (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n\ndata_fin = pd.DataFrame([])\ndata_fin[\'acc\'] = scaled_acc\ndata_fin[\'disp\'] = scaled_disp\ndata_fin[\'horse\'] = scaled_horse\ndata_fin[\'weight\'] = scaled_weight\ncyl_dummies = pd.get_dummies(data[\'cylinders\'], prefix=\'cyl\', drop_first=True)\nyr_dummies = pd.get_dummies(data[\'model year\'], prefix=\'yr\', drop_first=True)\norig_dummies = pd.get_dummies(data[\'origin\'], prefix=\'orig\', drop_first=True)\nmpg = data[\'mpg\']\ndata_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)\ndata_fin.info()\n<class \'pandas.core.frame.DataFrame\'>\nRangeIndex: 392 entries, 0 to 391\nData columns (total 23 columns):\nmpg       392 non-null float64\nacc       392 non-null float64\ndisp      392 non-null float64\nhorse     392 non-null float64\nweight    392 non-null float64\ncyl_4     392 non-null uint8\ncyl_5     392 non-null uint8\ncyl_6     392 non-null uint8\ncyl_8     392 non-null uint8\nyr_71     392 non-null uint8\nyr_72     392 non-null uint8\nyr_73     392 non-null uint8\nyr_74     392 non-null uint8\nyr_75     392 non-null uint8\nyr_76     392 non-null uint8\nyr_77     392 non-null uint8\nyr_78     392 non-null uint8\nyr_79     392 non-null uint8\nyr_80     392 non-null uint8\nyr_81     392 non-null uint8\nyr_82     392 non-null uint8\norig_2    392 non-null uint8\norig_3    392 non-null uint8\ndtypes: float64(5), uint8(18)\nmemory usage: 22.3 KB\n\nFor now, let\'s simplify the model and only inlude \'acc\', \'horse\' and the three \'orig\' categories in our final data.\ndata_ols = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis=1)\ndata_ols.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nmpg\nacceleration\nweight\norig_2\norig_3\n\n\n\n\n0\n18.0\n0.238095\n0.720986\n0\n0\n\n\n1\n15.0\n0.208333\n0.908047\n0\n0\n\n\n2\n18.0\n0.178571\n0.651205\n0\n0\n\n\n3\n16.0\n0.238095\n0.648095\n0\n0\n\n\n4\n17.0\n0.148810\n0.664652\n0\n0\n\n\n\n\nA linear model using statsmodels\nNow, let\'s use the statsmodels.api to run OLS on all of the data. Just like for linear regression with a single predictor, you can use the formula $y \\sim X$ with $n$ predictors where $X$ is represented as $x_1+\\ldots+x_n$.\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nformula = \'mpg ~ acceleration+weight+orig_2+orig_3\'\nmodel = ols(formula=formula, data=data_ols).fit()\nHaving to type out all the predictors isn\'t practical when you have many. Another better way than to type them all out is to seperate out the outcome variable \'mpg\' out of your DataFrame, and use the a \'+\'.join() command on the predictors, as done below:\noutcome = \'mpg\'\npredictors = data_ols.drop(\'mpg\', axis=1)\npred_sum = \'+\'.join(predictors.columns)\nformula = outcome + \'~\' + pred_sum\nmodel = ols(formula=formula, data=data_ols).fit()\nmodel.summary()\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 26 Sep 2019   Prob (F-statistic): 1.86e-107\n\n\nTime: 12:01:03   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nIntercept    20.7608     0.688    30.181  0.000    19.408    22.113\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_2     0.4124     0.639     0.645  0.519    -0.844     1.669\n\n\norig_3     1.7218     0.653     2.638  0.009     0.438     3.005\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.               8.47\n\nWarnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nOr even easier, simply use the ols() function from statsmodels.api. The advantage is that you don\'t have to create the summation string. Important to note, however, is that the intercept term is not included by default, so you have to make sure you manipulate your predictors DataFrame so it includes a constant term. You can do this using .add_constant.\nimport statsmodels.api as sm\npredictors_int = sm.add_constant(predictors)\nmodel = sm.OLS(data[\'mpg\'],predictors_int).fit()\nmodel.summary()\n//anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n  return ptp(axis=axis, out=out, **kwargs)\n\n\nOLS Regression Results\n\nDep. Variable: mpg   R-squared:             0.726\n\n\nModel: OLS   Adj. R-squared:        0.723\n\n\nMethod: Least Squares   F-statistic:           256.7\n\n\nDate: Thu, 26 Sep 2019   Prob (F-statistic): 1.86e-107\n\n\nTime: 12:01:03   Log-Likelihood:      -1107.2\n\n\nNo. Observations:    392   AIC:                   2224.\n\n\nDf Residuals:    387   BIC:                   2244.\n\n\nDf Model:      4    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [0.025 0.975]\n\n\nconst    20.7608     0.688    30.181  0.000    19.408    22.113\n\n\nacceleration     5.0494     1.389     3.634  0.000     2.318     7.781\n\n\nweight    -5.8764     0.282   -20.831  0.000    -6.431    -5.322\n\n\norig_2     0.4124     0.639     0.645  0.519    -0.844     1.669\n\n\norig_3     1.7218     0.653     2.638  0.009     0.438     3.005\n\n\n\n\nOmnibus: 37.427   Durbin-Watson:         0.840\n\n\nProb(Omnibus):  0.000   Jarque-Bera (JB):     55.989\n\n\nSkew:  0.648   Prob(JB):           6.95e-13\n\n\nKurtosis:  4.322   Cond. No.               8.47\n\nWarnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nInterpretation\nJust like for single multiple regression, the coefficients for the model should be interpreted as ""how does $y$ change for each additional unit $X$""? However, do note that since $X$ was transformed, the interpretation can sometimes require a little more attention. In fact, as the model is built on the transformed $X$, the actual relationship is ""how does $y$ change for each additional unit $X\'$"", where $X\'$ is the (log- and min-max, standardized,...) transformed data matrix.\nLinear regression using scikit-learn\nYou can also repeat this process using scikit-learn. The code to do this can be found below. The scikit-learn package is known for its machine learning functionalities and generally very popular when it comes to building a clear data science workflow. It is also commonly used by data scientists for regression. The disadvantage of scikit-learn compared to statsmodels is that it doesn\'t have some statistical metrics like the p-values of the parameter estimates readily available. For a more ad-hoc comparison of scikit-learn and statsmodels, you can read this blogpost: https://blog.thedataincubator.com/2017/11/scikit-learn-vs-statsmodels/.\nfrom sklearn.linear_model import LinearRegression\ny = data_ols[\'mpg\']\nlinreg = LinearRegression()\nlinreg.fit(predictors, y)\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n\n# coefficients\nlinreg.coef_\narray([ 5.04941007, -5.87640551,  0.41237454,  1.72184708])\n\nThe intercept of the model is stored in the .intercept_ attribute.\n# intercept\nlinreg.intercept_\n20.760757080821836\n\nSummary\nCongrats! You now know how to build a linear regression model with multiple predictors in statsmodel and scikit-learn. You also took a look at the statistical performance metrics pertaining to the overall model and its parameters!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', '1', 'Jupyter Notebook', 'Updated Mar 6, 2020', '1', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020']}","{'location': 'Kathmandu, Nepal', 'stats_list': [], 'contributions': '236 contributions\n        in the last year', 'description': ['Implementation of Classic Machine Learning Algorithm (Logistic Regression) to classify clickbait titles among online news\n'], 'url_profile': 'https://github.com/KamalShrest', 'info_list': ['Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', '1', 'Jupyter Notebook', 'Updated Mar 6, 2020', '1', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['Prediction-Students-grades-using-Bayesian-Linear-Regression-in-Python\nIn this notebook, we will implement a complete machine learning project, focusing on Bayesian Inference methods, in particular, Bayesian Linear Regression. We will go through the entire machine learning process, cleaning the data, exploring it to find trends, establishing a baseline model, evaluating several machine learning approaches for comparisons, implementing Bayesian Linear Regression, interpreting the results, and presenting the results.\n'], 'url_profile': 'https://github.com/chandrakanthab', 'info_list': ['Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', '1', 'Jupyter Notebook', 'Updated Mar 6, 2020', '1', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Logistic Regression - Recap\nKey Takeaways\n\nIn this section you learned about a different supervised learning technique: classification! Specifically, you practiced building a very basic classification model from scratch - a logistic regression model\nLogistic regression uses a sigmoid function which helps to plot an ""s""-like curve that enables a linear function to act as a binary classifier\nYou can evaluate logistic regression models using some combination of precision, recall, and accuracy\nA confusion matrix is another common way to visualize the performance of a classification model\nReceiver Operating Characteristic (ROC) curve and the Area Under the Curve (AUC) can be used to help determine the best precision-recall tradeoff for a given classifier\nClass weights, under/oversampling, and SMOTE can be used to deal with class imbalance problems\n\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', '1', 'Jupyter Notebook', 'Updated Mar 6, 2020', '1', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Logistic Regression - Recap\nKey Takeaways\n\nIn this section you learned about a different supervised learning technique: classification! Specifically, you practiced building a very basic classification model from scratch - a logistic regression model\nLogistic regression uses a sigmoid function which helps to plot an ""s""-like curve that enables a linear function to act as a binary classifier\nYou can evaluate logistic regression models using some combination of precision, recall, and accuracy\nA confusion matrix is another common way to visualize the performance of a classification model\nReceiver Operating Characteristic (ROC) curve and the Area Under the Curve (AUC) can be used to help determine the best precision-recall tradeoff for a given classifier\nClass weights, under/oversampling, and SMOTE can be used to deal with class imbalance problems\n\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', '1', 'Jupyter Notebook', 'Updated Mar 6, 2020', '1', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating Logistic Regression Models - Lab\nIntroduction\nIn regression, you are predicting continous values so it makes sense to discuss error as a distance of how far off our estimates were. When classifying a binary variable, however, a model is either correct or incorrect. As a result, we tend to quantify this in terms of how many false positives versus false negatives we come across. In particular, we examine a few different specific measurements when evaluating the performance of a classification algorithm. In this lab, you\'ll review precision, recall, accuracy, and F1 score in order to evaluate our logistic regression models.\nObjectives\nIn this lab you will:\n\nImplement evaluation metrics from scratch using Python\n\nTerminology review\nLet\'s take a moment and review some classification evaluation metrics:\n$$ \\text{Precision} = \\frac{\\text{Number of True Positives}}{\\text{Number of Predicted Positives}} $$\n$$ \\text{Recall} = \\frac{\\text{Number of True Positives}}{\\text{Number of Actual Total Positives}} $$\n$$ \\text{Accuracy} = \\frac{\\text{Number of True Positives + True Negatives}}{\\text{Total Observations}} $$\n$$ \\text{F1 score} = 2 * \\frac{\\text{Precision * Recall}}{\\text{Precision + Recall}} $$\nAt times, it may be best to tune a classification algorithm to optimize against precision or recall rather than overall accuracy. For example, imagine the scenario of predicting whether or not a patient is at risk for cancer and should be brought in for additional testing. In cases such as this, we often may want to cast a slightly wider net, and it is preferable to optimize for recall, the number of cancer positive cases, than it is to optimize precision, the percentage of our predicted cancer-risk patients who are indeed positive.\nSplit the data into training and test sets\nimport pandas as pd\ndf = pd.read_csv(\'heart.csv\')\ndf.head()\nSplit the data first into X and y, and then into training and test sets. Assign 25% to the test set and set the random_state to 0.\n# Import train_test_split\n\n\n# Split data into X and y\ny = None\nX = None\n\n# Split the data into a training and a test set\nX_train, X_test, y_train, y_test = None\nBuild a vanilla logistic regression model\n\nImport and instantiate LogisticRegression\nMake sure you do not use an intercept term and use the \'liblinear\' solver\nFit the model to training data\n\n# Import LogisticRegression\n\n\n# Instantiate LogisticRegression\nlogreg = None\n\n# Fit to training data\nmodel_log = None\nmodel_log\nWrite a function to calculate the precision\ndef precision(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the recall\ndef recall(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the accuracy\ndef accuracy(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the F1 score\ndef f1_score(y, y_hat):\n    # Your code here\n    pass\nCalculate the precision, recall, accuracy, and F1 score of your classifier\nDo this for both the training and test sets.\n# Your code here\ny_hat_train = None\ny_hat_test = None\nGreat job! Now it\'s time to check your work with sklearn.\nCalculate metrics with sklearn\nEach of the metrics we calculated above is also available inside the sklearn.metrics module.\nIn the cell below, import the following functions:\n\nprecision_score\nrecall_score\naccuracy_score\nf1_score\n\nCompare the results of your performance metrics functions above with the sklearn functions. Calculate these values for both your train and test set.\n# Your code here\nNicely done! Did the results from sklearn match that of your own?\nCompare precision, recall, accuracy, and F1 score for train vs test sets\nCalculate and then plot the precision, recall, accuracy, and F1 score for the test and training splits using different training set sizes. What do you notice?\nimport matplotlib.pyplot as plt\n%matplotlib inline\ntraining_precision = []\ntesting_precision = []\ntraining_recall = []\ntesting_recall = []\ntraining_accuracy = []\ntesting_accuracy = []\ntraining_f1 = []\ntesting_f1 = []\n\nfor i in range(10, 95):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= None) # replace the ""None"" here\n    logreg = LogisticRegression(fit_intercept=False, C=1e20, solver=\'liblinear\')\n    model_log = None\n    y_hat_test = None\n    y_hat_train = None \n    \n    # Your code here\nCreate four scatter plots looking at the train and test precision in the first one, train and test recall in the second one, train and test accuracy in the third one, and train and test F1 score in the fourth one.\nWe already created the scatter plot for precision:\n# Train and test precision\nplt.scatter(list(range(10, 95)), training_precision, label=\'training_precision\')\nplt.scatter(list(range(10, 95)), testing_precision, label=\'testing_precision\')\nplt.legend()\nplt.show()\n# Train and test recall\n# Train and test accuracy\n# Train and test F1 score\nSummary\nNice! In this lab, you calculated evaluation metrics for classification algorithms from scratch in Python. Going forward, continue to think about scenarios in which you might prefer to optimize one of these metrics over another.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', '1', 'Jupyter Notebook', 'Updated Mar 6, 2020', '1', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Fitting a Logistic Regression Model - Lab\nIntroduction\nIn the last lesson you were given a broad overview of logistic regression. This included an introduction to two separate packages for creating logistic regression models. In this lab, you\'ll be investigating fitting logistic regressions with statsmodels. For your first foray into logistic regression, you are going to attempt to build a model that classifies whether an individual survived the Titanic shipwreck or not (yes, it\'s a bit morbid).\nObjectives\nIn this lab you will:\n\nImplement logistic regression with statsmodels\nInterpret the statistical results associated with model parameters\n\nImport the data\nImport the data stored in the file \'titanic.csv\' and print the first five rows of the DataFrame to check its contents.\n# Import the data\n\n\ndf = None\nDefine independent and target variables\nYour target variable is in the column \'Survived\'. A 0 indicates that the passenger didn\'t survive the shipwreck. Print the total number of people who didn\'t survive the shipwreck. How many people survived?\n# Total number of people who survived/didn\'t survive\nOnly consider the columns specified in relevant_columns when building your model. The next step is to create dummy variables from categorical variables. Remember to drop the first level for each categorical column and make sure all the values are of type float:\n# Create dummy variables\nrelevant_columns = [\'Pclass\', \'Age\', \'SibSp\', \'Fare\', \'Sex\', \'Embarked\', \'Survived\']\ndummy_dataframe = None\n\ndummy_dataframe.shape\nDid you notice above that the DataFrame contains missing values? To keep things simple, simply delete all rows with missing values.\n\nNOTE: You can use the .dropna() method to do this.\n\n# Drop missing rows\ndummy_dataframe = None\ndummy_dataframe.shape\nFinally, assign the independent variables to X and the target variable to y:\n# Split the data into X and y\ny = None\nX = None\nFit the model\nNow with everything in place, you can build a logistic regression model using statsmodels (make sure you create an intercept term as we showed in the previous lesson).\n\nWarning: Did you receive an error of the form ""LinAlgError: Singular matrix""? This means that statsmodels was unable to fit the model due to certain linear algebra computational problems. Specifically, the matrix was not invertible due to not being full rank. In other words, there was a lot of redundant, superfluous data. Try removing some features from the model and running it again.\n\n# Build a logistic regression model using statsmodels\nAnalyze results\nGenerate the summary table for your model. Then, comment on the p-values associated with the various features you chose.\n# Summary table\n# Your comments here\nLevel up (Optional)\nCreate a new model, this time only using those features you determined were influential based on your analysis of the results above. How does this model perform?\n# Your code here\n# Your comments here\nSummary\nWell done! In this lab, you practiced using statsmodels to build a logistic regression model. You then interpreted the results, building upon your previous stats knowledge, similar to linear regression. Continue on to take a look at building logistic regression models in Scikit-learn!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', '1', 'Jupyter Notebook', 'Updated Mar 6, 2020', '1', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Regression Model Validation - Lab\nIntroduction\nIn this lab, you'll be able to validate your Ames Housing data model using train-test split.\nObjectives\nYou will be able to:\n\nCompare training and testing errors to determine if model is over or underfitting\n\nLet's use our Ames Housing Data again!\nWe included the code to preprocess below.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\names = pd.read_csv('ames.csv')\n\n# using 9 predictive categorical or continuous features, plus the target SalePrice\ncontinuous = ['LotArea', '1stFlrSF', 'GrLivArea', 'SalePrice']\ncategoricals = ['BldgType', 'KitchenQual', 'SaleType', 'MSZoning', 'Street', 'Neighborhood']\n\names_cont = ames[continuous]\n\n# log features\nlog_names = [f'{column}_log' for column in ames_cont.columns]\n\names_log = np.log(ames_cont)\names_log.columns = log_names\n\n# normalize (subract mean and divide by std)\n\ndef normalize(feature):\n    return (feature - feature.mean()) / feature.std()\n\names_log_norm = ames_log.apply(normalize)\n\n# one hot encode categoricals\names_ohe = pd.get_dummies(ames[categoricals], prefix=categoricals, drop_first=True)\n\npreprocessed = pd.concat([ames_log_norm, ames_ohe], axis=1)\nX = preprocessed.drop('SalePrice_log', axis=1)\ny = preprocessed['SalePrice_log']\nPerform a train-test split\n# Split the data into training and test sets. Use the default split size\nApply your model to the train set\n# Import and initialize the linear regression model class\n# Fit the model to train data\nCalculate predictions on training and test sets\n# Calculate predictions on training and test sets\nCalculate training and test residuals\n# Calculate residuals\nCalculate the Mean Squared Error (MSE)\nA good way to compare overall performance is to compare the mean squarred error for the predicted values on the training and test sets.\n# Import mean_squared_error from sklearn.metrics\n# Calculate training and test MSE\nIf your test error is substantially worse than the train error, this is a sign that the model doesn't generalize well to future cases.\nOne simple way to demonstrate overfitting and underfitting is to alter the size of our train-test split. By default, scikit-learn allocates 25% of the data to the test set and 75% to the training set. Fitting a model on only 10% of the data is apt to lead to underfitting, while training a model on 99% of the data is apt to lead to overfitting.\nEvaluate the effect of train-test split size\nIterate over a range of train-test split sizes from .5 to .95. For each of these, generate a new train/test split sample. Fit a model to the training sample and calculate both the training error and the test error (mse) for each of these splits. Plot these two curves (train error vs. training size and test error vs. training size) on a graph.\n# Your code here\nEvaluate the effect of train-test split size: Extension\nRepeat the previous example, but for each train-test split size, generate 10 iterations of models/errors and save the average train/test error. This will help account for any particularly good/bad models that might have resulted from poor/good splits in the data.\n# Your code here\nWhat's happening here? Evaluate your result!\nSummary\nCongratulations! You now practiced your knowledge of MSE and used your train-test split skills to validate your model.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', '1', 'Jupyter Notebook', 'Updated Mar 6, 2020', '1', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression with CART Trees\nIntroduction\nAs we\'ve learned, a decision tree is a supervised machine learning model that can be used both for classification and regression tasks. We have seen that a decision tree uses a tree structure to predict an output class for a given input example in a classification task. For regression analysis, each path in the tree from the root node to a leaf node represents a decision path that ends in a predicted value. In this lesson, we shall see how regression is performed in using a decision tree regressor using a simple example.\nNote: Kindly visit the official documentation for the regressor tree function used in this lesson.\nObjectives\nYou will be able to:\n\nExplain recursive partitioning\nFit a decision tree regression model with scikit-learn\n\nRecursive partitioning\nLinear regression is considered a global model as there is a single model holding over the entire sample space. For data containing complex features with complicated and nonlinear relations, assembling such a single global model can be a very difficult and computationally expensive task.\nAnother way to handle nonlinear regressions is to partition the sample space into smaller regions, as we have already seen in previous lessons with classification trees. This isn\'t much different in regression. Our goal is partition down to increasingly smaller, simpler subsets until we can fit simple regression models to them. Since each subset is a partition of a smaller subset that is itself a subset, this makes it a textbook example of Recursive Partioning.\nRecall that in classification trees, the leaf nodes (the deepest nodes, or the ones at the end of each particular path) are the ones that contain the purest subsets of the data. Regression trees work a bit differently, but the general idea is still the same. With regression trees, each leaf node of the tree represents a cell of the partition. These cells are the smallest unit where a simple regression can be fit to the data accurately. Splitting the data still works the same way as we saw in previous lessons for classification -- we use our tree model to continuously subset down to smaller, more specific subsets until we reach a level where we can build the simplest regression model to the most specific subset in our data. For example, a regression tree may recursively partition the model down further and further until it gets all customers over the age of 50 residing in Florida with an income over $60k/year, and then fit a simple regression model to only the data points that fit within this specific subset.\nSimple local models\nOne point worth noting is that the simple regression models for each partition aren\'t being used as regressions in real-time. Instead, they take the sample mean of the dependent variable for that partition. Whenever the model makes a prediction, it uses this sample mean rather than calculating the actual regression model. In practice, this works quite well, and has some distinct advantages. Models are easier to interpret, and faster to use for inference (making predictions) since they are just retrieving the stored mean value rather than calculating the actual output of the regression.\nThis is more easily understood when visualized. Consider the regression tree below, which predicts the price of cars based on wheelbase and horsepower:\n\nOnce we have created a decision tree, we can visualize the decision boundaries of that tree (assuming that the dimensionality is small enough for visualization). Notice that all the dividing lines are parallel to the axes because each internal node checks whether a single variable is above or below a given value. In simpler terms, all decision boundaries with decision trees will always be horizontal or vertical if visualized -- there are no diagonal, wavy, or curvy lines, because of the nature of the boolean (true/false) logic used by decision trees to determine the splits!\n\nThe tree correctly represents the interaction between Horsepower and Wheelbase, i.e. when Horsepower > 0.6, Wheelbase no longer matters. When both are equally important, the tree switches between them.\nOnce we train the tree, the local models are completely understood,  so all the effort should go into finding a good partitioning of the data.\nCART training algorithm\nIn this lab, we will focus on the CART algorithm (Classification and Regression Trees) for regression.\n\nThe CART algorithm builds a binary tree in which every non-leaf node has exactly two children (corresponding to a yes/no answer).\n\nGiven a set of training examples and their labels, the algorithm repeatedly splits the training examples $D$ into two subsets $D_{left}, D_{right}$ using some feature set $f$ and feature threshold $t_f$ such that samples with the same label are grouped together.\nAt each node, the algorithm selects the split $\\theta = (f, t_f)$ that produces the smallest mean squared error (MSE) (alternatively, we could use the mean absolute error).\nSo at each step, the algorithm selects the parameters $\\theta$ that minimizes the following cost function:\n\\begin{equation}\nJ(D, \\theta) = \\frac{n_{left}}{n_{total}} MSE_{left} + \\frac{n_{right}}{n_{total}} MSE_{right}\n\\end{equation}\n\n$D$: remaining training examples\n$n_{total}$ : number of remaining training examples\n$\\theta = (f, t_f)$: feature and feature threshold\n$n_{left}/n_{right}$: number of samples in the left/right subset\n$MSE_{left}/MSE_{right}$: MSE of the left/right subset\n\nThis step is repeated recursively until the maximum allowable depth is reached or the current number of samples $n_{total}$ drops below some minimum number. The original equations can be found here.\nAfter building the tree, new examples can be classified by navigating through the tree, testing at each node the corresponding feature until a leaf node/prediction is reached.\nMean Squared Error (MSE)\nWhen performing regression with CART trees (i.e. the target values are continuous) we can evaluate a split using its MSE. The MSE of node $m$ is computed as follows:\n\\begin{equation}\n\\hat{y}m = \\frac{1}{n{m}} \\sum_{i \\in D_m} y_i\n\\end{equation}\n\\begin{equation}\nMSE_m = \\frac{1}{n_{m}} \\sum_{i \\in D_m} (y_i - \\hat{y}_m)^2\n\\end{equation}\n\n$D_m$: training examples in node $m$\n$n_{m}$ : total number of training examples in node $m$\n$y_i$: target value of $i-$th example\n\nLet\'s see the above in action with a simple experiment. We shall generate some non-linear synthetic data for our X and y attributes and fit it to a regression tree. So let\'s move ahead with this. In order to have a visual understanding of how this works, we shall deal with a simple regression problem between two variables X and y, where y is a simple function of X that we want to learn. Let\'s see this below:\nGenarate data\nRun the cell below to generate the data we will be using in this lesson:\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nplt.style.use(\'seaborn\')\nnp.random.seed(124)\n\n# Generate 100 examples of X and y (a simple cubic function of X) \nX = np.linspace(-3, 3, 100)\ny = X ** 3 + np.random.randn(100)\n\n# Plot the data \nplt.figure(figsize=(15,6))\nplt.scatter(X, y)\nplt.title(""Simple quadratic dataset with noise"")\nplt.xlabel(""Feature values"")\nplt.ylabel(""Target values"")\nplt.show()\n\nLet\'s now create our features and labels, and also perform a 75/25 split for the training and test sets:\nX = X.reshape(-1, 1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n\n# Print the data dimensions\nprint(\'Shape X_train:\', X_train.shape)\nprint(\'Shape y_train:\', y_train.shape)\nprint(\'Shape X_test:\', X_test.shape)\nprint(\'Shape y_test:\', y_test.shape)\nShape X_train: (75, 1)\nShape y_train: (75,)\nShape X_test: (25, 1)\nShape y_test: (25,)\n\nFit a Regression tree\nYou can use DecisionTreeRegressor() to fit a decision tree regressor in Scikit-learn. Let\'s create an instance of this class just like the classification tasks and fit it to data. For now, we\'ll set the max depth parameter to 3, as we now know that increasing this could lead to overfitting. We can experiment with different depths later.\nfrom sklearn.tree import DecisionTreeRegressor\nregressor = DecisionTreeRegressor(random_state=42, max_depth=3)\nregressor.fit(X_train, y_train)\nDecisionTreeRegressor(criterion=\'mse\', max_depth=3, max_features=None,\n                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n                      min_impurity_split=None, min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      presort=False, random_state=42, splitter=\'best\')\n\nPrediction and evaluation\nThe output of the cell above shows us the default values for most hyperparameters. You are encouraged to check the official documentation for this class for details on options available to you for growing regression trees!\nWe can now predict labels with previously unseen data and calculate MSE. As an extra measure, we can also look at calculating the R-squared value to inspect the goodness of fit for our model.\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.metrics import r2_score\n\n# Make predictions and evaluate \ny_pred = regressor.predict(X_test)\nprint(\'MSE score:\', mse(y_test, y_pred))\nprint(\'R-sq score:\', r2_score(y_test,y_pred))\nMSE score: 7.651234359344747\nR-sq score: 0.9134119360857194\n\nVisualize the model fit\nOur R-squared score tells us that this appears to be a very good fit (remember $r^2$ ranges from 0 (poor) to 1 (best)). Let\'s visualize the learned function below with our scatter plot from earlier and see how well it fits.\nX_grid = np.arange(min(X), max(X), 0.01)\nX_grid = X_grid.reshape((len(X_grid), 1))\nplt.figure(figsize=(15,6))\nplt.scatter(X, y, color = \'red\', label=\'data\')\nplt.plot(X_grid, regressor.predict(X_grid), color = \'green\', label=\'Regression function\')\nplt.title(\'Decision Tree Regression\')\nplt.xlabel(\'Features\')\nplt.ylabel(\'Target\')\nplt.legend()\nplt.show()\n\nWe found this regression line without using any complex non-linear functions, in a fraction of time. This is the key benefit of regression trees over other regression techniques that we have seen earlier.\nSome observations\n\nThe function is not continuous\nHorizontal lines are averages of all data points in sections created\nThese horizontal lines represent sections. Predictions are averages of data points in these sections. So prediction for all values from the same section will be the same\n\nTry changing the max_depth parameter in the model and grow the tree again. The resulting visualization will clearly show you the impact of tree depth on overfitting.\nCaveats\nWithout regularization, decision trees are likely to overfit the training examples. This can be prevented using techniques like pruning or by providing a maximum allowed tree depth and/or a minimum number of samples required to split a node further as we saw with classification.\nAdditional resources\n\nAn Introduction to Recursive Partitioning: Rationale, Application and Characteristics of Classification and Regression Trees, Bagging and Random Forests\nCART: Classification And Regression Trees for Machine Learning\nPopular Decision Tree: Classification and Regression Trees (C&RT)\nYoutube: CART trees\n\nSummary\nIn this lesson, you learned about CART trees for regression. You looked at how the CART algorithm works, along with MSE as a loss measure which is used as a learning mechanism. You saw a simple experiment with some synthetic data where we used a tree regressor to learn a non-linear function.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Model Fit in Linear Regression - Lab\nIntroduction\nIn this lab, you\'ll learn how to evaluate your model results and you\'ll learn how to select the appropriate features using stepwise selection.\nObjectives\nYou will be able to:\n\nUse stepwise selection methods to determine the most important features for a model\nUse recursive feature elimination to determine the most important features for a model\n\nThe Ames Housing Data once more\nimport pandas as pd\nimport numpy as np\n\names = pd.read_csv(\'ames.csv\')\n\ncontinuous = [\'LotArea\', \'1stFlrSF\', \'GrLivArea\', \'SalePrice\']\ncategoricals = [\'BldgType\', \'KitchenQual\', \'SaleType\', \'MSZoning\', \'Street\', \'Neighborhood\']\n\names_cont = ames[continuous]\n\n# log features\nlog_names = [f\'{column}_log\' for column in ames_cont.columns]\n\names_log = np.log(ames_cont)\names_log.columns = log_names\n\n# normalize (subract mean and divide by std)\n\ndef normalize(feature):\n    return (feature - feature.mean()) / feature.std()\n\names_log_norm = ames_log.apply(normalize)\n\n# one hot encode categoricals\names_ohe = pd.get_dummies(ames[categoricals], prefix=categoricals, drop_first=True)\n\npreprocessed = pd.concat([ames_log_norm, ames_ohe], axis=1)\nPerform stepwise selection\nThe function for stepwise selection is copied below. Use this provided function on your preprocessed Ames Housing data.\nimport statsmodels.api as sm\n\ndef stepwise_selection(X, y, \n                       initial_list=[], \n                       threshold_in=0.01, \n                       threshold_out = 0.05, \n                       verbose=True):\n    """""" \n    Perform a forward-backward feature selection \n    based on p-value from statsmodels.api.OLS\n    Arguments:\n        X - pandas.DataFrame with candidate features\n        y - list-like with the target\n        initial_list - list of features to start with (column names of X)\n        threshold_in - include a feature if its p-value < threshold_in\n        threshold_out - exclude a feature if its p-value > threshold_out\n        verbose - whether to print the sequence of inclusions and exclusions\n    Returns: list of selected features \n    Always set threshold_in < threshold_out to avoid infinite looping.\n    See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n    """"""\n    included = list(initial_list)\n    while True:\n        changed=False\n        # forward step\n        excluded = list(set(X.columns)-set(included))\n        new_pval = pd.Series(index=excluded)\n        for new_column in excluded:\n            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n            new_pval[new_column] = model.pvalues[new_column]\n        best_pval = new_pval.min()\n        if best_pval < threshold_in:\n            best_feature = new_pval.idxmin()\n            included.append(best_feature)\n            changed=True\n            if verbose:\n                print(\'Add  {:30} with p-value {:.6}\'.format(best_feature, best_pval))\n\n        # backward step\n        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n        # use all coefs except intercept\n        pvalues = model.pvalues.iloc[1:]\n        worst_pval = pvalues.max() # null if pvalues is empty\n        if worst_pval > threshold_out:\n            changed=True\n            worst_feature = pvalues.argmax()\n            included.remove(worst_feature)\n            if verbose:\n                print(\'Drop {:30} with p-value {:.6}\'.format(worst_feature, worst_pval))\n        if not changed:\n            break\n    return included\n# Your code here\nBuild the final model again in Statsmodels\n# Your code here\nUse Feature ranking with recursive feature elimination\nUse feature ranking to select the 5 most important features\n# Your code here\nFit the linear regression model again using the 5 selected columns\n# Your code here\nNow, predict $\\hat y$ using your model. You can use .predict() in scikit-learn.\n# Your code here\nNow, using the formulas of R-squared and adjusted R-squared below, and your Python/numpy knowledge, compute them and contrast them with the R-squared and adjusted R-squared in your statsmodels output using stepwise selection. Which of the two models would you prefer?\n$SS_{residual} = \\sum (y - \\hat{y})^2 $\n$SS_{total} = \\sum (y - \\bar{y})^2 $\n$R^2 = 1- \\dfrac{SS_{residual}}{SS_{total}}$\n$R^2_{adj}= 1-(1-R^2)\\dfrac{n-1}{n-p-1}$\n# Your code here\n\n# r_squared is 0.239434  \n# adjusted_r_squared is 0.236818\nLevel up (Optional)\n\nPerform variable selection using forward selection, using this resource: https://planspace.org/20150423-forward_selection_with_statsmodels/. Note that this time features are added based on the adjusted R-squared!\nTweak the code in the stepwise_selection() function written above to just perform forward selection based on the p-value\n\nSummary\nGreat! You practiced your feature selection skills by applying stepwise selection and recursive feature elimination to the Ames Housing dataset!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Regression with CART Trees - Lab\nIntroduction\nIn this lab, we'll make use of what we learned in the previous lesson to build a model for the Petrol Consumption Dataset from Kaggle. This model will be used to predict gasoline consumption for a bunch of examples, based on features about the drivers.\nObjectives\nIn this lab you will:\n\nFit a decision tree regression model with scikit-learn\n\nImport necessary libraries\n#\xa0Import libraries \nimport pandas as pd  \nimport numpy as np  \nfrom sklearn.model_selection import train_test_split \nThe dataset\n\nImport the 'petrol_consumption.csv' dataset\nPrint the first five rows of the data\nPrint the dimensions of the data\n\n# Import the dataset\ndataset = None\n# Print the first five rows\n# Print the dimensions of the data\n\nPrint the summary statistics of all columns in the data:\n\n#\xa0Describe the dataset\nCreate training and test sets\n\nAssign the target column 'Petrol_Consumption' to y\nAssign the remaining independent variables to X\nSplit the data into training and test sets using a 80/20 split\nSet the random state to 42\n\n# Split the data into training and test sets\nX = None\ny = None\nX_train, X_test, y_train, y_test = None\nCreate an instance of CART regressor and fit the data to the model\nAs mentioned earlier, for a regression task we'll use a different sklearn class than we did for the classification task. The class we'll be using here is the DecisionTreeRegressor class, as opposed to the DecisionTreeClassifier from before.\n# Import the DecisionTreeRegressor class \n\n\n# Instantiate and fit a regression tree model to training data \nregressor = None\nMake predictions and calculate the MAE, MSE, and RMSE\nUse the above model to generate predictions on the test set.\nJust as with decision trees for classification, there are several commonly used metrics for evaluating the performance of our model. The most common metrics are:\n\nMean Absolute Error (MAE)\nMean Squared Error (MSE)\nRoot Mean Squared Error (RMSE)\n\nIf these look familiar, it's likely because you have already seen them before -- they are common evaluation metrics for any sort of regression model, and as we can see, regressions performed with decision tree models are no exception!\nSince these are common evaluation metrics, sklearn has functions for each of them that we can use to make our job easier. You'll find these functions inside the metrics module. In the cell below, calculate each of the three evaluation metrics.\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\n# Make predictions on the test set\ny_pred = None\n\n# Evaluate these predictions\nprint('Mean Absolute Error:', None)  \nprint('Mean Squared Error:', None)  \nprint('Root Mean Squared Error:', None)\nLevel Up (Optional)\n\n\nLook at the hyperparameters used in the regression tree, check their value ranges in official doc and try running some optimization by growing a number of trees in a loop\n\n\nUse a dataset that you are familiar with and run tree regression to see if you can interpret the results\n\n\nCheck for outliers, try normalization and see the impact on the output\n\n\nSummary\nIn this lesson, you implemented the architecture to train a tree regressor and predict values for unseen data. You saw that with a vanilla approach, the results were not so great, and thus we must further tune the model (what we described as hyperparameter optimization and pruning, in the case of trees).\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression with CART Trees\nIntroduction\nAs we\'ve learned, a decision tree is a supervised machine learning model that can be used both for classification and regression tasks. We have seen that a decision tree uses a tree structure to predict an output class for a given input example in a classification task. For regression analysis, each path in the tree from the root node to a leaf node represents a decision path that ends in a predicted value. In this lesson, we shall see how regression is performed in using a decision tree regressor using a simple example.\nNote: Kindly visit the official documentation for the regressor tree function used in this lesson.\nObjectives\nYou will be able to:\n\nExplain recursive partitioning\nFit a decision tree regression model with scikit-learn\n\nRecursive partitioning\nLinear regression is considered a global model as there is a single model holding over the entire sample space. For data containing complex features with complicated and nonlinear relations, assembling such a single global model can be a very difficult and computationally expensive task.\nAnother way to handle nonlinear regressions is to partition the sample space into smaller regions, as we have already seen in previous lessons with classification trees. This isn\'t much different in regression. Our goal is partition down to increasingly smaller, simpler subsets until we can fit simple regression models to them. Since each subset is a partition of a smaller subset that is itself a subset, this makes it a textbook example of Recursive Partioning.\nRecall that in classification trees, the leaf nodes (the deepest nodes, or the ones at the end of each particular path) are the ones that contain the purest subsets of the data. Regression trees work a bit differently, but the general idea is still the same. With regression trees, each leaf node of the tree represents a cell of the partition. These cells are the smallest unit where a simple regression can be fit to the data accurately. Splitting the data still works the same way as we saw in previous lessons for classification -- we use our tree model to continuously subset down to smaller, more specific subsets until we reach a level where we can build the simplest regression model to the most specific subset in our data. For example, a regression tree may recursively partition the model down further and further until it gets all customers over the age of 50 residing in Florida with an income over $60k/year, and then fit a simple regression model to only the data points that fit within this specific subset.\nSimple local models\nOne point worth noting is that the simple regression models for each partition aren\'t being used as regressions in real-time. Instead, they take the sample mean of the dependent variable for that partition. Whenever the model makes a prediction, it uses this sample mean rather than calculating the actual regression model. In practice, this works quite well, and has some distinct advantages. Models are easier to interpret, and faster to use for inference (making predictions) since they are just retrieving the stored mean value rather than calculating the actual output of the regression.\nThis is more easily understood when visualized. Consider the regression tree below, which predicts the price of cars based on wheelbase and horsepower:\n\nOnce we have created a decision tree, we can visualize the decision boundaries of that tree (assuming that the dimensionality is small enough for visualization). Notice that all the dividing lines are parallel to the axes because each internal node checks whether a single variable is above or below a given value. In simpler terms, all decision boundaries with decision trees will always be horizontal or vertical if visualized -- there are no diagonal, wavy, or curvy lines, because of the nature of the boolean (true/false) logic used by decision trees to determine the splits!\n\nThe tree correctly represents the interaction between Horsepower and Wheelbase, i.e. when Horsepower > 0.6, Wheelbase no longer matters. When both are equally important, the tree switches between them.\nOnce we train the tree, the local models are completely understood,  so all the effort should go into finding a good partitioning of the data.\nCART training algorithm\nIn this lab, we will focus on the CART algorithm (Classification and Regression Trees) for regression.\n\nThe CART algorithm builds a binary tree in which every non-leaf node has exactly two children (corresponding to a yes/no answer).\n\nGiven a set of training examples and their labels, the algorithm repeatedly splits the training examples $D$ into two subsets $D_{left}, D_{right}$ using some feature set $f$ and feature threshold $t_f$ such that samples with the same label are grouped together.\nAt each node, the algorithm selects the split $\\theta = (f, t_f)$ that produces the smallest mean squared error (MSE) (alternatively, we could use the mean absolute error).\nSo at each step, the algorithm selects the parameters $\\theta$ that minimizes the following cost function:\n\\begin{equation}\nJ(D, \\theta) = \\frac{n_{left}}{n_{total}} MSE_{left} + \\frac{n_{right}}{n_{total}} MSE_{right}\n\\end{equation}\n\n$D$: remaining training examples\n$n_{total}$ : number of remaining training examples\n$\\theta = (f, t_f)$: feature and feature threshold\n$n_{left}/n_{right}$: number of samples in the left/right subset\n$MSE_{left}/MSE_{right}$: MSE of the left/right subset\n\nThis step is repeated recursively until the maximum allowable depth is reached or the current number of samples $n_{total}$ drops below some minimum number. The original equations can be found here.\nAfter building the tree, new examples can be classified by navigating through the tree, testing at each node the corresponding feature until a leaf node/prediction is reached.\nMean Squared Error (MSE)\nWhen performing regression with CART trees (i.e. the target values are continuous) we can evaluate a split using its MSE. The MSE of node $m$ is computed as follows:\n\\begin{equation}\n\\hat{y}m = \\frac{1}{n{m}} \\sum_{i \\in D_m} y_i\n\\end{equation}\n\\begin{equation}\nMSE_m = \\frac{1}{n_{m}} \\sum_{i \\in D_m} (y_i - \\hat{y}_m)^2\n\\end{equation}\n\n$D_m$: training examples in node $m$\n$n_{m}$ : total number of training examples in node $m$\n$y_i$: target value of $i-$th example\n\nLet\'s see the above in action with a simple experiment. We shall generate some non-linear synthetic data for our X and y attributes and fit it to a regression tree. So let\'s move ahead with this. In order to have a visual understanding of how this works, we shall deal with a simple regression problem between two variables X and y, where y is a simple function of X that we want to learn. Let\'s see this below:\nGenarate data\nRun the cell below to generate the data we will be using in this lesson:\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nplt.style.use(\'seaborn\')\nnp.random.seed(124)\n\n# Generate 100 examples of X and y (a simple cubic function of X) \nX = np.linspace(-3, 3, 100)\ny = X ** 3 + np.random.randn(100)\n\n# Plot the data \nplt.figure(figsize=(15,6))\nplt.scatter(X, y)\nplt.title(""Simple quadratic dataset with noise"")\nplt.xlabel(""Feature values"")\nplt.ylabel(""Target values"")\nplt.show()\n\nLet\'s now create our features and labels, and also perform a 75/25 split for the training and test sets:\nX = X.reshape(-1, 1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n\n# Print the data dimensions\nprint(\'Shape X_train:\', X_train.shape)\nprint(\'Shape y_train:\', y_train.shape)\nprint(\'Shape X_test:\', X_test.shape)\nprint(\'Shape y_test:\', y_test.shape)\nShape X_train: (75, 1)\nShape y_train: (75,)\nShape X_test: (25, 1)\nShape y_test: (25,)\n\nFit a Regression tree\nYou can use DecisionTreeRegressor() to fit a decision tree regressor in Scikit-learn. Let\'s create an instance of this class just like the classification tasks and fit it to data. For now, we\'ll set the max depth parameter to 3, as we now know that increasing this could lead to overfitting. We can experiment with different depths later.\nfrom sklearn.tree import DecisionTreeRegressor\nregressor = DecisionTreeRegressor(random_state=42, max_depth=3)\nregressor.fit(X_train, y_train)\nDecisionTreeRegressor(criterion=\'mse\', max_depth=3, max_features=None,\n                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n                      min_impurity_split=None, min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      presort=False, random_state=42, splitter=\'best\')\n\nPrediction and evaluation\nThe output of the cell above shows us the default values for most hyperparameters. You are encouraged to check the official documentation for this class for details on options available to you for growing regression trees!\nWe can now predict labels with previously unseen data and calculate MSE. As an extra measure, we can also look at calculating the R-squared value to inspect the goodness of fit for our model.\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.metrics import r2_score\n\n# Make predictions and evaluate \ny_pred = regressor.predict(X_test)\nprint(\'MSE score:\', mse(y_test, y_pred))\nprint(\'R-sq score:\', r2_score(y_test,y_pred))\nMSE score: 7.651234359344747\nR-sq score: 0.9134119360857194\n\nVisualize the model fit\nOur R-squared score tells us that this appears to be a very good fit (remember $r^2$ ranges from 0 (poor) to 1 (best)). Let\'s visualize the learned function below with our scatter plot from earlier and see how well it fits.\nX_grid = np.arange(min(X), max(X), 0.01)\nX_grid = X_grid.reshape((len(X_grid), 1))\nplt.figure(figsize=(15,6))\nplt.scatter(X, y, color = \'red\', label=\'data\')\nplt.plot(X_grid, regressor.predict(X_grid), color = \'green\', label=\'Regression function\')\nplt.title(\'Decision Tree Regression\')\nplt.xlabel(\'Features\')\nplt.ylabel(\'Target\')\nplt.legend()\nplt.show()\n\nWe found this regression line without using any complex non-linear functions, in a fraction of time. This is the key benefit of regression trees over other regression techniques that we have seen earlier.\nSome observations\n\nThe function is not continuous\nHorizontal lines are averages of all data points in sections created\nThese horizontal lines represent sections. Predictions are averages of data points in these sections. So prediction for all values from the same section will be the same\n\nTry changing the max_depth parameter in the model and grow the tree again. The resulting visualization will clearly show you the impact of tree depth on overfitting.\nCaveats\nWithout regularization, decision trees are likely to overfit the training examples. This can be prevented using techniques like pruning or by providing a maximum allowed tree depth and/or a minimum number of samples required to split a node further as we saw with classification.\nAdditional resources\n\nAn Introduction to Recursive Partitioning: Rationale, Application and Characteristics of Classification and Regression Trees, Bagging and Random Forests\nCART: Classification And Regression Trees for Machine Learning\nPopular Decision Tree: Classification and Regression Trees (C&RT)\nYoutube: CART trees\n\nSummary\nIn this lesson, you learned about CART trees for regression. You looked at how the CART algorithm works, along with MSE as a loss measure which is used as a learning mechanism. You saw a simple experiment with some synthetic data where we used a tree regressor to learn a non-linear function.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Logistic Regression Model Comparisons - Lab\nIntroduction\nIn this lab, you\'ll further investigate how to tune your own logistic regression implementation, as well as that of scikit-learn in order to produce better models.\nObjectives\n\nCompare the different inputs with logistic regression models and determine the optimal model\n\nIn the previous lab, you were able to compare the output of your own implementation of the logistic regression model with that of scikit-learn. However, that model did not include an intercept or any regularization. In this investigative lab, you will analyze the impact of these two tuning parameters.\nImport the data\nAs with the previous lab, import the dataset stored in \'heart.csv\':\n# Import the data\n\ndf = None\n\n# Print the first five rows of the data\nSplit the data\nDefine X and y as with the previous lab. This time, follow best practices and also implement a standard train-test split. Assign 25% to the test set and set the random_state to 17.\n# Define X and y\ny = None\nX = None\n\n# Split the data into training and test sets\n\n\nX_train, X_test, y_train, y_test = None\nprint(y_train.value_counts(),\'\\n\\n\', y_test.value_counts())\nInitial Model - Personal Implementation\nUse your code from the previous lab to once again train a logistic regression algorithm on the training set.\n# Your code from previous lab\nimport numpy as np\n\ndef sigmoid(x):\n    x = np.array(x)\n    return 1/(1 + np.e**(-1*x))\n\ndef grad_desc(X, y, max_iterations, alpha, initial_weights=None):\n    """"""Be sure to set default behavior for the initial_weights parameter.""""""\n    if initial_weights is None:\n        initial_weights = np.ones((X.shape[1], 1)).flatten()\n    weights_col = pd.DataFrame(initial_weights)\n    weights = initial_weights\n    # Create a for loop of iterations\n    for iteration in range(max_iterations):\n        # Generate predictions using the current feature weights\n        predictions = sigmoid(np.dot(X, weights))\n        # Calculate an error vector based on these initial predictions and the correct labels\n        error_vector = y - predictions\n        # Calculate the gradient \n        # As we saw in the previous lab, calculating the gradient is often the most difficult task.\n        # Here, your are provided with the closed form solution for the gradient of the log-loss function derived from MLE\n        # For more details on the derivation, see the additional resources section below.\n        gradient = np.dot(X.transpose(), error_vector)\n        # Update the weight vector take a step of alpha in direction of gradient \n        weights += alpha * gradient\n        weights_col = pd.concat([weights_col, pd.DataFrame(weights)], axis=1)\n    # Return finalized weights\n    return weights, weights_col\n\nweights, weights_col = grad_desc(X_train, y_train, 50000, 0.001)\nMake [probability] predictions on the test set\n# Predict on test set\ny_hat_test = None\nnp.round(y_hat_test, 2)\nCreate an ROC curve for your predictions\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_hat_test)\n\nprint(\'AUC: {}\'.format(auc(test_fpr, test_tpr)))\n\n# Seaborn\'s beautiful styling\nsns.set_style(\'darkgrid\', {\'axes.facecolor\': \'0.9\'})\n\nplt.figure(figsize=(10, 8))\nlw = 2\n\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Test ROC curve\')\n\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=\'lower right\')\nplt.show()\nUpdate your ROC curve to include the training set\ny_hat_train = None\n\ntrain_fpr, train_tpr, train_thresholds = None\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_hat_test)\n\n# Train AUC\nprint(\'Train AUC: {}\'.format( None ))\nprint(\'AUC: {}\'.format(auc(test_fpr, test_tpr)))\n\n# Seaborn\'s beautiful styling\nsns.set_style(\'darkgrid\', {\'axes.facecolor\': \'0.9\'})\n\nplt.figure(figsize=(10, 8))\nlw = 2\n\nplt.plot(train_fpr, train_tpr, color=\'blue\',\n         lw=lw, label=\'Train ROC curve\')\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Test ROC curve\')\n\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=\'lower right\')\nplt.show()\nCreate a confusion matrix for your predictions\nUse a standard decision boundary of 0.5 to convert your probabilities output by logistic regression into binary classifications. (Again this should be for the test set.) Afterward, feel free to use the built-in scikit-learn function to compute the confusion matrix as we discussed in previous sections.\n# Your code here\nInitial Model - scikit-learn\nUse scikit-learn to build a similar model. To start, create an identical model as you did in the last section; turn off the intercept and set the regularization parameter, C, to a ridiculously large number such as 1e16.\n# Your code here\nCreate an ROC Curve for the scikit-learn model\nUse both the training and test sets\n# Your code here\n\ny_train_score = None\ny_test_score = None\n\ntrain_fpr, train_tpr, train_thresholds = None\ntest_fpr, test_tpr, test_thresholds = None\n\n\nprint(\'Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\nprint(\'Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\n\nplt.figure(figsize=(10, 8))\nlw = 2\n\nplt.plot(train_fpr, train_tpr, color=\'blue\',\n         lw=lw, label=\'Train ROC curve\')\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Test ROC curve\')\n\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=\'lower right\')\nplt.show()\nAdd an Intercept\nNow add an intercept to the scikit-learn model. Keep the regularization parameter C set to a very large number such as 1e16.\n# Create new model\nlogregi = None\nPlot all three models ROC curves on the same graph.\n# Initial model plots\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_hat_test)\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_hat_train)\n\n\nprint(\'Custom Model Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\nprint(\'Custome Model Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\n\nplt.figure(figsize=(10,8))\nlw = 2\n\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Custom Model Test ROC curve\')\nplt.plot(train_fpr, train_tpr, color=\'blue\',\n         lw=lw, label=\'Custom Model Train ROC curve\')\n\n\n# Second model plots\ny_test_score = logreg.decision_function(X_test)\ny_train_score = logreg.decision_function(X_train)\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_test_score)\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_train_score)\n\nprint(\'Scikit-learn Model 1 Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\nprint(\'Scikit-learn Model 1 Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\n\n\nplt.plot(test_fpr, test_tpr, color=\'yellow\',\n         lw=lw, label=\'Scikit learn Model 1 Test ROC curve\')\nplt.plot(train_fpr, train_tpr, color=\'gold\',\n         lw=lw, label=\'Scikit learn Model 1 Train ROC curve\')\n\n\n# Third model plots\ny_test_score = None\ny_train_score = None\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_test_score)\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_train_score)\n\nprint(\'Scikit-learn Model 2 with intercept Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\nprint(\'Scikit-learn Model 2 with intercept Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\n\n\nplt.plot(test_fpr, test_tpr, color=\'purple\',\n         lw=lw, label=\'Scikit learn Model 2 with intercept Test ROC curve\')\nplt.plot(train_fpr, train_tpr, color=\'red\',\n         lw=lw, label=\'Scikit learn Model 2 with intercept Train ROC curve\')\n\n# Formatting\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=""lower right"")\nplt.show()\nAltering the Regularization Parameter\nNow, experiment with altering the regularization parameter. At a minimum, create 5 different subplots with varying regularization (C) parameters. For each, plot the ROC curve of the training and test set for that specific model.\nRegularization parameters between 1 and 20 are recommended. Observe the difference in test and training AUC as you go along.\n# Your code here\nHow did the regularization parameter impact the ROC curves plotted above?\nSummary\nIn this lab, you reviewed many of the accuracy measures for classification algorithms and observed the impact of additional tuning models using intercepts and regularization.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Fitting a Logistic Regression Model - Lab\nIntroduction\nIn the last lesson you were given a broad overview of logistic regression. This included an introduction to two separate packages for creating logistic regression models. In this lab, you\'ll be investigating fitting logistic regressions with statsmodels. For your first foray into logistic regression, you are going to attempt to build a model that classifies whether an individual survived the Titanic shipwreck or not (yes, it\'s a bit morbid).\nObjectives\nIn this lab you will:\n\nImplement logistic regression with statsmodels\nInterpret the statistical results associated with model parameters\n\nImport the data\nImport the data stored in the file \'titanic.csv\' and print the first five rows of the DataFrame to check its contents.\n# Import the data\n\n\ndf = None\nDefine independent and target variables\nYour target variable is in the column \'Survived\'. A 0 indicates that the passenger didn\'t survive the shipwreck. Print the total number of people who didn\'t survive the shipwreck. How many people survived?\n# Total number of people who survived/didn\'t survive\nOnly consider the columns specified in relevant_columns when building your model. The next step is to create dummy variables from categorical variables. Remember to drop the first level for each categorical column and make sure all the values are of type float:\n# Create dummy variables\nrelevant_columns = [\'Pclass\', \'Age\', \'SibSp\', \'Fare\', \'Sex\', \'Embarked\', \'Survived\']\ndummy_dataframe = None\n\ndummy_dataframe.shape\nDid you notice above that the DataFrame contains missing values? To keep things simple, simply delete all rows with missing values.\n\nNOTE: You can use the .dropna() method to do this.\n\n# Drop missing rows\ndummy_dataframe = None\ndummy_dataframe.shape\nFinally, assign the independent variables to X and the target variable to y:\n# Split the data into X and y\ny = None\nX = None\nFit the model\nNow with everything in place, you can build a logistic regression model using statsmodels (make sure you create an intercept term as we showed in the previous lesson).\n\nWarning: Did you receive an error of the form ""LinAlgError: Singular matrix""? This means that statsmodels was unable to fit the model due to certain linear algebra computational problems. Specifically, the matrix was not invertible due to not being full rank. In other words, there was a lot of redundant, superfluous data. Try removing some features from the model and running it again.\n\n# Build a logistic regression model using statsmodels\nAnalyze results\nGenerate the summary table for your model. Then, comment on the p-values associated with the various features you chose.\n# Summary table\n# Your comments here\nLevel up (Optional)\nCreate a new model, this time only using those features you determined were influential based on your analysis of the results above. How does this model perform?\n# Your code here\n# Your comments here\nSummary\nWell done! In this lab, you practiced using statsmodels to build a logistic regression model. You then interpreted the results, building upon your previous stats knowledge, similar to linear regression. Continue on to take a look at building logistic regression models in Scikit-learn!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Regression Model Validation - Lab\nIntroduction\nIn this lab, you'll be able to validate your Ames Housing data model using train-test split.\nObjectives\nYou will be able to:\n\nCompare training and testing errors to determine if model is over or underfitting\n\nLet's use our Ames Housing Data again!\nWe included the code to preprocess below.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\names = pd.read_csv('ames.csv')\n\n# using 9 predictive categorical or continuous features, plus the target SalePrice\ncontinuous = ['LotArea', '1stFlrSF', 'GrLivArea', 'SalePrice']\ncategoricals = ['BldgType', 'KitchenQual', 'SaleType', 'MSZoning', 'Street', 'Neighborhood']\n\names_cont = ames[continuous]\n\n# log features\nlog_names = [f'{column}_log' for column in ames_cont.columns]\n\names_log = np.log(ames_cont)\names_log.columns = log_names\n\n# normalize (subract mean and divide by std)\n\ndef normalize(feature):\n    return (feature - feature.mean()) / feature.std()\n\names_log_norm = ames_log.apply(normalize)\n\n# one hot encode categoricals\names_ohe = pd.get_dummies(ames[categoricals], prefix=categoricals, drop_first=True)\n\npreprocessed = pd.concat([ames_log_norm, ames_ohe], axis=1)\nX = preprocessed.drop('SalePrice_log', axis=1)\ny = preprocessed['SalePrice_log']\nPerform a train-test split\n# Split the data into training and test sets. Use the default split size\nApply your model to the train set\n# Import and initialize the linear regression model class\n# Fit the model to train data\nCalculate predictions on training and test sets\n# Calculate predictions on training and test sets\nCalculate training and test residuals\n# Calculate residuals\nCalculate the Mean Squared Error (MSE)\nA good way to compare overall performance is to compare the mean squarred error for the predicted values on the training and test sets.\n# Import mean_squared_error from sklearn.metrics\n# Calculate training and test MSE\nIf your test error is substantially worse than the train error, this is a sign that the model doesn't generalize well to future cases.\nOne simple way to demonstrate overfitting and underfitting is to alter the size of our train-test split. By default, scikit-learn allocates 25% of the data to the test set and 75% to the training set. Fitting a model on only 10% of the data is apt to lead to underfitting, while training a model on 99% of the data is apt to lead to overfitting.\nEvaluate the effect of train-test split size\nIterate over a range of train-test split sizes from .5 to .95. For each of these, generate a new train/test split sample. Fit a model to the training sample and calculate both the training error and the test error (mse) for each of these splits. Plot these two curves (train error vs. training size and test error vs. training size) on a graph.\n# Your code here\nEvaluate the effect of train-test split size: Extension\nRepeat the previous example, but for each train-test split size, generate 10 iterations of models/errors and save the average train/test error. This will help account for any particularly good/bad models that might have resulted from poor/good splits in the data.\n# Your code here\nWhat's happening here? Evaluate your result!\nSummary\nCongratulations! You now practiced your knowledge of MSE and used your train-test split skills to validate your model.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Trees and Model Optimization - Lab\nIntroduction\nIn this lab, we\'ll see how to apply regression analysis using CART trees while making use of some hyperparameter tuning to improve our model.\nObjectives\nIn this lab you will:\n\nPerform the full process of cleaning data, tuning hyperparameters, creating visualizations, and evaluating decision tree models\nDetermine the optimal hyperparameters for a decision tree model and evaluate the performance of decision tree models\n\nAmes Housing dataset\nThe dataset is available in the file \'ames.csv\'.\n\nImport the dataset and examine its dimensions:\n\n# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use(\'ggplot\')\n%matplotlib inline\n\n# Load the Ames housing dataset \ndata = None\n\n# Print the dimensions of data\n\n\n# Check out the info for the dataframe\n\n\n# Show the first 5 rows\nIdentify features and target data\nIn this lab, we will use using 3 predictive continuous features:\nFeatures\n\nLotArea: Lot size in square feet\n1stFlrSF: Size of first floor in square feet\nGrLivArea: Above grade (ground) living area square feet\n\nTarget\n\n\nSalePrice\', the sale price of the home, in dollars\n\n\nCreate DataFrames for the features and the target variable as shown above\n\n\nInspect the contents of both the features and the target variable\n\n\n# Features and target data\ntarget = None\nfeatures = None\nInspect correlations\n\nUse scatter plots to show the correlation between the chosen features and the target variable\nComment on each scatter plot\n\n#\xa0Your code here \nCreate evaluation metrics\n\nImport r2_score and mean_squared_error from sklearn.metrics\nCreate a function performance(true, predicted) to calculate and return both the R-squared score and Root Mean Squared Error (RMSE) for two equal-sized arrays for the given true and predicted values\n\nDepending on your version of sklearn, in order to get the RMSE score you will need to either set squared=False or you will need to take the square root of the output of the mean_squared_error function - check out the documentation or this helpful and related StackOverflow post\nThe benefit of calculating RMSE instead of the Mean Squared Error (MSE) is that RMSE is in the same units at the target - here, this means that RMSE will be in dollars, calculating how far off in dollars our predictions are away from the actual prices for homes, on average\n\n\n\n#\xa0Import metrics\n\n\n# Define the function\ndef performance(y_true, y_predict):\n    """""" \n    Calculates and returns the two performance scores between \n    true and predicted values - first R-Squared, then RMSE\n    """"""\n\n    # Calculate the r2 score between \'y_true\' and \'y_predict\'\n\n    # Calculate the root mean squared error between \'y_true\' and \'y_predict\'\n\n    # Return the score\n\n    pass\n\n\n# Test the function\nscore = performance([3, -0.5, 2, 7, 4.2], [2.5, 0.0, 2.1, 7.8, 5.3])\nscore\n\n# [0.9228556485355649, 0.6870225614927066]\nSplit the data into training and test sets\n\nSplit features and target datasets into training/test data (80/20)\nFor reproducibility, use random_state=42\n\nfrom sklearn.model_selection import train_test_split \n\n# Split the data into training and test subsets\nx_train, x_test, y_train, y_test = None\nGrow a vanilla regression tree\n\nImport the DecisionTreeRegressor class\nRun a baseline model for later comparison using the datasets created above\nGenerate predictions for test dataset and calculate the performance measures using the function created above\nUse random_state=45 for tree instance\nRecord your observations\n\n#\xa0Import DecisionTreeRegressor\n\n\n# Instantiate DecisionTreeRegressor \n# Set random_state=45\nregressor = None\n\n# Fit the model to training data\n\n\n# Make predictions on the test data\ny_pred = None\n\n# Calculate performance using the performance() function \nscore = None\nscore\n\n# [0.5961521990414137, 55656.48543887347] - R2, RMSE\nHyperparameter tuning (I)\n\nFind the best tree depth using depth range: 1-30\nRun the regressor repeatedly in a for loop for each depth value\nUse random_state=45 for reproducibility\nCalculate RMSE and r-squared for each run\nPlot both performance measures for all runs\nComment on the output\n\n#\xa0Your code here \nHyperparameter tuning (II)\n\nRepeat the above process for min_samples_split\nUse a range of values from 2-10 for this hyperparameter\nUse random_state=45 for reproducibility\nVisualize the output and comment on results as above\n\n#\xa0Your code here \nRun the optimized model\n\nUse the best values for max_depth and min_samples_split found in previous runs and run an optimized model with these values\nCalculate the performance and comment on the output\n\n#\xa0Your code here \nLevel up (Optional)\n\nHow about bringing in some more features from the original dataset which may be good predictors?\nAlso, try tuning more hyperparameters like max_features to find a more optimal version of the model\n\n#\xa0Your code here \nSummary\nIn this lab, we looked at applying a decision-tree-based regression analysis on the Ames Housing dataset. We saw how to train various models to find the optimal values for hyperparameters.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Regression Trees and Model Optimization - Lab\nIntroduction\nIn this lab, we\'ll see how to apply regression analysis using CART trees while making use of some hyperparameter tuning to improve our model.\nObjectives\nIn this lab you will:\n\nPerform the full process of cleaning data, tuning hyperparameters, creating visualizations, and evaluating decision tree models\nDetermine the optimal hyperparameters for a decision tree model and evaluate the performance of decision tree models\n\nAmes Housing dataset\nThe dataset is available in the file \'ames.csv\'.\n\nImport the dataset and examine its dimensions:\n\n# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use(\'ggplot\')\n%matplotlib inline\n\n# Load the Ames housing dataset \ndata = None\n\n# Print the dimensions of data\n\n\n# Check out the info for the dataframe\n\n\n# Show the first 5 rows\nIdentify features and target data\nIn this lab, we will use using 3 predictive continuous features:\nFeatures\n\nLotArea: Lot size in square feet\n1stFlrSF: Size of first floor in square feet\nGrLivArea: Above grade (ground) living area square feet\n\nTarget\n\n\nSalePrice\', the sale price of the home, in dollars\n\n\nCreate DataFrames for the features and the target variable as shown above\n\n\nInspect the contents of both the features and the target variable\n\n\n# Features and target data\ntarget = None\nfeatures = None\nInspect correlations\n\nUse scatter plots to show the correlation between the chosen features and the target variable\nComment on each scatter plot\n\n#\xa0Your code here \nCreate evaluation metrics\n\nImport r2_score and mean_squared_error from sklearn.metrics\nCreate a function performance(true, predicted) to calculate and return both the R-squared score and Root Mean Squared Error (RMSE) for two equal-sized arrays for the given true and predicted values\n\nDepending on your version of sklearn, in order to get the RMSE score you will need to either set squared=False or you will need to take the square root of the output of the mean_squared_error function - check out the documentation or this helpful and related StackOverflow post\nThe benefit of calculating RMSE instead of the Mean Squared Error (MSE) is that RMSE is in the same units at the target - here, this means that RMSE will be in dollars, calculating how far off in dollars our predictions are away from the actual prices for homes, on average\n\n\n\n#\xa0Import metrics\n\n\n# Define the function\ndef performance(y_true, y_predict):\n    """""" \n    Calculates and returns the two performance scores between \n    true and predicted values - first R-Squared, then RMSE\n    """"""\n\n    # Calculate the r2 score between \'y_true\' and \'y_predict\'\n\n    # Calculate the root mean squared error between \'y_true\' and \'y_predict\'\n\n    # Return the score\n\n    pass\n\n\n# Test the function\nscore = performance([3, -0.5, 2, 7, 4.2], [2.5, 0.0, 2.1, 7.8, 5.3])\nscore\n\n# [0.9228556485355649, 0.6870225614927066]\nSplit the data into training and test sets\n\nSplit features and target datasets into training/test data (80/20)\nFor reproducibility, use random_state=42\n\nfrom sklearn.model_selection import train_test_split \n\n# Split the data into training and test subsets\nx_train, x_test, y_train, y_test = None\nGrow a vanilla regression tree\n\nImport the DecisionTreeRegressor class\nRun a baseline model for later comparison using the datasets created above\nGenerate predictions for test dataset and calculate the performance measures using the function created above\nUse random_state=45 for tree instance\nRecord your observations\n\n#\xa0Import DecisionTreeRegressor\n\n\n# Instantiate DecisionTreeRegressor \n# Set random_state=45\nregressor = None\n\n# Fit the model to training data\n\n\n# Make predictions on the test data\ny_pred = None\n\n# Calculate performance using the performance() function \nscore = None\nscore\n\n# [0.5961521990414137, 55656.48543887347] - R2, RMSE\nHyperparameter tuning (I)\n\nFind the best tree depth using depth range: 1-30\nRun the regressor repeatedly in a for loop for each depth value\nUse random_state=45 for reproducibility\nCalculate RMSE and r-squared for each run\nPlot both performance measures for all runs\nComment on the output\n\n#\xa0Your code here \nHyperparameter tuning (II)\n\nRepeat the above process for min_samples_split\nUse a range of values from 2-10 for this hyperparameter\nUse random_state=45 for reproducibility\nVisualize the output and comment on results as above\n\n#\xa0Your code here \nRun the optimized model\n\nUse the best values for max_depth and min_samples_split found in previous runs and run an optimized model with these values\nCalculate the performance and comment on the output\n\n#\xa0Your code here \nLevel up (Optional)\n\nHow about bringing in some more features from the original dataset which may be good predictors?\nAlso, try tuning more hyperparameters like max_features to find a more optimal version of the model\n\n#\xa0Your code here \nSummary\nIn this lab, we looked at applying a decision-tree-based regression analysis on the Ames Housing dataset. We saw how to train various models to find the optimal values for hyperparameters.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating Logistic Regression Models - Lab\nIntroduction\nIn regression, you are predicting continous values so it makes sense to discuss error as a distance of how far off our estimates were. When classifying a binary variable, however, a model is either correct or incorrect. As a result, we tend to quantify this in terms of how many false positives versus false negatives we come across. In particular, we examine a few different specific measurements when evaluating the performance of a classification algorithm. In this lab, you\'ll review precision, recall, accuracy, and F1 score in order to evaluate our logistic regression models.\nObjectives\nIn this lab you will:\n\nImplement evaluation metrics from scratch using Python\n\nTerminology review\nLet\'s take a moment and review some classification evaluation metrics:\n$$ \\text{Precision} = \\frac{\\text{Number of True Positives}}{\\text{Number of Predicted Positives}} $$\n$$ \\text{Recall} = \\frac{\\text{Number of True Positives}}{\\text{Number of Actual Total Positives}} $$\n$$ \\text{Accuracy} = \\frac{\\text{Number of True Positives + True Negatives}}{\\text{Total Observations}} $$\n$$ \\text{F1 score} = 2 * \\frac{\\text{Precision * Recall}}{\\text{Precision + Recall}} $$\nAt times, it may be best to tune a classification algorithm to optimize against precision or recall rather than overall accuracy. For example, imagine the scenario of predicting whether or not a patient is at risk for cancer and should be brought in for additional testing. In cases such as this, we often may want to cast a slightly wider net, and it is preferable to optimize for recall, the number of cancer positive cases, than it is to optimize precision, the percentage of our predicted cancer-risk patients who are indeed positive.\nSplit the data into training and test sets\nimport pandas as pd\ndf = pd.read_csv(\'heart.csv\')\ndf.head()\nSplit the data first into X and y, and then into training and test sets. Assign 25% to the test set and set the random_state to 0.\n# Import train_test_split\n\n\n# Split data into X and y\ny = None\nX = None\n\n# Split the data into a training and a test set\nX_train, X_test, y_train, y_test = None\nBuild a vanilla logistic regression model\n\nImport and instantiate LogisticRegression\nMake sure you do not use an intercept term and use the \'liblinear\' solver\nFit the model to training data\n\n# Import LogisticRegression\n\n\n# Instantiate LogisticRegression\nlogreg = None\n\n# Fit to training data\nmodel_log = None\nmodel_log\nWrite a function to calculate the precision\ndef precision(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the recall\ndef recall(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the accuracy\ndef accuracy(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the F1 score\ndef f1_score(y, y_hat):\n    # Your code here\n    pass\nCalculate the precision, recall, accuracy, and F1 score of your classifier\nDo this for both the training and test sets.\n# Your code here\ny_hat_train = None\ny_hat_test = None\nGreat job! Now it\'s time to check your work with sklearn.\nCalculate metrics with sklearn\nEach of the metrics we calculated above is also available inside the sklearn.metrics module.\nIn the cell below, import the following functions:\n\nprecision_score\nrecall_score\naccuracy_score\nf1_score\n\nCompare the results of your performance metrics functions above with the sklearn functions. Calculate these values for both your train and test set.\n# Your code here\nNicely done! Did the results from sklearn match that of your own?\nCompare precision, recall, accuracy, and F1 score for train vs test sets\nCalculate and then plot the precision, recall, accuracy, and F1 score for the test and training splits using different training set sizes. What do you notice?\nimport matplotlib.pyplot as plt\n%matplotlib inline\ntraining_precision = []\ntesting_precision = []\ntraining_recall = []\ntesting_recall = []\ntraining_accuracy = []\ntesting_accuracy = []\ntraining_f1 = []\ntesting_f1 = []\n\nfor i in range(10, 95):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= None) # replace the ""None"" here\n    logreg = LogisticRegression(fit_intercept=False, C=1e20, solver=\'liblinear\')\n    model_log = None\n    y_hat_test = None\n    y_hat_train = None \n    \n    # Your code here\nCreate four scatter plots looking at the train and test precision in the first one, train and test recall in the second one, train and test accuracy in the third one, and train and test F1 score in the fourth one.\nWe already created the scatter plot for precision:\n# Train and test precision\nplt.scatter(list(range(10, 95)), training_precision, label=\'training_precision\')\nplt.scatter(list(range(10, 95)), testing_precision, label=\'testing_precision\')\nplt.legend()\nplt.show()\n# Train and test recall\n# Train and test accuracy\n# Train and test F1 score\nSummary\nNice! In this lab, you calculated evaluation metrics for classification algorithms from scratch in Python. Going forward, continue to think about scenarios in which you might prefer to optimize one of these metrics over another.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Aug 18, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Coding Logistic Regression From Scratch - Lab\nIntroduction\nIn this lab, you\'ll practice your ability to translate mathematical algorithms into Python functions. This will deepen and solidify your understanding of logistic regression!\nObjectives\nIn this lab you will:\n\nBuild a logistic regression model from scratch using gradient descent\n\nOverview\nRecall that the logistic regression algorithm builds upon the intuition from linear regression. In logistic regression, you start by taking the input data, X, and multiplying it by a vector of weights for each of the individual features, which produces an output, y. Afterward, you\'ll work on using an iterative approach via gradient descent to tune these weights.\nLinear regression setup\nWrite a simple function predict_y() that takes in a matrix X of observations and a vector of feature weights w and outputs a vector of predictions for the various observations.\nRecall that this is the sum of the product of each of the feature observations and their corresponding feature weights:\n$\\large \\hat{y}i = X{i1} \\cdot w_1 + X_{i2} \\cdot w_2 + X_{i3} \\cdot w_3 + ... + X_{in} \\cdot w_n$\n\nHint: Think about which mathematical operation you\'ve seen previously that will take a matrix (X) and multiply it by a vector of weights (w). Use NumPy!\n\n# Your code here\nimport numpy as np\n\ndef predict_y(X, w): \n    pass\nThe sigmoid function\nRecall that the sigmoid function is used to map the linear regression model output to a range of 0 to 1, satisfying basic premises of probability. As a reminder, the sigmoid function is defined by:\n$S(x) = \\dfrac{1}{1+e^(-x)}$\nWrite this as a Python function where x is the input and the function outputs the result of the sigmoid function.\n\nHint: Use NumPy!\n\n# Your code here\nPlot the sigmoid\nFor good measure, let\'s do a brief investigation of your new function. Plot the output of your sigmoid() function using 10,000 values evenly spaced from -20 to 20.\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Plot sigmoid\nGradient descent with the sigmoid function\nRecall that gradient descent is a numerical method for finding a minimum to a cost function. In the case of logistic regression, you are looking to minimize the error between the model\'s predictions and the actual data labels. To do this, you first calculate an error vector based on the current model\'s feature weights. You then multiply the transpose of the training matrix itself by this error vector in order to obtain the gradient. Finally, you take the gradient, multiply it by the step size and add this to our current weight vector to update it. Below, write such a function. It will take 5 inputs:\n\nX\ny\nmax_iterations\nalpha (the step size)\ninitial_weights\n\nBy default, have your function set the initial_weights parameter to a vector where all feature weights are set to 1.\n# Your code here\ndef grad_desc(X, y, max_iterations, alpha, initial_weights=None):\n    """"""Be sure to set default behavior for the initial_weights parameter.""""""\n    # Create a for loop of iterations\n        # Generate predictions using the current feature weights\n        # Calculate an error vector based on these initial predictions and the correct labels\n        # Calculate the gradient \n        # As we saw in the previous lab, calculating the gradient is often the most difficult task.\n        # Here, your are provided with the closed form solution for the gradient of the log-loss function derived from MLE\n        # For more details on the derivation, see the additional resources section below.\n        gradient = np.dot(X.transpose(), error_vector) \n        # Update the weight vector take a step of alpha in direction of gradient \n    # Return finalized weights\n    \nRunning your algorithm\nNow that you\'ve coded everything from the ground up, you can further investigate the convergence behavior of the gradient descent algorithm. Remember that gradient descent does not guarantee a global minimum, only a local minimum, and that small deviations in the starting point or step size can lead to different outputs.\nFirst, run the following cell to import the data and create the predictor and target variables:\n# Import data\nimport pandas as pd\ndf = pd.read_csv(\'heart.csv\')\n\n# Create the predictor and target variables\ny = df[\'target\']\nX = df.drop(columns=[\'target\'], axis=1)\n\nprint(y.value_counts())\nX.head()\nRun your algorithm and plot the successive weights of the features through iterations. Below is a dataset, with X and y predefined for you. Use your logistic regression function to train a model. As the model trains, record the iteration cycle of the gradient descent algorithm and the weights of the various features. Then, plot this data on subplots for each of the individual features. Each graph should have the iteration number on the x-axis and the value of that feature weight for that iteration cycle on the y-axis. This will visually display how the algorithm is adjusting the weights over successive iterations, and hopefully show convergence to stable weights.\n# Your code here\nScikit-learn\nFor comparison, import scikit-learn\'s standard LogisticRegression() function. Initialize it with no intercept and C=1e16 or another very high number. The reason is as follows: our implementation has not used an intercept, and you have not performed any regularization such as Lasso or Ridge (scikit-learn uses l2 by default). The high value of C will essentially negate this. Also, set the random_state to 2 and use the \'liblinear\' solver.\nAfter initializing a regression object, fit it to X and y.\n# Your code here\nCompare the models\nCompare the coefficient weights of your model to that generated by scikit-learn.\n# Your code here\nLevel up (Optional)\nUpdate the gradient descent algorithm to also return the cost after each iteration. Then rerun the algorithm and create a graph displaying the cost versus the iteration number.\n# Your code here\nAdditional Resources\nIf you want to see more of the mathematics behind the gradient derivation above, check out section 4.4.1 from the Elements of Statistical Learning which can be found here: https://web.stanford.edu/~hastie/ElemStatLearn//.\nSummary\nCongratulations! You just coded logistic regression from the ground up using NumPy! With this, you should have a fairly deep understanding of logistic regression and how the algorithm works! In the upcoming labs, you\'ll continue to explore this from a few more angles, plotting your data along with the decision boundary for our predictions.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '58 contributions\n        in the last year', 'description': ['House-price-prediction-using-extreme-gradient-boost-regression\n'], 'url_profile': 'https://github.com/aryamonani', 'info_list': ['Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Logistic Regression in scikit-learn\nIntroduction\nGenerally, the process for fitting a logistic regression model using scikit-learn is very similar to that which you previously saw for statsmodels. One important exception is that scikit-learn will not display statistical measures such as the p-values associated with the various features. This is a shortcoming of scikit-learn, although scikit-learn has other useful tools for tuning models which we will investigate in future lessons.\nThe other main process of model building and evaluation which we didn't to discuss previously is performing a train-test split. As we saw in linear regression, model validation is an essential part of model building as it helps determine how our model will generalize to future unseen cases. After all, the point of any model is to provide future predictions where we don't already know the answer but have other informative data (X).\nWith that, let's take a look at implementing logistic regression in scikit-learn using dummy variables and a proper train-test split.\nObjectives\nYou will be able to:\n\nFit a logistic regression model using scikit-learn\n\nImport the data\nimport pandas as pd\n\ndf = pd.read_csv('titanic.csv')\ndf.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\nDefine X and y\nNote that we first have to create our dummy variables, and then we can use these to define X and y.\ndf = pd.get_dummies(df, drop_first=True)\nprint(df.columns)\ndf.head()\nIndex(['PassengerId', 'Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare',\n       'Name_Abbott, Mr. Rossmore Edward',\n       'Name_Abbott, Mrs. Stanton (Rosa Hunt)', 'Name_Abelson, Mr. Samuel',\n       ...\n       'Cabin_F G63', 'Cabin_F G73', 'Cabin_F2', 'Cabin_F33', 'Cabin_F38',\n       'Cabin_F4', 'Cabin_G6', 'Cabin_T', 'Embarked_Q', 'Embarked_S'],\n      dtype='object', length=1726)\n\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPassengerId\nSurvived\nPclass\nAge\nSibSp\nParch\nFare\nName_Abbott, Mr. Rossmore Edward\nName_Abbott, Mrs. Stanton (Rosa Hunt)\nName_Abelson, Mr. Samuel\n...\nCabin_F G63\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n1\n0\n3\n22.0\n1\n0\n7.2500\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n1\n2\n1\n1\n38.0\n1\n0\n71.2833\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n3\n1\n3\n26.0\n0\n0\n7.9250\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n3\n4\n1\n1\n35.0\n1\n0\n53.1000\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n4\n5\n0\n3\n35.0\n0\n0\n8.0500\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n\n5 rows √ó 1726 columns\n\nWow! That's a lot of columns! (Way more then is useful in practice: we now have columns for each of the passengers names. This is an example of what not to do. Let's try that again, this time being mindful of which variables we actually want to include in our model.\ndf = pd.read_csv('titanic.csv')\ndf.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\nx_feats = ['Pclass', 'Sex', 'Age', 'SibSp', 'Fare', 'Cabin', 'Embarked']\nX = pd.get_dummies(df[x_feats], drop_first=True)\ny = df['Survived']\nX.head() # Preview our data to make sure it looks reasonable\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPclass\nAge\nSibSp\nFare\nSex_male\nCabin_A14\nCabin_A16\nCabin_A19\nCabin_A20\nCabin_A23\n...\nCabin_F G63\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n3\n22.0\n1\n7.2500\n1\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n1\n1\n38.0\n1\n71.2833\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n3\n26.0\n0\n7.9250\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n3\n1\n35.0\n1\n53.1000\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n4\n3\n35.0\n0\n8.0500\n1\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n\n5 rows √ó 153 columns\n\nNormalization\nAnother important model tuning practice is to normalize your data. That is, if the features are on different scales, some features may impact the model more heavily then others. To level the playing field, we often normalize all features to a consistent scale of 0 to 1.\n# Fill missing values\nX = X.fillna(value=0) \nfor col in X.columns:\n    # Subtract the minimum and divide by the range forcing a scale of 0 to 1 for each feature\n    X[col] = (X[col] - min(X[col]))/ (max(X[col]) - min(X[col])) \n\nX.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPclass\nAge\nSibSp\nFare\nSex_male\nCabin_A14\nCabin_A16\nCabin_A19\nCabin_A20\nCabin_A23\n...\nCabin_F G63\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n1.0\n0.2750\n0.125\n0.014151\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n1\n0.0\n0.4750\n0.125\n0.139136\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\n1.0\n0.3250\n0.000\n0.015469\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n3\n0.0\n0.4375\n0.125\n0.103644\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n4\n1.0\n0.4375\n0.000\n0.015713\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n\n5 rows √ó 153 columns\n\nTrain-test split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nFit a model\nFit an initial model to the training set. In scikit-learn, you do this by first creating an instance of the LogisticRegression class. From there, then use the .fit() method from your class instance to fit a model to the training data.\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression(fit_intercept=False, C=1e12, solver='liblinear')\nmodel_log = logreg.fit(X_train, y_train)\nmodel_log\nLogisticRegression(C=1000000000000.0, class_weight=None, dual=False,\n                   fit_intercept=False, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',\n                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n                   warm_start=False)\n\nPredict\nNow that we have a model, lets take a look at how it performs.\ny_hat_test = logreg.predict(X_test)\ny_hat_train = logreg.predict(X_train)\nimport numpy as np\n# We could subtract the two columns. If values or equal, difference will be zero. Then count number of zeros \nresiduals = np.abs(y_train - y_hat_train)\nprint(pd.Series(residuals).value_counts())\nprint(pd.Series(residuals).value_counts(normalize=True))\n0    563\n1    105\nName: Survived, dtype: int64\n0    0.842814\n1    0.157186\nName: Survived, dtype: float64\n\nNot bad; our classifier was about 85% correct on our training data!\nresiduals = np.abs(y_test - y_hat_test)\nprint(pd.Series(residuals).value_counts())\nprint(pd.Series(residuals).value_counts(normalize=True))\n0    174\n1     49\nName: Survived, dtype: int64\n0    0.780269\n1    0.219731\nName: Survived, dtype: float64\n\nAnd still about 80% accurate on our test data!\nSummary\nIn this lesson, you took a more complete look at a data science pipeline for logistic regression, splitting the data into training and test sets and using the model to make predictions. You'll practice this on your own in the upcoming lab before having a more detailed discussion of more nuanced methods for evaluating a classifier's performance.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Linear to Logistic regression\nIntroduction\nIn this lesson, you\'ll be introduced to the logistic regression model. You\'ll start with an introductory example using linear regression, which you\'ve seen before, to act as a segue into logistic regression. After that, you\'ll learn about the formal notation of logistic regression models. Then, you\'ll conclude this lesson by looking at a real-world example.\nObjectives\nYou will be able to:\n\nDescribe the need for logistic regression\nInterpret the parameters of a logistic regression model\n\nRecap of the linear regression model\nYou have previously learned about linear regression models. In these models, you are trying to fit a linear relationship between two variables. An example is given below. In this example, you want to find a relationship between age and monthly income. It is reasonable to assume that, on average, older people have a higher income than younger people who are newer to the job market and have less experience. A potential relationship could look like the plot below. The monthly income is shown in 1000s of USD.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nnp.random.seed(1234)\n\nage = np.random.uniform(18, 65, 100)\nincome = np.random.normal((age/10), 0.5)\nage = age.reshape(-1,1)\n\nfig = plt.figure(figsize=(8,6))\nfig.suptitle(\'age vs income\', fontsize=16)\nplt.scatter(age, income)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.show()\n\nIn linear regression, you would try to find a relationship between age and monthly income. Conceptually, this means fitting a line that represents the relationship between age and monthly income, as shown below.\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'linear regression\', fontsize=16)\nplt.scatter(age, income)\nplt.plot(age, age/10, c=\'black\')\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.show()\n\nThe idea is that you could use this line to make predictions in the future. In this case, the relationship is modeled as follows: the expected monthly income for someone who is, say, 40 years old, is 3000 (3 on the y-axis). Of course, the actual income will most likely be different, but this indicates what the model predicts as the salary value.\nSo how is this related to logistic regression?\nNow, imagine you get a dataset where no information on exact income is given (after all, people don\'t like to talk about how much they earn!), but you only have information on whether or not they earn more than 4000 USD per month. Starting from the generated data we used before, the new variable income_bin was transformed to 1 when someone\'s income is over 4000 USD, and 0 when the income is less than 4000 USD.\nincome_bin = income > 4\nincome_bin = income_bin.astype(int)  \nprint(income_bin)\n[0 1 0 1 1 0 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0\n 1 0 1 0 0 0 1 1 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0\n 0 1 0 0 0 0 1 1 1 1 1 0 1 1 1 0 0 1 0 1 0 0 1 1 0 1]\n\nHave a look at what happens when you plot this.\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'age vs binary income\', fontsize=16)\nplt.scatter(age, income_bin)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income (> or < 4000)\', fontsize=14)\nplt.show()\n\nYou can already tell that fitting a straight line will not work here. Take a look at what happens when you fit a regression line to these data.\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LinearRegression\n\n# Create linear regression model\nlin_reg = LinearRegression()\nlin_reg.fit(age, income_bin)\n# Store the coefficients\ncoef = lin_reg.coef_\ninterc = lin_reg.intercept_\n# Create the line\nlin_income = (interc + age * coef)\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'linear regression\', fontsize=16)\nplt.scatter(age, income_bin)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.plot(age, lin_income, c=\'black\')\nplt.show()\n\nYou can see that this doesn\'t make a lot of sense. This straight line cannot grasp the true structure of what is going on when using a linear regression model. Now, without going into the mathematical details for now, look at a logistic regression model and fit that to the dataset.\n# Instantiate a Logistic regression model\n# Solver must be specified to avoid warning, see documentation for more information\n# liblinear is recommended for small datasets\n# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\nregr = LogisticRegression(C=1e5, solver=\'liblinear\')\n\n# Fit the model to the training set\nregr.fit(age, income_bin)\nLogisticRegression(C=100000.0, class_weight=None, dual=False,\n                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class=\'warn\', n_jobs=None, penalty=\'l2\',\n                   random_state=None, solver=\'liblinear\', tol=0.0001, verbose=0,\n                   warm_start=False)\n\n# Store the coefficients\ncoef = regr.coef_\ninterc = regr.intercept_\n\n# Create the linear predictor\nlin_pred = (age * coef + interc)\n\n# Perform the log transformation\nmod_income = 1 / (1 + np.exp(-lin_pred))\n\n# Sort the numbers to make sure plot looks right\nage_ordered, mod_income_ordered = zip(*sorted(zip(age ,mod_income.ravel()),key=lambda x: x[0]))\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'logistic regression\', fontsize=16)\nplt.scatter(age, income_bin)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.plot(age_ordered, mod_income_ordered, c=\'black\')\nplt.show()\n\nThis already looks a lot better! You can see that this function has an S-shape which plateaus to 0 in the left tale and 1 to the right tale. This is exactly what we needed here. Hopefully this example was a good way of showing why logistic regression is useful. Now, it\'s time to dive into the mathematics that make logistic regression possible.\nLogistic regression model formulation\nThe model\nAs you might remember from the linear regression lesson, a linear regression model can be written as:\n$$ \\hat y = \\hat\\beta_0 + \\hat\\beta_1 x_1 + \\hat\\beta_2 x_2 +\\ldots + \\beta_n x_n $$\nWhen there are $n$ predictors $x_1,\\ldots,x_n$ and $n+1$ parameter estimates that are estimated by the model $\\hat\\beta_0, \\hat\\beta_1,\\ldots, \\hat\\beta_n$.  $ \\hat y $ is an estimator for the outcome variable.\nTranslating this model formulation to our example, this boils down to:\n$$ \\text{income} = \\beta_0 + \\beta_1 \\text{age} $$\nWhen you want to apply this to a binary dataset, what you actually want to do is perform a classification of your data in one group versus another one. In our case, we want to classify our observations (the 100 people in our dataset) as good as possible in ""earns more than 4k"" and ""earns less than 4k"". A model will have to guess what the probability is of belonging to one group versus another. And that is exactly what logistic regression models can do!\nEssentially, what happens is, the linear regression is transformed in a way that the outcome takes a value between 0 and 1. This can then be interpreted as a probability (e.g., 0.2 is a probability of 20%). Applied to our example, the expression for a logistic regression model would look like this:\n$$ P(\\text{income} > 4000) = \\displaystyle \\frac{1}{1+e^{-(\\hat \\beta_0+\\hat \\beta_1 \\text{age})}}$$\nNote that the outcome is written as $P(\\text{income} > 4000)$. This means that the output should be interpreted as the probability that the monthly income is over 4000 USD.\nIt is important to note that this is the case because the income variable was relabeled to be equal to 1 when the income is bigger than 4000, and 0 when smaller than 4000. In other words, the outcome variable should be interpreted as the probability of the class label to be equal to 1.\nInterpretation\nAs mentioned before, the probability of an income over 4000 can be calculated using:\n$$ P(\\text{income} > 4000) = \\displaystyle \\frac{1}{1+e^{-(\\hat \\beta_o+\\hat \\beta_1 \\text{age})}}$$\nYou can show that, by multiplying both numerator and denominator by $e^{(\\hat \\beta_0+\\hat \\beta_1 \\text{age})}$\n$$ P(\\text{income} > 4000) = \\displaystyle \\frac{e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}}}{1+e^{\\hat \\beta_o+\\hat \\beta_1 \\text{age}}}$$\nAs a result, you can compute $P(\\text{income} \\leq 4000)$ as:\n$$ P(\\text{income} < 4000) = 1- \\displaystyle \\frac{e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}}}{1+e^{\\hat \\beta_o+\\hat \\beta_1 \\text{age}}}= \\displaystyle \\frac{1}{1+e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}}}$$\nThis doesn\'t seem to be very spectacular, but combining these two results leads to an easy interpretation of the model parameters, triggered by the odds\n$$ \\dfrac{P(\\text{income} > 4000)}{P(\\text{income} < 4000)} = e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}} $$\nThis expression can be interpreted as the odds in favor of an income greater than 4000 USD.\nThis result, in combination with mathematical properties of exponential functions, leads to the fact that, applied to our example:\nif age goes up by 1, the odds are multiplied by $e^{\\beta_1}$\nIn our example, there is a positive relationship between age and income, this will lead a positive $\\beta_1 > 0$, so $e^{\\beta_1}>1$, and the odds will increase as age increases.\nA real-world example\nNow you will apply these concepts to a real-world dataset:\nimport statsmodels as sm\nimport sklearn.preprocessing as preprocessing\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom scipy import stats\nsalaries = pd.read_csv(\'salaries_final.csv\', index_col=0)\nsalaries.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nAge\nEducation\nOccupation\nRelationship\nRace\nSex\nTarget\n\n\n\n\n0\n39\nBachelors\nAdm-clerical\nNot-in-family\nWhite\nMale\n<=50K\n\n\n1\n50\nBachelors\nExec-managerial\nHusband\nWhite\nMale\n<=50K\n\n\n2\n38\nHS-grad\nHandlers-cleaners\nNot-in-family\nWhite\nMale\n<=50K\n\n\n3\n53\n11th\nHandlers-cleaners\nHusband\nBlack\nMale\n<=50K\n\n\n4\n28\nBachelors\nProf-specialty\nWife\nBlack\nFemale\n<=50K\n\n\n\n\nFor this example, you will fit a logistic regression model to Target using Age, Race, and Sex. Since Target, Race, and Sex are categorical, they need to be be converted to a numeric datatype first.\nThe get_dummies() function will only convert object and category datatypes to dummy variables so it is safe to pass Age to get_dummies(). Note that we also pass two additional arguments, drop_first=True and dtype=float. The drop_first=True argument removes the first level for each categorical variable and the dtype=float argument converts the datatype of all the dummy variables to float. The data must be float in order to obtain accurate statistical results from statsmodels.\n# Convert race and sex using get_dummies() \nx_feats = [\'Race\', \'Sex\', \'Age\']\nX = pd.get_dummies(salaries[x_feats], drop_first=True, dtype=float)\n\n# Convert target using get_dummies\ny = pd.get_dummies(salaries[\'Target\'], drop_first=True, dtype=float)\ny = y[\'>50K\']\nimport statsmodels.api as sm\n\n# Create intercept term required for sm.Logit, see documentation for more information\nX = sm.add_constant(X)\n\n# Fit model\nlogit_model = sm.Logit(y, X)\n\n# Get results of the fit\nresult = logit_model.fit()\nOptimization terminated successfully.\n         Current function value: 0.498651\n         Iterations 6\n\n\n//anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n  return ptp(axis=axis, out=out, **kwargs)\n\nresult.summary()\n\nLogit Regression Results\n\nDep. Variable: >50K   No. Observations:    32561\n\n\nModel: Logit   Df Residuals:        32554\n\n\nMethod: MLE   Df Model:                6\n\n\nDate: Wed, 20 Nov 2019   Pseudo R-squ.:      0.09666\n\n\nTime: 14:55:31   Log-Likelihood:      -16237.\n\n\nconverged: True   LL-Null:             -17974.\n\n\nCovariance Type: nonrobust   LLR p-value:         0.000\n\n\n\n\n coef std err z P>|z| [0.025 0.975]\n\n\nconst    -4.4248     0.189   -23.380  0.000    -4.796    -4.054\n\n\nAge     0.0387     0.001    38.530  0.000     0.037     0.041\n\n\nRace_Asian-Pac-Islander     0.9991     0.197     5.079  0.000     0.614     1.385\n\n\nRace_Black     0.1812     0.191     0.950  0.342    -0.193     0.555\n\n\nRace_Other    -0.1143     0.282    -0.406  0.685    -0.667     0.438\n\n\nRace_White     0.8742     0.183     4.782  0.000     0.516     1.232\n\n\nSex_Male     1.2069     0.035    34.380  0.000     1.138     1.276\n\n\nnp.exp(result.params)\nconst                      0.011977\nAge                        1.039480\nRace_Asian-Pac-Islander    2.715861\nRace_Black                 1.198638\nRace_Other                 0.891987\nRace_White                 2.396965\nSex_Male                   3.343142\ndtype: float64\n\nYou can also use scikit-learn to retrieve the parameter estimates. The disadvantage here though is that there are no p-values for your parameter estimates!\nlogreg = LogisticRegression(fit_intercept = False, C = 1e15, solver=\'liblinear\')\nmodel_log = logreg.fit(X, y)\nmodel_log\nLogisticRegression(C=1000000000000000.0, class_weight=None, dual=False,\n                   fit_intercept=False, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class=\'warn\', n_jobs=None, penalty=\'l2\',\n                   random_state=None, solver=\'liblinear\', tol=0.0001, verbose=0,\n                   warm_start=False)\n\nmodel_log.coef_\narray([[-4.38706342,  0.03871011,  0.96178902,  0.14397983, -0.14384057,\n         0.83689457,  1.2067121 ]])\n\nSummary\nIn this lab you built upon your previous knowledge of linear regression and built an intuitive understanding of how this could be adapted for classification. We then demonstrated tools for performing logistic regression. In the upcoming lessons you will continue to investigate logistic regression from other viewpoints.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Logistic Regression in scikit-learn\nIntroduction\nGenerally, the process for fitting a logistic regression model using scikit-learn is very similar to that which you previously saw for statsmodels. One important exception is that scikit-learn will not display statistical measures such as the p-values associated with the various features. This is a shortcoming of scikit-learn, although scikit-learn has other useful tools for tuning models which we will investigate in future lessons.\nThe other main process of model building and evaluation which we didn't to discuss previously is performing a train-test split. As we saw in linear regression, model validation is an essential part of model building as it helps determine how our model will generalize to future unseen cases. After all, the point of any model is to provide future predictions where we don't already know the answer but have other informative data (X).\nWith that, let's take a look at implementing logistic regression in scikit-learn using dummy variables and a proper train-test split.\nObjectives\nYou will be able to:\n\nFit a logistic regression model using scikit-learn\n\nImport the data\nimport pandas as pd\n\ndf = pd.read_csv('titanic.csv')\ndf.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\nDefine X and y\nNote that we first have to create our dummy variables, and then we can use these to define X and y.\ndf = pd.get_dummies(df, drop_first=True)\nprint(df.columns)\ndf.head()\nIndex(['PassengerId', 'Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare',\n       'Name_Abbott, Mr. Rossmore Edward',\n       'Name_Abbott, Mrs. Stanton (Rosa Hunt)', 'Name_Abelson, Mr. Samuel',\n       ...\n       'Cabin_F G63', 'Cabin_F G73', 'Cabin_F2', 'Cabin_F33', 'Cabin_F38',\n       'Cabin_F4', 'Cabin_G6', 'Cabin_T', 'Embarked_Q', 'Embarked_S'],\n      dtype='object', length=1726)\n\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPassengerId\nSurvived\nPclass\nAge\nSibSp\nParch\nFare\nName_Abbott, Mr. Rossmore Edward\nName_Abbott, Mrs. Stanton (Rosa Hunt)\nName_Abelson, Mr. Samuel\n...\nCabin_F G63\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n1\n0\n3\n22.0\n1\n0\n7.2500\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n1\n2\n1\n1\n38.0\n1\n0\n71.2833\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n3\n1\n3\n26.0\n0\n0\n7.9250\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n3\n4\n1\n1\n35.0\n1\n0\n53.1000\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n4\n5\n0\n3\n35.0\n0\n0\n8.0500\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n\n5 rows √ó 1726 columns\n\nWow! That's a lot of columns! (Way more then is useful in practice: we now have columns for each of the passengers names. This is an example of what not to do. Let's try that again, this time being mindful of which variables we actually want to include in our model.\ndf = pd.read_csv('titanic.csv')\ndf.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\nx_feats = ['Pclass', 'Sex', 'Age', 'SibSp', 'Fare', 'Cabin', 'Embarked']\nX = pd.get_dummies(df[x_feats], drop_first=True)\ny = df['Survived']\nX.head() # Preview our data to make sure it looks reasonable\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPclass\nAge\nSibSp\nFare\nSex_male\nCabin_A14\nCabin_A16\nCabin_A19\nCabin_A20\nCabin_A23\n...\nCabin_F G63\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n3\n22.0\n1\n7.2500\n1\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n1\n1\n38.0\n1\n71.2833\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n3\n26.0\n0\n7.9250\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n3\n1\n35.0\n1\n53.1000\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n4\n3\n35.0\n0\n8.0500\n1\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n\n5 rows √ó 153 columns\n\nNormalization\nAnother important model tuning practice is to normalize your data. That is, if the features are on different scales, some features may impact the model more heavily then others. To level the playing field, we often normalize all features to a consistent scale of 0 to 1.\n# Fill missing values\nX = X.fillna(value=0) \nfor col in X.columns:\n    # Subtract the minimum and divide by the range forcing a scale of 0 to 1 for each feature\n    X[col] = (X[col] - min(X[col]))/ (max(X[col]) - min(X[col])) \n\nX.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nPclass\nAge\nSibSp\nFare\nSex_male\nCabin_A14\nCabin_A16\nCabin_A19\nCabin_A20\nCabin_A23\n...\nCabin_F G63\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n1.0\n0.2750\n0.125\n0.014151\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n1\n0.0\n0.4750\n0.125\n0.139136\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\n1.0\n0.3250\n0.000\n0.015469\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n3\n0.0\n0.4375\n0.125\n0.103644\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n4\n1.0\n0.4375\n0.000\n0.015713\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n\n5 rows √ó 153 columns\n\nTrain-test split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nFit a model\nFit an initial model to the training set. In scikit-learn, you do this by first creating an instance of the LogisticRegression class. From there, then use the .fit() method from your class instance to fit a model to the training data.\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression(fit_intercept=False, C=1e12, solver='liblinear')\nmodel_log = logreg.fit(X_train, y_train)\nmodel_log\nLogisticRegression(C=1000000000000.0, class_weight=None, dual=False,\n                   fit_intercept=False, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',\n                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n                   warm_start=False)\n\nPredict\nNow that we have a model, lets take a look at how it performs.\ny_hat_test = logreg.predict(X_test)\ny_hat_train = logreg.predict(X_train)\nimport numpy as np\n# We could subtract the two columns. If values or equal, difference will be zero. Then count number of zeros \nresiduals = np.abs(y_train - y_hat_train)\nprint(pd.Series(residuals).value_counts())\nprint(pd.Series(residuals).value_counts(normalize=True))\n0    563\n1    105\nName: Survived, dtype: int64\n0    0.842814\n1    0.157186\nName: Survived, dtype: float64\n\nNot bad; our classifier was about 85% correct on our training data!\nresiduals = np.abs(y_test - y_hat_test)\nprint(pd.Series(residuals).value_counts())\nprint(pd.Series(residuals).value_counts(normalize=True))\n0    174\n1     49\nName: Survived, dtype: int64\n0    0.780269\n1    0.219731\nName: Survived, dtype: float64\n\nAnd still about 80% accurate on our test data!\nSummary\nIn this lesson, you took a more complete look at a data science pipeline for logistic regression, splitting the data into training and test sets and using the model to make predictions. You'll practice this on your own in the upcoming lab before having a more detailed discussion of more nuanced methods for evaluating a classifier's performance.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}","{'location': 'Canada', 'stats_list': [], 'contributions': '74 contributions\n        in the last year', 'description': ['Linear Regression Model of Experience Level on Salary in Tech Industry\nauthor: Zhaotian Li, Yonghao Li, Hongtianxu Hua\ndate: 1 March, 2020\nThis report is concerned with the potential effects of personal and professional experience on the\naverage salary received by full-time employees within tech companies in the US. We used Stack\nOverflow Developer Survey Results (2019) to build a linear regression model to identify the\nmost influential factors. We found out that having a graduate degree has the highest impact on\nsalary, along with other strong predictors . The linear regression model we build could provide\ninsights to seasoned developers, job-seekers, and employers, regarding potential salary\nredistribution, job satisfaction, and work environment.\n'], 'url_profile': 'https://github.com/liyongh1', 'info_list': ['Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Evaluating Logistic Regression Models - Lab\nIntroduction\nIn regression, you are predicting continous values so it makes sense to discuss error as a distance of how far off our estimates were. When classifying a binary variable, however, a model is either correct or incorrect. As a result, we tend to quantify this in terms of how many false positives versus false negatives we come across. In particular, we examine a few different specific measurements when evaluating the performance of a classification algorithm. In this lab, you\'ll review precision, recall, accuracy, and F1 score in order to evaluate our logistic regression models.\nObjectives\nIn this lab you will:\n\nImplement evaluation metrics from scratch using Python\n\nTerminology review\nLet\'s take a moment and review some classification evaluation metrics:\n$$ \\text{Precision} = \\frac{\\text{Number of True Positives}}{\\text{Number of Predicted Positives}} $$\n$$ \\text{Recall} = \\frac{\\text{Number of True Positives}}{\\text{Number of Actual Total Positives}} $$\n$$ \\text{Accuracy} = \\frac{\\text{Number of True Positives + True Negatives}}{\\text{Total Observations}} $$\n$$ \\text{F1 score} = 2 * \\frac{\\text{Precision * Recall}}{\\text{Precision + Recall}} $$\nAt times, it may be best to tune a classification algorithm to optimize against precision or recall rather than overall accuracy. For example, imagine the scenario of predicting whether or not a patient is at risk for cancer and should be brought in for additional testing. In cases such as this, we often may want to cast a slightly wider net, and it is preferable to optimize for recall, the number of cancer positive cases, than it is to optimize precision, the percentage of our predicted cancer-risk patients who are indeed positive.\nSplit the data into training and test sets\nimport pandas as pd\ndf = pd.read_csv(\'heart.csv\')\ndf.head()\nSplit the data first into X and y, and then into training and test sets. Assign 25% to the test set and set the random_state to 0.\n# Import train_test_split\n\n\n# Split data into X and y\ny = None\nX = None\n\n# Split the data into a training and a test set\nX_train, X_test, y_train, y_test = None\nBuild a vanilla logistic regression model\n\nImport and instantiate LogisticRegression\nMake sure you do not use an intercept term and use the \'liblinear\' solver\nFit the model to training data\n\n# Import LogisticRegression\n\n\n# Instantiate LogisticRegression\nlogreg = None\n\n# Fit to training data\nmodel_log = None\nmodel_log\nWrite a function to calculate the precision\ndef precision(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the recall\ndef recall(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the accuracy\ndef accuracy(y, y_hat):\n    # Your code here\n    pass\nWrite a function to calculate the F1 score\ndef f1_score(y, y_hat):\n    # Your code here\n    pass\nCalculate the precision, recall, accuracy, and F1 score of your classifier\nDo this for both the training and test sets.\n# Your code here\ny_hat_train = None\ny_hat_test = None\nGreat job! Now it\'s time to check your work with sklearn.\nCalculate metrics with sklearn\nEach of the metrics we calculated above is also available inside the sklearn.metrics module.\nIn the cell below, import the following functions:\n\nprecision_score\nrecall_score\naccuracy_score\nf1_score\n\nCompare the results of your performance metrics functions above with the sklearn functions. Calculate these values for both your train and test set.\n# Your code here\nNicely done! Did the results from sklearn match that of your own?\nCompare precision, recall, accuracy, and F1 score for train vs test sets\nCalculate and then plot the precision, recall, accuracy, and F1 score for the test and training splits using different training set sizes. What do you notice?\nimport matplotlib.pyplot as plt\n%matplotlib inline\ntraining_precision = []\ntesting_precision = []\ntraining_recall = []\ntesting_recall = []\ntraining_accuracy = []\ntesting_accuracy = []\ntraining_f1 = []\ntesting_f1 = []\n\nfor i in range(10, 95):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= None) # replace the ""None"" here\n    logreg = LogisticRegression(fit_intercept=False, C=1e20, solver=\'liblinear\')\n    model_log = None\n    y_hat_test = None\n    y_hat_train = None \n    \n    # Your code here\nCreate four scatter plots looking at the train and test precision in the first one, train and test recall in the second one, train and test accuracy in the third one, and train and test F1 score in the fourth one.\nWe already created the scatter plot for precision:\n# Train and test precision\nplt.scatter(list(range(10, 95)), training_precision, label=\'training_precision\')\nplt.scatter(list(range(10, 95)), testing_precision, label=\'testing_precision\')\nplt.legend()\nplt.show()\n# Train and test recall\n# Train and test accuracy\n# Train and test F1 score\nSummary\nNice! In this lab, you calculated evaluation metrics for classification algorithms from scratch in Python. Going forward, continue to think about scenarios in which you might prefer to optimize one of these metrics over another.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Logistic Regression Model Comparisons - Lab\nIntroduction\nIn this lab, you\'ll further investigate how to tune your own logistic regression implementation, as well as that of scikit-learn in order to produce better models.\nObjectives\n\nCompare the different inputs with logistic regression models and determine the optimal model\n\nIn the previous lab, you were able to compare the output of your own implementation of the logistic regression model with that of scikit-learn. However, that model did not include an intercept or any regularization. In this investigative lab, you will analyze the impact of these two tuning parameters.\nImport the data\nAs with the previous lab, import the dataset stored in \'heart.csv\':\n# Import the data\n\ndf = None\n\n# Print the first five rows of the data\nSplit the data\nDefine X and y as with the previous lab. This time, follow best practices and also implement a standard train-test split. Assign 25% to the test set and set the random_state to 17.\n# Define X and y\ny = None\nX = None\n\n# Split the data into training and test sets\n\n\nX_train, X_test, y_train, y_test = None\nprint(y_train.value_counts(),\'\\n\\n\', y_test.value_counts())\nInitial Model - Personal Implementation\nUse your code from the previous lab to once again train a logistic regression algorithm on the training set.\n# Your code from previous lab\nimport numpy as np\n\ndef sigmoid(x):\n    x = np.array(x)\n    return 1/(1 + np.e**(-1*x))\n\ndef grad_desc(X, y, max_iterations, alpha, initial_weights=None):\n    """"""Be sure to set default behavior for the initial_weights parameter.""""""\n    if initial_weights is None:\n        initial_weights = np.ones((X.shape[1], 1)).flatten()\n    weights_col = pd.DataFrame(initial_weights)\n    weights = initial_weights\n    # Create a for loop of iterations\n    for iteration in range(max_iterations):\n        # Generate predictions using the current feature weights\n        predictions = sigmoid(np.dot(X, weights))\n        # Calculate an error vector based on these initial predictions and the correct labels\n        error_vector = y - predictions\n        # Calculate the gradient \n        # As we saw in the previous lab, calculating the gradient is often the most difficult task.\n        # Here, your are provided with the closed form solution for the gradient of the log-loss function derived from MLE\n        # For more details on the derivation, see the additional resources section below.\n        gradient = np.dot(X.transpose(), error_vector)\n        # Update the weight vector take a step of alpha in direction of gradient \n        weights += alpha * gradient\n        weights_col = pd.concat([weights_col, pd.DataFrame(weights)], axis=1)\n    # Return finalized weights\n    return weights, weights_col\n\nweights, weights_col = grad_desc(X_train, y_train, 50000, 0.001)\nMake [probability] predictions on the test set\n# Predict on test set\ny_hat_test = None\nnp.round(y_hat_test, 2)\nCreate an ROC curve for your predictions\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_hat_test)\n\nprint(\'AUC: {}\'.format(auc(test_fpr, test_tpr)))\n\n# Seaborn\'s beautiful styling\nsns.set_style(\'darkgrid\', {\'axes.facecolor\': \'0.9\'})\n\nplt.figure(figsize=(10, 8))\nlw = 2\n\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Test ROC curve\')\n\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=\'lower right\')\nplt.show()\nUpdate your ROC curve to include the training set\ny_hat_train = None\n\ntrain_fpr, train_tpr, train_thresholds = None\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_hat_test)\n\n# Train AUC\nprint(\'Train AUC: {}\'.format( None ))\nprint(\'AUC: {}\'.format(auc(test_fpr, test_tpr)))\n\n# Seaborn\'s beautiful styling\nsns.set_style(\'darkgrid\', {\'axes.facecolor\': \'0.9\'})\n\nplt.figure(figsize=(10, 8))\nlw = 2\n\nplt.plot(train_fpr, train_tpr, color=\'blue\',\n         lw=lw, label=\'Train ROC curve\')\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Test ROC curve\')\n\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=\'lower right\')\nplt.show()\nCreate a confusion matrix for your predictions\nUse a standard decision boundary of 0.5 to convert your probabilities output by logistic regression into binary classifications. (Again this should be for the test set.) Afterward, feel free to use the built-in scikit-learn function to compute the confusion matrix as we discussed in previous sections.\n# Your code here\nInitial Model - scikit-learn\nUse scikit-learn to build a similar model. To start, create an identical model as you did in the last section; turn off the intercept and set the regularization parameter, C, to a ridiculously large number such as 1e16.\n# Your code here\nCreate an ROC Curve for the scikit-learn model\nUse both the training and test sets\n# Your code here\n\ny_train_score = None\ny_test_score = None\n\ntrain_fpr, train_tpr, train_thresholds = None\ntest_fpr, test_tpr, test_thresholds = None\n\n\nprint(\'Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\nprint(\'Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\n\nplt.figure(figsize=(10, 8))\nlw = 2\n\nplt.plot(train_fpr, train_tpr, color=\'blue\',\n         lw=lw, label=\'Train ROC curve\')\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Test ROC curve\')\n\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=\'lower right\')\nplt.show()\nAdd an Intercept\nNow add an intercept to the scikit-learn model. Keep the regularization parameter C set to a very large number such as 1e16.\n# Create new model\nlogregi = None\nPlot all three models ROC curves on the same graph.\n# Initial model plots\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_hat_test)\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_hat_train)\n\n\nprint(\'Custom Model Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\nprint(\'Custome Model Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\n\nplt.figure(figsize=(10,8))\nlw = 2\n\nplt.plot(test_fpr, test_tpr, color=\'darkorange\',\n         lw=lw, label=\'Custom Model Test ROC curve\')\nplt.plot(train_fpr, train_tpr, color=\'blue\',\n         lw=lw, label=\'Custom Model Train ROC curve\')\n\n\n# Second model plots\ny_test_score = logreg.decision_function(X_test)\ny_train_score = logreg.decision_function(X_train)\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_test_score)\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_train_score)\n\nprint(\'Scikit-learn Model 1 Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\nprint(\'Scikit-learn Model 1 Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\n\n\nplt.plot(test_fpr, test_tpr, color=\'yellow\',\n         lw=lw, label=\'Scikit learn Model 1 Test ROC curve\')\nplt.plot(train_fpr, train_tpr, color=\'gold\',\n         lw=lw, label=\'Scikit learn Model 1 Train ROC curve\')\n\n\n# Third model plots\ny_test_score = None\ny_train_score = None\n\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_test_score)\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_train_score)\n\nprint(\'Scikit-learn Model 2 with intercept Test AUC: {}\'.format(auc(test_fpr, test_tpr)))\nprint(\'Scikit-learn Model 2 with intercept Train AUC: {}\'.format(auc(train_fpr, train_tpr)))\n\n\nplt.plot(test_fpr, test_tpr, color=\'purple\',\n         lw=lw, label=\'Scikit learn Model 2 with intercept Test ROC curve\')\nplt.plot(train_fpr, train_tpr, color=\'red\',\n         lw=lw, label=\'Scikit learn Model 2 with intercept Train ROC curve\')\n\n# Formatting\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i/20.0 for i in range(21)])\nplt.xticks([i/20.0 for i in range(21)])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic (ROC) Curve\')\nplt.legend(loc=""lower right"")\nplt.show()\nAltering the Regularization Parameter\nNow, experiment with altering the regularization parameter. At a minimum, create 5 different subplots with varying regularization (C) parameters. For each, plot the ROC curve of the training and test set for that specific model.\nRegularization parameters between 1 and 20 are recommended. Observe the difference in test and training AUC as you go along.\n# Your code here\nHow did the regularization parameter impact the ROC curves plotted above?\nSummary\nIn this lab, you reviewed many of the accuracy measures for classification algorithms and observed the impact of additional tuning models using intercepts and regularization.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Ridge and Lasso Regression\nIntroduction\nAt this point, you\'ve seen a number of criteria and algorithms for fitting regression models to data. You\'ve seen the simple linear regression using ordinary least squares, and its more general regression of polynomial functions. You\'ve also seen how we can overfit models to data using polynomials and interactions. With all of that, you began to explore other tools to analyze this general problem of overfitting versus underfitting, all this using training and test splits, bias and variance, and cross validation.\nNow you\'re going to take a look at another way to tune the models you create. These methods all modify the mean squared error function that you are optimizing against. The modifications will add a penalty for large coefficient weights in the resulting model.\nObjectives\nYou will be able to:\n\nDefine Lasso regression\nDefine Ridge regression\nDescribe why standardization is necessary before Ridge and Lasso regression\nCompare and contrast Lasso, Ridge, and non-regularized regression\nUse Lasso and Ridge regression with scikit-learn\n\nOur regression cost function\nFrom an earlier lesson, you know that when solving for a linear regression, you can express the cost function as\n$$ \\text{cost_function}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - (mx_i + b))^2$$\nThis is the expression for simple linear regression (for 1 predictor $x$). If you have multiple predictors, you would have something that looks like:\n$$ \\text{cost_function}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij} ) -b )^2$$\nwhere $k$ is the number of predictors.\nPenalized estimation\nYou\'ve seen that when the number of predictors increases, your model complexity increases, with a higher chance of overfitting as a result. We\'ve previously seen fairly ad-hoc variable selection methods (such as forward/backward selection), to simply select a few variables from a longer list of variables as predictors.\nNow, instead of completely ""deleting"" certain predictors from a model (which is equal to setting coefficients equal to zero), wouldn\'t it be interesting to just reduce the values of the coefficients to make them less sensitive to noise in the data? Penalized estimation operates in a way where parameter shrinkage effects are used to make some or all of the coefficients smaller in magnitude (closer to zero). Some of the penalties have the property of performing both variable selection (setting some coefficients exactly equal to zero) and shrinking the other coefficients. Ridge and Lasso regression are two examples of penalized estimation. There are multiple advantages to using these methods:\n\nThey reduce model complexity\nThe may prevent from overfitting\nSome of them may perform variable selection at the same time (when coefficients are set to 0)\nThey can be used to counter multicollinearity\n\nLasso and Ridge are two commonly used so-called regularization techniques. Regularization is a general term used when one tries to battle overfitting. Regularization techniques will be covered in more depth when we\'re moving into machine learning!\nRidge regression\nIn ridge regression, the cost function is changed by adding a penalty term to the square of the magnitude of the coefficients.\n$$ \\text{cost_function_ridge}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij})-b)^2 + \\lambda \\sum_{j=1}^p m_j^2$$\nIf you have two predictors the full equation would look like this (notice that there is a penalty term m for each predictor in the model - in this case, two) :\n$$ \\text{cost_function_ridge}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = $$\n$$ \\sum_{i=1}^n(y_i - ((m_1x_{1i})-b)^2 + \\lambda m_1^2 + (m_2x_{2i})-b)^2 + \\lambda m_2^2)$$\nRemember that you want to minimize your cost function, so by adding the penalty term $\\lambda$, ridge regression puts a constraint on the coefficients $m$. This means that large coefficients penalize the optimization function. That\'s why ridge regression leads to a shrinkage of the coefficients and helps to reduce model complexity and multicollinearity.\n$\\lambda$ is a so-called hyperparameter, which means you have to specify the value for lambda. For a small lambda, the outcome of your ridge regression will resemble a linear regression model. For large lambda, penalization will increase and more parameters will shrink.\nRidge regression is often also referred to as L2 Norm Regularization.\nLasso regression\nLasso regression is very similar to Ridge regression, except that the magnitude of the coefficients are not squared in the penalty term. So, while Ridge regression keeps the sum of the squared regression coefficients (except for the intercept) bounded, the Lasso method bounds the sum of the absolute values.\nThe resulting cost function looks like this:\n$$ \\text{cost_function_lasso}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij})-b)^2 + \\lambda \\sum_{j=1}^p \\mid m_j \\mid$$\nIf you have two predictors the full equation would look like this (notice that there is a penalty term m for each predictor in the model - in this case, two):\n$$ \\text{cost_function_lasso}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = $$\n$$\\sum_{i=1}^n(y_i - ((m_1x_{1i})-b)^2 + \\lambda \\mid m_1 \\mid) + ((m_2x_{2i})-b)^2 + \\lambda \\mid m_2 \\mid) $$\nThe name ""Lasso"" comes from ""Least Absolute Shrinkage and Selection Operator"".\nWhile it may look similar to the definition of the Ridge estimator, the effect of the absolute values is that some coefficients might be set exactly equal to zero, while other coefficients are shrunk towards zero. Hence the Lasso method is attractive because it performs estimation and selection simultaneously. Especially for variable selection when the number of predictors is very high.\nLasso regression is often also referred to as L1 Norm Regularization.\nStandardization before Regularization\nAn important step before using either Lasso or Ridge regularization is to first standardize your data such that it is all on the same scale. Regularization is based on the concept of penalizing larger coefficients, so if you have features that are on different scales, some will get unfairly penalized. Below, you can see that we are using a MinMaxScaler to standardize our data to the same scale. A downside of standardization is that the value of the coefficients become less interpretable and must be transformed back to their original scale if you want to interpret how a one unit change in a feature impacts the target variable.\nAn example using our auto-mpg data\nLet\'s transform our continuous predictors in auto-mpg and see how they perform as predictors in a Ridge versus Lasso regression.\nWe import the dataset and, seperate the target and predictors and then split the data into training and test sets:\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import Lasso, Ridge, LinearRegression\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndata = pd.read_csv(\'auto-mpg.csv\') \n\ny = data[[\'mpg\']]\nX = data.drop([\'mpg\', \'car name\', \'origin\'], axis=1)\n\n# Perform test train split\nX_train , X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12)\nAfter splitting the data into training and test sets, we use the MixMaxScaler() to fit and transform X_train and transform X_test.\n\nNOTE: You want to fit and transform only the training data because in a real-world setting, you only have access to this data. You can then use the same scalar object to transform the test data. It\'s not uncommon for people to first transform the data and then split into training and test sets -- which leads to data-leakage.\n\nscale = MinMaxScaler()\nX_train_transformed = scale.fit_transform(X_train)\nX_test_transformed = scale.transform(X_test)\nWe will not fit the Ridge, Lasso, and Linear regression models to the transformed training data. Notice that the Ridge and Lasso models have the parameter alpha, which is Scikit-Learn\'s version of $\\lambda$ in the regularization cost functions.\n# Build a Ridge, Lasso and regular linear regression model  \n# Note that in scikit-learn, the regularization parameter is denoted by alpha (and not lambda)\nridge = Ridge(alpha=0.5)\nridge.fit(X_train_transformed, y_train)\n\nlasso = Lasso(alpha=0.5)\nlasso.fit(X_train_transformed, y_train)\n\nlin = LinearRegression()\nlin.fit(X_train_transformed, y_train)\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n\nNext, let\'s generate predictions for both the training and test sets:\n# Generate preditions for training and test sets\ny_h_ridge_train = ridge.predict(X_train_transformed)\ny_h_ridge_test = ridge.predict(X_test_transformed)\n\ny_h_lasso_train = np.reshape(lasso.predict(X_train_transformed), (274, 1))\ny_h_lasso_test = np.reshape(lasso.predict(X_test_transformed), (118, 1))\n\ny_h_lin_train = lin.predict(X_train_transformed)\ny_h_lin_test = lin.predict(X_test_transformed)\nLook at the RSS for training and test sets for each of the three models:\nprint(\'Train Error Ridge Model\', np.sum((y_train - y_h_ridge_train)**2))\nprint(\'Test Error Ridge Model\', np.sum((y_test - y_h_ridge_test)**2))\nprint(\'\\n\')\n\nprint(\'Train Error Lasso Model\', np.sum((y_train - y_h_lasso_train)**2))\nprint(\'Test Error Lasso Model\', np.sum((y_test - y_h_lasso_test)**2))\nprint(\'\\n\')\n\nprint(\'Train Error Unpenalized Linear Model\', np.sum((y_train - lin.predict(X_train_transformed))**2))\nprint(\'Test Error Unpenalized Linear Model\', np.sum((y_test - lin.predict(X_test_transformed))**2))\nTrain Error Ridge Model mpg    2684.673787\ndtype: float64\nTest Error Ridge Model mpg    2067.795707\ndtype: float64\n\n\nTrain Error Lasso Model mpg    4450.979518\ndtype: float64\nTest Error Lasso Model mpg    3544.087085\ndtype: float64\n\n\nTrain Error Unpenalized Linear Model mpg    2658.043444\ndtype: float64\nTest Error Unpenalized Linear Model mpg    1976.266987\ndtype: float64\n\nWe note that Ridge is clearly better than Lasso here, but that the unpenalized model performs best here. Let\'s see how including Ridge and Lasso changed our parameter estimates.\nprint(\'Ridge parameter coefficients:\', ridge.coef_)\nprint(\'Lasso parameter coefficients:\', lasso.coef_)\nprint(\'Linear model parameter coefficients:\', lin.coef_)\nRidge parameter coefficients: [[ -2.06904445  -2.88593443  -1.81801505 -15.23785349  -1.45594148\n    8.1440177 ]]\nLasso parameter coefficients: [-9.09743525 -0.         -0.         -4.02703963  0.          3.92348219]\nLinear model parameter coefficients: [[ -1.33790698  -1.05300843  -0.08661412 -19.26724989  -0.37043697\n    8.56051229]]\n\nDid you notice that Lasso shrinked a few parameters to 0? The Ridge regression mostly affected the fourth parameter (estimated to be -19.26 for the linear regression model).\nAdditional reading\nFull code examples for Ridge and Lasso regression, advantages and disadvantages, and how to code ridge and Lasso in Python can be found here.\nMake sure to have a look at the Scikit-Learn documentation for Ridge and Lasso.\nSummary\nGreat! You now know how to perform Lasso and Ridge regression. Let\'s move on to the lab so you can use these!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Logistic Regression in scikit-learn - Lab\nIntroduction\nIn this lab, you are going to fit a logistic regression model to a dataset concerning heart disease. Whether or not a patient has heart disease is indicated in the column labeled 'target'. 1 is for positive for heart disease while 0 indicates no heart disease.\nObjectives\nIn this lab you will:\n\nFit a logistic regression model using scikit-learn\n\nLet's get started!\nRun the following cells that import the necessary functions and import the dataset:\n# Import necessary functions\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\n# Import data\ndf = pd.read_csv('heart.csv')\ndf.head()\nDefine appropriate X and y\nRecall the dataset contains information about whether or not a patient has heart disease and is indicated in the column labeled 'target'. With that, define appropriate X (predictors) and y (target) in order to model whether or not a patient has heart disease.\n# Split the data into target and predictors\ny = None\nX = None\nNormalize the data\nNormalize the data (X) prior to fitting the model.\n# Your code here\nX = None\nX.head()\nTrain- test split\n\nSplit the data into training and test sets\nAssign 25% to the test set\nSet the random_state to 0\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = None\nFit a model\n\nInstantiate LogisticRegression\n\nMake sure you don't include the intercept\nset C to a very large number such as 1e12\nUse the 'liblinear' solver\n\n\nFit the model to the training data\n\n# Instantiate the model\nlogreg = None\n\n# Fit the model\nPredict\nGenerate predictions for the training and test sets.\n# Generate predictions\ny_hat_train = None\ny_hat_test = None\nHow many times was the classifier correct on the training set?\n# Your code here\nHow many times was the classifier correct on the test set?\n# Your code here\nAnalysis\nDescribe how well you think this initial model is performing based on the training and test performance. Within your description, make note of how you evaluated performance as compared to your previous work with regression.\n# Your analysis here\nSummary\nIn this lab, you practiced a standard data science pipeline: importing data, split it into training and test sets, and fit a logistic regression model. In the upcoming labs and lessons, you'll continue to investigate how to analyze and tune these models for various scenarios.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Linear to Logistic regression\nIntroduction\nIn this lesson, you\'ll be introduced to the logistic regression model. You\'ll start with an introductory example using linear regression, which you\'ve seen before, to act as a segue into logistic regression. After that, you\'ll learn about the formal notation of logistic regression models. Then, you\'ll conclude this lesson by looking at a real-world example.\nObjectives\nYou will be able to:\n\nDescribe the need for logistic regression\nInterpret the parameters of a logistic regression model\n\nRecap of the linear regression model\nYou have previously learned about linear regression models. In these models, you are trying to fit a linear relationship between two variables. An example is given below. In this example, you want to find a relationship between age and monthly income. It is reasonable to assume that, on average, older people have a higher income than younger people who are newer to the job market and have less experience. A potential relationship could look like the plot below. The monthly income is shown in 1000s of USD.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nnp.random.seed(1234)\n\nage = np.random.uniform(18, 65, 100)\nincome = np.random.normal((age/10), 0.5)\nage = age.reshape(-1,1)\n\nfig = plt.figure(figsize=(8,6))\nfig.suptitle(\'age vs income\', fontsize=16)\nplt.scatter(age, income)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.show()\n\nIn linear regression, you would try to find a relationship between age and monthly income. Conceptually, this means fitting a line that represents the relationship between age and monthly income, as shown below.\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'linear regression\', fontsize=16)\nplt.scatter(age, income)\nplt.plot(age, age/10, c=\'black\')\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.show()\n\nThe idea is that you could use this line to make predictions in the future. In this case, the relationship is modeled as follows: the expected monthly income for someone who is, say, 40 years old, is 3000 (3 on the y-axis). Of course, the actual income will most likely be different, but this indicates what the model predicts as the salary value.\nSo how is this related to logistic regression?\nNow, imagine you get a dataset where no information on exact income is given (after all, people don\'t like to talk about how much they earn!), but you only have information on whether or not they earn more than 4000 USD per month. Starting from the generated data we used before, the new variable income_bin was transformed to 1 when someone\'s income is over 4000 USD, and 0 when the income is less than 4000 USD.\nincome_bin = income > 4\nincome_bin = income_bin.astype(int)  \nprint(income_bin)\n[0 1 0 1 1 0 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0\n 1 0 1 0 0 0 1 1 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0\n 0 1 0 0 0 0 1 1 1 1 1 0 1 1 1 0 0 1 0 1 0 0 1 1 0 1]\n\nHave a look at what happens when you plot this.\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'age vs binary income\', fontsize=16)\nplt.scatter(age, income_bin)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income (> or < 4000)\', fontsize=14)\nplt.show()\n\nYou can already tell that fitting a straight line will not work here. Take a look at what happens when you fit a regression line to these data.\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LinearRegression\n\n# Create linear regression model\nlin_reg = LinearRegression()\nlin_reg.fit(age, income_bin)\n# Store the coefficients\ncoef = lin_reg.coef_\ninterc = lin_reg.intercept_\n# Create the line\nlin_income = (interc + age * coef)\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'linear regression\', fontsize=16)\nplt.scatter(age, income_bin)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.plot(age, lin_income, c=\'black\')\nplt.show()\n\nYou can see that this doesn\'t make a lot of sense. This straight line cannot grasp the true structure of what is going on when using a linear regression model. Now, without going into the mathematical details for now, look at a logistic regression model and fit that to the dataset.\n# Instantiate a Logistic regression model\n# Solver must be specified to avoid warning, see documentation for more information\n# liblinear is recommended for small datasets\n# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\nregr = LogisticRegression(C=1e5, solver=\'liblinear\')\n\n# Fit the model to the training set\nregr.fit(age, income_bin)\nLogisticRegression(C=100000.0, class_weight=None, dual=False,\n                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class=\'warn\', n_jobs=None, penalty=\'l2\',\n                   random_state=None, solver=\'liblinear\', tol=0.0001, verbose=0,\n                   warm_start=False)\n\n# Store the coefficients\ncoef = regr.coef_\ninterc = regr.intercept_\n\n# Create the linear predictor\nlin_pred = (age * coef + interc)\n\n# Perform the log transformation\nmod_income = 1 / (1 + np.exp(-lin_pred))\n\n# Sort the numbers to make sure plot looks right\nage_ordered, mod_income_ordered = zip(*sorted(zip(age ,mod_income.ravel()),key=lambda x: x[0]))\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle(\'logistic regression\', fontsize=16)\nplt.scatter(age, income_bin)\nplt.xlabel(\'age\', fontsize=14)\nplt.ylabel(\'monthly income\', fontsize=14)\nplt.plot(age_ordered, mod_income_ordered, c=\'black\')\nplt.show()\n\nThis already looks a lot better! You can see that this function has an S-shape which plateaus to 0 in the left tale and 1 to the right tale. This is exactly what we needed here. Hopefully this example was a good way of showing why logistic regression is useful. Now, it\'s time to dive into the mathematics that make logistic regression possible.\nLogistic regression model formulation\nThe model\nAs you might remember from the linear regression lesson, a linear regression model can be written as:\n$$ \\hat y = \\hat\\beta_0 + \\hat\\beta_1 x_1 + \\hat\\beta_2 x_2 +\\ldots + \\beta_n x_n $$\nWhen there are $n$ predictors $x_1,\\ldots,x_n$ and $n+1$ parameter estimates that are estimated by the model $\\hat\\beta_0, \\hat\\beta_1,\\ldots, \\hat\\beta_n$.  $ \\hat y $ is an estimator for the outcome variable.\nTranslating this model formulation to our example, this boils down to:\n$$ \\text{income} = \\beta_0 + \\beta_1 \\text{age} $$\nWhen you want to apply this to a binary dataset, what you actually want to do is perform a classification of your data in one group versus another one. In our case, we want to classify our observations (the 100 people in our dataset) as good as possible in ""earns more than 4k"" and ""earns less than 4k"". A model will have to guess what the probability is of belonging to one group versus another. And that is exactly what logistic regression models can do!\nEssentially, what happens is, the linear regression is transformed in a way that the outcome takes a value between 0 and 1. This can then be interpreted as a probability (e.g., 0.2 is a probability of 20%). Applied to our example, the expression for a logistic regression model would look like this:\n$$ P(\\text{income} > 4000) = \\displaystyle \\frac{1}{1+e^{-(\\hat \\beta_0+\\hat \\beta_1 \\text{age})}}$$\nNote that the outcome is written as $P(\\text{income} > 4000)$. This means that the output should be interpreted as the probability that the monthly income is over 4000 USD.\nIt is important to note that this is the case because the income variable was relabeled to be equal to 1 when the income is bigger than 4000, and 0 when smaller than 4000. In other words, the outcome variable should be interpreted as the probability of the class label to be equal to 1.\nInterpretation\nAs mentioned before, the probability of an income over 4000 can be calculated using:\n$$ P(\\text{income} > 4000) = \\displaystyle \\frac{1}{1+e^{-(\\hat \\beta_o+\\hat \\beta_1 \\text{age})}}$$\nYou can show that, by multiplying both numerator and denominator by $e^{(\\hat \\beta_0+\\hat \\beta_1 \\text{age})}$\n$$ P(\\text{income} > 4000) = \\displaystyle \\frac{e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}}}{1+e^{\\hat \\beta_o+\\hat \\beta_1 \\text{age}}}$$\nAs a result, you can compute $P(\\text{income} \\leq 4000)$ as:\n$$ P(\\text{income} < 4000) = 1- \\displaystyle \\frac{e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}}}{1+e^{\\hat \\beta_o+\\hat \\beta_1 \\text{age}}}= \\displaystyle \\frac{1}{1+e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}}}$$\nThis doesn\'t seem to be very spectacular, but combining these two results leads to an easy interpretation of the model parameters, triggered by the odds\n$$ \\dfrac{P(\\text{income} > 4000)}{P(\\text{income} < 4000)} = e^{\\hat \\beta_0+\\hat \\beta_1 \\text{age}} $$\nThis expression can be interpreted as the odds in favor of an income greater than 4000 USD.\nThis result, in combination with mathematical properties of exponential functions, leads to the fact that, applied to our example:\nif age goes up by 1, the odds are multiplied by $e^{\\beta_1}$\nIn our example, there is a positive relationship between age and income, this will lead a positive $\\beta_1 > 0$, so $e^{\\beta_1}>1$, and the odds will increase as age increases.\nA real-world example\nNow you will apply these concepts to a real-world dataset:\nimport statsmodels as sm\nimport sklearn.preprocessing as preprocessing\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom scipy import stats\nsalaries = pd.read_csv(\'salaries_final.csv\', index_col=0)\nsalaries.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nAge\nEducation\nOccupation\nRelationship\nRace\nSex\nTarget\n\n\n\n\n0\n39\nBachelors\nAdm-clerical\nNot-in-family\nWhite\nMale\n<=50K\n\n\n1\n50\nBachelors\nExec-managerial\nHusband\nWhite\nMale\n<=50K\n\n\n2\n38\nHS-grad\nHandlers-cleaners\nNot-in-family\nWhite\nMale\n<=50K\n\n\n3\n53\n11th\nHandlers-cleaners\nHusband\nBlack\nMale\n<=50K\n\n\n4\n28\nBachelors\nProf-specialty\nWife\nBlack\nFemale\n<=50K\n\n\n\n\nFor this example, you will fit a logistic regression model to Target using Age, Race, and Sex. Since Target, Race, and Sex are categorical, they need to be be converted to a numeric datatype first.\nThe get_dummies() function will only convert object and category datatypes to dummy variables so it is safe to pass Age to get_dummies(). Note that we also pass two additional arguments, drop_first=True and dtype=float. The drop_first=True argument removes the first level for each categorical variable and the dtype=float argument converts the datatype of all the dummy variables to float. The data must be float in order to obtain accurate statistical results from statsmodels.\n# Convert race and sex using get_dummies() \nx_feats = [\'Race\', \'Sex\', \'Age\']\nX = pd.get_dummies(salaries[x_feats], drop_first=True, dtype=float)\n\n# Convert target using get_dummies\ny = pd.get_dummies(salaries[\'Target\'], drop_first=True, dtype=float)\ny = y[\'>50K\']\nimport statsmodels.api as sm\n\n# Create intercept term required for sm.Logit, see documentation for more information\nX = sm.add_constant(X)\n\n# Fit model\nlogit_model = sm.Logit(y, X)\n\n# Get results of the fit\nresult = logit_model.fit()\nOptimization terminated successfully.\n         Current function value: 0.498651\n         Iterations 6\n\n\n//anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n  return ptp(axis=axis, out=out, **kwargs)\n\nresult.summary()\n\nLogit Regression Results\n\nDep. Variable: >50K   No. Observations:    32561\n\n\nModel: Logit   Df Residuals:        32554\n\n\nMethod: MLE   Df Model:                6\n\n\nDate: Wed, 20 Nov 2019   Pseudo R-squ.:      0.09666\n\n\nTime: 14:55:31   Log-Likelihood:      -16237.\n\n\nconverged: True   LL-Null:             -17974.\n\n\nCovariance Type: nonrobust   LLR p-value:         0.000\n\n\n\n\n coef std err z P>|z| [0.025 0.975]\n\n\nconst    -4.4248     0.189   -23.380  0.000    -4.796    -4.054\n\n\nAge     0.0387     0.001    38.530  0.000     0.037     0.041\n\n\nRace_Asian-Pac-Islander     0.9991     0.197     5.079  0.000     0.614     1.385\n\n\nRace_Black     0.1812     0.191     0.950  0.342    -0.193     0.555\n\n\nRace_Other    -0.1143     0.282    -0.406  0.685    -0.667     0.438\n\n\nRace_White     0.8742     0.183     4.782  0.000     0.516     1.232\n\n\nSex_Male     1.2069     0.035    34.380  0.000     1.138     1.276\n\n\nnp.exp(result.params)\nconst                      0.011977\nAge                        1.039480\nRace_Asian-Pac-Islander    2.715861\nRace_Black                 1.198638\nRace_Other                 0.891987\nRace_White                 2.396965\nSex_Male                   3.343142\ndtype: float64\n\nYou can also use scikit-learn to retrieve the parameter estimates. The disadvantage here though is that there are no p-values for your parameter estimates!\nlogreg = LogisticRegression(fit_intercept = False, C = 1e15, solver=\'liblinear\')\nmodel_log = logreg.fit(X, y)\nmodel_log\nLogisticRegression(C=1000000000000000.0, class_weight=None, dual=False,\n                   fit_intercept=False, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class=\'warn\', n_jobs=None, penalty=\'l2\',\n                   random_state=None, solver=\'liblinear\', tol=0.0001, verbose=0,\n                   warm_start=False)\n\nmodel_log.coef_\narray([[-4.38706342,  0.03871011,  0.96178902,  0.14397983, -0.14384057,\n         0.83689457,  1.2067121 ]])\n\nSummary\nIn this lab you built upon your previous knowledge of linear regression and built an intuitive understanding of how this could be adapted for classification. We then demonstrated tools for performing logistic regression. In the upcoming lessons you will continue to investigate logistic regression from other viewpoints.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Ridge and Lasso Regression - Lab\nIntroduction\nIn this lab, you\'ll practice your knowledge of Ridge and Lasso regression!\nObjectives\nIn this lab you will:\n\nUse Lasso and Ridge regression with scikit-learn\nCompare and contrast Lasso, Ridge and non-regularized regression\n\nHousing Prices Data\nLet\'s look at yet another house pricing dataset:\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings(\'ignore\')\n\ndf = pd.read_csv(\'Housing_Prices/train.csv\')\n\nLook at .info() of the data:\n# Your code here\n\n\nFirst, split the data into X (predictor) and y (target) variables\nSplit the data into 75-25 training-test sets. Set the random_state to 10\nRemove all columns of object type from X_train and X_test and assign them to X_train_cont and X_test_cont, respectively\n\n# Create X and y\ny = None\nX = None\n\n# Split data into training and test sets\nX_train, X_test, y_train, y_test = None\n\n# Remove ""object""-type features from X\ncont_features = None\n\n# Remove ""object""-type features from X_train and X_test\nX_train_cont = None\nX_test_cont = None\n\nLet\'s use this data to build a first naive linear regression model\n\nFill the missing values in data using median of the columns (use SimpleImputer)\nFit a linear regression model to this data\nCompute the R-squared and the MSE for both the training and test sets\n\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.impute import SimpleImputer\n\n# Impute missing values with median using SimpleImputer\nimpute = None\nX_train_imputed = None\nX_test_imputed = None\n\n# Fit the model and print R2 and MSE for training and test sets\nlinreg = None\n\n# Print R2 and MSE for training and test sets\n\n\nNormalize your data\n\nNormalize your data using a StandardScalar\nFit a linear regression model to this data\nCompute the R-squared and the MSE for both the training and test sets\n\nfrom sklearn.preprocessing import StandardScaler\n\n# Scale the train and test data\nss = None\nX_train_imputed_scaled = None\nX_test_imputed_scaled = None\n\n# Fit the model\nlinreg_norm = None\n\n\n# Print R2 and MSE for training and test sets\n\n\nInclude categorical variables\nThe above models didn\'t include categorical variables so far, let\'s include them!\n\nInclude all columns of object type from X_train and X_test and assign them to X_train_cat and X_test_cat, respectively\nFill missing values in all these columns with the string \'missing\'\n\n# Create X_cat which contains only the categorical variables\nfeatures_cat = None\nX_train_cat = None\nX_test_cat = None\n\n# Fill missing values with the string \'missing\'\n\n\n\n\nOne-hot encode all these categorical columns using OneHotEncoder\nTransform the training and test DataFrames (X_train_cat) and (X_test_cat)\nRun the given code to convert these transformed features into DataFrames\n\nfrom sklearn.preprocessing import OneHotEncoder\n\n# OneHotEncode categorical variables\nohe = None\n\n# Transform training and test sets\nX_train_ohe = None\nX_test_ohe = None\n\n# Convert these columns into a DataFrame\ncolumns = ohe.get_feature_names(input_features=X_train_cat.columns)\ncat_train_df = pd.DataFrame(X_train_ohe.todense(), columns=columns)\ncat_test_df = pd.DataFrame(X_test_ohe.todense(), columns=columns)\n\n\nCombine X_train_imputed_scaled and cat_train_df into a single DataFrame\nSimilarly, combine X_test_imputed_scaled and cat_test_df into a single DataFrame\n\n# Your code here\nX_train_all = None\nX_test_all = None\n\nNow build a linear regression model using all the features (X_train_all). Also, print the R-squared and the MSE for both the training and test sets.\n# Your code here\n\nNotice the severe overfitting above; our training R-squared is very high, but the test R-squared is negative! Similarly, the scale of the test MSE is orders of magnitude higher than that of the training MSE.\nRidge and Lasso regression\nUse all the data (normalized features and dummy categorical variables, X_train_all) to build two models - one each for Lasso and Ridge regression. Each time, look at R-squared and MSE.\nLasso\nWith default parameter (alpha = 1)\n# Your code here\n\nWith a higher regularization parameter (alpha = 10)\n# Your code here\n\nRidge\nWith default parameter (alpha = 1)\n# Your code here\n\nWith default parameter (alpha = 10)\n# Your code here\n\nCompare the metrics\nWrite your conclusions here:\n\nCompare number of parameter estimates that are (very close to) 0 for Ridge and Lasso\nUse 10**(-10) as an estimate that is very close to 0.\n# Number of Ridge params almost zero\n\n# Number of Lasso params almost zero\n\nprint(len(lasso.coef_))\nprint(sum(abs(lasso.coef_) < 10**(-10))/ len(lasso.coef_))\n\nLasso was very effective to essentially perform variable selection and remove about 25% of the variables from your model!\nPut it all together\nTo bring all of our work together lets take a moment to put all of our preprocessing steps for categorical and continuous variables into one function. This function should take in our features as a dataframe X and target as a Series y and return a training and test DataFrames with all of our preprocessed features along with training and test targets.\ndef preprocess(X, y):\n    \'\'\'Takes in features and target and implements all preprocessing steps for categorical and continuous features returning \n    train and test DataFrames with targets\'\'\'\n    \n    # Train-test split (75-25), set seed to 10\n\n    \n    # Remove ""object""-type features and SalesPrice from X\n\n\n    # Impute missing values with median using SimpleImputer\n\n\n    # Scale the train and test data\n\n\n    # Create X_cat which contains only the categorical variables\n\n\n    # Fill nans with a value indicating that that it is missing\n\n\n    # OneHotEncode Categorical variables\n\n    \n    # Combine categorical and continuous features into the final dataframe\n    \n    return X_train_all, X_test_all, y_train, y_test\n\nGraph the training and test error to find optimal alpha values\nEarlier we tested two values of alpha to see how it effected our MSE and the value of our coefficients. We could continue to guess values of alpha for our Ridge or Lasso regression one at a time to see which values minimize our loss, or we can test a range of values and pick the alpha which minimizes our MSE. Here is an example of how we would do this:\nX_train_all, X_test_all, y_train, y_test = preprocess(X, y)\n\ntrain_mse = []\ntest_mse = []\nalphas = []\n\nfor alpha in np.linspace(0, 200, num=50):\n    lasso = Lasso(alpha=alpha)\n    lasso.fit(X_train_all, y_train)\n    \n    train_preds = lasso.predict(X_train_all)\n    train_mse.append(mean_squared_error(y_train, train_preds))\n    \n    test_preds = lasso.predict(X_test_all)\n    test_mse.append(mean_squared_error(y_test, test_preds))\n    \n    alphas.append(alpha)\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfig, ax = plt.subplots()\nax.plot(alphas, train_mse, label=\'Train\')\nax.plot(alphas, test_mse, label=\'Test\')\nax.set_xlabel(\'Alpha\')\nax.set_ylabel(\'MSE\')\n\n# np.argmin() returns the index of the minimum value in a list\noptimal_alpha = alphas[np.argmin(test_mse)]\n\n# Add a vertical line where the test MSE is minimized\nax.axvline(optimal_alpha, color=\'black\', linestyle=\'--\')\nax.legend();\n\nprint(f\'Optimal Alpha Value: {int(optimal_alpha)}\')\n\nTake a look at this graph of our training and test MSE against alpha. Try to explain to yourself why the shapes of the training and test curves are this way. Make sure to think about what alpha represents and how it relates to overfitting vs underfitting.\nSummary\nWell done! You now know how to build Lasso and Ridge regression models, use them for feature selection and find an optimal value for $\\text{alpha}$.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Logistic Regression in scikit-learn - Lab\nIntroduction\nIn this lab, you are going to fit a logistic regression model to a dataset concerning heart disease. Whether or not a patient has heart disease is indicated in the column labeled 'target'. 1 is for positive for heart disease while 0 indicates no heart disease.\nObjectives\nIn this lab you will:\n\nFit a logistic regression model using scikit-learn\n\nLet's get started!\nRun the following cells that import the necessary functions and import the dataset:\n# Import necessary functions\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\n# Import data\ndf = pd.read_csv('heart.csv')\ndf.head()\nDefine appropriate X and y\nRecall the dataset contains information about whether or not a patient has heart disease and is indicated in the column labeled 'target'. With that, define appropriate X (predictors) and y (target) in order to model whether or not a patient has heart disease.\n# Split the data into target and predictors\ny = None\nX = None\nNormalize the data\nNormalize the data (X) prior to fitting the model.\n# Your code here\nX = None\nX.head()\nTrain- test split\n\nSplit the data into training and test sets\nAssign 25% to the test set\nSet the random_state to 0\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = None\nFit a model\n\nInstantiate LogisticRegression\n\nMake sure you don't include the intercept\nset C to a very large number such as 1e12\nUse the 'liblinear' solver\n\n\nFit the model to the training data\n\n# Instantiate the model\nlogreg = None\n\n# Fit the model\nPredict\nGenerate predictions for the training and test sets.\n# Generate predictions\ny_hat_train = None\ny_hat_test = None\nHow many times was the classifier correct on the training set?\n# Your code here\nHow many times was the classifier correct on the test set?\n# Your code here\nAnalysis\nDescribe how well you think this initial model is performing based on the training and test performance. Within your description, make note of how you evaluated performance as compared to your previous work with regression.\n# Your analysis here\nSummary\nIn this lab, you practiced a standard data science pipeline: importing data, split it into training and test sets, and fit a logistic regression model. In the upcoming labs and lessons, you'll continue to investigate how to analyze and tune these models for various scenarios.\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '65 contributions\n        in the last year', 'description': ['Parallel-implementation-of-Linear-Regression-from-scratch-in-Distributed-Setting\nDataset are used in the project:- KDD Cup 1998 Data Data Set https://archive.ics.uci.edu/ml/datasets/KDD+Cup+1998+Data\n'], 'url_profile': 'https://github.com/NipamNayan', 'info_list': ['Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Regression with CART Trees - Lab\nIntroduction\nIn this lab, we'll make use of what we learned in the previous lesson to build a model for the Petrol Consumption Dataset from Kaggle. This model will be used to predict gasoline consumption for a bunch of examples, based on features about the drivers.\nObjectives\nIn this lab you will:\n\nFit a decision tree regression model with scikit-learn\n\nImport necessary libraries\n#\xa0Import libraries \nimport pandas as pd  \nimport numpy as np  \nfrom sklearn.model_selection import train_test_split \nThe dataset\n\nImport the 'petrol_consumption.csv' dataset\nPrint the first five rows of the data\nPrint the dimensions of the data\n\n# Import the dataset\ndataset = None\n# Print the first five rows\n# Print the dimensions of the data\n\nPrint the summary statistics of all columns in the data:\n\n#\xa0Describe the dataset\nCreate training and test sets\n\nAssign the target column 'Petrol_Consumption' to y\nAssign the remaining independent variables to X\nSplit the data into training and test sets using a 80/20 split\nSet the random state to 42\n\n# Split the data into training and test sets\nX = None\ny = None\nX_train, X_test, y_train, y_test = None\nCreate an instance of CART regressor and fit the data to the model\nAs mentioned earlier, for a regression task we'll use a different sklearn class than we did for the classification task. The class we'll be using here is the DecisionTreeRegressor class, as opposed to the DecisionTreeClassifier from before.\n# Import the DecisionTreeRegressor class \n\n\n# Instantiate and fit a regression tree model to training data \nregressor = None\nMake predictions and calculate the MAE, MSE, and RMSE\nUse the above model to generate predictions on the test set.\nJust as with decision trees for classification, there are several commonly used metrics for evaluating the performance of our model. The most common metrics are:\n\nMean Absolute Error (MAE)\nMean Squared Error (MSE)\nRoot Mean Squared Error (RMSE)\n\nIf these look familiar, it's likely because you have already seen them before -- they are common evaluation metrics for any sort of regression model, and as we can see, regressions performed with decision tree models are no exception!\nSince these are common evaluation metrics, sklearn has functions for each of them that we can use to make our job easier. You'll find these functions inside the metrics module. In the cell below, calculate each of the three evaluation metrics.\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\n# Make predictions on the test set\ny_pred = None\n\n# Evaluate these predictions\nprint('Mean Absolute Error:', None)  \nprint('Mean Squared Error:', None)  \nprint('Root Mean Squared Error:', None)\nLevel Up (Optional)\n\n\nLook at the hyperparameters used in the regression tree, check their value ranges in official doc and try running some optimization by growing a number of trees in a loop\n\n\nUse a dataset that you are familiar with and run tree regression to see if you can interpret the results\n\n\nCheck for outliers, try normalization and see the impact on the output\n\n\nSummary\nIn this lesson, you implemented the architecture to train a tree regressor and predict values for unseen data. You saw that with a vanilla approach, the results were not so great, and thus we must further tune the model (what we described as hyperparameter optimization and pruning, in the case of trees).\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['A Complete Data Science Project Using Multiple Regression - Introduction\nIntroduction\nIn this section, you\'ll get a chance to synthesize your skills and work through the entire Data Science workflow. To start, you\'ll extract appropriate data from a SQL database. From there, you\'ll continue exploring and cleaning your data, modeling the data, and conducting statistical analyses!\nData Science Processes\nYou\'ll take a look at three general frameworks for conducting Data Science processes using the skills you\'ve learned thus far:\n\nCRoss-Industry Standard Process for Data Mining - CRISP-DM\nKnowledge Discovery in Databases - KDD\nObtain Scrub Explore Model iNterpret - OSEMN\n\n\nNote: OSEMN is pronounced ""OH-sum"" and rhymes with ""possum""\n\nFrom there, the lessons follow a similar structure:\nObtaining Data\nYou\'ll review SQL and practice importing data from a relational database using the ETL (Extract, Transform and Load) process.\nScrubbing Data\nFrom there, you\'ll practice cleaning data:\n\nCasting columns to the appropriate data types\nIdentifying and dealing with null values appropriately\nRemoving columns that aren\'t required for modeling\nChecking for and dealing with multicollinearity\nNormalizing the data\n\nExploring Data\nOnce you\'ve the cleaned data, you\'ll then do some further EDA (Exploratory Data Analysis) to check out the distributions of the various columns, examine the descriptive statistics for the dataset, and to create some initial visualizations to better understand the dataset.\nModeling Data\nFinally, you\'ll create a definitive model. This will include fitting an initial regression model, and then conducting statistical analyses of the results. You\'ll take a look at the p-values of the various features and perform some feature selection. You\'ll test for regression assumptions including normality, heteroscedasticity, and independence. From these tests, you\'ll then refine and improve the model, not just for performance, but for interpretability as well.\nSummary\nIn this section, you\'ll conduct end-to-end review of the Data Science process!\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Simple Linear Regression - Lab\nIntroduction\nIn this lab, you'll get some hand-on practice developing a simple linear regression model. You'll also use your model to make a prediction about new data!\nObjectives\nYou will be able to:\n\nPerform a linear regression using self-constructed functions\nInterpret the parameters of a simple linear regression model in relation to what they signify for specific data\n\nLet's get started\nThe best-fit line's slope $\\hat m$ can be calculated as:\n$$\\hat m = \\rho \\frac{S_Y}{S_X}$$\nWith $\\rho$ being the correlation coefficient and ${S_Y}$ and ${S_X}$ being the standard deviation of $x$ and $y$, respectively. It can be shown that this is also equal to:\n$$\\hat m = \\dfrac{\\overline{x}*\\overline{y}-\\overline{xy}}{(\\overline{x})^2-\\overline{x^2}}$$\nYou'll use the latter formula in this lab. First, break down the formula into its parts. To do this, you'll import the required libraries and define some data points to work with. Next, you'll use some pre-created toy data in NumPy arrays. Let's do this for you to give you a head start.\n# import necessary libraries\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nstyle.use('ggplot')\n%matplotlib inline\n\n# Initialize arrays X and Y with given values\n# X = Independent Variable\nX = np.array([1,2,3,4,5,6,8,8,9,10], dtype=np.float64)\n# Y = Dependent Variable\nY = np.array([7,7,8,9,9,10,10,11,11,12], dtype=np.float64)\nCreate a scatter plot of X and Y and comment on the output\n# Scatter plot\n# Your observations about the relationship between X and Y \n\n\n\n#\nWrite a function calc_slope()\nWrite a function calc_slope() that takes in X and Y and calculates the slope using the formula shown above.\n# Write the function to calculate slope as: \n# (mean(x) * mean(y) ‚Äì mean(x*y)) / ( mean (x)^2 ‚Äì mean( x^2))\ndef calc_slope(xs,ys):\n    \n    pass\n\ncalc_slope(X,Y)\n\n# 0.5393518518518512\nGreat, so we have our slope. Next we calculate the intercept.\nAs a reminder, the calculation for the best-fit line's y-intercept is:\n$$\\hat c = \\overline y - \\hat m \\overline x $$\nWrite a function best_fit()\nWrite a function best_fit() that takes in X and Y, calculates the slope and intercept using the formula. The function should return slope and intercept values.\n# use the slope function with intercept formula to return calculate slope and intercept from data points\n\ndef best_fit(xs,ys):\n    \n    pass\n\n# Uncomment below to test your function\n\n#m, c = best_fit(X,Y)\n#m, c\n\n# (0.5393518518518512, 6.379629629629633)\nWe now have a working model with m and c as model parameters. We can create a line for the data points using the calculated slope and intercept:\n\nRecall that $y = mx + c$. We can now use slope and intercept values along with X data points (features) to calculate the Y data points (labels) of the regression line.\n\nWrite a function reg_line()\nWrite a function reg_line() that takes in slope, intercept and X vector and calculates the regression line using $y= mx + c$ for each point in X\ndef reg_line (m, c, xs):\n    \n    pass\n\n# Uncomment below\n#regression_line = reg_line(m,c,X)\nPlot the (x,y) data points and draw the calculated regression line for visual inspection\n# Plot data and regression line\nSo there we have it, our least squares regression line. This is the best fit line and does describe the data pretty well (still not perfect though).\nDescribe your Model Mathematically and in Words\n# Your answer here\n\n\nPredicting new data\nSo, how might you go about actually making a prediction based on this model you just made?\nNow that we have a working model with m and b as model parameters. We can fill in a value of x with these parameters to identify a corresponding value of $\\hat y$ according to our model. Recall the formula:\n$$\\hat y = \\hat mx + \\hat c$$\nLet's try to find a y prediction for a new value of $x = 7$, and plot the new prediction with existing data\nx_new = 7\ny_predicted = None\ny_predicted\n\n# 10.155092592592592\nPlot the prediction with the rest of the data\n# Plot as above and show the predicted value\nYou now know how to create your own models, which is great! Next, you'll find out how to determine the accuracy of your model!\nSummary\nIn this lesson, you learned how to perform linear regression for data that are linearly related. You first calculated the slope and intercept parameters of the regression line that best fit the data. You then used the regression line parameters to predict the value ($\\hat y$-value) of a previously unseen feature ($x$-value).\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['Malaria is a life-threatening mosquito-borne blood disease. It originates from the infected\nAnopheles mosquito that transmits a parasite. This enters the victim‚Äôs blood system and into their liver\nwhere the parasite reproduces. The ending result is the victim getting a high fever that entails shaking\nchills and pain or in worst case scenarios, coma and death. With the rising epidemic of malaria, it has\nbeen a concern within the health care industry on the parasite resistance to antimalaria drugs and the\nclearance rates.\nAccording to the World Health Organization (WHO), an estimated 3.2 billion people are at risk of\nmalaria which has made this mosquito-borne infection a critical public health problem. There was an\nestimated 438,000 people that died from malaria in 2015 and 620,000 in 2017.\nResistance to anti-malarial drugs has led malaria researchers to investigate what covariates\n(parasite and host factors) are associated with resistance. In this regard, investigation of how covariates\nimpact malaria parasites clearance is often performed using a two-stage approach in which the WWARN\nParasite Clearance Estimator or PCE is used to estimate parasite clearance rates and then the estimated\nparasite clearance is regressed on the covariates. However, the recently developed Bayesian Clearance\nEstimator instead leads to more accurate results for hierarchical regression modelling which motivated\nthe authors to implement the method as an R package, called ‚Äúbhrcr‚Äù.\n'], 'url_profile': 'https://github.com/femyba', 'info_list': ['Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Multiple Linear Regression in Statsmodels - Lab\nIntroduction\nIn this lab, you'll practice fitting a multiple linear regression model on the Ames Housing dataset!\nObjectives\nYou will be able to:\n\nDetermine if it is necessary to perform normalization/standardization for a specific model or set of data\nUse standardization/normalization on features of a dataset\nIdentify if it is necessary to perform log transformations on a set of features\nPerform log transformations on different features of a dataset\nUse statsmodels to fit a multiple linear regression model\nEvaluate a linear regression model by using statistical performance metrics pertaining to overall model and specific parameters\n\nThe Ames Housing Data\nUsing the specified continuous and categorical features, preprocess your data to prepare for modeling:\n\nSplit off and one hot encode the categorical features of interest\nLog and scale the selected continuous features\n\nimport pandas as pd\nimport numpy as np\n\names = pd.read_csv('ames.csv')\n\ncontinuous = ['LotArea', '1stFlrSF', 'GrLivArea', 'SalePrice']\ncategoricals = ['BldgType', 'KitchenQual', 'SaleType', 'MSZoning', 'Street', 'Neighborhood']\nContinuous Features\n# Log transform and normalize\nCategorical Features\n# One hot encode categoricals\nCombine Categorical and Continuous Features\n# combine features into a single dataframe called preprocessed\nRun a linear model with SalePrice as the target variable in statsmodels\n# Your code here\nRun the same model in scikit-learn\n# Your code here - Check that the coefficients and intercept are the same as those from Statsmodels\nPredict the house price given the following characteristics (before manipulation!!)\nMake sure to transform your variables as needed!\n\nLotArea: 14977\n1stFlrSF: 1976\nGrLivArea: 1976\nBldgType: 1Fam\nKitchenQual: Gd\nSaleType: New\nMSZoning: RL\nStreet: Pave\nNeighborhood: NridgHt\n\nSummary\nCongratulations! You pre-processed the Ames Housing data using scaling and standardization. You also fitted your first multiple linear regression model on the Ames Housing data using statsmodels and scikit-learn!\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Ridge and Lasso Regression - Lab\nIntroduction\nIn this lab, you\'ll practice your knowledge of Ridge and Lasso regression!\nObjectives\nIn this lab you will:\n\nUse Lasso and Ridge regression with scikit-learn\nCompare and contrast Lasso, Ridge and non-regularized regression\n\nHousing Prices Data\nLet\'s look at yet another house pricing dataset:\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings(\'ignore\')\n\ndf = pd.read_csv(\'Housing_Prices/train.csv\')\nLook at .info() of the data:\n# Your code here\n\nFirst, split the data into X (predictor) and y (target) variables\nSplit the data into 75-25 training-test sets. Set the random_state to 10\nRemove all columns of object type from X_train and X_test and assign them to X_train_cont and X_test_cont, respectively\n\n# Create X and y\ny = None\nX = None\n\n# Split data into training and test sets\nX_train, X_test, y_train, y_test = None\n\n# Remove ""object""-type features from X\ncont_features = None\n\n# Remove ""object""-type features from X_train and X_test\nX_train_cont = None\nX_test_cont = None\nLet\'s use this data to build a first naive linear regression model\n\nFill the missing values in data using median of the columns (use SimpleImputer)\nFit a linear regression model to this data\nCompute the R-squared and the MSE for both the training and test sets\n\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.impute import SimpleImputer\n\n# Impute missing values with median using SimpleImputer\nimpute = None\nX_train_imputed = None\nX_test_imputed = None\n\n# Fit the model and print R2 and MSE for training and test sets\nlinreg = None\n\n# Print R2 and MSE for training and test sets\nNormalize your data\n\nNormalize your data using a StandardScalar\nFit a linear regression model to this data\nCompute the R-squared and the MSE for both the training and test sets\n\nfrom sklearn.preprocessing import StandardScaler\n\n# Scale the train and test data\nss = None\nX_train_imputed_scaled = None\nX_test_imputed_scaled = None\n\n# Fit the model\nlinreg_norm = None\n\n\n# Print R2 and MSE for training and test sets\nInclude categorical variables\nThe above models didn\'t include categorical variables so far, let\'s include them!\n\nInclude all columns of object type from X_train and X_test and assign them to X_train_cat and X_test_cat, respectively\nFill missing values in all these columns with the string \'missing\'\n\n# Create X_cat which contains only the categorical variables\nfeatures_cat = None\nX_train_cat = None\nX_test_cat = None\n\n# Fill missing values with the string \'missing\'\n\n\nOne-hot encode all these categorical columns using OneHotEncoder\nTransform the training and test DataFrames (X_train_cat) and (X_test_cat)\nRun the given code to convert these transformed features into DataFrames\n\nfrom sklearn.preprocessing import OneHotEncoder\n\n# OneHotEncode categorical variables\nohe = None\n\n# Transform training and test sets\nX_train_ohe = None\nX_test_ohe = None\n\n# Convert these columns into a DataFrame\ncolumns = ohe.get_feature_names(input_features=X_train_cat.columns)\ncat_train_df = pd.DataFrame(X_train_ohe.todense(), columns=columns)\ncat_test_df = pd.DataFrame(X_test_ohe.todense(), columns=columns)\n\nCombine X_train_imputed_scaled and cat_train_df into a single DataFrame\nSimilarly, combine X_test_imputed_scaled and cat_test_df into a single DataFrame\n\n# Your code here\nX_train_all = None\nX_test_all = None\nNow build a linear regression model using all the features (X_train_all). Also, print the R-squared and the MSE for both the training and test sets.\n# Your code here\nNotice the severe overfitting above; our training R-squared is very high, but the test R-squared is negative! Similarly, the scale of the test MSE is orders of magnitude higher than that of the training MSE.\nRidge and Lasso regression\nUse all the data (normalized features and dummy categorical variables, X_train_all) to build two models - one each for Lasso and Ridge regression. Each time, look at R-squared and MSE.\nLasso\nWith default parameter (alpha = 1)\n# Your code here\nWith a higher regularization parameter (alpha = 10)\n# Your code here\nRidge\nWith default parameter (alpha = 1)\n# Your code here\nWith default parameter (alpha = 10)\n# Your code here\nCompare the metrics\nWrite your conclusions here:\n\nCompare number of parameter estimates that are (very close to) 0 for Ridge and Lasso\nUse 10**(-10) as an estimate that is very close to 0.\n# Number of Ridge params almost zero\n# Number of Lasso params almost zero\nprint(len(lasso.coef_))\nprint(sum(abs(lasso.coef_) < 10**(-10))/ len(lasso.coef_))\nLasso was very effective to essentially perform variable selection and remove about 25% of the variables from your model!\nPut it all together\nTo bring all of our work together lets take a moment to put all of our preprocessing steps for categorical and continuous variables into one function. This function should take in our features as a dataframe X and target as a Series y and return a training and test DataFrames with all of our preprocessed features along with training and test targets.\ndef preprocess(X, y):\n    \'\'\'Takes in features and target and implements all preprocessing steps for categorical and continuous features returning \n    train and test DataFrames with targets\'\'\'\n    \n    # Train-test split (75-25), set seed to 10\n\n    \n    # Remove ""object""-type features and SalesPrice from X\n\n\n    # Impute missing values with median using SimpleImputer\n\n\n    # Scale the train and test data\n\n\n    # Create X_cat which contains only the categorical variables\n\n\n    # Fill nans with a value indicating that that it is missing\n\n\n    # OneHotEncode Categorical variables\n\n    \n    # Combine categorical and continuous features into the final dataframe\n    \n    return X_train_all, X_test_all, y_train, y_test\nGraph the training and test error to find optimal alpha values\nEarlier we tested two values of alpha to see how it effected our MSE and the value of our coefficients. We could continue to guess values of alpha for our Ridge or Lasso regression one at a time to see which values minimize our loss, or we can test a range of values and pick the alpha which minimizes our MSE. Here is an example of how we would do this:\nX_train_all, X_test_all, y_train, y_test = preprocess(X, y)\n\ntrain_mse = []\ntest_mse = []\nalphas = []\n\nfor alpha in np.linspace(0, 200, num=50):\n    lasso = Lasso(alpha=alpha)\n    lasso.fit(X_train_all, y_train)\n    \n    train_preds = lasso.predict(X_train_all)\n    train_mse.append(mean_squared_error(y_train, train_preds))\n    \n    test_preds = lasso.predict(X_test_all)\n    test_mse.append(mean_squared_error(y_test, test_preds))\n    \n    alphas.append(alpha)\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfig, ax = plt.subplots()\nax.plot(alphas, train_mse, label=\'Train\')\nax.plot(alphas, test_mse, label=\'Test\')\nax.set_xlabel(\'Alpha\')\nax.set_ylabel(\'MSE\')\n\n# np.argmin() returns the index of the minimum value in a list\noptimal_alpha = alphas[np.argmin(test_mse)]\n\n# Add a vertical line where the test MSE is minimized\nax.axvline(optimal_alpha, color=\'black\', linestyle=\'--\')\nax.legend();\n\nprint(f\'Optimal Alpha Value: {int(optimal_alpha)}\')\nTake a look at this graph of our training and test MSE against alpha. Try to explain to yourself why the shapes of the training and test curves are this way. Make sure to think about what alpha represents and how it relates to overfitting vs underfitting.\nSummary\nWell done! You now know how to build Lasso and Ridge regression models, use them for feature selection and find an optimal value for $\\text{alpha}$.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', 'Updated Mar 9, 2020', 'Jupyter Notebook', 'Updated Dec 24, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020']}"
"{'location': 'Winnipeg', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': ['AirPollutionPredictionRegression_MyoArmbandKeyboardNN\nTO RUN AIR POLLUTION PREDICTION:\npython air_pollution_prediction.py\nThree functions to help with regression.\nPolynomial regression, linear regression for categorial data, linear regression for normal data.\nYOu would have to change what portion of the data you want to train with. I left comments\nTO RUN MYO ARMBAND KEYBOARD CLASSIFICATION:\npython CNN.py\n\nIf you want to process multiple types of data, separate them with commas (without space). For example, if you want to process both accelerometer and gyro, type in: ""accelerometer,gyro"", when asked for data types for multiple axes case.\n\n'], 'url_profile': 'https://github.com/olayinkade', 'info_list': ['Python', 'Updated Mar 15, 2020', '1', 'R', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', '1', 'Python', 'Updated Feb 9, 2021', '2', 'HTML', 'Updated Nov 15, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Updated Dec 29, 2020', 'Python', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'Jupyter Notebook', 'Updated Sep 23, 2020']}","{'location': 'Boston', 'stats_list': [], 'contributions': '232 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Jayesh88', 'info_list': ['Python', 'Updated Mar 15, 2020', '1', 'R', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', '1', 'Python', 'Updated Feb 9, 2021', '2', 'HTML', 'Updated Nov 15, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Updated Dec 29, 2020', 'Python', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'Jupyter Notebook', 'Updated Sep 23, 2020']}","{'location': 'Dallas', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/rupalshrivastava', 'info_list': ['Python', 'Updated Mar 15, 2020', '1', 'R', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', '1', 'Python', 'Updated Feb 9, 2021', '2', 'HTML', 'Updated Nov 15, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Updated Dec 29, 2020', 'Python', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'Jupyter Notebook', 'Updated Sep 23, 2020']}","{'location': 'Mumbai, India', 'stats_list': [], 'contributions': '378 contributions\n        in the last year', 'description': [""SmartPass\nA Smarter Approach towards Password Profiling\nAbstract\nIn today's world of inter-connectivity and vast ocean of services on the Internet, Security seems to be one of the important factor. Passwords have always been the most primitive security and the first line of defense against any unauthorized access. Over the years, with the advancement in computational power and new smarter ways for password-based breaches, passwords need to be stronger than ever.\nThe determination of Strength of a passwords has always been a topic of discussion for many Security Scientist and experts. The most widely used approach today is the Rule-Based Approach. A password that is able to satisfy a set of rules, such as length, special characters and numbers is considered to be a strong password\nAbout the Project\nIn this project we decided to to study the various passwords structures and determine a new way of password profiling using Machine Learning Tools.\nOur objective was to study these passwords and isolate attributes that play an important role in determining the strength of the passwords and use these attributes to profile passwords and determine their strength and Probablity of being Breached.\nFEATURES\n\nA Smarter Way\nLatest Dataset of Breached Passwords\nData-Driven Approach\n2 Approaches\n\nBased on Breached Passwords (Clustering)\nBased on Breached and UnBreached Passwords (Regression)\n\n\nReal-time Profiling\nCould be used Individually or as second Layer of ensuring password Strength\n\n\n\n\nRule-Based Approach\nSmart Approach\n\n\n\n\nA set of Regex rules are defined in order to force the user into creating Complex Passwords\nThere are no fixed Defined Rules\n\n\nThese Rules are generic in nature\nThe Rules are auto generated and data dependent\n\n\nThey increase the probability of Randomness in the password\nThey leave the creation of the password to the users creativity\n\n\nThey can be used as guidelines to a malicious attacker\nThere are no fixed guidelines that impact the creation of the password\n\n\nThe Rules are fixed and have no scope of updating or scalability in future\nThe model can be retrained with fresher and more recent data, to generate better results\n\n\nThe Rules are derived from computational ideal strategies\nThe Rules are generated on the basis of actual human generated passwords\n\n\nGives Probability of Complexity\nGives Probability of being Breached and Complexity\n\n\n\nDevelopers\n\nAgnellus Fernandes \nAnisha Fernandes\nClarice D'silva\nSancia D'cunha\n\nScreenshots\n1.Clustering Based\n\n\n2.Regression Based\n\n\nTools used\n\nPython 3 (Obviously)\nSci-kit Learn Machine Learning Library (Machine Learning Models)\nPycharm (IDE of Choice)\nFlask (Web Based GUI)\nSecLists (Dataset)\n\n""], 'url_profile': 'https://github.com/AgnellusX1', 'info_list': ['Python', 'Updated Mar 15, 2020', '1', 'R', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', '1', 'Python', 'Updated Feb 9, 2021', '2', 'HTML', 'Updated Nov 15, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Updated Dec 29, 2020', 'Python', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'Jupyter Notebook', 'Updated Sep 23, 2020']}","{'location': 'BIT MESRA RANCHI', 'stats_list': [], 'contributions': '272 contributions\n        in the last year', 'description': [""physics_lab_bit_mesra\n\nActually the whole software is around 700MB so it's not possible to upload in github so just open the pycache/new_edit.cpython-36.pyc and download the new_edit.cpython-36.pyc file it's around 2kb file :)\nEnjoyyy!!!!\nRegression lines are very useful for forecasting procedures. The purpose of the line is to describe the interrelation of a dependent variable (Y variable) with one or many independent variables (X variable).\n""], 'url_profile': 'https://github.com/shivamkumar0611', 'info_list': ['Python', 'Updated Mar 15, 2020', '1', 'R', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', '1', 'Python', 'Updated Feb 9, 2021', '2', 'HTML', 'Updated Nov 15, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Updated Dec 29, 2020', 'Python', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'Jupyter Notebook', 'Updated Sep 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Mennaaah', 'info_list': ['Python', 'Updated Mar 15, 2020', '1', 'R', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', '1', 'Python', 'Updated Feb 9, 2021', '2', 'HTML', 'Updated Nov 15, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Updated Dec 29, 2020', 'Python', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'Jupyter Notebook', 'Updated Sep 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '73 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/TathagataBardhan', 'info_list': ['Python', 'Updated Mar 15, 2020', '1', 'R', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', '1', 'Python', 'Updated Feb 9, 2021', '2', 'HTML', 'Updated Nov 15, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Updated Dec 29, 2020', 'Python', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'Jupyter Notebook', 'Updated Sep 23, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['Weather-Determination\nI have created a model which will predict the temperature for the particular day. In order to predict temperature I have used Linear regression technique.\n'], 'url_profile': 'https://github.com/AkshayBhasin26', 'info_list': ['Python', 'Updated Mar 15, 2020', '1', 'R', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', '1', 'Python', 'Updated Feb 9, 2021', '2', 'HTML', 'Updated Nov 15, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Updated Dec 29, 2020', 'Python', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'Jupyter Notebook', 'Updated Sep 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/akshat-01', 'info_list': ['Python', 'Updated Mar 15, 2020', '1', 'R', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', '1', 'Python', 'Updated Feb 9, 2021', '2', 'HTML', 'Updated Nov 15, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Updated Dec 29, 2020', 'Python', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'Jupyter Notebook', 'Updated Sep 23, 2020']}","{'location': 'Toronto, Canada', 'stats_list': [], 'contributions': '83 contributions\n        in the last year', 'description': ['Employment Equity Study - CIBC Bank\nThis project used self reported data from CIBC bank that was sourced from ESDC (Employment and Social Development Canada).\nThe data was national full-time employee data from 2017.\nData Source: https://equity.esdc.gc.ca/sgiemt-weims/pub/f.4rm2@-eng.jsp?eid=281&cal=2017&id=1\nThe purpose of this project was to observe female representation among senior occupational groups at CIBC.\nThis project:\n\nAnalyzed the connection between gender and other employee statuses (i.e. Indigenous background and disability status) and how this impacted employees achieving specific salary quartiles within various senior level occupational groups.\nProduced an intersectional analysis to study competition among women to achieve various salary quartiles by observing how gender coupled with Indigenous background or disability status impacted earnings potential.\n\nThis labour analysis was done using exploratory data analysis and ordinal logistic regression modeling to predict salary quartile achievement using the R programming language.\nUsing a training and testing data split, different versions of the regression models were created and assessed for performance.\nThe model with the best relative performance was used to create visualizations to show joint effects of gender and Indigenous/disability status.\nThis analysis uses the R programming libraries (i.e. ggplot2) to showcase advanced skills in data manipulation and visualization.\nThis analysis used Microsoft Excel to store the datasets that were used to produce data visualizations on Tableau.\nThe link to the Tableau workbook is: https://public.tableau.com/profile/harry.parameswaran#!/vizhome/CIBC-FTEmployeeAnalysis/GenderDistributionbyOccupation\nThe R script and output using Jupyter Notebooks via the RStudio editor is attached above as ""CIBC FT Employee Analysis.ipynb"".\nR packages used include: ggplot2, dplyr, corrplot, ggpubr, ggthemes, GGally, vcd, car, MASS, ""effects"", jtools and huxtable.\n'], 'url_profile': 'https://github.com/harry-para', 'info_list': ['Python', 'Updated Mar 15, 2020', '1', 'R', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', '1', 'Python', 'Updated Feb 9, 2021', '2', 'HTML', 'Updated Nov 15, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Updated Dec 29, 2020', 'Python', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'Jupyter Notebook', 'Updated Sep 23, 2020']}"
"{'location': 'Ann Arbor', 'stats_list': [], 'contributions': '190 contributions\n        in the last year', 'description': [""NHANES\nThis repository stores the Python codes (partial) for the course project National Health and Nutrition Examination Survey Actigraphy Singular Spectrum and Regression Analysis\nDataset:\nFor each year between 1999 and 2015, the NHANES data are stored in multiple separate files, all of which are in XPT (SAS transport/export) format. Nicely, both Python and R are compatibe to read this SAS data format.\nWithin each wave, there is one file for each group of attributes. Those separate files collaboratively capture various aspects of the health issues of the U.S. population and purport for cross-sectional study. Within a wave, the ‚Äúsequence number‚Äù variable SEQN uniquely identifies one person (primary key). Different files from the same wave can be linked together by joining the unique SEQN values.\nStudy:\nThis project examines how individual's risk factors and demographics affect actigraphy. We propose statistical models which take actigraphy as the dependent variable, and risk factors (Diastolic Blood Pressure, Systolic Blood Pressure, High-density Lipoprotein, Low-density Lipoprotein, Triglycerides) and demographics (Non-smoke, age, height, weight, ethnicity) as independent variables.\nDocumentation:\nThe detailed NHANES data and documentation are available at https://wwwn.cdc.gov/nchs/nhanes/Default.aspx\n""], 'url_profile': 'https://github.com/son520804', 'info_list': ['Python', 'Updated Mar 9, 2020', '1', 'HTML', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'R', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', '1', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jul 23, 2020', 'Updated Mar 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '65 contributions\n        in the last year', 'description': ['""Diabetes predictor""\nThis is a small project which takes input such as you BMI, Diabetes pedigree function etc and then analsis them and uses it to predict whether you are a diabteic patient or not.\nThe project is trained using Logistic regression model and i have implemented K cross validation technique to increase the accuracy.\nThe GUI is build using flask.\nIn order to run the project simply open the command prompt and type the following command:\n""python app.py""\nThis command will give you a localhost URL through which you can run the project!\nHappy coding!\n'], 'url_profile': 'https://github.com/jaskirat23', 'info_list': ['Python', 'Updated Mar 9, 2020', '1', 'HTML', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'R', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', '1', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jul 23, 2020', 'Updated Mar 2, 2020']}","{'location': 'New York', 'stats_list': [], 'contributions': '79 contributions\n        in the last year', 'description': ['time-series-homework\nIn this assignment, I used both time series models and a linear regression model predict future movements in the exchange rate of the Japanese Yen against the US Dollar.\nUsing 29 years of USDJPY exchange rate data (from January 1990 through October 2019) as an input, I ran the following models to make predictions on various aspects of the Yen over the next 5 trading days:\n\nARMA Model - to forecast returns\nARIMA Model - to forecast price\nGARCH Model - to forecast volatility\n\nBased on these models, I predicted that returns on the Yen should decrease, its value should fall vs. the dollar, and volatility should rise. As such, I can draw the conclusion that it would be unwise to go long Yen at this time. However, due to a high p-value for each of these models, I cannot fully rely on the predictions they produced.\nFollowing this analysis, I used the same dataset to construct a linear regression model aimed at predicting returns for the yen. I trained the model on returns data from 1990 to 2018. Once trained, I fed this model both the in-sample data used to train the model and out-of-sample from 2019 to ""predict"" historical returns. When comparing the outputs of the in-sample and out-of-sample applications of this model, I found that the root mean squared error for the out-of-sample application was actually lower than for the in-sample application, giving me confidence that this model could be used to make somewhat accurate predictions on data it is not familiar with.\n'], 'url_profile': 'https://github.com/bwacker1', 'info_list': ['Python', 'Updated Mar 9, 2020', '1', 'HTML', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'R', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', '1', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jul 23, 2020', 'Updated Mar 2, 2020']}","{'location': 'Chicago, IL', 'stats_list': [], 'contributions': '205 contributions\n        in the last year', 'description': ['Drug_Use_Classification_LogReg_Blog\nLogistic Regression used to predict Drug Use. This content was used for a blog post and later turned into a full project.\nThis notebook uses data from a study done about drug consumption and logistic regression to predict whether or not an individual has used a certain drug given certain demographic and personality traits about the individual.\nUPDATE: The work here only utilizes logistic regression and is used for a blog, however this problem was turned into a full scale project using other types of classifiers. This work can be found in this repository\nData from: UCI Machine Learning Repository\nMain Python File:\ndrug_consumption_blog.ipynb\nBlog hosted:\nBlog\n'], 'url_profile': 'https://github.com/BenGeissel', 'info_list': ['Python', 'Updated Mar 9, 2020', '1', 'HTML', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'R', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', '1', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jul 23, 2020', 'Updated Mar 2, 2020']}","{'location': 'Atlanta, GA', 'stats_list': [], 'contributions': '35 contributions\n        in the last year', 'description': ['Predicting-House-Prices\nPredicted house prices on Kaggle dataset and achieved a MAPE score of 0.08044 using Multivariate Analysis, XGBoost, Random Forest Regressor in python\n'], 'url_profile': 'https://github.com/suprajaravipati', 'info_list': ['Python', 'Updated Mar 9, 2020', '1', 'HTML', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'R', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', '1', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jul 23, 2020', 'Updated Mar 2, 2020']}","{'location': 'The Netherlands', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Rasul-89', 'info_list': ['Python', 'Updated Mar 9, 2020', '1', 'HTML', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'R', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', '1', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jul 23, 2020', 'Updated Mar 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['Prediction-of-defaulting-customers-in-famous-Bank-Data-set-using-python.\nUsed Python for predicting defaulting customers in Bank given The description of the dataset ‚Ä¢ http://archive.ics.uci.edu/ml/datasets/Bank+Marketing\nUse the following estimation models for predicting customer\nsubscription (y in the table).\n‚Ä¢ Linear model ‚Ä¢ Logit model ‚Ä¢ Decision tree ‚Ä¢ KNN ‚Ä¢ SVM\n'], 'url_profile': 'https://github.com/vikalpmehta', 'info_list': ['Python', 'Updated Mar 9, 2020', '1', 'HTML', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'R', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', '1', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jul 23, 2020', 'Updated Mar 2, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['Property_Price_Prediction\nThere are a number of factors which determine property prices, some are logical, based on economic theories and population density and some are based on more intangible factors, like availability of amenities & necessities, neighborhood, etc.\nIn this we build a linear regression model with stochastic gradient descent to predict the price of the property from the dataset having attributes such as sale type, sale condition etc.\n'], 'url_profile': 'https://github.com/ArchanaKPrasad', 'info_list': ['Python', 'Updated Mar 9, 2020', '1', 'HTML', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'R', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', '1', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jul 23, 2020', 'Updated Mar 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '235 contributions\n        in the last year', 'description': ['Soft or Hard Bank?\nTime Series ‚Äî A Yen for the Future\nZooming in Yen Futures\nWhile Zoom is trending up, we decide to ""zoom"" in Yen Futures. Time series and regression analysis are conducted on historical records of Yen futures starting from 1990. Futures on Yen is interpreted as the amount in Japanese Yen bought by one U.S. dollar. Based on outputs, GARCH(2,1) model is robust for yen futures while its\n1. Time Series Forecasting\nSince we are interested in future movements of yen, a time series analysis is one go-to method. Before we decide on the model, let\'s take a look at the yen daily futures settlement prices in the past three decades below:\n\nIn the graph above, we see cyclical movement with waves peak in 1996, 2000, 2006, 2012, 2017. Two top ones took place in 2012 and 1996. It suggests that Yen depreciated the most against US dollars in those years.\nFurthermore, there appears to be a long-term strengthening of US Dollar against Yen. The depreciation of Yen could serve to improve the balance of trade in Japan. There do seem to be some more medium consistent trends over one, three or five years. However, on a daily basis, there are a lot of short-term ups and downs.\nAn Ad-Fuller test on the settlement prices of Yen concludes there is trend over time as its p-value is 0.138, greater than the 10% probability to be statistically significant. We failed to reject the null hypothesis that the yen futures are non-stationary.\nResults from ACF and PACF\n\nAccording to the autocorrelation graph, potentially all lags need to be included in the model as all of them appear to be significant, i.e. outside the shaded zone around the x-axis for time.\nBased on the figure of partial autocorrelation, on the other hand, only the first lag is helpful to be included in the time-series model. In other words, most of the trends can be explained by the first lag.\n\nIt is possible for the second lag to be included as well since it is on the boarder of the shaded zone indicating statistical significance.\n\n\n\n\n\nHodrick-Prescott Filter\nNext, we decompose the daily Yen settlement prices into a trend and noice using Hodrick-Prescott Filter.\nSmoothing with the filter and plotting the resulting trend against the actual futures settlements. Short term fluctuations would represent profitable trading opportunities. For example, when the orange line for trend runs above the blue actual settlement, it could signal that Yen is temporarily more undervalued than it should be. A US dollar is predicted to trade for more yen than the actual quote. Therefore, we\'d buy or long Yen short-term.\n\nNoises from filter deviates around a mean of zero as follows:\n\nARMA Forecast\nShall we long or short yen?\nIn order to answer the question, an ARIMA(2,1) model is built. Returns are calculated by taking the daily percent changes on quotes of yen futures settlement.\n\nYt = beta1*(Y_t-1) + beta2*(Y_t-2) + alpha1*(e_t-1)\n\nSince p-values for AR(1), AR(2), MA(1) are 0.421, 0.810, 0.921, 0.818 respectively, none of those are less than 0.05. The model is not statistically significant enough to be a good fit for returns on yen futures based on the data.\nA five-day forecast on returns of yen futures settlement prices shows decreasing positive returns on settlement quotes. One USD is predicted to exchange for more yen over the next five days. The rate of increase would drop from 1.2 basis points on the next day to the lowest of 0.5 basis points on the second day then bouncing back to a range of 0.63 to 0.68 basis point in the following three days. Hence, we should buy or long yen futures in order to lock up the price for exchange in a future point in time.\n\nARIMA Forecast\nSince our ARMA(2,1) model on returns is not convincing due to insignificant p-values on coefficients of model terms, we decided to put together an ARIMA(5,1,1) model on yen settlement prices with AR(5), DIFF(1) and MA(1). The idea is supported by the partitial autocorrelation plot that suggests possibility of a three or five-day lagging period.\nThe output shows that the settlement price today is positively correlated with that of the previous couple of days\' because coefficients of AR(1) and AR(2) are positive. On the other hand, however, yen settlements three to five days before contribute negatively to its price today. Moreover, the moving average error term is moving in oppositive directions to the price today.\nThe p-values for all coefficients are greater than 0.3. Therefore, the ARIMA(5,1,1) model on raw Yen settlement prices is not statistically significant. Thus, it is not a good fit based on historical performances. We would not feel too confident in trading based on this result.\nThe figure below shows five-day forecast of yen futures settlement prices based on ARIMA(5,1,1) model. As the price is expected to rise, it is an buying opportunity now to lock up the price to purchase yen in the future. We see the price curve rise at a decreasing speed that is consistent with our conclusion on the ARMA(2,1) model for returns on yen futures. Potential hedging strategy involves purchasing yen futures, long USD and short JPY.\n\nGARCH Forecast on Volatilities\nA GARCH(2,1) model on yen futures returns shows robust output on covariance estimator. It infers heteroskedasticity on returns for yen futures. There are statistically significant alpha1 and beta1 terms. The model implies that 3.8% of today\'s volativity gets positively passed onto from yesterday. Besides, yesterday\'s moving average could explain 95.4% of today\'s.\nConclusion on GARCH(2,1) on Yen Futures Returns\n\n\nAfter taking heteroskedasticity into account, the GARCH(2,1) on returns of Yen futures settlement prices is a better model than ARMA(2,1) on returns and ARIMA(5,1,1) on settlement prices as the coefficient of baseline variance term (omega), coefficient for moving averages (beta), how much previous\' day\'s volatility add to today\'s (alpha1) are statistically significant. Their p-values are less than 0.025.\n\nOmega is close to zero as we initialized it to be. The variance is zero if there were no information from the past variances being passed onto today.\nbeta1 is the moving average term of this GARCH(2,1) model. Beta1=0.9536 means 95.36% of today\' returns can be explained by that of the last settlement date on Yen futures. This term should be less than 1.\nalpha1 suggests 3.8% of the previous day\'s volatility would be passed onto today\'s volatility\nalpha2 is statistically insignificant so the second to the last day\'s volatility contribute little to that of today\'s.\n\n\n\nNote: to ensure staionarity, the sum of alphas and beta(s) are less than one.\n\n\nAs shown below, the plots of residuals and annualized conditional volatility on the GARCH(2,1) model sugguests that volatilities of returns on yen futures have become calmer over the past thirty years.\n\nA five-day forecast for JPY futures returns suggests an upward sloping staight line. The figure below displays our conclusions from the output statistics. Volatilities on returns grow higher over time. The constant rate of increase is passed on from volatility of the returns from the previous day. Over the five-day window, volatility on yen futures returns would increase by 0.17% to 7.6%.\nBecause GARCH output is statistically robust, we should be more confident trading based on information implied by this GARCH(2,1) model.\n\nBased on your time series analysis, would you buy the yen now?\n\nAccording to the graph above on Garch(2,1) forecast, the returns on yen futures is expected to increase. That means we should buy Yen futures now and lock up the price to purchase Yen on a certain date in the future. It is supported by the forecast from ARIMA(5,1,1) model that the raw settlement price of Yen is expected to rise in the next five days, meaning one dollar can purchase more Yen in the next week.\n\nIs the risk of the yen expected to increase or decrease?\n\nBased on Garch(2,1), the risk of Yen is expected to rise in the next five days while staying relatively quiet for a period of time in the long run. The volatility will keep quiet as the sum of beta1 and alpha1 is 0.99 < 1. If it were one, we have a random walk. It is consistent with the plots on residuals and conditional volatilities on Yen futures returns above.\n\nBased on the model evaluation, would you feel confident in using these models for trading?\n\n\nI would trade based on Garch(2,1) model because of its statistically significant omega, beta1 and apha1 coefficients. Besides, it has the lowest AIC and BIC scores out of the three models.\n\nIn comparison, I am not comfortable trading based on ARCH(2,1) and ARIMA(5,1,1) models as none of the parameters of the models are statistically significant for the training data.\n\n\n\nBased on the following seasonal decomposition, it is observed that Yen settlement price spike during the second and third quarters of a year. Therefore, seasonality will be taken into consideration for trading as well.\n\nSince the last record date was November 15, 2019, it may not be as profitable choosing to hold the Yen futures contracts for too long even though the Garch(2,1) forcasts an increase in returns.\n\n\n\nFurther Analysis: Do people need more JPY for the summer?\nA possible direction to identify a better model is to look for underlying seasonality in the data. A seasonal decomposition is plotted below on average monthly yen settlement prices.\n\nFrom the decomposed plot on seasonal effects, there is a cyclical movement with settlement prices. Quotes increase starting from the second season to the third season of a year approximately. Decreases in settlement prices occur roughly during the fourth season of a year and last into the first season of the following year.\nWe will further explore seasonal effects in the linear regression analysis below.\n2. Linear Regression Modeling\nWe explore seasonal effects by setting lagged JPY futures returns to daily, weekly, monthly and seasonal averages. A series of models are propsed using lagging periods by day, month, season, windows of 26 weeks, half a year and multiple months. Dummy variables are created for analysis of weekly, monthly and seasonal effects. Rolling linear regressions are performed to make the model structures more rigorous. Results from output statistics are compared to conclude the model that best captures the features of underlying data.\nThe models are fitted using training period from 1990 to 2017. The testing data are based on historical yen futures returns from 2018 to 2019.\nRegression models are named based on their period of lags, dummy variables and how lagged returns are calculated. Statistics that  measures robustness of linear models are:\n\nRoot Mean Squred Errors (RMSE)\nR-squared (R2)\n\nRule of Thumb: For models of good fit, we are looking for those with lower RMSEs and higher R2s.\nThe following is a table of statistics on linear regression models that were performed using Sklearn in python:\n\n\n\nModel\nOut-of-Sample RMSE\nOut-of-Sample R2\nIn-Sample RMSE\nIn-Sample R2\n\n\n\n\nDaily\n0.4137\n0.0010\n0.7059\n0.0001\n\n\nDaily_Week_Dummies\n0.4160\n-0.0105\n0.7026\n0.0094\n\n\nDaily_DayofWeek_Dummies\n0.4159\n-0.010\n0.7053\n0.0019\n\n\nDaily_Month_Dummies\n0.4152\n-0.0064\n0.7055\n0.0015\n\n\nDaily_Season_Dummies\n0.4142\n-0.0016\n0.7059\n0.0004\n\n\nDaily_Summer_Winter_Dummies\n0.4142\n0.0001\n0.7059\n0.0003\n\n\nDaily_Fall_Dummies\n0.4139\n-0.0003\n0.7059\n0.0003\n\n\nDaily_Rolling_36_Weeks\n0.6961\n-0.0031\nNA\nNA\n\n\nDaily_Rolling_26_Weeks\n0.7000\n-0.0032\nNA\nNA\n\n\nMonthly_Avg\n0.0888\n-0.0085\n0.1495\n0.0028\n\n\nMonthly_Rolling_9_Months\n0.1714\n-0.3794\nNA\nNA\n\n\nSeasonal_Avg\n0.0675\n0.0328\n0.0910\n0.0136\n\n\nSeasonal_Rolling_3_Seasons\n0.1286\n-1.0262\nNA\nNA\n\n\nSeasonal_Rolling_7_Seasons\n0.1052\n-0.3315\nNA\nNA\n\n\n\nPlease see conclusions and plots for the above models as follows:\nDaily\nConclusion: The root MSEs are 0.4137 for out-of-sample performance and 0.7059 for in-sample performance while R-squares are close to zero, 0.01 and 0.001 respectively, we conclude that the above OLS regression is not a statistically enough to represent the data provided.\n\n\n\n\nDaily_Week_Dummies\nConclusion: This OLS linear regression model is not a statistically enough to represent the data provided based on both in and out-of-sample high RMSEs and low R2s.\n\n\nDaily_DayofWeek_Dummies\nConclusion: This OLS linear regression model is not a statistically enough to represent the data provided based on both in and out-of-sample high RMSEs and low R2s.\nDaily_Month_Dummies\nConclusion: This OLS linear regression model is not a statistically enough to represent the data provided based on both in and out-of-sample high RMSEs and low R2s.\nDaily_Season_Dummies\nConclusion: This OLS linear regression model is not a statistically enough to represent the data provided based on both in and out-of-sample high RMSEs and low R2s.\nDaily_Summer_Winter_Dummies\nConclusion: This OLS linear regression model is not a statistically enough to represent the data provided based on both in and out-of-sample high RMSEs and low R2s.\nDaily_Fall_Dummies\nConclusion: This OLS linear regression model is not a statistically enough to represent the data provided based on both in and out-of-sample high RMSEs and low R2s.\nDaily_Rolling_36_Weeks\nConclusion: This OLS linear regression model is not a statistically enough to represent the data provided based on both out-of-sample high RMSEs and low R2s.\n\nA rolling training window of 9 months, i.e. 36 weeks, with time frame of 321 in iterations\n\n\n\nDaily_Rolling_26_Weeks\nConclusion: This OLS linear regression model is not a statistically enough to represent the data provided based on both out-of-sample high RMSEs and low R2s.\n\nA rolling training window of 6.5 months, i.e. 26 weeks, with time frame of 1528 in iterations\n\n\n.png)\n\n.png)\nMonthly_Avg\nConclusion: This OLS linear regression is a better model.\n\nAverage monthly returns with one lag\n\n\n\n\n\nMonthly_Rolling_9_Months\nConclusion: This OLS linear regression model is a better model.\n\nAverage monthly returns with rolling period of 9 months and 347 iterations\n\n\n\nSeaonal_Avg\nConclusion: This OLS linear regression is a better model.\n\nAverage seasonal returns with rolling period of 9 months and 347 iterations\n\n\n\n\n\n\n\nSeasonal_Rolling_3_Seasons\nConclusion: This OLS linear regression is a better model.\n\nAverage seasonal returns with rolling period of 3 seasons and iteration over a time frame of 115 seasons\n\n\n\nSeasonal_Rolling_7_Seasons\nConclusion: This OLS linear regression is a better model.\n\nAverage seasonal returns of one lag are tested with prior rolling period of multiple seasons\nA 7-season rolling window provides the lowest RMSEs and highest R2s with iteration time frame of 111 seasons\n\n\n\n\nConclusions:\n\n\nThe best linear regression models are as follows:\n\nFor test statistics\n\nSeasonal_Rolling_3_Seasons\nMonthly_Avg\nSeasonal_Avg\nSeasonal_Rolling_7_Seasons\nMonthly_Rolling_9_Months\n\n\nFor combination of rigorous structure and test statistics, in that order\n\nSeasonal_Rolling_3_Seasons\nSeasonal_Rolling_7_Seasons\nMonthly_Rolling_9_Months\n\n\n\n\n\nOut-of-Sample performance has higher R2s and lower RMSEs for each of the models because they have less data compared to in-sample training\n\nOne year of testing data vs. 27 years of training data\n\n\n\nThe fall seasonal dummy produces output statistics close to those of summer_winter dummies due to possible following reasons:\n\nTravel is most frequent during months of August to October\nSchool starts in the fall\n\n\n\n\nFiles\nTime-Series Forecasting Notebook\nLinear Regression Modeling Notebook\nYen Data CSV File\n\nReferences:\n\nCU Bootcamp GitLab\nhttps://stackoverflow.com/questions/18691084/what-does-1-mean-in-numpy-reshape\nhttps://stackoverflow.com/questions/17679089/pandas-dataframe-groupby-two-columns-and-get-counts\nhttps://stackoverflow.com/questions/44124436/python-datetime-to-season\nhttps://stackoverflow.com/questions/25146121/extracting-just-month-and-year-separately-from-pandas-datetime-column\nhttps://pypi.org/project/arch/4.13/\nhttps://machinelearningmastery.com/develop-arch-and-garch-models-for-time-series-forecasting-in-python/\nhttp://www.blackarbs.com/blog/time-series-analysis-in-python-linear-models-to-garch/11/1/2016#ARMA\n\n'], 'url_profile': 'https://github.com/Ava33343', 'info_list': ['Python', 'Updated Mar 9, 2020', '1', 'HTML', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'R', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', '1', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jul 23, 2020', 'Updated Mar 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/akshit222sharma', 'info_list': ['Python', 'Updated Mar 9, 2020', '1', 'HTML', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Apr 4, 2020', 'R', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', '1', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Apache-2.0 license', 'Updated Jul 23, 2020', 'Updated Mar 2, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '60 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/arshiful-gwu', 'info_list': ['HTML', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 5, 2020', '1', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated May 28, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'C', 'LGPL-3.0 license', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'Python', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020']}","{'location': 'Melbourne', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': ['Predicting-the-Critical-Temperature-of-a-Superconductor\nPredicting the Critical Temperature of a Superconductor\n'], 'url_profile': 'https://github.com/roshanwahane', 'info_list': ['HTML', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 5, 2020', '1', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated May 28, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'C', 'LGPL-3.0 license', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'Python', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': ['project_text_classification\nClassifying a book summary to its respective genres using Multi-label Text Classification with the help of Stemming, SVM, logistic regression and tf-idf Statistic.\n'], 'url_profile': 'https://github.com/arnanta', 'info_list': ['HTML', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 5, 2020', '1', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated May 28, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'C', 'LGPL-3.0 license', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'Python', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020']}","{'location': 'IIT Kharagpur', 'stats_list': [], 'contributions': '222 contributions\n        in the last year', 'description': [""PCA\nBreaking down a n dimension data to m dimension to visualize it in order to find out best fittable function or other regression.\nIn the code of PCA take the input data in dataset and choose the desired output after auto pre-processing in the code.\nIt is tested on 2.24 million 6 dimensional data's analysis for prediction using only 2 dimensions with an efficiency of 90.4% (on prediciting for 10,000 new test data fitting a simple linear using stochastic gradient method) as compared with using all six dimensions produces accuracy of 96.4% on 10K test entires.\n""], 'url_profile': 'https://github.com/kartikpunjabi111', 'info_list': ['HTML', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 5, 2020', '1', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated May 28, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'C', 'LGPL-3.0 license', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'Python', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020']}","{'location': 'Somerville, MA', 'stats_list': [], 'contributions': '45 contributions\n        in the last year', 'description': ['In this project I see how well we can predict the probability of a Recession within 3 years using 10 Yr - 3Mo Treasury Yield Spread and Unemployment Rate as predictors. These two indicators were mentioned in a recent Forbes article (https://www.forbes.com/sites/simonmoore/2020/02/24/what-is-the-best-recession-indicator) as great predictors for a recession and I wanted to take a look for myself. In the Jupyter notebook included here, I source the data from the Federal Reserve API, wrangle it, perform EDA, and fit/test a Logistic Regression and a Gradient Boosting model for this purpose. Click on the US_Recession_Probability.ipynb file to take a look!\n'], 'url_profile': 'https://github.com/rguseynov23', 'info_list': ['HTML', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 5, 2020', '1', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated May 28, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'C', 'LGPL-3.0 license', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'Python', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['Predict house price using regression\nIn this project, i would be doing exploratory analysis and predictive modeling for House price prediction data for King County in US.  It is a regression based prediction.\nThis kaggle dataset holds 2014 and 2015 records of houses sold in King County.\n'], 'url_profile': 'https://github.com/ansks', 'info_list': ['HTML', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 5, 2020', '1', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated May 28, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'C', 'LGPL-3.0 license', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'Python', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020']}","{'location': 'Oslo', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': ['egg_chick_intervals\nContains program code for logistic regression on number of eggs and chicks as a function of predictors, when these numbers can be intervals. The inference tool is Bayesian, using MCMC. Random variables and fixed factorialvariables are allows. These should start with ""rfactor_"" and ""ffactor_"" respectively. The program is called with a csv file and the set of predictors in that file denominated with + or minus the column number, plus for egg variables and minus for chick variables. The file should contain four columns labeled ""egg.start"", ""egg.end"", ""kids.start"" and ""kids.end"" to specify the start and end of the egg/chick intervals. (If start and end is the same, it means the number is fixed). It is assumed that a maximum of eggs is 7, but this should be easy to change in the code. A prior file can be given, containing the predictor number, range and the upper limit of the 95% credibility interval for the multiplication factor that the explanation variable can affect the odds.\nA help text is shown when calling the program without input arguments. This also shows the available options.\nThe program uses the Hydrasub library, found at http://folk.uio.no/hydrasub, but a packed copy of that library is found here. This library will later be moved to Github.\n'], 'url_profile': 'https://github.com/trondreitan', 'info_list': ['HTML', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 5, 2020', '1', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated May 28, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'C', 'LGPL-3.0 license', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'Python', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020']}","{'location': 'Banglore', 'stats_list': [], 'contributions': '45 contributions\n        in the last year', 'description': ['personalloan_modelling_py\nThis is supervised learning problem where we have identified potential loan customers for Thera Bank using classification techniques. Compared models built with Logistic Regression and KNN algorithm in order to select the best performing one based on the confusion matrix, performance matrics and ROC curve.\nKNN algorithm is the best classifer here because bank should be okay if potential customers left unpredicted(negative case) who will not respond to the campaign but should predict the potential customers who will respond to the campaign accurately.\nKNN model is the better classifer as compare to logistic and naive bayes as it has good recall, high AUC and accuracy that will help to predict the positive customers but still we can improve the sensitivity i.e. not to lose out on potential customers who will respond to the loan campaign then our model will perform much better.\nActionable Insights: adjusting the classification threshold to achieve better sensitivity :I As concluded in logistic regression model,we are getting better senstivity at low threshold values.\n'], 'url_profile': 'https://github.com/mbansal21', 'info_list': ['HTML', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 5, 2020', '1', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated May 28, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'C', 'LGPL-3.0 license', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'Python', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020']}","{'location': 'Hyderabad', 'stats_list': [], 'contributions': '118 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/psaikumar48', 'info_list': ['HTML', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 5, 2020', '1', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated May 28, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'C', 'LGPL-3.0 license', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'Python', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['Identifying-potential-customers-for-loans\nIdentified potential loan customers for Thera Bank using classification techniques. Compared models built with Logistic Regression and KNN algorithm in order to select the best performing one.\n'], 'url_profile': 'https://github.com/anchalbhatia', 'info_list': ['HTML', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Python', 'Updated Mar 5, 2020', '1', 'Jupyter Notebook', 'Updated Apr 20, 2020', 'Jupyter Notebook', 'Updated May 28, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', 'C', 'LGPL-3.0 license', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'Python', 'Updated Mar 6, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': [""Student-Performance-Prediction\nA linear regression model created to find the student's final performance based on the attributes provided and correlation with the target label.\nDataset Link\nhttps://archive.ics.uci.edu/ml/datasets/Student+Performance\n""], 'url_profile': 'https://github.com/joshisameer343', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'Jupyter Notebook', 'Updated Nov 7, 2020', 'Updated Mar 2, 2020', '1', 'Python', 'MIT license', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'R', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020']}","{'location': 'Seattle, WA', 'stats_list': [], 'contributions': '351 contributions\n        in the last year', 'description': ['King County Housing Data\nOverview\nThis project takes a look at housing data for King County in the state of Washington. The student notebook follows the OSEMN framework (Obtaining, Scrubbing, Exploring, Modeling, and Interpretting the data) to see what features of a house will help predict the selling price.\nObjectives\nI explored 3 questions in the data:\n\nWhat are the unique features of the most expensive houses?\nHow do houses built after the year 2000 compare to houses built before 2000?\nWhich season of the year is best for buying? Which season of the year is best for selling?\n\nI iterated through the different house features and settled on a model with features that predict the cost of the house.\nResults\nAfter limiting the houses to the affordable range, I was able to create a prediction with a r^2 value of .922.\n\nFiles\nColumn names gives a description of the data listed in each column. Student is the jupyter notebook used for this project. Presentation gives a brief overview of the findings in the project. KC_housing data is the data file.\nBlog\nHere is the link for the blog I wrote while completing this project: https://roweyerboat.github.io/cleaning_geographical_data\nContact Info\nLinkedIn\n'], 'url_profile': 'https://github.com/roweyerboat', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'Jupyter Notebook', 'Updated Nov 7, 2020', 'Updated Mar 2, 2020', '1', 'Python', 'MIT license', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'R', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020']}","{'location': 'United States', 'stats_list': [], 'contributions': '178 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/vinayreddy115', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'Jupyter Notebook', 'Updated Nov 7, 2020', 'Updated Mar 2, 2020', '1', 'Python', 'MIT license', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'R', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '93 contributions\n        in the last year', 'description': ['Prediciton-of-House-Prices-Across-Bangalore\nIn this project we create a website which helps users predict the price of property across Bangalore using various Regression and Classification techniques. For this project I have used Python with Numpy, Pandas, BeautifulSoup, Requests, Regex for WebScrapping, Orange for Data Analysis and Preprocessing, Ensembling of different Regression and Classification models for Forecasting the Property Price and Flask for Building WebFramework.\n'], 'url_profile': 'https://github.com/ronithbinny', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'Jupyter Notebook', 'Updated Nov 7, 2020', 'Updated Mar 2, 2020', '1', 'Python', 'MIT license', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'R', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Regression and Model Validation - Recap\nIntroduction\nIn this section you extended your knowledge of building regression models by adding additional predictive variables and subsequently validating those models using train-test-split and cross validation.\nMultiple Regression\nYou saw a number of techniques and concepts related to regression. This included the idea of using multiple predictors in order to build a stronger estimator. That said, there were caveats to using multiple predictors. For example, multicollinearity between variables should be avoided. One option for features with particularly high correlation is to only use one of these features. This improves model interpretability. In addition, linear regression is also most effective when features are of a similar scale. Typically, feature scaling and normalization are used to achieve this. There are also other data preparation techniques such as creating dummy variables for categorical variables, and transforming non-normal distributions using functions such as logarithms. Finally, in order to validate models it is essential to always partition your dataset such as with train-test splits or k-fold cross validation.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'Jupyter Notebook', 'Updated Nov 7, 2020', 'Updated Mar 2, 2020', '1', 'Python', 'MIT license', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'R', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': ['Understanding-significant-indicators-in-predicting-the-price-of-a-car-using-Linear-Regression\nProblem Statement:\nA Chinese automobile company Geely Auto aspires to enter the US market by setting up their manufacturing unit there and producing cars\nlocally to give competition to their US and European counterparts.      They have contracted an automobile consulting company to\nunderstand the factors on which the pricing of cars depends. Specifically, they want to understand the factors affecting the pricing of\ncars in the American market, since those may be very different from the Chinese market. The company wants to know:  Which variables are\nsignificant in predicting the price of a car How well those variables describe the price of a car Based on various market surveys, the\nconsulting firm has gathered a large dataset of different types of cars across the Americal market.\nBusiness Goal\nYou are required to model the price of cars with the available independent variables.\nIt will be used by the management to understand how exactly the prices vary with the independent variables.\nThey can accordingly manipulate the design of the cars, the business strategy etc. to meet certain price levels.\nFurther, the model will be a good way for management to understand the pricing dynamics of a new market.\n'], 'url_profile': 'https://github.com/bharti2810', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'Jupyter Notebook', 'Updated Nov 7, 2020', 'Updated Mar 2, 2020', '1', 'Python', 'MIT license', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'R', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['Statistical-Analysis-on-Feature-Selection\n'], 'url_profile': 'https://github.com/KennethY319', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'Jupyter Notebook', 'Updated Nov 7, 2020', 'Updated Mar 2, 2020', '1', 'Python', 'MIT license', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'R', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020']}","{'location': 'Boston, MA, United States', 'stats_list': [], 'contributions': '58 contributions\n        in the last year', 'description': ['Black-Friday-Sales-Prediction-and-Analysis-using-Regression-and-Clustering-Techniques\n\nThis project is an attempt to uncover insights on consumer behavior against various products on Black Friday which in turn affects the total purchase made at a store.\nWe used regression techniques like Linear Regression and Random Forest for the prediction of total purchase and K ‚Äì means Clustering to segment the data into a set of homogenous clusters of records for generating the insight of how many people belonged to each city and how that ranked in terms of total purchase.\nThe project is particularly helpful to both stores as well as e-commerce businesses as they can get a better understanding on which products are making the most money with what sectors of the buyers. This in turn can help the retail brands to identify their target audience as well as position and market their products better.\n\n'], 'url_profile': 'https://github.com/darshandurve20', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'Jupyter Notebook', 'Updated Nov 7, 2020', 'Updated Mar 2, 2020', '1', 'Python', 'MIT license', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'R', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Introduction to Linear Regression - Recap\nIntroduction\nThis short lesson summarizes the topics we covered in this section and why they'll be important to you as a data scientist.\nKey Takeaways\nIn this section, the nominal focus was on how to perform a linear regression, but the real value was learning how to think about the application of machine learning models to data sets.\nKey takeaways include:\n\nStatistical learning theory deals with the problem of finding a predictive function based on data\nA loss function calculates how well a given model represents the relationship between data values\nA linear regression is simply a (straight) line of best fit for predicting a continuous value (y = mx + c)\nThe Coefficient of Determination (R Squared) can be used to determine how well a given line fits a given data set\nCertain assumptions must hold true for a least squares linear regression to be useful - linearity, normality and heteroscedasticity\nQ-Q plots can check for normality in residual errors\nThe Jarque-Bera test can be used to test for normality - especially when the number of data points is large\nThe Goldfeld-Quant test can be used to check for homoscedasticity\n\n""], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'Jupyter Notebook', 'Updated Nov 7, 2020', 'Updated Mar 2, 2020', '1', 'Python', 'MIT license', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'R', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['PERFORMANCE-EVALUATION-OF-MINI-BATCH-LINEAR-REGRESSION-ON-CPU-AND-GPU\n'], 'url_profile': 'https://github.com/simonaprudente23', 'info_list': ['1', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'Jupyter Notebook', 'Updated Nov 7, 2020', 'Updated Mar 2, 2020', '1', 'Python', 'MIT license', 'Updated Mar 5, 2020', 'Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'R', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Aug 10, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Multiple Regression and Model Validation - Recap\nIntroduction\nIn this section you extended your knowledge of building regression models by adding additional predictive variables and subsequently validating those models using train-test-split and cross validation.\nMultiple Regression\nYou saw a number of techniques and concepts related to regression. This included the idea of using multiple predictors in order to build a stronger estimator. That said, there were caveats to using multiple predictors. For example, multicollinearity between variables should be avoided. One option for features with particularly high correlation is to only use one of these features. This improves model interpretability. In addition, linear regression is also most effective when features are of a similar scale. Typically, feature scaling and normalization are used to achieve this. There are also other data preparation techniques such as creating dummy variables for categorical variables, and transforming non-normal distributions using functions such as logarithms. Finally, in order to validate models it is essential to always partition your dataset such as with train-test splits or k-fold cross validation.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'Python', 'Updated Oct 7, 2020', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Updated Jun 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/anujlitoriyaa555', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'Python', 'Updated Oct 7, 2020', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Updated Jun 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Introduction to Linear Regression - Introduction\nIntroduction\nIn this section, you\'re going to learn about one of the most basic machine learning models, linear regression! Many of the ideas you learn in this section will be foundational knowledge for more complex machine learning models.\nStatistical Learning Theory\nWe\'ll start this section by exploring Statistical Learning Theory and how dependent and independent variables relate to it. Statistical Learning Theory provides an important framework for understanding machine learning.\nLinear Regression\nIn this section, we\'ll introduce our first machine learning model - linear regression. It\'s really just a fancy way of saying ""(straight) line of best fit"", but it will introduce a number of concepts that will be important as you continue to learn about more sophisticated models.\nCoefficient of Determination\nWe\'re then going to introduce the idea of ""R squared"" as the coefficient of determination to quantify how well a particular line fits a particular data set.\nA Complete Regression\nFrom there we look at calculating a complete linear regression, just using code. We\'ll cover some of the assumptions that must be held for a ""least squares regression"", introduce Ordinary Least Squares in Statsmodels and introduce some tools for diagnosing your linear regression such as Q-Q plots, the Jarque-Bera test for normal distribution of residuals and the Goldfield-Quandt test for heteroscedasticity. We then look at the interpretation of significance and p-value and finish up by doing a regression model of the Boston Housing data set.\nSummary\nCongratulations! You\'ve made it through much of the introductory data and we\'ve finally got enough context to take a look at our first machine learning model, while broadening our experience of both coding and math so we\'ll be able to introduce more sophisticated machine learning models as the course progresses.\n'], 'url_profile': 'https://github.com/learn-co-students', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'Python', 'Updated Oct 7, 2020', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Updated Jun 22, 2020']}","{'location': 'France', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['RFR_BS_IS\nRandom Forest regressor and gradient boosting for predicting Air France close price over fundamentals data\n'], 'url_profile': 'https://github.com/Lucas-BLP', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'Python', 'Updated Oct 7, 2020', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Updated Jun 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '53 contributions\n        in the last year', 'description': ['Zomato_Bangalore\nData visualization and prediction of restaurant rating using random forest regressor on Zomato Bangalore dataset\n'], 'url_profile': 'https://github.com/chaudhary-vivek', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'Python', 'Updated Oct 7, 2020', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Updated Jun 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/JMorsch22', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'Python', 'Updated Oct 7, 2020', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Updated Jun 22, 2020']}","{'location': 'Munich, Germany', 'stats_list': [], 'contributions': '134 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/hollowcodes', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'Python', 'Updated Oct 7, 2020', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Updated Jun 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '53 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/spati-java', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'Python', 'Updated Oct 7, 2020', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Updated Jun 22, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '567 contributions\n        in the last year', 'description': ['Position-Salaries\nThis is a dataset providing information about various level positions of any employee and respective salaries. This is a regression problem to predict salry for any given input position.\n'], 'url_profile': 'https://github.com/Sakshi2k', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'Python', 'Updated Oct 7, 2020', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Updated Jun 22, 2020']}","{'location': 'Boston, MA', 'stats_list': [], 'contributions': '56 contributions\n        in the last year', 'description': ['Predictive-Modelling-Application-to-Bank-Telemarketing\nThis case is based around a real-world dataset about telemarketing calls made by a Portuguese bank. You can find more information about this dataset here:\nhttps://archive.ics.uci.edu/ml/datasets/bank+marketing\nThe bank is interested in a predictive model because it will allow them to call the right customers at the right times. From an analytics perspective, the primary distinguishing feature in this case is that solving a predictive problem is directly useful to the firm.\nPlease hand in both documents and outputs preferably using \u200bRmarkdown\u200b.\nBasic Explanatory Analysis\n\nLoad the data contained in the file data_telebank.csv and name the variable \u200bdta_bank\nIn one sentence, describe variables in each column paying special attention to\na. Type of variable (categorical/numerical) and what are the units (for the numerical only)\nb. For the ones that are numerical study whether they have outliers. There is no definition for what an outlier so we can define an outlier as any observation with a value that is more than 4 times its standard deviation.\nThe variable that will focus our study is y and it indicates whether the household actually decided to join the bank. We will see how we can use the predictive modeling techniques seen in class to improve the efficiency making marketing phone calls.\nCreate a corr-plot using the package corrplot. You will have to install it using the command \u200binstall.packages()\nRun the following command \u200blm(y~.,data=dta_bank)\na. Write the structural equation that R is estimating?\nb. Comment the results.\ni. Best time to perform telemarketing tasks?\nii. Best income groups?\niii. Potential concerns of omitted variable Bias\n\nPredictive Modeling and Tuning\nThis is a predictive modeling exercise and we have seen in class that we always divide the data\nset in 1. 2.3. 4. 5.\n\u200bdta_bank_training\u200b, \u200bdta_bank_validating\u200b, \u200bdta_bank_test\u200b.\nExplain (in sentences) why and how we always do that.\nFrom the point of view of the firm and given that we are running a predictive exercise, is there any variable that should not be included as X? If yes, please drop it.\nExplain the problems of overfitting and underfitting.\nExplain the meaning of the no free lunch theorem.\nFor the following 4 models, write their structural equations and comment:\nlm(y~., data=)\nlm(y~.^2, data=)\na. Which one overfits more?\nb. Which one underfits more?\nc. Is the model that fits the training data the best one that has the best predictive\npower?\nd. Can we use a confusion matrix to analyze the problems a problem of\nunderfitting?\ne. Which data set should we use to run these regressions?\nImproving the predictive power\n\nMake a visualization to inspect the relationship between the Y and each of the X that you have included in the regressions above.\na. Does it look linear?\nUse the other predictive methods seen in class (like NB classifiers or KNN) to check if you can improve the performance.\nDo they make it better? Worse?\n\nCausal Questions\n\nWhen we study causality we always focus on the parameters multiplying the X variables instead of the predictive capacity of the model. We then give a causal interpretation to the estimated coefficients.\na. Explain when in marketing is preferable a causal analysis to a predictive analysis.\nb. In the context of a linear regression, explain the concepts of a biased estimated.\nWhich of the variables could be interesting to analyze from a causal point of view. Give\nexamples.\nFor those variables what would be the potential omitted variables problem?\n\n'], 'url_profile': 'https://github.com/jemmarong0704', 'info_list': ['Jupyter Notebook', 'Updated Jul 11, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Jul 10, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 5, 2020', '1', 'Python', 'Updated Oct 7, 2020', 'Python', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 9, 2020', 'Updated Jun 22, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': [""Fake-Real-News-Classifier\nIn the jupyter notebook we'll provide you with a simple implementation of a machine learning classifier on text data using term frequency for text representation and logistic regression for classification task I hope you will enjoy this.\n""], 'url_profile': 'https://github.com/Houssem96', 'info_list': ['Jupyter Notebook', 'Updated Oct 10, 2020', 'Jupyter Notebook', 'Updated Jun 30, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'MATLAB', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', '1', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Updated Mar 7, 2020']}","{'location': 'Boston, MA', 'stats_list': [], 'contributions': '352 contributions\n        in the last year', 'description': ['Statistics\nThat‚Äôs the magic of revisions ‚Äì every cut is necessary, and every cut hurts, but something new always grows. I took 10 Days statistics challenge in Hackerrank to practice statistics from calculating mean, median, mode to regression.\n'], 'url_profile': 'https://github.com/Chauhanshi', 'info_list': ['Jupyter Notebook', 'Updated Oct 10, 2020', 'Jupyter Notebook', 'Updated Jun 30, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'MATLAB', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', '1', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Updated Mar 7, 2020']}","{'location': 'Bangalore, India', 'stats_list': [], 'contributions': '586 contributions\n        in the last year', 'description': ['Mood_Prediction\nA machine learning project done to analyse the sentiment/mood of demonetization-related tweets. Some models used are SVM, Random Forest Classifier and Logistic Regression. The classification is done as positive, negative or neutral.\n'], 'url_profile': 'https://github.com/SaiKrishna1207', 'info_list': ['Jupyter Notebook', 'Updated Oct 10, 2020', 'Jupyter Notebook', 'Updated Jun 30, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'MATLAB', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', '1', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Updated Mar 7, 2020']}","{'location': 'Thessaloniki', 'stats_list': [], 'contributions': '96 contributions\n        in the last year', 'description': ['Energy-Predictor\nExperimental data used to create regression models of appliances energy use in a low energy building. Applied cros-corellation, partial cros-corellation, autocorrelations, non linear autocorrelations , networks, Granger and Conditional Granger causality and dimensionality reduction\n'], 'url_profile': 'https://github.com/GiorgosTsal', 'info_list': ['Jupyter Notebook', 'Updated Oct 10, 2020', 'Jupyter Notebook', 'Updated Jun 30, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'MATLAB', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', '1', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Updated Mar 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""ai-accelerator-recipe-Bank-Credit-Card-Eligibility\nThe dataset used for this recipe is 'creditcardeligibility.csv'. The recipe helps in performing Exploratory Data Analysis, data cleaning etc. After scaling the data, Logistic Regression is used to predict the target variable\n""], 'url_profile': 'https://github.com/hcl-rbs-ai-accelerator-3', 'info_list': ['Jupyter Notebook', 'Updated Oct 10, 2020', 'Jupyter Notebook', 'Updated Jun 30, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'MATLAB', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', '1', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Updated Mar 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '50 contributions\n        in the last year', 'description': ['Large-Scale-Optimization\nPart 1 :\nImplementation of first order and second order methods including Hessian calculations; all done for multiclass logistic Regression. Gradient Descent, Newton Raphson, Stoichastic Gradient Descent, Minibatch SGD, minbatch GD, SVRG\nFor Hessian calculations and derivation for this setting, please refer to this excellent blog. http://fourier.eng.hmc.edu/e176/lectures/ch7/node14.html\nPart 2:\nImplementation of Subgradients and Proximal gradients. Looked at both of them for data denoising task.\n'], 'url_profile': 'https://github.com/anuragrpatil', 'info_list': ['Jupyter Notebook', 'Updated Oct 10, 2020', 'Jupyter Notebook', 'Updated Jun 30, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'MATLAB', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', '1', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Updated Mar 7, 2020']}","{'location': 'Gandhainagr', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/JuberGandharv', 'info_list': ['Jupyter Notebook', 'Updated Oct 10, 2020', 'Jupyter Notebook', 'Updated Jun 30, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'MATLAB', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', '1', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Updated Mar 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '669 contributions\n        in the last year', 'description': ['Bluff-or-Truth\nThis is a demo project based on Polynomial Regression to predict the Accurate salary of the different post ie. CEO,manager etc. Such that the hiring work become easy.\nThis is a demo project so we have kept our dataset small.\nSo  there is no any need of spliting and labeling the data.\nAll the program has been written in python on jyupyter notebook.\nThere is a small comprision between Multiple linear regression and Polynomial Regresssion for this type of data\nYou can vary the value of DEGREE For better Result\n'], 'url_profile': 'https://github.com/devil-cyber', 'info_list': ['Jupyter Notebook', 'Updated Oct 10, 2020', 'Jupyter Notebook', 'Updated Jun 30, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'MATLAB', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', '1', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Updated Mar 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['COVID-19-Data-Analysis-and-Prediction\nCOVID data analysis using pandas and matplotlib. Prediction is done by using random forest regressor and SVM\n'], 'url_profile': 'https://github.com/kria7', 'info_list': ['Jupyter Notebook', 'Updated Oct 10, 2020', 'Jupyter Notebook', 'Updated Jun 30, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'MATLAB', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', '1', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Updated Mar 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['fuzzy-pancake\nCOVID data analysis using pandas and matplotlib. Prediction is done by using random forest regressor and SVM\n'], 'url_profile': 'https://github.com/kria7', 'info_list': ['Jupyter Notebook', 'Updated Oct 10, 2020', 'Jupyter Notebook', 'Updated Jun 30, 2020', 'Jupyter Notebook', 'Updated Mar 10, 2020', 'MATLAB', 'Updated Apr 9, 2020', 'Jupyter Notebook', 'Updated Mar 17, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', '1', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Updated Mar 7, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '223 contributions\n        in the last year', 'description': ['Automatic RandomForestImputer: Handling missing values with a random forest automatically\nFor supervised and semi-supervised learning\nThis library uses a random forest(regressor or classifier) to replace missing values in a dataset. It tackles:\n\nSamples having missing values in one or more features\nSamples having a missing target value and missing values in one or more features: both of them will be predicted and replaced.\n\nDependencies\n\nPython(version>=3.6)\nNumpy\nPandas\nMatplolib\nSklearn\nTensorflow (version>=2.2.0)\nDataTypeIdentifier\n\nInstructions\n\n\nYou can get the library with pip install MissingValuesHandler\n\n\nImport a dataset\n\n\nThe type of Random Forest is automatically handled: if the target variable is numerical, a RandomForestRegressor is selected and if it is categorical, the algorithm will choose a RandomForestClassifier.\n\n\nClass instantiation: training_resilience is a parameter that lets the algorithm know how many times it must keep striving for convergence when there are still some values that didn\'t converge\n\n\nThe class possesses three important arguments among others:\n\nforbidden_variables_list: variables that don\'t require encoding will be put in that list\nordinal_variables_list: suited for ordinal categorical variables encoding\nn_iterations_for_convergence: checks after n rounds if the predicted values converged. 4 or 5 rounds are usually enough\n\n\n\nSet up the parameters of the random forest except for the criterion since it is also taken care of by the software: it is gini or entropy for a random forest classifier and mse (mean squared error) for a regressor. Set up essential parameters like the number of iterations, the additional trees, the base estimator‚Ä¶\n\n\nThe method train() contains two important arguments among others:\n\nsample_size [0;1[: allows to draw a representative sample from the data(can be used when the dataset is too big). 0 for no sampling\nn_quantiles: allows to draw a representative sample from the data when the target variable is numerical(default value at 0 if the variable is categorical)\n\n\n\nCoding example:\nfrom MissingValuesHandler.missing_data_handler import RandomForestImputer\nfrom os.path import join\nfrom pandas import read_csv\n""""""\n############################################\n############# IMPORT DATA  #################\n############################################\n""""""\ndata = read_csv(join(""data"",""Loan_approval.csv""), sep="","", index_col=False)\n\n""""""\n############################################\n############### RUN TIME ###################\n############################################\n""""""\n#Main object\nrandom_forest_imputer = RandomForestImputer(data=data,\n                                            target_variable_name=""Status"",\n                                            training_resilience=3, \n                                            n_iterations_for_convergence=5,\n                                            forbidden_features_list=[""Credit_History""],\n                                            ordinal_features_list=[])\n\n#Setting the ensemble model parameters: it could be a random forest regressor or classifier\nrandom_forest_imputer.set_ensemble_model_parameters(n_estimators=40, additional_estimators=10)\n\n#Launching training and getting our new dataset\nnew_data = random_forest_imputer.train(sample_size=0.3, \n                                       path_to_save_dataset=join(""data"", ""Loan_approval_no_nan.csv""))\n""""""\n############################################\n########## DATA RETRIEVAL ##################\n############################################\n""""""\nsample_used                         = random_forest_imputer.get_sample()\nfeatures_type_prediction            = random_forest_imputer.get_features_type_predictions()\ntarget_variable_type_prediction     = random_forest_imputer.get_target_variable_type_prediction()\nencoded_features                    = random_forest_imputer.get_encoded_features()\nencoded_target_variable             = random_forest_imputer.get_target_variable_encoded()\nfinal_proximity_matrix              = random_forest_imputer.get_proximity_matrix()\nfinal_distance_matrix               = random_forest_imputer.get_distance_matrix()\nweighted_averages                   = random_forest_imputer.get_nan_features_predictions(option=""all"")\nconvergent_values                   = random_forest_imputer.get_nan_features_predictions(option=""conv"")\ndivergent_values                    = random_forest_imputer.get_nan_features_predictions(option=""div"")\nensemble_model_parameters           = random_forest_imputer.get_ensemble_model_parameters()\nall_target_value_predictions        = random_forest_imputer.get_nan_target_values_predictions(option=""all"")\ntarget_value_predictions            = random_forest_imputer.get_nan_target_values_predictions(option=""one"")\n\n\n""""""\n############################################\n######## WEIGHTED AVERAGES PLOT ############\n############################################\n""""""\nrandom_forest_imputer.create_weighted_averages_plots(directory_path=""graphs"", both_graphs=1)\n\n""""""\n############################################\n######## TARGET VALUE(S) PLOT ##############\n############################################\n""""""\nrandom_forest_imputer.create_target_pred_plot(directory_path=""graphs"")\n\n""""""\n############################################\n##########      MDS PLOT    ################\n############################################\n""""""\nmds_coordinates = random_forest_imputer.get_mds_coordinates(n_dimensions=3, distance_matrix=final_distance_matrix)\nrandom_forest_imputer.show_mds_plot(mds_coordinates, plot_type=""3d"")\n3d Multidimensional Scaling(MDS):\nWe can use the distance matrix to plot the samples and observe how they are related to one another\n\nWe can use the K-means algorithm to cluster the data and analyze the features of every cluster\n\nReferences for the supervised algorithm:\n\n[1]: Leo Breiman‚Äôs website. Random Forests Leo Breiman and Adele Cutler stat.berkeley.edu/~breiman/RandomForests/cc_home.htm\n[2]: John Starmer‚Äôs video on Youtube Channel StatQuest. Random Forests Part 2: Missing data and clustering https://youtu.be/nyxTdL_4Q-Q\n\n'], 'url_profile': 'https://github.com/YA26', 'info_list': ['Python', 'Updated Nov 20, 2020', '2', 'Python', 'MIT license', 'Updated Jun 26, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '93 contributions\n        in the last year', 'description': ['Prediction-Of-Stock-Prices\nIn this project we predict the Stock prices of various Stocks (30+) using various machine learning models such as Support Vector Regressor, decision Tree, Random Forest, CatBoost, XGBoost, LGB etc and evaluate the best performing model.\n'], 'url_profile': 'https://github.com/ronithbinny', 'info_list': ['Python', 'Updated Nov 20, 2020', '2', 'Python', 'MIT license', 'Updated Jun 26, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '27 contributions\n        in the last year', 'description': ['Machine Learning Engineer Nanodegree\nModel Evaluation and Validation\nProject: Predicting Boston Housing Prices\nInstall\nThis project requires Python and the following Python libraries installed:\n\nNumPy\nPandas\nmatplotlib\nscikit-learn\n\nYou will also need to have software installed to run and execute a Jupyter Notebook\nIf you do not have Python installed yet, it is highly recommended that you install the Anaconda distribution of Python, which already has the above packages and more included.\nCode\nTemplate code is provided in the boston_housing.ipynb notebook file. You will also be required to use the included visuals.py Python file and the housing.csv dataset file to complete your work. While some code has already been implemented to get you started, you will need to implement additional functionality when requested to successfully complete the project. Note that the code included in visuals.py is meant to be used out-of-the-box and not intended for students to manipulate. If you are interested in how the visualizations are created in the notebook, please feel free to explore this Python file.\nRun\nIn a terminal or command window, navigate to the top-level project directory boston_housing/ (that contains this README) and run one of the following commands:\nipython notebook boston_housing.ipynb\nor\njupyter notebook boston_housing.ipynb\nThis will open the Jupyter Notebook software and project file in your browser.\nData\nThe modified Boston housing dataset consists of 489 data points, with each datapoint having 3 features. This dataset is a modified version of the Boston Housing dataset found on the UCI Machine Learning Repository.\nFeatures\n\nRM: average number of rooms per dwelling\nLSTAT: percentage of population considered lower status\nPTRATIO: pupil-teacher ratio by town\n\nTarget Variable\n4. MEDV: median value of owner-occupied homes\n'], 'url_profile': 'https://github.com/sgoyal12499', 'info_list': ['Python', 'Updated Nov 20, 2020', '2', 'Python', 'MIT license', 'Updated Jun 26, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020']}","{'location': 'Charlotte, NC', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/anirahul', 'info_list': ['Python', 'Updated Nov 20, 2020', '2', 'Python', 'MIT license', 'Updated Jun 26, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""ai-accelerator-recipe-Loan-Data-\nThis use case helps in determining the status of loan of customers. The dataset used is 'loan_data.csv'. EDA and data preprocessing is done using standard python libraries. Logistic Regression, Decision Tree and Random Forest Classifiers have been used to predict the Loan Status.\n""], 'url_profile': 'https://github.com/hcl-rbs-ai-accelerator-3', 'info_list': ['Python', 'Updated Nov 20, 2020', '2', 'Python', 'MIT license', 'Updated Jun 26, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""ai-accelerator-recipe-german-credit\nThis use case helps in identifying credit risk of customers. The dataset used for this use case is 'german_credit_data.csv'. After initial exploratory data analysis, classification algorithms like Logistic Regression, KNN, SVM and Decision Tree have been used to predit the credit risk.\n""], 'url_profile': 'https://github.com/hcl-rbs-ai-accelerator-3', 'info_list': ['Python', 'Updated Nov 20, 2020', '2', 'Python', 'MIT license', 'Updated Jun 26, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['Predicting-the-term-deposit-subscription\nLeveraged customer information of bank marketing campaigns to predict whether a customer will subscribe to term deposit or not. Different classification algorithms like Decision tree, Logistic Regression were used. Ensemble techniques like Random forest were used to further improve the classification results.\n'], 'url_profile': 'https://github.com/anchalbhatia', 'info_list': ['Python', 'Updated Nov 20, 2020', '2', 'Python', 'MIT license', 'Updated Jun 26, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""ai-accelerator-recipe-credit-card-fraud-detection\nThis use case is used to find out fraudulent credit card transactions. The dataset used for this use case is 'credit_card_fraud.csv'. Standarard libraries like seaborn, matplotlib etc have been used for visualization and EDA. Logistic Regression, Gradient Boosting Classifier and Multinomial Naive Bayes algorithms have been used to predict the fraudulent transactions.\n""], 'url_profile': 'https://github.com/hcl-rbs-ai-accelerator-3', 'info_list': ['Python', 'Updated Nov 20, 2020', '2', 'Python', 'MIT license', 'Updated Jun 26, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Teju101994', 'info_list': ['Python', 'Updated Nov 20, 2020', '2', 'Python', 'MIT license', 'Updated Jun 26, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': [""ai-accelerator-recipe-Churn-Modelling\nThis use case helps to find out customer churn in banking. It uses the 'Churn_Modelling.csv' dataset. It uses standard Python modules. After necessary data cleaning, EDA, feature engineering and data preparation for fitting models, Logistic Regression, SVM and Ensemble models have been used to predict customer churn.\n""], 'url_profile': 'https://github.com/prgupta1705', 'info_list': ['Python', 'Updated Nov 20, 2020', '2', 'Python', 'MIT license', 'Updated Jun 26, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Jupyter Notebook', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated Mar 2, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Logeshlogii', 'info_list': ['Python', 'Updated Mar 4, 2020', 'R', 'Updated Mar 5, 2020', 'Python', 'MIT license', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', '1', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'R', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', '1', 'Python', 'Updated Aug 14, 2020', '1', 'R', 'Updated Jan 6, 2021', 'Jupyter Notebook', 'Updated Mar 6, 2020']}","{'location': 'Philadelphia', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['Screening-Tool-Chronic-Kidney-Disease\nABSTRACT\nChronic kidney disease\u202f(CKD) is a type of\u202fkidney disease\u202fin which there is gradual loss of\u202fkidney function\u202fover a period of months to years. Initially there are generally no symptoms; later, symptoms may include\u202fleg swelling, feeling tired,\u202fvomiting, loss of appetite, and\u202fconfusion. Complications include an increased risk of\u202fheart disease,\u202fhigh blood pressure,\u202fbone disease, and\u202fanemia. CKD can affect almost every body system. Early recognition and intervention are essential to slowing disease progression, maintaining quality of life, and improving outcomes.\nOur study implements logistic regression and develops a model to identify whether a person has CKD. We cited various research papers and consulted various Doctors and implemented logistic regression to measure the model‚Äôs accuracy. We conducted (a) Correlation Analysis, (b) Lasso Regression and Scientific citation for feature Selection. Logistic regression gave a training accuracy of 80.3 % with a validation accuracy of 79.46%. The study also consists of a simple screening tool which most likely indicates the presence of CKD. This study concludes by using predictive model and the screening tool to predict the risk of CKD in 2819 people.\nMETHODOLOGY\nDATA DESCRPTION: The dataset for the case study consists of responses for specifically designed questionnaire from 8819 individuals, aged 20 years or older taken between 1999-2000 and 2001-2002 during a survey across various states in USA.  The dataset is divided into two sets 1. Training set with 6000 observations in which 33 variables along with the status of CKD is provided. 2. Testing set consisting of 2819 observations with same set of variables in which the CKD has to be predicted. Table1 has all the 33 variables given in our dataset.\nMISSING DATA\nMissing data can occur because of nonresponse: no information is provided for one or more items or for a whole unit (""subject""). Some items are more likely to generate a nonresponse than others: for example, items about private subjects such as income.\nOur dataset consists of 8819 responses against 33 attributes (8819 x 33) 291027 individual responses are to be recorded. But only 283285 are recorded and 7742 records are missing (which is about 2.6 % of the data set). Four dummy variables have been created for Race group (Black, White, Hispanic and others).\nIMPUTATION\nMissing data reduces the representativeness of the sample and can therefore distort inferences about the population. The choice of\u202fmethod to impute missing values, largely influences the model‚Äôs predictive ability.\u202fIf the data is missing completely at random then deletion does not add any bias, but it might decrease the power of the analysis by decreasing the sample size. To deal with the missing data here, MICE package has been used with mean imputation so that the overall mean will not be affected.\nVARIABLE SELECTION\nAttribute selection methods are used to identify and remove unneeded, irrelevant and redundant attributes from data that do not contribute to the accuracy of a predictive model or may in fact decrease the accuracy of the model. We have used correlation analysis and cited many research papers online and eliminated following variables: Income, Unmarried, CareSource, Insured, Education, Height, Weight, LDL, Total Cholesterol for the initial selection. Then to further filter out the insignificant variables we have used several approaches.\nANALYTICAL APPROACH\nWe used Lasso regression for feature selection in remaining 24 variables. Based on the lasso model, following 13 variables have higher significance: Age, DBP, HDL, PVD, Activity, Hypertension, Diabetes, Stroke, CVD, Anemia, Racegroup Hispanic.\nSCIENTIFIC APPROACH\nConsidering real life scenario, we cross validated the variables with Nephrologist and modified the above variables and finalized the following 13 variables for our predictive model:  Age, Female, BMI, Dyslipidemia, PVD, Hypertension, Diabetes, Family Diabetes, Stroke, Family CVD, CHF, Anemia and Race Group (Black, White, Hispanic and others).\nCRITERION BASED APPROACH\nThe Akaike Information Criterion (AIC) is an estimator of the relative quality of statistical models for a given set of data. Given a collection of models for the data, AIC estimates the quality of each model, relative to each of the other models. We want to minimize AIC. Larger models will fit better and so have smaller RSS but use more parameters. Thus, the best choice of model will balance fit with model size.\nDifferent Logit models were run on the different combination of variable selected. We used AIC as an estimator on the all the models and picked out the best one with the lowest AIC value.\nPREDICTION MODEL\nThis logit model gave an AIC value of 1478.8. Then to train and test the model we split the 6000 training set data into three sets 1) Main Training Set (4000) 2) Testing (1000) Set 3) Validation Set (1000).\nACCURACY OF THE MODEL\nAfter training the model, we tested it with the Testing set consisting of 1000 responses to check the accuracy of the model and select the threshold for prediction. We generated a for loop on this set to predict CKD for this testing set for various thresholds and compared with the actual CKD values to get confusion matrix, accuracies and corresponding costs.\nTHRESHOLD SELECTION\nWe can convert the probabilities to predictions using what‚Äôs called a threshold value,\u202ft. If the probability of CKD is greater than this threshold value,\u202ft, we predict that person has CKD. But if the probability of CKD is less than the threshold value,\u202ft, then we predict that the person does not have CKD.\nHow to select the value for t: The threshold value,\u202ft, is often selected based on which errors are better. This would imply that\u202ft\u202fwould be best for no errors but it‚Äôs rare to have a model that predicts perfectly. In this model we have selected based on money as well. For taking test it takes 100 dollars for FP and TP we would lose 100$ and for TP the hospital will gain 1000$.\nAUC VALUE: We selected the threshold based on the ROC curve. AUC - ROC curve is a performance measurement for classification problem at various thresholds settings. ROC is a probability curve and AUC represent degree or measure of separability. It tells how much model is capable of distinguishing between classes. Higher the AUC, better the model is at predicting 0s as 0s and 1s as 1s. By analogy, higher the AUC, better the model is at distinguishing between patients with CKD and without CKD. The ROC curve is plotted with TPR against the FPR where TPR is on y-axis and FPR is on the x-axis. The AUC value for the above ROC curve 82 %.\nOur main aim was to reduce the FP(False Positive), so we went with a threshold which gave us less FP at the same time more profit. That threshold came out to be 0.07 with an accuracy of 77.2%.\nFor further validation this logit model was validated with the validation set of 1000. The model gave out an accuracy of 78.2%.\nWith the above threshold we got a cost of $74,200. Running the model on the Validation set further confirmed our model‚Äôs accuracy and AUC were almost similar for both the Testing and Validation sets. So, we went with the threshold of 0.07 and finally ran the model to predict whether a person has CKD on the 2819 Prediction Set.\nLIMITATIONS OF THE MODEL\nThe data set doesn‚Äôt talk about the severity of CKD (Symptoms like CVD develop only at the final stage. So, if a person who is prone to CKD might not have CVD at the initial stage).\nData about main causes of CKD are missing (Protein Urea, IHD, eGFR). Two simple tests can detect CKD urine, albumin and serum, creatinine. We don‚Äôt have these in the data set.\nThe variables selected might not be the best combination as more research work is needed on each variable.\nUnder-representation of certain race may lead bias in the prediction.\nImbalance in the data set -We have less people with CKD. This will lead to bias again.\nThe data set is focused only for people of US state and during a certain time period. So, putting a general Prediction model is very tough.\nThe model was focused on reducing FP and maximizing the profit but the most dangerous one is FN (False Negative).\nCONCLUSION\nThe model can accurately identify patients receiving low-quality care with test set accuracy being equal to 78 % with 13 attributes. In practice, the probabilities returned by the logistic regression model can be used to prioritize patients for intervention.\u202f Any individual‚Äôs risk can be estimated as the probability of that individual with the questions used in the simple screening tool. Further research is needed to simultaneously assess the role of multiple risk factors which were not provided in the case study (as mentioned in the limitation section) to validate this model in other population.\nREFERENCES\nChronic Kidney Disease: Early Education Intervention by Judy Kauffman, MSN, RN, CNN Charlottesville, Virginia ( A DNP Scholarly Project presented to the Graduate Faculty of the University of Virginia in Candidacy for the Degree of Doctor of Nursing Practice School of Nursing University of Virginia May 2017).\nDetection of Chronic Kidney Disease and Selecting Important Predictive Attributes by Asif Salekin and John Stankovic (Department of Computer Science University of Virginia Charlottesville, Virginia).\nAn Introduction to Statistical Learning with Applications in R by Gareth James and Daniela Witten.\nCentre for Disease Control and Prevention ( https://www.cdc.gov/kidneydisease/publications-resources/2019-national-facts.html )\nAfrican Health Sciences https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4915439/)\n'], 'url_profile': 'https://github.com/aishwarya-rt', 'info_list': ['Python', 'Updated Mar 4, 2020', 'R', 'Updated Mar 5, 2020', 'Python', 'MIT license', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', '1', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'R', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', '1', 'Python', 'Updated Aug 14, 2020', '1', 'R', 'Updated Jan 6, 2021', 'Jupyter Notebook', 'Updated Mar 6, 2020']}","{'location': 'The edge of knowing', 'stats_list': [], 'contributions': '906 contributions\n        in the last year', 'description': ['HOA\nSUTD 2020 10.008 Hands-on Activity Code Dump\nThis repo serves as a personal code dump of the script used to generate the answers and the resulting graphs/plots for the Thermal Physics HOA 1 and HOA 3 assignments.\nUsage\nFor the HOA 1 script, add in your Plotly username and API key into the script and run by executing python3 hoa1.py.\nFor the HOA 3 script, just run python3 hoa3.py.\n'], 'url_profile': 'https://github.com/jamestiotio', 'info_list': ['Python', 'Updated Mar 4, 2020', 'R', 'Updated Mar 5, 2020', 'Python', 'MIT license', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', '1', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'R', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', '1', 'Python', 'Updated Aug 14, 2020', '1', 'R', 'Updated Jan 6, 2021', 'Jupyter Notebook', 'Updated Mar 6, 2020']}","{'location': 'Paris,France', 'stats_list': [], 'contributions': '150 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Abonia1', 'info_list': ['Python', 'Updated Mar 4, 2020', 'R', 'Updated Mar 5, 2020', 'Python', 'MIT license', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', '1', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'R', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', '1', 'Python', 'Updated Aug 14, 2020', '1', 'R', 'Updated Jan 6, 2021', 'Jupyter Notebook', 'Updated Mar 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '65 contributions\n        in the last year', 'description': ['GloVe-Word-Embeddings-and-Deep-Learning-for-prediction-of-Mental-Illness\nIt is a Natural Language Processing Project on Reddit tweets. The Task of Anorexia Prediction was performed on unstructured data using Word Embeddings and algorithms such as Multi-Channelled Deep-Convolutional Neural Networks were implemented after preprocessing multiple chunks of data. Other Machine Learning models such as Random Forest, Logistic Regression, AdaBoost etc were also implemented. The data was taken from ‚ÄòeRisk 2018: Early risk prediction on the Internet‚Äô, an annual International Competition. The initial data was in XML format.\n'], 'url_profile': 'https://github.com/NipamNayan', 'info_list': ['Python', 'Updated Mar 4, 2020', 'R', 'Updated Mar 5, 2020', 'Python', 'MIT license', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', '1', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'R', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', '1', 'Python', 'Updated Aug 14, 2020', '1', 'R', 'Updated Jan 6, 2021', 'Jupyter Notebook', 'Updated Mar 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '26 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Nagarjuna485', 'info_list': ['Python', 'Updated Mar 4, 2020', 'R', 'Updated Mar 5, 2020', 'Python', 'MIT license', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', '1', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'R', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', '1', 'Python', 'Updated Aug 14, 2020', '1', 'R', 'Updated Jan 6, 2021', 'Jupyter Notebook', 'Updated Mar 6, 2020']}","{'location': 'Boston, MA', 'stats_list': [], 'contributions': '80 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/relishyeah', 'info_list': ['Python', 'Updated Mar 4, 2020', 'R', 'Updated Mar 5, 2020', 'Python', 'MIT license', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', '1', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'R', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', '1', 'Python', 'Updated Aug 14, 2020', '1', 'R', 'Updated Jan 6, 2021', 'Jupyter Notebook', 'Updated Mar 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '248 contributions\n        in the last year', 'description': ['Heart Disease Prediction with Ensemble Learning\n\n\nA disease is an unnatural medical condition that negatively affects the functional state of an organism and is generally associated with certain signs of illness. As reported by World Health Organization (WHO), Heart Disease and Stroke are the world‚Äôs biggest killers and have remained the leading causes of death globally in the last 15 years.\nIn the direction of predicting heart disease, Machine Learning can present remarkable features that simplify the identification of unseen patterns, eventually providing clinical insights that assist physicians in planning and providing care.\nIn this analysis, the presence of heart disease is predicted by employing Support VectorMachine (SVM), Multinomial Na√Øve Bayes, Logistic Regression (LR), Decision Tree (DT) & Random Forest (RF), Ensemble combination rules i.e., Majority Voting & Weighted Average Voting and Ensemble classifiers i.e., Bagging, Adaptive Boosting & Gradient Boosting. Parameters such as Accuracy, Precision, Recall and F1-score were estimated to analyze the performance and a comparative study of these classifiers was carried out.\nPython Libraries\nPython libraries are a collection of functions and methods that allows us to perform many actions without writing the code.\nNumPy: NumPy is a very popular python library for large multi-dimensional array and matrix processing, with the help of a large collection of high-level mathematical functions. It is very useful for fundamental scientific computations in Machine Learning.\nPandas: Pandas is a popular Python library for data analysis. Pandas is developed specifically for data extraction and preparation.\nMatplotlib: Matpoltlib is a very popular Python library for data visualization. It provides various kinds of graphs and plots for data visualization, viz., histogram, error charts, bar chats, etc.\nScikit-learn: Scikit-learn is one of the most popular ML libraries for classical ML algorithms.\nScikit-learn supports most of the supervised and unsupervised learning algorithms. Scikit-learncan also be used for data-mining and data-analysis, which makes it a great tool who is starting out with ML.\nSeaborn: Seaborn is a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics.\ntrain_test_split: It splits the dataset into a training set and a test set.\nDataset\nThe Heart Disease dataset has been taken from Kaggle. This database contains 76 attributes, but all published experiments refer to using a subset of 14 of them. It has a total number of 303 rows and 14 columns among which 165 have a heart disease.\nData Source: Heart Disease Dataset\nage: age in years\nsex: (1 = male; 0 = female)\ncp: chest pain type\ntrestbps: resting blood pressure (in mm Hg on admission to the hospital)\nchol: serum cholestoral in mg/dl\nfbs: (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)\nrestecg: resting electrocardiographic results\nthalach: maximum heart rate achieved\nexang: exercise induced angina (1 = yes; 0 = no)\noldpeak: ST depression induced by exercise relative to rest\nslope: the slope of the peak exercise ST segment\nca: number of major vessels (0-3) colored by flourosopy\nthal: thalassemia (1 = normal; 2 = fixed defect; 3 = reversable defect)\ntarget: (1= heart disease or 0= no heart disease)\n\n'], 'url_profile': 'https://github.com/Harshita9511', 'info_list': ['Python', 'Updated Mar 4, 2020', 'R', 'Updated Mar 5, 2020', 'Python', 'MIT license', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', '1', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'R', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', '1', 'Python', 'Updated Aug 14, 2020', '1', 'R', 'Updated Jan 6, 2021', 'Jupyter Notebook', 'Updated Mar 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '117 contributions\n        in the last year', 'description': ['Machine_Learning_exercises\nThis is repository made to dive into Machine Learning. After going through theoretical definitions of algorithms, every algorithm is implemented using R and then changed in order to make a coding template. Repository contains extensive exercises on Machine Learning algorithms. Organised into logical parts and programmed in R.\nThe whole project is organised into 10 parts - in order to understand every aspect of Machine Learning:\nPart 1 - Data Preprocessing (+ made a template to use in all the other steps of ML),\nPart 2 - Regression (Simple Linear Regression, Multiple Linear Regression, Polynomial Regression, SVR, Decision Tree Regression, Random Forest Regression),\nPart 3 - Classification (Logistic Regression, K-NN, SVM, Kernel SVM, Naive Bayes, Decision Tree Classification, Random Forest Classification),\nPart 4 - Clustering (K-Means, Hierarchical Clustering),\nPart 5 - Association Rule Learning (Apriori, Eclat),\nPart 6 - Reinforcement Learning (Upper Confidence Bound, Thompson Sampling),\nPart 7 - Natural Language Processing (Bag-of-words model and algorithms for NLP),\nPart 8 - Deep Learning (Artificial Neural Networks, Convolutional Neural Networks),\nPart 9 - Dimensionality Reduction (PCA, LDA, Kernel PCA),\nPart 10 - Model Selection & Boosting (k-fold Cross Validation, Parameter Tuning, Grid Search, XGBoost)\n'], 'url_profile': 'https://github.com/anastazijaverovic', 'info_list': ['Python', 'Updated Mar 4, 2020', 'R', 'Updated Mar 5, 2020', 'Python', 'MIT license', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', '1', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'R', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', '1', 'Python', 'Updated Aug 14, 2020', '1', 'R', 'Updated Jan 6, 2021', 'Jupyter Notebook', 'Updated Mar 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['MIDAS_Data_Challenge\nIn this project, we use data collected from Walter P. Moore‚Äôs historical projects to model a project‚Äôs profitability with high precision, and also identify the metrics that are good predictors of the project‚Äôs profitability. We pre-process the data, conduct exploratory data analysis and perform feature engineering, then apply some classification algorithm based on our understanding of the data and the question. Models include Logistics Regression, Random Forest, XGBoost. We found XGBoost to have the best performance as well as strong interpretability. From the model inference, we are able to determine the important variables that are highly influential on the profitability of the projects, and we visualize it.\n'], 'url_profile': 'https://github.com/gangyang0912', 'info_list': ['Python', 'Updated Mar 4, 2020', 'R', 'Updated Mar 5, 2020', 'Python', 'MIT license', 'Updated Apr 23, 2020', 'Jupyter Notebook', 'Updated Mar 6, 2020', '1', 'Jupyter Notebook', 'Updated Mar 7, 2020', 'R', 'Updated Mar 7, 2020', 'Jupyter Notebook', 'Updated Mar 7, 2020', '1', 'Python', 'Updated Aug 14, 2020', '1', 'R', 'Updated Jan 6, 2021', 'Jupyter Notebook', 'Updated Mar 6, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '30 contributions\n        in the last year', 'description': ['wordClassification\nLinear regression was used to find a correlation between four words and to classify them as a Noun or a Verb. To be able to make computations with words, the word embedded vectors approach was used.  Word embeddings representation puts every word into a vector space.  The place of the word in vector space is based on how words are related to each other.  There are many ways to put words into vector space. In this implementation, an existing pre-trained word embeddings was used.\n'], 'url_profile': 'https://github.com/ismaelvillanuevamiranda', 'info_list': ['Python', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', '1', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'MATLAB', 'Updated Apr 12, 2020', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'R', 'Apache-2.0 license', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated May 18, 2020']}","{'location': 'Surat, Gujarat', 'stats_list': [], 'contributions': '160 contributions\n        in the last year', 'description': ['BigMart-Sales-Prediction\nBigMart sales dataset consists of 2013 sales data for 1559 products across 10 different outlets in different cities. The goal of the following project is to build a regression model to predict the sales of each of 1559 products for the following year in each of the 10 different BigMart outlets. The BigMart sales dataset also consists of certain attributes for each product and store. This model helps BigMart understand the properties of products and stores that play an important role in increasing their overall sales.\nThis project is proudly created by Harsh Nagoriya.\n'], 'url_profile': 'https://github.com/harshnagoriya', 'info_list': ['Python', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', '1', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'MATLAB', 'Updated Apr 12, 2020', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'R', 'Apache-2.0 license', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated May 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['stock-market-intraday-predictor\nMini project on stock market intra day prediction in python -\n\nTop 10 companies for trading are found out using the stock screener from investing.com.\nThe details of a particular company selected from this list by the user are scraped from screener.in. Using the values scraped here like ROCE, ROE etc., we can figure out whether the stock is good for intraday trading and/or investment.\nOpen, close, high, low, adj. close, volume details are scraped for the particular stock from yahoo finance.\nA basic linear regression model is used to predict the open price for next day.\n\n'], 'url_profile': 'https://github.com/adititanna', 'info_list': ['Python', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', '1', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'MATLAB', 'Updated Apr 12, 2020', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'R', 'Apache-2.0 license', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated May 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['optimization_tutorial\nThis workshop introduces basic concepts, models and algorithms in linear programming, convex optimization and stochastic optimization. A MATLAB-based modeling system for convex optimization, CVX, is covered. Case studies are presented including an production plan problem, smart electric vehicle charging, a newsvendor problem, and a regression model. The codes are provided for practice.  The workshop is organized by IEEE South Sask section & PES/IAS Joint Chapter in collaboration with Engineering Graduate Student Association (EGSA) and the Faculty of Engineering and Applied Science at the University of Regina.\n'], 'url_profile': 'https://github.com/UofR-ESI-Lab', 'info_list': ['Python', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', '1', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'MATLAB', 'Updated Apr 12, 2020', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'R', 'Apache-2.0 license', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated May 18, 2020']}","{'location': 'Athens, Greece', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['Bike-Sharing-Demand-Prediction\nAs part of the course ""Data science & Web Mining"", we worked on a regression problem. Specifically, given a dataset consisting of a few thousand records containing the hourly count of rental bikes between years 2011 and 2012 in Capital bikeshare system with the corresponding weather and seasonal information. Our\xa0goal was\xa0to predict how many bikes would be rented each hour of a day, based on data including weather, time, temperature, whether or not its a workday, and much more. The dataset was taken from UCI Machine Learning Repository. Project was written in Python.\n'], 'url_profile': 'https://github.com/AthNtal', 'info_list': ['Python', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', '1', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'MATLAB', 'Updated Apr 12, 2020', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'R', 'Apache-2.0 license', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated May 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': ['Twitter-Sentiments\nThe objective of this task is to detect hate speech in tweets. For the sake of simplicity, we say a tweet contains hate speech if it has a\nracist or sexist sentiment associated with it. So, the task is to classify racist or sexist tweets from other tweets. Label ‚Äò1‚Äô denotes\nthe tweet is racist/sexist and label ‚Äò0‚Äô denotes the tweet is not racist/sexist, the objective was to predict the labels on the given test\ndataset. The evaluation metric from this practice problem is F1-Score. Skills used: Text Mining, Xgboost, Logistics Regression, SVM,\nDecision Tree, Ensembles, Hyper-Tuning parameters\nHate  speech  is  an  unfortunately  common  occurrence  on  the  Internet.  Often social media sites like Facebook and Twitter face the problem of identifying and censoring  problematic  posts  while weighing the right to freedom of speech. The  importance  of  detecting  and  moderating hate  speech  is  evident  from  the  strong  connection between hate speech and actual hate crimes. Early identification of users promoting  hate  speech  could  enable  outreach  programs that attempt to prevent an escalation from speech to action. Sites such as Twitter and Facebook have been seeking  to  actively  combat  hate  speech. In spite of these reasons, NLP research on hate speech has been very limited, primarily due to the lack of a general definition of hate speech, an analysis of its demographic influences, and an investigation of the most effective features.\nThe overall collection of tweets was split in the ratio of 65:35 into training and testing data. Out of the testing data, 30% is public and the rest is private.\n'], 'url_profile': 'https://github.com/dwivedi1997', 'info_list': ['Python', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', '1', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'MATLAB', 'Updated Apr 12, 2020', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'R', 'Apache-2.0 license', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated May 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['The-Prediction-of-Liver-Disease\nAim of this project is to build a best model to predict liver disease and use that model for human being to avoid risk of health, money and to give better health. In that i worked on near about 8 machine learning methods namely  Logistic Regression Naive Bayes Classifier Random Forest Decision Tree K-Nearest Neighbors Support Vector Machine Artificial Neural Network Bagging and Boosting by using R-Studio This was the first Real life project in my life in that i have learned so many things like how to find out errors and how to deal with it, Data Pre-processing part, Data Visualization, Model Building, etc.\n'], 'url_profile': 'https://github.com/NawajShaikh', 'info_list': ['Python', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', '1', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'MATLAB', 'Updated Apr 12, 2020', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'R', 'Apache-2.0 license', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated May 18, 2020']}","{'location': 'Copenhagen ', 'stats_list': [], 'contributions': '159 contributions\n        in the last year', 'description': ['Detecting-Heart-Arrhythmias-\nThis project aims at predicting if a heart beat from a ECG signal has an arrhythmia for each 0.4 second window centered on the peak of the heart beat. In this context, different classifiers including Random Forest, Logistic Regression, K Nearest Neighbors, Neural Networks and Decision Tree are used to detect abnormal beats. We use the MIT-BIH Arrythmia dataset from https://physionet.org/content/mitdb/1.0.0/ which is made available under the ODC Attribution License. This is a dataset with 48 half-hour two-channel ECG recordings measured at 360 Hz from the 1970s.\n'], 'url_profile': 'https://github.com/Meghdad-DTU', 'info_list': ['Python', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 14, 2020', '1', 'Jupyter Notebook', 'Updated Apr 15, 2020', 'MATLAB', 'Updated Apr 12, 2020', 'Updated Mar 3, 2020', 'Jupyter Notebook', 'Updated Mar 8, 2020', 'R', 'Apache-2.0 license', 'Updated Mar 4, 2020', 'Jupyter Notebook', 'Updated May 18, 2020']}",,
