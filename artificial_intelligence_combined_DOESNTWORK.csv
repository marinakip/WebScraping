"{'location': 'London', 'stats_list': [], 'contributions': '856 contributions\n        in the last year', 'description': ['AI for Trading\nUdacity nano-degree to learn practical AI application in trading algo.\nDesigned by WorldQuant.\nSyllabus:\n\nBasic Quantitative Trading - Trading with Momentum\nAdvanced Quantitative Trading - Breakout Strategy\nStocks, Indices, and ETFs - Smart Beta and Portfolio Optimization\nFactor Investing and Alpha Research - Alpha Research and Factor Modeling\nSentiment Analysis with Natural Language Processing\nAdvanced Natural Language Processing with Deep Leaning\nCombining Multiple Signals for Enhanced Alpha\nSimulating Trades with Historical Data - Backtesting\n\nMain Libraries:\n\nNumpy, Pandas, Matplotlib\nScikit-learn\nPytorch\nQuantopian/zipline\nQuantmedia\n\nMy Course:\n\nStarted: September 2019\nTarget End: February 2020\nActual End: January 2020\n\nProject Details:\n1. Basic Quantitative Trading - Trading with Momentum\n\nimport pandas, numpy, helper\nLoad Quatemedia EOD Price Data\nResample to Month-end close_price.resample(\'M\').last()\nCompute Log Return\nShift Returns returns.shift(n)\nGenerate Trading Signal\n\nStrategy tried:\n\nFor each month-end observation period, rank the stocks by previous returns, from the highest to the lowest. Select the top performing stocks for the long portfolio, and the bottom performing stocks for the short portfolio.\n\n\n\n for i, row in prev_price:\n   top_stock.loc[i] = row.nlargest(top_n)\n\n\n\n\nProjected Return portfolio_returns = (lookahead_returns * (df_long - df_short))/n_stocks\nStatistical Test\n\nAnnualized Rate of Return (np.exp(portfolio_returns.T.sum().dropna().mean()*12) - 1) * 100\nT-Test\n\nNull hypothesis (H0): Actual mean return from the signal is zero.\nWhen p value < 0.05, the null hypothesis is rejected\nOne-sample, one-sided t-test (t_value, p_value) = scipy.stats.ttest_1samp(portfolio_return, hypothesis)\n\n\n\n\n\n2. Advanced Quantitative Trading - Breakout Strategy\n\nimport pandas, numpy, helper\nLoad Quatemedia EOD Price Data\nThe Alpha Research Process\n\nWhat feature of markets or investor behaviour would lead to a persistent anomaly that my signal will try to use?\nExample Hypothesis:\n\nStocks oscillate in a range without news or significant interest\nTraders seek to sell at the top of the range and buy at the bottom\nWhen stocks break out of the range,\n\nthe liquidity traders seek to cover the losses, which magnify the move out of the range\nthe move out of the range attract other investor interst due to herd behaviour which favor continuation of the trend\n\n\n\n\nProcess:\n\nObserv & Research\nForm Hypothesis\nValidate Hypothesis, back to #1\nCode Expression\nEvaluate in-sample\nEvaluate out-of-sample\n\n\n\n\nCompute Highs and Lows in a Window\n\ne.g., Rolling max/min for the past 50 days\n\n\nCompute Long and Short Signals\n\nlong = close > high, short = close < low, position = long - short\n\n\nFilter Signals (5, 10, 20 day signal window)\n\nCheck if there was a signal in the past window_size of days\nhas_past_signal = bool(sum(clean_signals[signal_i:signal_i+window_size]))\nUse the current signal if there\'s no past signal, else 0/False\nclean_signal.append(not has_past_signal and current_signal)\nApply the above to short (signal[signal == -1].fillna(0.astype(int))) and long, add them up\n\n\nLookahead Close Price\n\nHow many days to short or long close_price.shift(lookahead_days*-1)\n\n\nLookahead Price Return\n\nLog return between lookahead_price and close_price\n\n\nCompute the Signal Return\n\nsignal * lookahead_returns\n\n\nTest for Significance\n\nPlot a histogram of the signal returns\n\n\nCheck Outliers in the histogram\nKolmogorov-Smirnov Test (KS-Test)\n\nCheck which stock is causing the outlying returns\nRun KS-Test on a normal distribution against each stock\'s signal returns\nks_value, p_value = scipy.stats.kstest(rvs=group[\'signal_return\'].values, cdf=\'norm\', args=(mean_all, std_all))\n\n\nFind outliers\n\nSymbols that pass the null hypothesis with a p-value less than 0.05\nSymbols that with a KS value above ks_threshod(0.8)\nRemove them by good_tickers = list(set(close.column) - outlier_tickers)\n\n\n\n3. Stocks, Indices, and ETFs - Smart Beta and Portfolio Optimization\n\nLoad large dollar volume stocks from quotemedia\n\nSmart Beta by alternative weighting - dividend yield to choose the portfolio weight\n\nCalculate Index Weights (dollar volume weights)\nCalculate Portfolio Weights based on Dividend\nCalculate Returns, Weighted Returns, Cumulative Returns\nTracking Error  np.sqrt(252) * np.std(benchmark_returns_daily - etf_returns_daily, ddof=1)\n\nPortfolio Optimization - minimize the portfolio variance and closely track the index\n$Minimize \\left [ \\sigma^2_p + \\lambda \\sqrt{\\sum_{1}^{m}(weight_i - indexWeight_i)^2} \\right  ]$ where $m$ is the number of stocks in the portfolio, and $\\lambda$ is a scaling factor that you can choose.\n6. Calculate the covariance of the returns np.cov(returns.fillna(0).values, rowvar=False)\n7. Calculate optimal weights\n\nPortfolio Variance: $\\sigma^2_p = \\mathbf{x^T} \\mathbf{P} \\mathbf{x}$\n\ncov_quad = cvx.quad_form(x, P)\n\n\nDistance from index weights: $\\left | \\mathbf{x} - \\mathbf{index} \\right |2$ = $\\sqrt{\\sum{1}^{n}(weight_i - indexWeight_i)^2}$\n\nindex_diff = cvx.norm(x, p=2, axix=None)\n\n\nObjective function = $\\mathbf{x^T} \\mathbf{P} \\mathbf{x} + \\lambda \\left | \\mathbf{x} - \\mathbf{index} \\right |_2$\n\ncvx.Minimize(cov_quad + scale * index_diff)\n\n\nConstraints\n\n\nx = cvx.Variable()\nconstraints = [x >= 0, sum(x) == 1]\n\n\n\n\nOptimization\n\n\nproblem = cvx.Problem(objective, constraints\nproblem.solve()\n\n\n\n\n\n\nRebalance Portfolio over time\nPortfolio Turnover\n\n$ AnnualizedTurnover =\\frac{SumTotalTurnover}{NumberOfRebalanceEvents} * NumberofRebalanceEventsPerYear $\n$ SumTotalTurnover =\\sum_{t,n}{\\left | x_{t,n} - x_{t+1,n} \\right |} $ Where $ x_{t,n} $ are the weights at time $ t $ for equity $ n $.\n$ SumTotalTurnover $ is just a different way of writing $ \\sum \\left | x_{t_1,n} - x_{t_2,n} \\right | $\nMinimum volatility ETF\n\n\n\n4. Factor Investing and Alpha Research - Alpha Research and Factor Modeling\n\nimport cvxpy, numpy, pandas, time, matplotlib.pyplot\nLoad equitiies EOD price (zipline.data.bundles)\n\nbundles.register(bundle_name, ingest_func)\nbundles.load(bundle_name)\n\n\nBuild Pipeline Engine\n\nuniverse = AverageDollarVolume(window_length=120).top(500) <- 490 Tickers\nengine = SimplePipelineEngine(get_loader, calendar, asset_finder)\n\n\nGet Returns\n\ndata_portal = DataPotal()\nget_pricing = data_portal.get_history_window()\nreturns = get_pricing().pct_change()[1:].fillna(0) <- e.g. 5 year: 1256x490\n\n\n\nStatistical Risk Model\n\nFit PCA\n\npca = sklearn.decomposition.PCA(n_components, svd_solver=\'full\')\npca.fit()\npca.components_ <- 20x490\n\n\nFactor Betas\n\npd.DataFrame(pca.components_.T, index=returns.columns.values, columns=np.arange(20)) <- 20x490\n\n\nFactor Returns\n\npd.DataFrame(pca.transform(returns), index=returns.index , columns=np.arange(20)) <- 490x20\n\n\nFactor Coveriance Matrix\n\nnp.diag(np.var(factor_returns, axix=0, ddof=1)*252) <- 20x20\n\n\nIdiosyncratic Variance Matrix\n\n\n_common_returns = pd.DataFrame(np.dot(factor_returns, factor_betas.T), returns.index, returns.columns)\n_residuals = (returns - _common_returns)\npd.DataFrame(np.diag(np.var(_residuals)*252), returns.columns, returns.columns) <- 490x490\n\n\n\n\nIdiosyncratic Variance Vector\n\n# np.dot(idiosyncratic_variance_matrix, np.ones(len(idiosyncratic_variance_matrix)))\npd.DaraFrame(np.diag(idiosyncratic_variance_matrix), returns.columns)\n\n\nPredict Portfolio Risk using the Risk Model\n\n$ \\sqrt{X^{T}(BFB^{T} + S)X} $ where:\n\n$ X $ is the portfolio weights\n$ B $ is the factor betas\n$ F $ is the factor covariance matrix\n$ S $ is the idiosyncratic variance matrix\n\n\nnp.sqrt(weight_df.T.dot(factor_betas.dot(factor_cov_matrix).dot(factor_betas.T) + idiosyncratic_var_matrix).dot(weight_df))\n\n\n\nCreate Alpha Factors\n\nMomentum 1 Year Factor\nMean Reversion 5 Day Sector Neutral Factor\nMean Reversion 5 Day Sector Neutral Smoothed Factor\nOvernight Sentiment Factor\nOvernight Sentiment Smoothed Factor\nCombine the Factors to a single Pipeline\n\n\npipeline = Pipeline(screen=universe)\npipeline.add(momentum_1yr(252, universe, sector), \'Momentum_1YR\')\n:\nall_factors = engine.run_pipeline(pipeline, start, end)\n\n\n\n\n\nEvaluate Alpha Factors\n\nGet Pricing Data\n\nassets = all_factors.index.level[1].values.tolist()\n\n\nFormat Alpha Factors and Pricing for Alphalens\n\nclean_factor_data = {factor: alphalens.get_clean_factor_and_forward_returns(factor, prices, period=[1])}\nunixt_factor_data = {factor: factor_data.set_index(pd.MultiIndex.from_tuples([(x.timestamp(), y) for x, y in factor_data.index.values], names=[\'date\', \'asset\']))}\n\n\nQuantile Analysis\n\nFactor Returns:\n\nalphalens.performance.factor_returns(factor_data).iloc[:, 0].cumprod().plot()\nThis should be generally move up and to the right\n\n\nBasis Points Per Day per Quantile\n\nalphalens.performance.mean_return_by_quantile(factor_data)[0].iloc[:, 0].plot.bar()\nShould be monotonic, not too much on short that is not practical to implement\nReturn spread (Q1 minus Q5)*252, considering transaction cost to cut this half, should be clear that these alphas can only survive in an institutional setting and that leverage will likely need to be applied\n\n\n\n\nTurnover Analysis\n\nLight test before full backtest to see the stability of the alphas over time\nFactor Rank Autocorrelation (FRA) should be close to 1\nalphalens.performance.factor_rank_autocorrelation(factor_data).plot()\n\n\nSharpe Ratio of the Alphas\n\npd.Series(data=252*factor_returns.mean()/factor_returns.std())\n\n\nThe Combined Alpha Vector\n\nML like Random Forest to get a single score per stock\nSimpler approach is to jsut average\n\n\n\nOptimal Portfolio Constrained by Risk Model\n\nObjective and Constraints\n\nObjective Function:\n\nCVXPY objective function that maximizes $ \\alpha^T * x \\ $, where $ x $ is the portfolio weights and $ \\alpha $ is the alpha vector.\ncvx.Minimize(-alpha_vector.values.flatten()*weights)\n\n\nConstraints\n\n$ r \\leq risk_{\\text{cap}}^2 \\ $ risk <= self.risk_cap **2\n$ B^T * x \\preceq factor_{\\text{max}} \\ $ factor_betas.T*weights <= self.factor_max\n$ B^T * x \\succeq factor_{\\text{min}} \\ $ factor_betas.T*weight >= self.factor_min\n$ x^T\\mathbb{1} = 0 \\ $ sum(weights) == 0.0\n$ |x|_1 \\leq 1 \\ $ sum(cvs.abs(weights)) <= 1.0\n$ x \\succeq weights_{\\text{min}} \\ $ weights >= self.weights_min\n$ x \\preceq weights_{\\text{max}} $ weights <= self.weights_max\nWhere $ x $ is the portfolio weights, $ B $ is the factor betas, and $ r $ is the portfolio risk\n\n\nOptimalHoldings(ABC).find()\n\n\nweights = cvx.Variable(len(alpha_vector))\nrisk = cvx.quad_form(f, X) + cvx.quad_form(weights, S)\nprob = cvx.Problem(obj, constraints)\nprob.solve(max_iter=500)\noptimal_weights = np.asarray(weights.value).flatten()\nreturns pd.DataFrame(data=optimal_weights, index=alpha_vector.index)\n\n\n\n\n\n\nOptimize with a Regularization Parameter\n\nTo enforce diversification, change Objective Function\nCVXPY objective function that maximize $ \\alpha^T * x + \\lambda|x|_2\\ $, where $ x $ is the portfolio weights, $ \\alpha $ is the alpha vector, and $ \\lambda $ is the regularization parameter.\nobjective = cvx.Minimize(-alpha_vector.values.flatten()*weights + self.lambda_reg*cvx.norm(weights, 2))\n\n\nOptimize with a Strict Factor Constrains and Target Weighting\n\nAnother common constraints is to take a predefined target weighting, $x^*$ (e.g., a quantile portfolio), and solve to get as close to that portfolio while respecting portfolio-level constraints.\nMinimize on on $ |x - x^|_2 $, where $ x $ is the portfolio weights  $ x^ $ is the target weighting\nobjective = cvs.Minimize(cvx.norm(alpha_vector.values.flatten()-weights), 2)\n\n\n\n5. Sentiment Analysis with NLP - NLP on Financial Statement\n\nimport nltk, numpy, pandas, pickle, pprint, tqdm.tqdm, bs4.BeautifulSoup, re\n\nnltk.download(\'stopwords\'), nltk.download(\'wordnet\')\n\n\nGet 10-k documents\n\nLimit number of request per second by @limits\nfeed = BeautifulSoup(request.get.text).feed\nentries = [entry.content.find(\'filing-href\').getText(), ... for entry in feed.find_all(\'entry\')]\nDownload 10-k documents\nExtract Documents\n\ndoc_start_pattern = re.compile(r\'<DOCUMENT>\')\ndoc_start_position_list = [x.end() for x in doc_start_pattern.finditer(text)]\n\n\nGet Document Types\n\ndoc_type_pattern = re.compile(r\'<TYPE>[^\\n]+\')\ndoc_type = doc_type_pattern.findall(doc)[0][len(""<TYPE>""):].lower()\n\n\n\n\nProcess the Data\n\nClean up\n\ntext.lower()\nBeautifulSoup(text, \'html.parser\').get_text()\n\n\nLemmatize\n\nnltk.stem.WordNetLemmatizer, nltk.corpus.wordnet\n\n\nRemove Stopwords\n\nnltk.corpus.stopwords\n\n\n\n\nAnalysis on 10ks\n\nLoughran and McDonald sentiment word list\n\nNegative, Positive, Uncertainty, Litigious, Constraining, Superfluous, Modal\n\n\nSentiment Bag of Words (Count for each ticker, sentiment)\n\n\nsklearn.feature_extraction.text.CountVectorizer(analyzer=\'word\', vocabulary=sentiment)\nX = vectorizer.fit_transform(docs)\nfeatures = vectorizer.get_feature_names()\n\n\n\n\nJaccard Similarity\n\nsklearn.metrics.jaccard_similarity_score(u, v)\nGet the similarity between neighboring bag of words\n\n\nTF-IDF\n\nsklearn.feature_extraction.text.TfidfVectorizer(analyzer=\'word\', vocabulary=sentiments)\n\n\nCosine Similarity\n\nsklearn.metrics.pairwise.cosine_similarity(u, v)\nGet the similarity between neighboring IFIDF vectors\n\n\n\n\nEvaluate Alpha Factors\n\nUse yearly pricing to match with 10K frequency of annual production\nTurn the sentiment dictionary into a dataframe so that alphalens can read\nAlphalens Format\n\ndata = alphalens.utils.get_clean_factor_and_forward_return(df.stack(), pricing, quantiles=5, bins=None, period=[1])\n\n\nAlphalens Format with Unix Timestamp\n\n{factor: data.set_index(pd.MultiIndex.from_tuples([(x.timestamp(), y) for x, y in data.index.values], names=[\'date\', \'asset\'])) for factor, data in factor_data.items()}\n\n\nFactor Returns\n\nalphalens.performance.factor_returns(data)\nShould move up and to the right\n\n\nBasis Points Per Day per Quantile\n\nalphalens.performance.mean_return_by_quantile(data)\nShould be monotonic in quantiles\n\n\nTurnover Analysis\n\nFactor Rank Autocorrelation (FRA) to measure the stability without full backtest\nalphalens.factor_rank_autocorrelation(data)\n\n\nSharpe Ratio of the Alphas\n\nShould be 1 or higher\nnp.sqrt(252)*factor_returns.mean() / factor_returns.std()\n\n\n\n\n\n6. Advanced NLP with Deep Leaning - Analizing Stock Sentiment from Twits (requiring GPU)\n\nimport json, nltk, os, random, re, torch, torch.nn, torch.optim, torch.nn.functional, numpy\nImport Twits\n\njson.load()\n\n\nPreprocessing the Data\n\nPre-Processing\n\n\nnltk.download(\'wordnet\')\nnltk.download(\'stopwords\')\ntext = message.lower()\ntext = re.sub(\'https?:\\/\\/[a-zA-Z0-9@:%._\\/+~#=?&;-]*\', \' \', text)\ntext = re.sub(\'\\$[a-zA-Z0-9]*\', \' \', text)\ntext = re.sub(\'\\@[a-zA-Z0-9]*\', \' \', text)\ntext = re.sub(\'[^a-zA-Z]\', \' \', text)\ntokens = text.split()\nwnl = nltk.stem.WordNetLemmatizer()\ntokens = [wnl.lemmatize(wnl.lemmatize(word, \'n\'), \'v\') for word in tokens]\n\n\n\n\nBag of Words\n\nbow = sorted(Counter(all_words), key=counts.get, reverse=True)\n\n\nRemove most common words such as \'the, \'and\' by high_cutoff=20, rare words by low_cutoff=1e-6\nCreate Dictionaries\n\n\nvocab = {word: ii for ii, word in enumarate(filtered_words, 1)}\nid2vodab = {v: k for k, v in vocab.items()}\nfiltered = [[word for word in message if word in vocab] for message in tokenized]\n\n\n\n\nBalancing the classes\n\n50% is neutral --> make it 20% by dropping some neutral twits\nRemove messages with zero length\n\n\n\n\nNeural Network\n\nEmbed -> RNN -> Dense -> Softmax\nText Classifier\n\n\nclass TextClassifier(nn.Module):\n    def __init__(self, vocab_size, embed_size, lstm_size, output_size, lstm_layers=1, dropout=0.1):\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.embed_size = embed_size\n        self.lstm_size = lstm_size\n        self.output_size = output_size\n        self.lstm_layers = lstm_layers\n        self.dropout = dropout\n\n        self.embedding = nn.Embedding(vodab_size, embed_size)\n        self.lsfm = nn.LSTM(embed_size, lstm_size, lstm_layers, dropout=dropout, batch_first=False)\n        self.dropout = nn.Dropout(-0.2)\n        self.fc = nn.Linear(lstm_size, output_size)\n        self.softmax = nn.LogSoftmax(dim=1)\n    def init_hidden(self, batch_size):\n        weight = next(self.parameters()).data\n        hidden = (weight.new(self.lstm_layers, batch_size, self.lstm_size).zero_(),\n                  weight.new(self.lstm_layers, batch_size, self.lstm_size).zero_())\n        return hidden\n    def forward(self, nn_input, hidden_state)\n        batch_size = nn_input.size(0)\n        nn_input = nn_input.long()\n        embeds = self.embedding(nn_input)\n        lstm_out, hidden_state = self.lstm(embeds, hidden_state)\n        lstm_out = lstm_out[-1,:,:] # Stack up LSMT Outputs\n        out = self.dropout(lstm_out)\n        out = self.fc(out)\n        logps = self.softmax(out)\n        return logps, hidden_state\n\n\n\n\n\n\nTraining\n\nDataLoaders and Batching\n\nInput Tensor shape should be (sequence_length, batch_size)\nLeft pad with zeros if a message has less tokens than sequence_length.\nIf a message has more token than sequence_length, keep the first sequence_length tokens\nBuild a DataLoader as a generator\ndef dataloader(): \n    yield batch, label_tensor # both variables are torch.tensor()\n\n\n\n\nTraining and Validation\n\nSplit data to training set and validation set, then check the model\ntext_batch, labels = next(iter(dataloader(train_features, train_labels, sequence_length=20, batch_size=64)))\nmodel = TextClassifier(len(vocab)+1, 200, 128, 5, dropout=0.)\nhidden = model.init_hidden(64)\nlogps, hidden = model.forward(text_batch, hidden)\nprint(logps)\n\n\nModel\ndevice = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\nmodel = TextClassifier(len(vocab)+1, 1024, 512, 5, lstm_layers=2, dropout=0.2)\nmodel.embedding.weight.data.uniform_(-1,1)\nmodel.to(device)\n\n\nTrain!\nepochs = 3\nbatch_size = 1024\nlearning_rate = 0.001\nclip = 5\nprint_every = 100\ncriterion = nn.NLLLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\nmodel.train()\nfor epoch in range(epochs):\n    print (\'Starting epoch {}\'.format(epoch + 1))\n    hidden = model.init_hidden(batch_size)\n    steps = 0\n    for text_batch, labels in dataloader(train_features, train_labels, batch_size=batch_size, sequence_length=20, shuffle=True):\n        steps += 1\n        if text_batch.size(1) != batch_size:\n            break\n        hidden = tuple([each.data for each in hidden])\n        text_batch, labels = text_batch.to(device), labels.to(device)\n        for each in hidden:\n            each.to(device)\n        model.zero_grad()\n        output, hidden = model(text_batch, hidden)\n        loss = criterion(output, labels)\n        loss.backwards()\n        nn.utils.clip_grad_norm_(model.parameters(), clip)\n        optimizer.step() # Optimize\n        if steps % print_every == 0:\n            model.eval()\n            valid_losses = []\n            accuracy = []\n            valid_hidden = model.init_hidden(batch_size)\n            for text_batch, labels in dataloader(valid_features, valid_labels, batch_size=batch_size, sequence_length=20, shuffle=False):\n                if text_batch.size(1) != batch_size:\n                    break\n                valid_hidden = tuple([each.data for each in valid_hidden])\n                text_batch, lables = text_batch.to(device), labels.to(device)\n                for each in valid_hidden:\n                    each.to(device)\n                valid_output, valid_hidden = model(text_batch, valid_hidden)\n                valid_loss = criterion(valid_output.squeeze(), labels)\n                valid_losses.append(valid_loss.item())\n                ps = torch.exp(valid_output)\n                top_p, top_class = ps.topk(1, dim=1)\n                equals = top_class == labels.view(*top_class.shape)\n                accuracy.append(torch.mean(equals.type(torch.FloatTensor)).item())\n            model.train()\n            print(""Epoch: {}/{}..."".format(epoch+1, epochs),\n                  ""Step: {}..."".format(steps),\n                  ""Loss: {:.6f}..."".format(loss.item()),\n                  ""Val Loss: {:.6f}"".format(np.mean(valid_losses)),\n                  ""Accuracy: {:.6f}"".format(np.mean(accuracy)))\n\n\n\n\n\n\nMaking Predictions\n\npreprocess, filter non-vocab words, convert words to ids, add a batch dimention (torch.tensor(tokens).view(-1,1))\nhidden = model.init_hidden(1)\nlogps, _ = model.forward(text_input, hidden)\npred = torch.exp(logps)\n\n\n\n\nTesting\n\n7. Combining Multiple Signals for Enhanced Alpha\n\nimport numpy, pandas, tqdm, matplotlib.pyplot\nData Pipeline\n\nzipline.data.bundles - register, load\nzipline.pipeline.Pipeline\nuniverse = zipline.pipeline.AverageDollarVolume\nzipline.utils.calendar.get_calendar(\'NYSE\')\nzipline.pipeline.loaders.USEquityPricingLoader\nengine = zipline.pipeline.engine.SimplePipelineEngine\nzipline.data.data_portal.DataPortal\n\n\nAlpha Factors\n\nMomentum 1 Year Factor\n\nzipline.pipeline.factors.Returns().demean(groupby=Sector).rank().zscore()\n\n\nMean Reversion 5 Day Sector Neutral Smoothed Factor\n\nunsmoothed = -Returns().demean(groupby=Sector).rank().zscore()\nsmoothed = zipline.pipeline.factors.SimpleMovingAverage(unsmoothed).rank().zscore()\n\n\nOvernight Sentiment Smoothed Factor\n\nCTO(Returns), TrainingOvernightReturns(Returns)\n\n\nCombine the three factors by pipeline.add()\n\n\nFeatures and Labels\n\nUniversal Quant Features\n\nStock Volatility 20d, 120d: pipeline.add(zipline.pipeline.factors.AnnualizedVolatility)\nStock Dollar Volume 20d, 120d: pipeline.add(zipline.pipeline.factors.AverageDollarVolume)\nSector\n\n\nRegime Features\n\nHigh and low volatility 20d, 120d: MarketVolatility(CustomFactor)\nHigh and low dispersion 20d, 120d: SimpleMovingAverage(MarketDispersion(CustomFactor))\n\n\nTarget\n\n1 Week Return, Quantized: pipeline.add(Returns().quantiles(2)), pipeline.add(Returns().quantiles(25))\n\n\nengine.run_pipeline()\nDate Feature\n\nJanuary, December, Weekday, Quarter, Qtr-Year, Month End, Month Start, Qtr Start, Qtr End\n\n\nOne-hot encode Sector\nShift Target\nIID Check (Independent and Identically Distributed)\n\nCheck rolling autocorelation between 1d to 5d shifted target using scipy.stats.speamanr\n\n\nTrain/Validation/Test Splits\n\n\nRandom Forests\n\nVisualize a Simple Tree\n\nclf = sklearn.tree.DecisionTreeClassifier()\nGraph: IPython.display.display\nRank features by importance clf.feature_importances_\n\n\nRandom Forest\n\nclf = sklearn.ensemble.RandomForestClassifier()\nScores: clf.score(), clf.oob_score_, clf.feature_importances_\n\n\nModel Results\n\nSharpe Ratios sqrt(252)*factor_returns.mean()/factor_returns.std()\nFactor Returns alphalens.performance.factor_returns()\nFactor Rank Autocorelation alphalens.performance.factor_rank_autocorrelation()\nScores: clf.predict_proba()\n\n\nCheck the above for Training Data and Validation Data\n\n\nOverlapping Samples\n\nOption 1) Drop Overlapping Samples\nOption 2) Use sklearn.ensemble.BaggingClassifier\'s max_samples with base_clf = DecisionTreeClassifier()\nOption 3) Build an ensemble of non-overlapping trees\n\n\nsklearn.ensemble.VotingClassifier\nsklearn.base.clone\nsklearn.preprocessing.LavelEncoder\nsklearn.utils.Bunch\n\n\n\n\n\n\nFinal Model\n\nRe-Training Model using Training Set + Validation Set\n\n\n\n8. Simulating Trades with Historical Data - Backtesting\n\n\n\nLoad Price, Covariance and Factor Exposure from Barra - data.update(pickle.load())\n\n\nShift daily returns by 2 days\n\n\nWinsorize\n\nnp.where(x <= a,a, np.where(x >= b, b, x)) and Density plot\n\n\n\nFactor Exposures and Factor Returns\n\nmodel = ols (Ordinary Least Squares)\nuniverse = Market Cap > 1e9, Winsorize\nvariable: dependent = Daily Return, independent = Factor Exposures\nestimation: Factor Returns\n\n\n\nChoose 4 Alpha Factors\n\n1 Day Reversal, Earnings Yield, Value, Sentiment\n\n\n\nMerge Previous Portfolio Holdings and Add h.opt.previous with 0\n\n\nConvert all NaN to 0, and median for 0 Specific Risk\n\n\nBuild Universe - (df[\'IssuerMarketCap\'] >= 1e9) | (abs(df[\'h.opt.previous\']) > 0.0)\n\n\nSet Risk Factors (B)\n\nAll Factors - Alpha Factors\npatsy.dmatrices to one-hot encode categories\n\n\n\nCalculate Specific Variance\n\n(Specific Risk * 0.01)**2\n\n\n\nBuild Factor Covariance Matrix\n\nTake off diagonal\n\n\n\nEstimate Transaction Cost\n\nLambda\n\n\n\nCombine the four Alpha Factors\n\nsum(B_Alpha(Design Matrix)) * 1e-4\n\n\n\nDefine Objective Function\n\n$$ f(\\mathbf{h}) = \\frac{1}{2}\\kappa \\mathbf{h}_t^T\\mathbf{Q}^T\\mathbf{Q}\\mathbf{h}t + \\frac{1}{2} \\kappa \\mathbf{h}t^T \\mathbf{S} \\mathbf{h}t - \\mathbf{\\alpha}^T \\mathbf{h}t + (\\mathbf{h}{t} - \\mathbf{h}{t-1})^T \\mathbf{\\Lambda} (\\mathbf{h}{t} - \\mathbf{h}{t-1}) $$\n\n\n\nDefine Gradient of Objective Function\n\n$$ f\'(\\mathbf{h}) = \\frac{1}{2}\\kappa (2\\mathbf{Q}^T\\mathbf{Qh}) + \\frac{1}{2}\\kappa (2\\mathbf{Sh}) - \\mathbf{\\alpha} + 2(\\mathbf{h}{t} - \\mathbf{h}{t-1}) \\mathbf{\\Lambda} $$\n\n\n\nOptimize Portfolio\n\nh = scipy.optimize.fmin_l_bfgs_b(func, initial_guess, func_gradient)\n\n\n\nCalculate Risk Exposure\n\nB.T * h\n\n\n\nCalculate Alpha Exposure\n\nB_Alpha.T * h\n\n\n\nCalculate Transaction Cost\n\n$$ tcost = \\sum_i^{N} \\lambda_{i} (h_{i,t} - h_{i,t-1})^2 $$\n\n\n\nBuild Tradelist\n\nh - h_previous\n\n\n\nSave optimal holdings as previous optimal holdings\n\nh_previous = h\n\n\n\nRun the Backtest\n\nLoop #6 to #21 for all the dates\n\n\n\nPnL Attrribution\n\n$$ {PnL}{alpha}= f \\times b{alpha} $$\n$$ {PnL}{risk} = f \\times b{risk} $$\n\n\n\nBuild Portfolio Characteristics\n\ncalculate the sum of long positions, short positions, net positions, gross market value, and amount of dollars traded.\n\n\n\n'], 'url_profile': 'https://github.com/yuki678', 'info_list': ['10', 'HTML', 'Updated Nov 13, 2020', '10', 'Jupyter Notebook', 'Updated Jan 1, 2020', '1', 'Python', 'Updated Feb 15, 2020', 'Kotlin', 'Updated Jan 5, 2020', 'Updated Jan 3, 2020', 'C++', 'Updated Jan 19, 2020', 'JavaScript', 'Updated Feb 12, 2021', 'Updated Dec 30, 2019', 'C++', 'EPL-2.0 license', 'Updated Jan 2, 2020', 'Updated Jan 6, 2020']}","{'location': 'Daejeon', 'stats_list': [], 'contributions': '42 contributions\n        in the last year', 'description': ['CH485---Artificial-Intelligence-and-Chemistry\nLecturer: Prof. Woo Youn Kim,  TA: Jaechang Lim\n\nAI has become a big social issue as it spreads rapidly to science, industry, and even daily life. Deep\nlearning techniques has attracted great attention as a new powerful tool for chemical research. In this course, we will discuss the role of artificial intelligence in modern chemistry and look at the latest trends in this\nfield. It aims to learn practical knowledge that can be used in actual research field through theory and\npractice focused on deep learning.\nThis is the repository for materials of KAIST 2019 fall Artificial Intelligence and Chemistry.\nWe use Google colab for all practices.\nWebiste: https://aceteamkaist.wixsite.com/home\n\nPractice\nPractice 02-06: Predicting molecular property, LogP in this practice, using various architectures of neural networks\nPractice 07: Generating new SMILES string using variational autoencoder\n\nPractice 02: Linear regression\nPractice 03: Multilayer perceptron with non-linear activation\nPractice 04: Convolutional neural network\nPractice 05: Recurrent neural network\nPractice 06: Graph convolutional neural network\nPractice 07: Variational autoencoder\n\n'], 'url_profile': 'https://github.com/jaechanglim', 'info_list': ['10', 'HTML', 'Updated Nov 13, 2020', '10', 'Jupyter Notebook', 'Updated Jan 1, 2020', '1', 'Python', 'Updated Feb 15, 2020', 'Kotlin', 'Updated Jan 5, 2020', 'Updated Jan 3, 2020', 'C++', 'Updated Jan 19, 2020', 'JavaScript', 'Updated Feb 12, 2021', 'Updated Dec 30, 2019', 'C++', 'EPL-2.0 license', 'Updated Jan 2, 2020', 'Updated Jan 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Künstliche Intelligenz\nAufgaben.\n\n Open Tech School AI Anmeldung (zum 06.01.2020).\n Open Tech School AI Anmeldung (zum 09.01.2020).\n Open Tech School besuchen.\n Kennenlernen der Pythonbibliotheken:\n\n matplot\n numpy\n[-] keras\n[-] TensorFlow etc.\n\n\n Xor gate K.I. Programm implementieren.\n pytest zum laufen kriegen.\n Text ""Noahs Lernmodell"" fertigstellen\n\n[-] Formeln besser leserlich machen\n\n\n Erklärung zu ""Neuronale Netzwerke"" outline\n Codereview von Tic Tac Toe mit Mat\n logfile fertigstellen\n Erklärung zu ""Neuronale Netzwerke"" fertigstellen\n\nLang-Zeit-Aufgaben.\n\n TicTacToe K.I. Programmieren.\n\nLOG.\n06.01.2020:\nBetriebspraktikumsbeginn.\nUeber die Bibliothek “Numpy” recherchiert.\nMit dem gesammelten Wissen hab ich anschließend einen Mathematischen Graphenzeichner programmiert.\nIn den Zwischenzeiten hab ich mich über neuronale Netzwerke informiert.\n07.01.2020:\nAm zweiten Tag des Praktikums hab ich mich weiter über neuronale Netzwerke informiert.\n08.01.2020:\nAm Mittwoch hab ich mich weiter informiert und mich für das Thema meines ersten neuronalen Netzwerks in Python entschieden.\n09.01.2020:\nDas “Xor Gate Neuronales Netzwerk” fertig programmiert, aber es funktioniert noch nicht.\nAbends bei dem “Open Tech School AI Meetup” habe ich einige neue “Numpy” Funktionen kennengelernt.\n10.01.2020:\nIch habe über neuronale Netzwerke gelernt und “Bugfixes” an dem “Xor Gate” Programm durchgeführt aber es funktioniert immer noch nicht. Ich bin zur Erkenntnis gelangt das die Fehler meines Programmes schwer zu finden sein werden da ich “unordentlich” programmiert habe.\n11.01.2020:\nPause.\n12.01.2020:\nPause.\n13.01.2020:\nAm Montag habe ich über ein Konzept mit dem Namen “Test Oriented Programming” gelernt, und die nötigen “Python libraries” installiert und ausprobiert.\n14.01.2020:\nAm Dienstag der zweiten Woche habe ich ein “Lernprozess für Neuronale Netzwerke” entwickelt und begonnen zu dokumentieren.\n15.01.2020:\nIch habe am Mittwoch meine Dokumentation zu meinem “Lernprozess für Neuronale Netzwerke” abgeschlossen, Pytest zum laufen gebracht und mich mit “Clean Coding” auseinandergesetzt.\nSpäter besuchte ich wieder die Open Tech Schule.\n16.01.2020 - 17.01.2020:\nAls ansatz um mit meiner Tic Tac Toe K.I. zu beginnen habe ich ein gewöhnliches 2 Spieler Tic Tac Toe Spiel programmiert.\n18.01.2020 - 22.01.2020:\nIn dieser Zeitspanne begann und beendete ich ein mein Tic Tac Toe neuronales Netzwerk. Es funktioniert auf Basis meines Lernprozesses. Nach 3.5 Stunden übung, was in diesem Kontext nicht viel ist, hat das Netzwerk schon zum Teil Gewinnorientiert gespielt.\n23.01.2020 - 24.01.2020:\nIn den letzten 2 Tagen machte ich ""Bugfixes"" an meinem neuralen Netzwerk. Mit hilfe einer der Mitarbeiter wurde mein neuronales Netzwerk dann anschließend zum Tic Tac Toe üben, auf einem Server deponiert, wo es ungestört eine Woche lang trainieren wird.\nEinige meiner recherchierten Links.\nhttps://www.youtube.com/watch?v=GB9ByFAIAH4 : Ein Numpy Lernvideo.\nhttps://www.youtube.com/watch?v=CliW7kSxxWU : Was sind Tensoren.\nhttps://www.youtube.com/watch?v=f5liqUk0ZTw&t=7s : Erlärt was Tensoren sind.\nhttps://en.wikipedia.org/wiki/Tensor : Tensoren.\nhttps://www.youtube.com/watch?v=oJNHXPs0XDk : neuronale Netzwerke: Topologische Architekturen.\nhttps://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=2&t=0s : Neural networks: Deep learning, chapter 1 (3Blue1Brown).\nhttps://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=3&t=0s : Neural networks: Deep learning, chapter 2 (3Blue1Brown).\nhttps://www.youtube.com/watch?v=Ilg3gGewQ5U&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=4&t=1s : Neural networks: Deep learning, chapter 3 (3Blue1Brown).\nhttps://www.youtube.com/watch?v=tIeHLnjs5U8&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=5&t=0s : Neural networks: Deep learning, chapter 4 (3Blue1Brown).\nhttps://www.youtube.com/watch?v=K-wIZuAA3EY : Eine K.I. erlernt das Gehen.\nhttps://www.youtube.com/watch?v=kvo1B7r7Xtk : Induktions Urteil.\nhttps://www.youtube.com/watch?v=ZDa-Z5JzLYM&t=62s : Python OOP Tutorial 1: Classes and Instances.\nhttps://en.wikipedia.org/wiki/Types_of_artificial_neural_networks : Unterschiedliche Sorten des neuronalen Netzwerks.\nhttps://en.wikipedia.org/wiki/Rectifier_(neural_networks) : Rectifier (neuronale Netzworke).\nhttps://towardsdatascience.com/the-mostly-complete-chart-of-neural-networks-explained-3fb6f2367464 : Erklärte Topologien für Neuronale Netzwerke.\nhttps://www.youtube.com/watch?v=QVdf0LgmICw : Python: ""variable scopes"".\n'], 'url_profile': 'https://github.com/Orangetree5', 'info_list': ['10', 'HTML', 'Updated Nov 13, 2020', '10', 'Jupyter Notebook', 'Updated Jan 1, 2020', '1', 'Python', 'Updated Feb 15, 2020', 'Kotlin', 'Updated Jan 5, 2020', 'Updated Jan 3, 2020', 'C++', 'Updated Jan 19, 2020', 'JavaScript', 'Updated Feb 12, 2021', 'Updated Dec 30, 2019', 'C++', 'EPL-2.0 license', 'Updated Jan 2, 2020', 'Updated Jan 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/fsjal', 'info_list': ['10', 'HTML', 'Updated Nov 13, 2020', '10', 'Jupyter Notebook', 'Updated Jan 1, 2020', '1', 'Python', 'Updated Feb 15, 2020', 'Kotlin', 'Updated Jan 5, 2020', 'Updated Jan 3, 2020', 'C++', 'Updated Jan 19, 2020', 'JavaScript', 'Updated Feb 12, 2021', 'Updated Dec 30, 2019', 'C++', 'EPL-2.0 license', 'Updated Jan 2, 2020', 'Updated Jan 6, 2020']}","{'location': 'Malvern, PA', 'stats_list': [], 'contributions': '187 contributions\n        in the last year', 'description': ['Artificial-Intelligence\nWhat is Artificial-Intelligence and Machine Learning?\n\n\nArtifical Intelligence is used go perform tasks that requires human intelligence, such as speech recognition and decision making.\n\n\nMachine learning is part of AI which provides the system for the ability to learn from experiences in order for these\ntechnologies to become better.\n\n\nArtificial-Intelligence in Software Testing\nArtificial-Intelligence Under Software Development\n\n\nAlthough AI has been small as comapred to technolgoies, such as smart speakers or self driving cars, it is still driving foward\n\n\nTools to test software are being made using AI are being used to make software development life cycle easier.\n\n\nIt can be used to reduce the amount of tasks in development and testing.\n\n\nAI enabled bots can identify recent code changes and check current state of tests.\n\n\nAI Techniques\nNeural Machine Translation\n\nIt predicts the sequence of words.\nIt uses neural machine translation.\nIt learns from experience.\n\nHow much memory do they need?\nThey only need a fraction a memory unlike statistical machine translation.\nHow much time do they need?\nThey do not need as much time as statistical machine translation.\nHow does neural machine translation work?\n\nThe neural network takes an input and convert it to vectors in matrices for the computer to understand.\nThe neural network converts the number back.\n\nExamples of nerual machine translation\n\nGoogle translate\nFacial recognition\nSnapchat\n\nHow does neural machine translation work?\n\nDeeping learning is worked through neural networks that handles the info. After that, machine learning does its job where it takes its data to do what it is supposed to do.\nHow does a artificial neural network apply to nerual machine translation ?\n\n\nModel of a Artificial Neural Network\nReal World Example: Google Translate\n\nA word is taken and the neural network converts it to vertices\nThe vertices convert it back to the input in a different language\n\n\nIn this case music can be passed to a neural network in order to make better music.\nResources\n\n\nhttps://blog.parasoft.com/what-is-artificial-intelligence-in-software-testing\n\n\nhttps://youtu.be/bfmFfD2RIcg\n\n\n'], 'url_profile': 'https://github.com/fayedraza', 'info_list': ['10', 'HTML', 'Updated Nov 13, 2020', '10', 'Jupyter Notebook', 'Updated Jan 1, 2020', '1', 'Python', 'Updated Feb 15, 2020', 'Kotlin', 'Updated Jan 5, 2020', 'Updated Jan 3, 2020', 'C++', 'Updated Jan 19, 2020', 'JavaScript', 'Updated Feb 12, 2021', 'Updated Dec 30, 2019', 'C++', 'EPL-2.0 license', 'Updated Jan 2, 2020', 'Updated Jan 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/NikolaiDimitrov', 'info_list': ['10', 'HTML', 'Updated Nov 13, 2020', '10', 'Jupyter Notebook', 'Updated Jan 1, 2020', '1', 'Python', 'Updated Feb 15, 2020', 'Kotlin', 'Updated Jan 5, 2020', 'Updated Jan 3, 2020', 'C++', 'Updated Jan 19, 2020', 'JavaScript', 'Updated Feb 12, 2021', 'Updated Dec 30, 2019', 'C++', 'EPL-2.0 license', 'Updated Jan 2, 2020', 'Updated Jan 6, 2020']}","{'location': 'Toronto, Canada', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['Artificial Intelligence projects\n'], 'url_profile': 'https://github.com/shafaaf', 'info_list': ['10', 'HTML', 'Updated Nov 13, 2020', '10', 'Jupyter Notebook', 'Updated Jan 1, 2020', '1', 'Python', 'Updated Feb 15, 2020', 'Kotlin', 'Updated Jan 5, 2020', 'Updated Jan 3, 2020', 'C++', 'Updated Jan 19, 2020', 'JavaScript', 'Updated Feb 12, 2021', 'Updated Dec 30, 2019', 'C++', 'EPL-2.0 license', 'Updated Jan 2, 2020', 'Updated Jan 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/touzanimo', 'info_list': ['10', 'HTML', 'Updated Nov 13, 2020', '10', 'Jupyter Notebook', 'Updated Jan 1, 2020', '1', 'Python', 'Updated Feb 15, 2020', 'Kotlin', 'Updated Jan 5, 2020', 'Updated Jan 3, 2020', 'C++', 'Updated Jan 19, 2020', 'JavaScript', 'Updated Feb 12, 2021', 'Updated Dec 30, 2019', 'C++', 'EPL-2.0 license', 'Updated Jan 2, 2020', 'Updated Jan 6, 2020']}","{'location': 'Bhopal', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/SRsk786', 'info_list': ['10', 'HTML', 'Updated Nov 13, 2020', '10', 'Jupyter Notebook', 'Updated Jan 1, 2020', '1', 'Python', 'Updated Feb 15, 2020', 'Kotlin', 'Updated Jan 5, 2020', 'Updated Jan 3, 2020', 'C++', 'Updated Jan 19, 2020', 'JavaScript', 'Updated Feb 12, 2021', 'Updated Dec 30, 2019', 'C++', 'EPL-2.0 license', 'Updated Jan 2, 2020', 'Updated Jan 6, 2020']}","{'location': 'Bozeman, MT', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['Artificial-Intelligence\nMontana State University, CSCI 446 projects\nProjects Overview\nProject 1: Search Algorithms\nProject 1 uses breadth/depth first searches, greedy algorithm, and A-star to ultimately solve puzzles, in this case, randomly generated mazes of different sizes.\nThis involves structuring the maze such that it can be represented as a graph and or tree.\nProject 2: Constraint Satisfaction Problems\nProject 2 focuses on CSP, a manner in which the agent must satisfy constaints using heuristics. In this case, a common phone game is implemented but rather a user solving the maze, the agent solves it. The constraints are used to tether the agent in which it will actually solve the maze. This is implemented as a decision tree such that the contraints eliminate possible options.\nProject 3: First-Order Logic\nProject 3 uses reinforcement learning with the application of first-order logic. The program is known as ""Wumpus World"" where an agent is to perform a task while avoiding obstacles using the first order logic. The overall idea is that the agent makes a decision with respect to its current state in training. Once the training is complete, a pruning process is used to eliminate decisions in which the agent would not receive a reward.\n'], 'url_profile': 'https://github.com/Jrkeeling23', 'info_list': ['10', 'HTML', 'Updated Nov 13, 2020', '10', 'Jupyter Notebook', 'Updated Jan 1, 2020', '1', 'Python', 'Updated Feb 15, 2020', 'Kotlin', 'Updated Jan 5, 2020', 'Updated Jan 3, 2020', 'C++', 'Updated Jan 19, 2020', 'JavaScript', 'Updated Feb 12, 2021', 'Updated Dec 30, 2019', 'C++', 'EPL-2.0 license', 'Updated Jan 2, 2020', 'Updated Jan 6, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['Artificial-Intelligence\n'], 'url_profile': 'https://github.com/jawahar17', 'info_list': ['Jupyter Notebook', 'Updated Jan 26, 2020', '1', 'C++', 'Updated Dec 31, 2019', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 3, 2020', 'Updated Jan 5, 2020', 'Updated Jan 5, 2020', 'Updated Jan 3, 2020', 'ShaderLab', 'Updated Apr 17, 2020', 'Updated Dec 12, 2020', '1', 'Python', 'Updated Jan 23, 2020']}","{'location': 'Lahore, Pakistan', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Hammad-Ikhlaq', 'info_list': ['Jupyter Notebook', 'Updated Jan 26, 2020', '1', 'C++', 'Updated Dec 31, 2019', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 3, 2020', 'Updated Jan 5, 2020', 'Updated Jan 5, 2020', 'Updated Jan 3, 2020', 'ShaderLab', 'Updated Apr 17, 2020', 'Updated Dec 12, 2020', '1', 'Python', 'Updated Jan 23, 2020']}","{'location': 'Raleigh, North Carolina', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': ['Artificial-Intelligence\nThis repository contains problems of artificial intelligences and their solutions in python3\nAll code files are written in python3\nSuitable sample data is also given to run the code\n'], 'url_profile': 'https://github.com/shaival2905', 'info_list': ['Jupyter Notebook', 'Updated Jan 26, 2020', '1', 'C++', 'Updated Dec 31, 2019', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 3, 2020', 'Updated Jan 5, 2020', 'Updated Jan 5, 2020', 'Updated Jan 3, 2020', 'ShaderLab', 'Updated Apr 17, 2020', 'Updated Dec 12, 2020', '1', 'Python', 'Updated Jan 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '44 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/bilaleluneis', 'info_list': ['Jupyter Notebook', 'Updated Jan 26, 2020', '1', 'C++', 'Updated Dec 31, 2019', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 3, 2020', 'Updated Jan 5, 2020', 'Updated Jan 5, 2020', 'Updated Jan 3, 2020', 'ShaderLab', 'Updated Apr 17, 2020', 'Updated Dec 12, 2020', '1', 'Python', 'Updated Jan 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['AI\nArtificial Intelligence\n'], 'url_profile': 'https://github.com/pradeeprt10', 'info_list': ['Jupyter Notebook', 'Updated Jan 26, 2020', '1', 'C++', 'Updated Dec 31, 2019', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 3, 2020', 'Updated Jan 5, 2020', 'Updated Jan 5, 2020', 'Updated Jan 3, 2020', 'ShaderLab', 'Updated Apr 17, 2020', 'Updated Dec 12, 2020', '1', 'Python', 'Updated Jan 23, 2020']}","{'location': 'Dublin', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/sajevk', 'info_list': ['Jupyter Notebook', 'Updated Jan 26, 2020', '1', 'C++', 'Updated Dec 31, 2019', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 3, 2020', 'Updated Jan 5, 2020', 'Updated Jan 5, 2020', 'Updated Jan 3, 2020', 'ShaderLab', 'Updated Apr 17, 2020', 'Updated Dec 12, 2020', '1', 'Python', 'Updated Jan 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/gokul-code', 'info_list': ['Jupyter Notebook', 'Updated Jan 26, 2020', '1', 'C++', 'Updated Dec 31, 2019', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 3, 2020', 'Updated Jan 5, 2020', 'Updated Jan 5, 2020', 'Updated Jan 3, 2020', 'ShaderLab', 'Updated Apr 17, 2020', 'Updated Dec 12, 2020', '1', 'Python', 'Updated Jan 23, 2020']}","{'location': 'Chicago, Illinois', 'stats_list': [], 'contributions': '171 contributions\n        in the last year', 'description': ['UnityArtificialIntelligencePlayground\n\n\nA playground for my personal learning of AI using Unity\n\nFor a technical write up about the making of this game, please check out my portfolio entry.\n'], 'url_profile': 'https://github.com/hodge47', 'info_list': ['Jupyter Notebook', 'Updated Jan 26, 2020', '1', 'C++', 'Updated Dec 31, 2019', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 3, 2020', 'Updated Jan 5, 2020', 'Updated Jan 5, 2020', 'Updated Jan 3, 2020', 'ShaderLab', 'Updated Apr 17, 2020', 'Updated Dec 12, 2020', '1', 'Python', 'Updated Jan 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['Artificial-Intelligence-Research\n'], 'url_profile': 'https://github.com/ackais', 'info_list': ['Jupyter Notebook', 'Updated Jan 26, 2020', '1', 'C++', 'Updated Dec 31, 2019', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 3, 2020', 'Updated Jan 5, 2020', 'Updated Jan 5, 2020', 'Updated Jan 3, 2020', 'ShaderLab', 'Updated Apr 17, 2020', 'Updated Dec 12, 2020', '1', 'Python', 'Updated Jan 23, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/renanaya48', 'info_list': ['Jupyter Notebook', 'Updated Jan 26, 2020', '1', 'C++', 'Updated Dec 31, 2019', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 3, 2020', 'Updated Jan 5, 2020', 'Updated Jan 5, 2020', 'Updated Jan 3, 2020', 'ShaderLab', 'Updated Apr 17, 2020', 'Updated Dec 12, 2020', '1', 'Python', 'Updated Jan 23, 2020']}"
"{'location': 'Evanston', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['Intro-to-Artificial-Intelligence\nadversarial games, classification, and regression\nWPI CS:4341 (Introduction to Artificial Intelligence)\nGroup 28 (Nikolas Gamarra, Adam Moran, Ying Zhang)\nTerm Project: Bomberman AI\nProject Strategy\nOur overall strategy for this project was to begin with implementing A* to ensure\nwe had a simple basic algorithm that could tackle most of the challenges. Once A* was\nimplemented we set about improving our A* heuristic until we hit a limit of what we\nthought was achievable with A*. In order to try and solve some of the shortcomings of\nA*, we attempted to implement more advanced algorithms like expectimax and policy\niteration.\nA* Search Algorithm\nIn order to make our path we did an implementation of A*. There were no major\ndifferences between our implementation and traditional A* except that our priority\nqueue was structured as a tuple of a location tuple and a priority. We also created a\ncouple of simple helper functions to find the distance between two tuples, and check if\na monster is within a certain radius of a cell in order to inform our heuristic. We also\nneeded to implement a simple priority queue class. For our heuristic, normal cells were\ngiven a value of 1, walls were given a value proportional to the fuse time +20, cost of 60\nwas given to cells within a radius of 5 of a monster and a cost of 80 was given to cells\nwithin a radius of 2 of a monster. The A* algorithm returned a dictionary of the came\nfrom path. This dictionary was iterated backward into a list and then the next move was\nextracted. In the event that A* could not find a path (which happened when we were\nstuck in a corner between an explosion) a try-catch statement would prevent the\nprogram from dying and move the character to a random safe cell. Because walls had a\nvery high cost, our agent generally avoids them but will path to them if there is no other\noption or if the other option gets too close to monsters. In the general motion of the\ncharacter, a bomb is placed whenever a move is attempted into a wall.\nIn order to avoid pathing into cells that would be exploding in the next turn,\nseveral helper functions were added to keep track of where a bomb had been placed\nand how long till it exploded. If the character ever detects that it is attempting to move\ninto a cell that will explode it will instead make a random safe choice.\nA* 7 was made in an attempt to solve the issue of pathing into corners with\navoid_monster3. Unfortunately, this implementation did not produce any better results\nfor any variant than the other implementations we had.\nUtility Functions\nIn order to tackle the problem, we created several helper functions that are\nuniversally helpful.\nPlace smart bombs and several other helper functions keep a record of which\ncells will be exploding and when. Using this A* can have the ability to peer into the\nfuture without the help of looking into future states of the map or using expectimax.\nUsing this we could simply and effectively create an A* solution that would never path\ninto a cell that is about to explode.\nAnother important helper function we created for our A* was avoid_monster. This\nfunction was improved over three versions. In the event that a monster is found within a\ncertain radius of the character A* stops and instead the character tries to run away from\nthe monster. If the monster is in line with a potential bomb placement or within a certain\nradius a bomb will be placed. The avoid_monster2 behavior has a three part heuristic.\nFirst, the valid neighbors are considered. Of those, cells closer to the exit, further away\nfrom the monster, and bordering fewer walls score better. Avoid walls was added to\nattempt to avoid getting cornered. Ultimately We found that this was not enough to\nalways avoid getting trapped in corners.\nTo solve getting cornered we created avoid_monster3. This function does a\nsimple look into the future with sensed worlds and checks if it will survive running in for\na certain depth in each direction if the monster is aggressive and chases it. The set of\nresults where it survived is examined and the one furthest from the closest monster is\nchosen as the next move.\nExpectimax\nIn expectimax search, we estimated the probabilistic model of how the monster\nwill move in any state from the sensed world, we obtained utilities from the outcomes of\nthe current states of the world, and emerge the action. The implement of the\nprobability-based algorithm and reward system allows our agent to evaluate the risks\nand rewards of each move, which largely increased the pass rate.\nThe pseudocode of Expectimax we used is as follow:\ndef value(s):\nif s is a max node\nreturn maxValue(s)\nif s is an exp node\nreturn expValue(s)\nif s is a terminal node\nreturn evaluation(s)\ndef maxValue(s):\nvalues = [value(s’) for s’ in successors(s)]\nreturn max(values)\ndef expValue(s):\nvalues = [value(s’) for s’ in successors(s)]\nweights = [probability(s, s’) for s’ in successors(s)]\nreturn expectation(values, weights)\nFor future improvement, we would spend more time on developing the reward\nsystem. The current one is working but sometimes making unnecessary actions which\nmade the agent vulnerable from the placed bomb and monsters. More importantly, we\nwould increase the search level to go deeper for more accurate results.\nPolicy Iteration\nIn policy iteration, we basically wanted the agent to find the most beneficial\nactions in any given state. The idea was that we used a “SensedWorld” to modify the\nreturned world by performing actions or change the state of the environment without\naffecting other existing world instances. With the exception of the agent occupying the\nsame coordinates as the exit cell, all rewards would be negative to incentivize moving\ntowards the exit. It used the same code used to avoid corners, calculate proximity to a\nmonster, detect explosions before they happen, and calculate distances between two\ntuples to produce rewards. Unfortunately, how we would end up finding states would\nnot include all of the possible states (especially since we would have to consider each\ncombination of agent position, monster position, bomb position, explosion position, and\nwall position to have all the states and therefore the best action to do for each state)\nand would therefore create an incomplete policy, which defeats the purpose of doing\npolicy iteration.\nSolutions\nIn order to evaluate which test character we would use for each variant, we created a\nspreadsheet to track the performance of each one across multiple unseeded random trials. This\nway we could ensure we had the best performance possible for each variant and scenario.\nBelow this table is a simple explanation of why we chose that algorithm and why it performed\nwell. For an in-depth explanation of how the algorithm works look into the above sections on\neach algorithm.\nA4(retreat) A5(normal) A6(cheese) Expectimax 3 Expectimax 4 Policy Iteration A7(avoid3) BEST\nTotal: 72.36% 74.27% 33.00% 64.00% 77.73% 0.00% 67.00% 81.36%\nS1 ADV 66.0% 72.5% 66.0% 70.0% 76.0% 0.0% 74.0% 80.0%\nS1 V1 100.0% 100.0% 100.0% 100.0% 100.0% 0.0% 100.0% 100.0%\nS1 V2 90.0% 100.0% 100.0% 90.0% 90.0% 0.0% 100.0% 100.0%\nS1 V3 50.0% 72.7% 80.0% 50.0% 90.0% 0.0% 70.0% 90.0%\nS1 V4 40.0% 50.0% 50.0% 60.0% 60.0% 0.0% 50.0% 60.0%\nS1 V5 50.0% 40.0% 0.0% 50.0% 40.0% 0.0% 50.0% 50.0%\nS2 ADV 78.7% 76.0% 0.0% 58.0% 79.5% 0.0% 60.0% 82.7%\nS2 V1 100.0% 100.0% 0.0% 100.0% 100.0% 0.0% 100.0% 100.0%\nS2 V2 100.0% 100.0% 0.0% 90.0% 90.0% 0.0% 100.0% 100.0%\nS2 V3 75.0% 66.7% 0.0% 40.0% 73.3% 0.0% 70.0% 75.0%\nS2 V4 68.6% 66.7% 0.0% 50.0% 64.0% 0.0% 20.0% 68.6%\nS2 V5 50.0% 46.7% 0.0% 10.0% 70.0% 0.0% 10.0% 70.0%\nScenario 1:\nVariant 1:\nWe simply used our simplest version of A* (#5) for maps without monsters as it was reliable and\neffective. We chose to use the simplest versions for these as it would be less likely to run into\nerrors.\nVariant 2:\nFor scenario 1 we used our normal A* (#5) as it scored the best in our trials\nVariant 3:\nFor this variant we used our implementation of Expectimax (#4) as is scored best in our trials.\nVariant 4:\nFor this variant we used our implementation of Expectimax (#4) as is scored best in our trials.\nVariant 5:\nFor scenario 1 we used our retreating A* (#4) as it scored the best in our trials\nScenario 2:\nVariant 1:\nWe simply used our simplest version of A* (#5) for maps without monsters as it was reliable and\neffective. We chose to use the simplest versions for these as it would be less likely to run into\nerrors.\nVariant 2:\nWe again used A* (#5) for this variant as it consistently scored 100%\nVariant 3:\nFor scenario 2 we used a special version of A* (#4) that would retreat to the y location of the\nstart position. This behavior was implemented because we realized it was dangerous to enter\nthe tight spaces while a monster was present.\nVariant 4:\nFor scenario 2 we used a special version of A* (#4) that would retreat to the y location of the\nstart position. This behavior was implemented because we realized it was dangerous to enter\nthe tight spaces while a monster was present.\nVariant 5:\nFor this variant we used our implementation of Expectimax (#4) as is scored best in our trials.\nThis was because the different heuristic that was used for it performed better in variants with a\nlot of monsters as it moved more cautiously and placed more bombs.\nConclusion\nIn conclusion we were able to successfully exit the map over 80% of the time in our own testing\nwhen using the best algorithm for each scenario and variant. While we would have liked to\nimplement some more advanced AIs given more time, we successfully met our target of being\nable to get a good grade based on the rubric in our own tests.\n'], 'url_profile': 'https://github.com/PRINTF-yzhang', 'info_list': ['Python', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Aug 27, 2020', 'Updated Jan 4, 2020', 'Python', 'Updated Dec 31, 2019', '1', 'MIT license', 'Updated Jan 2, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 2, 2020', 'Updated Feb 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/faheelsattar', 'info_list': ['Python', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Aug 27, 2020', 'Updated Jan 4, 2020', 'Python', 'Updated Dec 31, 2019', '1', 'MIT license', 'Updated Jan 2, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 2, 2020', 'Updated Feb 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '928 contributions\n        in the last year', 'description': ['MNIST-Artificial-Intelligence\nArtificial intelligence projects using the MNIST dataset and Python, Scikit-learn, and TensorFlow.\n\nPerceptron, Logistic Regression, SVM 분류기를 이용하여 최상의 MNIST학습 결과 도출하기\nSVM Classifier + Stochastic Gradient Descen 으로 MNIST의 Multi-Label Classification Problem 해결하기\nStandard Scaling, PCA, Convolution, Polynomial 전처리작업 및 작업물을 그래프로 도식화시키기\n\n\nPBL01\nThe given PBL Case is making a system to read the serial numbers of the car parts produced in their factory.\nNamely, It is creating an accurate digit classification system.\nAnd then, our object is finding the best model out of the given classifiers and hyperparameters.\nThe classifiers are Logistic Regression and Support Vector Machine, and a data is MNIST as we saw in class, and a criterion is F1-score, which is the harmonic mean of precision and recall.\nThe problem of our project is to find out which Machine Learning model would be best to classify this MNIST.\n\nPBL02\nUsing an SVM classifier, implement a maximized margin aolong with the data can be classified. \nStochastic Gradient Descent would then be used along with Mini-Batches to update the weights in order to create a solution for the multi-label classification prolbem at hand.\nTo conclude,\nData scaling proved incredibly important, having increased the accuracy in every case where we implemented it.\nPCA also allowed our algorithms to run in a much shorter time however retained accuracy all the while.\nHowever, ran on its own PCA without scaling is not recommendable since normalization is needed prior to PCA.\nGridSearch although time consuming proved incredibly important in the grand scheme of hyperparameter tuning.\n\nPBL03\nThe training data consisted of a total of 80,000 data points including the D1, D2 and the new1k data set. Of the 80,000 data, 30% of it was separated and used as the test set.\nWe set our hyperparameters; C, learning rate and batch size to the following values.\nConvolution, PCA and Polynomial were used to preprocess the data.\n\n'], 'url_profile': 'https://github.com/ChaeLinYeo', 'info_list': ['Python', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Aug 27, 2020', 'Updated Jan 4, 2020', 'Python', 'Updated Dec 31, 2019', '1', 'MIT license', 'Updated Jan 2, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 2, 2020', 'Updated Feb 17, 2020']}","{'location': 'Stony Brook', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Artificial-Intelligence-Projects\nProjects of AI Course\n'], 'url_profile': 'https://github.com/DeepthiCherukuri', 'info_list': ['Python', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Aug 27, 2020', 'Updated Jan 4, 2020', 'Python', 'Updated Dec 31, 2019', '1', 'MIT license', 'Updated Jan 2, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 2, 2020', 'Updated Feb 17, 2020']}","{'location': 'Dhaka', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/showkoth', 'info_list': ['Python', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Aug 27, 2020', 'Updated Jan 4, 2020', 'Python', 'Updated Dec 31, 2019', '1', 'MIT license', 'Updated Jan 2, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 2, 2020', 'Updated Feb 17, 2020']}","{'location': 'Luxembourg', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': [""Artificial Intelligence cheatsheets for Stanford's CS 221\nAvailable in English - Français - Türkçe\nGoal\nThis repository aims at summing up in the same place all the important notions that are covered in Stanford's CS 221 Artificial Intelligence course, and include:\n\nCheatsheets for each artificial intelligence field\nAll elements of the above combined in an ultimate compilation of concepts, to have with you at all times!\n\nContent\nVIP Cheatsheets\n\n\n\n\n\n\n\n\n\n\n\nReflex-based models\nStates-based models\nVariables-based models\nLogic-based models\n\n\n\nSuper VIP Cheatsheet\n\n\n\n\n\n\n\n\nAll the above gathered in one place\n\n\n\nWebsite\nThis material is also available on a dedicated website, so that you can enjoy reading it from any device.\nAuthors\nAfshine Amidi (Ecole Centrale Paris, MIT) and Shervine Amidi (Ecole Centrale Paris, Stanford University)\n""], 'url_profile': 'https://github.com/Moado', 'info_list': ['Python', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Aug 27, 2020', 'Updated Jan 4, 2020', 'Python', 'Updated Dec 31, 2019', '1', 'MIT license', 'Updated Jan 2, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 2, 2020', 'Updated Feb 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '316 contributions\n        in the last year', 'description': ['This assignment is from Free University of Tbilisi\'s AI course, which is based on University of California, Berkeley\'s ""CS 188 | Introduction to Artificial Intelligence"" course.\n'], 'url_profile': 'https://github.com/lkito', 'info_list': ['Python', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Aug 27, 2020', 'Updated Jan 4, 2020', 'Python', 'Updated Dec 31, 2019', '1', 'MIT license', 'Updated Jan 2, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 2, 2020', 'Updated Feb 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '316 contributions\n        in the last year', 'description': ['This assignment is from Free University of Tbilisi\'s AI course, which is based on University of California, Berkeley\'s ""CS 188 | Introduction to Artificial Intelligence"" course.\n'], 'url_profile': 'https://github.com/lkito', 'info_list': ['Python', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Aug 27, 2020', 'Updated Jan 4, 2020', 'Python', 'Updated Dec 31, 2019', '1', 'MIT license', 'Updated Jan 2, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 2, 2020', 'Updated Feb 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '61 contributions\n        in the last year', 'description': ['AI-ACW\nArtificial Intelligence ACW project\n'], 'url_profile': 'https://github.com/dcontini5', 'info_list': ['Python', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Aug 27, 2020', 'Updated Jan 4, 2020', 'Python', 'Updated Dec 31, 2019', '1', 'MIT license', 'Updated Jan 2, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 2, 2020', 'Updated Feb 17, 2020']}","{'location': 'Visakhapatnam', 'stats_list': [], 'contributions': '364 contributions\n        in the last year', 'description': ['Article-on-AI\nAn article on Artificial Intelligence.\n'], 'url_profile': 'https://github.com/SudeepaNoble', 'info_list': ['Python', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 3, 2020', 'Jupyter Notebook', 'Updated Aug 27, 2020', 'Updated Jan 4, 2020', 'Python', 'Updated Dec 31, 2019', '1', 'MIT license', 'Updated Jan 2, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 2, 2020', 'Updated Feb 17, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '116 contributions\n        in the last year', 'description': [""Nestor\nAn artificial intelligence based chatbot powered by Google's Dialogflow that possesses human capabilities.\nCurrently putting a currency conversion and language-deciphering components.\n""], 'url_profile': 'https://github.com/ykabusalah', 'info_list': ['Updated Jan 3, 2020', 'GPL-3.0 license', 'Updated Jan 5, 2020', '1', 'TeX', 'Updated Apr 20, 2020', 'MATLAB', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 3, 2020', '1', 'Java', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', '1', 'Python', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['intellect-supreme\nArtificial Intelligence research\n'], 'url_profile': 'https://github.com/josephaan', 'info_list': ['Updated Jan 3, 2020', 'GPL-3.0 license', 'Updated Jan 5, 2020', '1', 'TeX', 'Updated Apr 20, 2020', 'MATLAB', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 3, 2020', '1', 'Java', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', '1', 'Python', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 5, 2020']}","{'location': 'Montreal, Quebec', 'stats_list': [], 'contributions': '104 contributions\n        in the last year', 'description': ['Aritifical Intelligence - Winter 2020\nProject 1 - State Space Search\nProject 2 - Naive Bayes Classifier\n'], 'url_profile': 'https://github.com/matteo-esposito', 'info_list': ['Updated Jan 3, 2020', 'GPL-3.0 license', 'Updated Jan 5, 2020', '1', 'TeX', 'Updated Apr 20, 2020', 'MATLAB', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 3, 2020', '1', 'Java', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', '1', 'Python', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Pet-Dog-Recommendation-Expert-System\nProject of Artificial Intelligence\n'], 'url_profile': 'https://github.com/qjwmelody', 'info_list': ['Updated Jan 3, 2020', 'GPL-3.0 license', 'Updated Jan 5, 2020', '1', 'TeX', 'Updated Apr 20, 2020', 'MATLAB', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 3, 2020', '1', 'Java', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', '1', 'Python', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mertunal0', 'info_list': ['Updated Jan 3, 2020', 'GPL-3.0 license', 'Updated Jan 5, 2020', '1', 'TeX', 'Updated Apr 20, 2020', 'MATLAB', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 3, 2020', '1', 'Java', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', '1', 'Python', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 5, 2020']}","{'location': 'Iran', 'stats_list': [], 'contributions': '205 contributions\n        in the last year', 'description': ['AI951\nArtificial Intelligence - Fall 2016\n'], 'url_profile': 'https://github.com/SadraSamadi', 'info_list': ['Updated Jan 3, 2020', 'GPL-3.0 license', 'Updated Jan 5, 2020', '1', 'TeX', 'Updated Apr 20, 2020', 'MATLAB', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 3, 2020', '1', 'Java', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', '1', 'Python', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '316 contributions\n        in the last year', 'description': ['This assignment is from Free University of Tbilisi\'s AI course, which is based on University of California, Berkeley\'s ""CS 188 | Introduction to Artificial Intelligence"" course.\n'], 'url_profile': 'https://github.com/lkito', 'info_list': ['Updated Jan 3, 2020', 'GPL-3.0 license', 'Updated Jan 5, 2020', '1', 'TeX', 'Updated Apr 20, 2020', 'MATLAB', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 3, 2020', '1', 'Java', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', '1', 'Python', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '316 contributions\n        in the last year', 'description': ['This assignment is from Free University of Tbilisi\'s AI course, which is based on University of California, Berkeley\'s ""CS 188 | Introduction to Artificial Intelligence"" course.\n'], 'url_profile': 'https://github.com/lkito', 'info_list': ['Updated Jan 3, 2020', 'GPL-3.0 license', 'Updated Jan 5, 2020', '1', 'TeX', 'Updated Apr 20, 2020', 'MATLAB', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 3, 2020', '1', 'Java', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', '1', 'Python', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/plamenpasliev', 'info_list': ['Updated Jan 3, 2020', 'GPL-3.0 license', 'Updated Jan 5, 2020', '1', 'TeX', 'Updated Apr 20, 2020', 'MATLAB', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 3, 2020', '1', 'Java', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', '1', 'Python', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 5, 2020']}","{'location': 'Colombo, Sri Lanka', 'stats_list': [], 'contributions': '26 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Sanuja91', 'info_list': ['Updated Jan 3, 2020', 'GPL-3.0 license', 'Updated Jan 5, 2020', '1', 'TeX', 'Updated Apr 20, 2020', 'MATLAB', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 3, 2020', '1', 'Java', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', '1', 'Python', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 5, 2020']}"
"{'location': 'Paraguay', 'stats_list': [], 'contributions': '92 contributions\n        in the last year', 'description': ['PrivAI\n\nPrivacy Preserving Artificial Intelligence Algorithms\n\nIntroduccion\nImplementacion de algoritmos para entrenar modelos de machine learning preservando la privacidad de los usuarios\nRequirements\n\npysyft[udacity]\n\n'], 'url_profile': 'https://github.com/sgaseretto', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 29, 2020', 'Python', 'Updated Sep 13, 2020', '3', 'JavaScript', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 5, 2020', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 26, 2020', 'Updated Jan 3, 2020', 'Updated Jan 3, 2020', 'CSS', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 14, 2020']}","{'location': 'USA', 'stats_list': [], 'contributions': '54 contributions\n        in the last year', 'description': ['Artificial Intelligence\nThis is the pallanghuzhi game code. Pallanguzhi is a two player game. It is a sister game to mancala which is popular in AI since a lot of agents were created to play mancala. Pallenguzhi differs slightly in its rules when compared to mancala, regardless, it’s still a counting game.\nAI Agent: This projects work with different agents playing the game “Pallanguzhi”. The agents employed the algorithms: Greedy, Min-Max, Min-Max with alpha-beta pruning, Genetic Algorithm and A3C. Each agent played against another agent to test how well the different heuristics (strategies) helped the agent win and the observations were recorded.\nThere are two python files game.py and board.py.\nboard.py file contains game board, no of pits and pieces in each pit along with some of the methods.\ngame.py file contains all agent algorithms that is naive algorithm, greedy algorithm, min max algorithm, min max with alpha beta prunning algorithm.\nTo run the program.\nYou can use python version 2.7 or 3.0 or greater.\nOpen Console\nGo to the directory of the program file and then execute below line in the console\npython game.py\n\nYou can see an output in the console.\nDEMO\nNaïve vs Naïve\n\n\n\n\nNaïve vs Greedy\n\n\n\n\nNaïve vs Min-Max with alpha-beta pruning\n\n\n\n\nMin-Max vs Greedy\n\n\n\n\nCheckout project section for more details: https://kepy.online/\nCheckout Demo of this project here: https://youtu.be/TBmMEyluORQ\n'], 'url_profile': 'https://github.com/kepy97', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 29, 2020', 'Python', 'Updated Sep 13, 2020', '3', 'JavaScript', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 5, 2020', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 26, 2020', 'Updated Jan 3, 2020', 'Updated Jan 3, 2020', 'CSS', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['CS181-BBTan-Project\nWe are 4 sophomores from ShanghaiTech University. This is our final project for course CS181 which aims to play BBTan using basic Q-learning and advanced DQN.\nOur work contains 2 parts.\n1. Using basic Q-learning with human-manufactured features.\n2. Combining convolution neutral network to Q-learning based on visual input.\n'], 'url_profile': 'https://github.com/sheyining', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 29, 2020', 'Python', 'Updated Sep 13, 2020', '3', 'JavaScript', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 5, 2020', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 26, 2020', 'Updated Jan 3, 2020', 'Updated Jan 3, 2020', 'CSS', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 14, 2020']}","{'location': 'Fairfax, VA', 'stats_list': [], 'contributions': '70 contributions\n        in the last year', 'description': ['CS221: Artificial Intelligence - Autumn 2019 - 2020\nFor overview, look at cheatsheet by Shervine Amidi\n'], 'url_profile': 'https://github.com/domhuh', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 29, 2020', 'Python', 'Updated Sep 13, 2020', '3', 'JavaScript', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 5, 2020', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 26, 2020', 'Updated Jan 3, 2020', 'Updated Jan 3, 2020', 'CSS', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 14, 2020']}","{'location': 'Bengaluru', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/LubnaFirdouse', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 29, 2020', 'Python', 'Updated Sep 13, 2020', '3', 'JavaScript', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 5, 2020', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 26, 2020', 'Updated Jan 3, 2020', 'Updated Jan 3, 2020', 'CSS', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Matrix\nMy first project on artificial intelligence\n'], 'url_profile': 'https://github.com/Uganda-Knuckles', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 29, 2020', 'Python', 'Updated Sep 13, 2020', '3', 'JavaScript', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 5, 2020', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 26, 2020', 'Updated Jan 3, 2020', 'Updated Jan 3, 2020', 'CSS', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['A5\nVision recognition technology based on Artificial Intelligence\n'], 'url_profile': 'https://github.com/songshiweilai', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 29, 2020', 'Python', 'Updated Sep 13, 2020', '3', 'JavaScript', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 5, 2020', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 26, 2020', 'Updated Jan 3, 2020', 'Updated Jan 3, 2020', 'CSS', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 14, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['A5\nVision recognition technology based on Artificial Intelligence\n'], 'url_profile': 'https://github.com/fallinlovewitheattingshit', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 29, 2020', 'Python', 'Updated Sep 13, 2020', '3', 'JavaScript', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 5, 2020', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 26, 2020', 'Updated Jan 3, 2020', 'Updated Jan 3, 2020', 'CSS', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 14, 2020']}","{'location': 'Bangalore, India', 'stats_list': [], 'contributions': '329 contributions\n        in the last year', 'description': ['2048 - Solving using Artificial Intelligence\nObjective:\nIn this project we aim to create an artificial intelligence to solve the 2048 game consistently i.e. trying to reach the score of 2048. We will be using the minimax algorithm for the main game play with alpha-beta pruning to reduce the redundant moves and use the combination of several heuristics to get the ideal heuristic to compare the validity of the different generated states in helping us reach the 2048 tile most consistently and efficiently.\n'], 'url_profile': 'https://github.com/tskk97', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 29, 2020', 'Python', 'Updated Sep 13, 2020', '3', 'JavaScript', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 5, 2020', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 26, 2020', 'Updated Jan 3, 2020', 'Updated Jan 3, 2020', 'CSS', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 14, 2020']}","{'location': 'Bucharest', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Madaist', 'info_list': ['Jupyter Notebook', 'Apache-2.0 license', 'Updated Jan 29, 2020', 'Python', 'Updated Sep 13, 2020', '3', 'JavaScript', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 5, 2020', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 26, 2020', 'Updated Jan 3, 2020', 'Updated Jan 3, 2020', 'CSS', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 14, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '156 contributions\n        in the last year', 'description': [""rl-hypothesis-1\nThis is an attempt at artificial general intelligence (AGI), defining AGI to be capable of arbitrary problem solving even if not particularly intelligent. To date, the closest solution is reinforcement learning (RL). RL is great at solving simulatable games, so is now used to solve video games. However, a true automaton should not be constrained to video games. To an RL agent, the only difference between reality and video games is the amount of data--there is far less data in the real world. To work with this constraint, I fit a variational autoencoder (VAE) to the data distribution and can thus pad the dataset with simulants. So, the fundamental question is: can VAEs pad AGI agents' data sets sufficiently to achieve real-world RL? This repo test this question.\nOne major caveat is that VAEs aren't powerful enough to simulate totally-accurate game transitions (RL data). To get around this, I'm applying transfer learning. The transfer learning transform is summarized in this notebook. The majority of my q-Net (RL deep net) is trained on a very large RL dataset. Only the upper-most dense net will be fit on VAE-simulated data. Simulation quality is analyzed in this notebook. This enables the VAE to simulate an embedding instead of exact game states. This should also abstract-away visual processing information, leaving distilled strategic information. Transfer learning will gives VAEs a chance to succeed. I feel this concession is acceptable because a similar design choice has effectively been applied in biological evolution--even humans' preceptive mechanisms are largely based in our animal origins.\nThis experiment's engineering has been fascinating, involving non-trivial scaling challenges. PySpark manages work communication and execution, but only executes workers' Python as child processes. This enables workers' Python to be arbitrarily complex, not constrained to PySpark's definitions nor the JVM, and is necessary for RL and VAE software. PySpark is executed in containers orchestrated by Kubernetes. Containers' builds abstract-away RL and VAE complexity, exposing a simple Python interface to PySpark.\nThe hypothesis is tested by executing an RL experiment with variably padded data, playing Breakout. If VAE-padding can increase the average score, we are one important step closer to releasing AGI into the real world. Latest experimental results are summarized here.\nResources\nFast and easy experimentation is prioritized. Content will be copied from public repos to avoid manual programming when possible.\n\nbase q-net software here\nexecution environment here\n\nNVidia TOS denies distribution of cudnn lib, so environment is CPU-only. This isn't a big setback since game simulation eats most of the time and is largely CPU-driven.\n\n\ncVAE here\n\nI used a conditional generative method because I would otherwise doubt its capability to simulate non-continuous values.\nDiscrete values are simulated from their empirical distributions.\nOriginally, I used GANs but prefer to work with the VAE's clearly defined loss function.\n\n\n\nI've tested the q-net and VAE software. It's good stuff. The q-net really needs some heavy parallelization, ideally a parameter server. However, I don't have the dev time and will eat the cycles instead. Fortunately, it seems to be using available CPU cores (up to 14) and does seem to need a lot of RAM--so, I'll use a beefy node.\nBuild\n\nBuild environment: GCP console.\nConfigure with config.sh\n\nService account required.\n\n\nExecute build with bash build.sh\n\nExecution\n\nFit initial model\n\nRun bash 1-initial-fit.sh\n\n\nRun transfer learning transform\n\nRun bash 2-transform.sh\n\n\nFit VAE\n\nRun bash 3-fit-vae.sh\n\n\nSimple evaluation\n\nRun bash 4-scaled-simple-eval.sh\nLaunches Spark on Kubernetes cluster\nGenerates samples from VAE\nFits a transfer-learned q-net\nReturns game play performance statistics\n\n\n\nHow it works\n\nEngineering details\n\nCompute environment is a single, multi-purpose container.\nVMs and Clusters will auto-terminate after executing work.\nSpark worker nodes are assigned to preemptible machines, reducing compute costs by about 5 times.\n\nResults\nData suggests a weak negative result. VAE-padding isn't providing statistically significant lift. Fortunately, the hypothesis isn't necessarily voided. Sample sizes have been low so far, so increasing sample sizes could illustrate small, yet-positive effect sizes. Also, not enough time has been put into designing a high-quality generative process. There's still plenty of room for a positive result here.\n""], 'url_profile': 'https://github.com/wdurno', 'info_list': ['Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', 'MIT license', 'Updated Dec 31, 2019', 'C++', 'Apache-2.0 license', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 9, 2020', 'Python', 'Updated Dec 30, 2019', '1', 'JavaScript', 'GPL-3.0 license', 'Updated Jan 2, 2020', 'JavaScript', 'Updated Jan 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '157 contributions\n        in the last year', 'description': ['SYNBIO_AI\nArtificial Intelligence tools for synthetic biology\n'], 'url_profile': 'https://github.com/Gonza10V', 'info_list': ['Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', 'MIT license', 'Updated Dec 31, 2019', 'C++', 'Apache-2.0 license', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 9, 2020', 'Python', 'Updated Dec 30, 2019', '1', 'JavaScript', 'GPL-3.0 license', 'Updated Jan 2, 2020', 'JavaScript', 'Updated Jan 6, 2020']}","{'location': 'Bhubaneswar', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/amjadbhai', 'info_list': ['Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', 'MIT license', 'Updated Dec 31, 2019', 'C++', 'Apache-2.0 license', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 9, 2020', 'Python', 'Updated Dec 30, 2019', '1', 'JavaScript', 'GPL-3.0 license', 'Updated Jan 2, 2020', 'JavaScript', 'Updated Jan 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '316 contributions\n        in the last year', 'description': ['This assignment is from Free University of Tbilisi\'s AI course, which is based on University of California, Berkeley\'s ""CS 188 | Introduction to Artificial Intelligence"" course.\n'], 'url_profile': 'https://github.com/lkito', 'info_list': ['Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', 'MIT license', 'Updated Dec 31, 2019', 'C++', 'Apache-2.0 license', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 9, 2020', 'Python', 'Updated Dec 30, 2019', '1', 'JavaScript', 'GPL-3.0 license', 'Updated Jan 2, 2020', 'JavaScript', 'Updated Jan 6, 2020']}","{'location': 'Kerala, India', 'stats_list': [], 'contributions': '69 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/rans0000', 'info_list': ['Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', 'MIT license', 'Updated Dec 31, 2019', 'C++', 'Apache-2.0 license', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 9, 2020', 'Python', 'Updated Dec 30, 2019', '1', 'JavaScript', 'GPL-3.0 license', 'Updated Jan 2, 2020', 'JavaScript', 'Updated Jan 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '27 contributions\n        in the last year', 'description': ['HexAI\nHex game with artificial intelligence using Monte Carlo simulation.\nAuthor:\nRomain Garnier <rom1{dot}garnier{at}yahoo{dot}fr>.\nLicensing provisions:\nApache 2.0 license.\nCompile with\n$ g++ -Wall -Wextra -Wpedantic -Wconversion HexAI.cpp -o HexAI\nExecute as\n$ ./HexAI dimension HumanVsHuman\nHuman can play against human if second argument > 0.\nMachine chooses positions in the hex table and computes best move from\na chosen number of Monte Carlo simulations (minimum 100, default 1000).\nHex table is showed on terminal with played positions.\nPositions are numbered as a grid.\nFor Hex board description see https://en.wikipedia.org/wiki/Hex_(board_game)\nX should take left<->right path to win.\nO should take up<->down path to win.\nThe human might want to play in first or take machine position.\nMachine might want to take human position if the latest plays first.\nPlayer should hit (row number enter button, then column enter).\nThe algorithm finds shortest paths from src to all other vertices\nwith union-find data structure\n(see https://en.wikipedia.org/wiki/Disjoint-set_data_structure)\n'], 'url_profile': 'https://github.com/4Rom1', 'info_list': ['Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', 'MIT license', 'Updated Dec 31, 2019', 'C++', 'Apache-2.0 license', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 9, 2020', 'Python', 'Updated Dec 30, 2019', '1', 'JavaScript', 'GPL-3.0 license', 'Updated Jan 2, 2020', 'JavaScript', 'Updated Jan 6, 2020']}","{'location': 'Hangzhou, China', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['AIUE\n💫 Artificial Intelligence UI Exploration for Web/Desktop/APP testing\n'], 'url_profile': 'https://github.com/slxiao', 'info_list': ['Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', 'MIT license', 'Updated Dec 31, 2019', 'C++', 'Apache-2.0 license', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 9, 2020', 'Python', 'Updated Dec 30, 2019', '1', 'JavaScript', 'GPL-3.0 license', 'Updated Jan 2, 2020', 'JavaScript', 'Updated Jan 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['atificial_intellegence_neu\n'], 'url_profile': 'https://github.com/wepstein712', 'info_list': ['Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', 'MIT license', 'Updated Dec 31, 2019', 'C++', 'Apache-2.0 license', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 9, 2020', 'Python', 'Updated Dec 30, 2019', '1', 'JavaScript', 'GPL-3.0 license', 'Updated Jan 2, 2020', 'JavaScript', 'Updated Jan 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['JavascriptUAI\nUser interface Artificial Intelligence\n'], 'url_profile': 'https://github.com/JavaScriptUAI', 'info_list': ['Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', 'MIT license', 'Updated Dec 31, 2019', 'C++', 'Apache-2.0 license', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 9, 2020', 'Python', 'Updated Dec 30, 2019', '1', 'JavaScript', 'GPL-3.0 license', 'Updated Jan 2, 2020', 'JavaScript', 'Updated Jan 6, 2020']}","{'location': 'Hanover, NH', 'stats_list': [], 'contributions': '363 contributions\n        in the last year', 'description': ['AI for basic game logic (Tic-Tac-Toe).\nThis algorithm utilizes a minimax decision tree in order to determine the optimal move when playing tic-tac-toe.\nBuilt using React and JS, this application runs in the browser and can be run here: http://tictactoe-ai314.herokuapp.com/\n\nAvailable Scripts\nIn the project directory, you can run:\nyarn start\nRuns the app in the development mode.\nOpen http://localhost:3000 to view it in the browser.\nThe page will reload if you make edits.\nYou will also see any lint errors in the console.\nyarn test\nLaunches the test runner in the interactive watch mode.\nSee the section about running tests for more information.\nyarn build\nBuilds the app for production to the build folder.\nIt correctly bundles React in production mode and optimizes the build for the best performance.\nThe build is minified and the filenames include the hashes.\nYour app is ready to be deployed!\nSee the section about deployment for more information.\nyarn eject\nNote: this is a one-way operation. Once you eject, you can’t go back!\nIf you aren’t satisfied with the build tool and configuration choices, you can eject at any time. This command will remove the single build dependency from your project.\nInstead, it will copy all the configuration files and the transitive dependencies (Webpack, Babel, ESLint, etc) right into your project so you have full control over them. All of the commands except eject will still work, but they will point to the copied scripts so you can tweak them. At this point you’re on your own.\nYou don’t have to ever use eject. The curated feature set is suitable for small and middle deployments, and you shouldn’t feel obligated to use this feature. However we understand that this tool wouldn’t be useful if you couldn’t customize it when you are ready for it.\nLearn More\nYou can learn more in the Create React App documentation.\nTo learn React, check out the React documentation.\nCode Splitting\nThis section has moved here: https://facebook.github.io/create-react-app/docs/code-splitting\nAnalyzing the Bundle Size\nThis section has moved here: https://facebook.github.io/create-react-app/docs/analyzing-the-bundle-size\nMaking a Progressive Web App\nThis section has moved here: https://facebook.github.io/create-react-app/docs/making-a-progressive-web-app\nAdvanced Configuration\nThis section has moved here: https://facebook.github.io/create-react-app/docs/advanced-configuration\nDeployment\nThis section has moved here: https://facebook.github.io/create-react-app/docs/deployment\nyarn build fails to minify\nThis section has moved here: https://facebook.github.io/create-react-app/docs/troubleshooting#npm-run-build-fails-to-minify\n'], 'url_profile': 'https://github.com/johnmccambridge7', 'info_list': ['Jupyter Notebook', 'Updated Apr 24, 2020', 'Jupyter Notebook', 'Updated Jan 30, 2020', 'Updated Jan 4, 2020', 'Python', 'Updated Jan 4, 2020', 'MIT license', 'Updated Dec 31, 2019', 'C++', 'Apache-2.0 license', 'Updated Jan 3, 2020', 'Python', 'Updated Jan 9, 2020', 'Python', 'Updated Dec 30, 2019', '1', 'JavaScript', 'GPL-3.0 license', 'Updated Jan 2, 2020', 'JavaScript', 'Updated Jan 6, 2020']}"
"{'location': 'Florianopolis', 'stats_list': [], 'contributions': '52 contributions\n        in the last year', 'description': ['\nProjeto-X\n""Project-X"" is a program that aims to create an artificial intelligence system that can recognize people and communicate with them.\nThe system is based on Python and its own functions to create chatbot artificial intelligence.\n\n\nThe purpose of this project is to create an artificial intelligence that most closely resembles the AI \u200b""Jarvis used in the Iron Man movie.\nHow artificial intelligence works\nThe communication system searches your database for the closest phrase to what you said. If she finds nothing and training mode is enabled, she will ask if she wants to add her phrase to the database and a possible answer.\nIf training mode is disabled and the AI \u200b\u200bconfidence level is low, it will be ignored and say ""I don\'t understand"".\nInstallation\nFor project startup the repository must be cloned:\ngit clone https://github.com/bobyzoo/ProjetoX.git\n\nBasic Usage\nfrom classBot import *\n\nBot = classBot(\'Assist.db\', mode_train=False)\n\nwhile True:\n    Quest = input(\'You: \')\n    answer = Bot.procuraResposta(Quest)\n    print(f\'Bot: {answer}\')\n\n\nDevelopment pattern for contributors\n\nCreate a fork of\nthe main ProjetoX repository on GitHub.\nMake your changes in a branch named something different from master, e.g. create\na new branch my-pull-request.\nCreate a pull request.\n\n'], 'url_profile': 'https://github.com/bobyzoo', 'info_list': ['2', 'Python', 'Updated Jul 20, 2020', 'Updated Jan 4, 2020', '1', 'Python', 'MIT license', 'Updated Dec 30, 2019', 'Updated Dec 30, 2019', 'Python', 'Updated May 7, 2020', '1', 'Python', 'Updated Dec 19, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Python', 'Updated Jan 1, 2020', 'Updated Jan 2, 2020', 'Java', 'Updated Jan 5, 2020']}","{'location': 'Lagos, Nigeria', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['Face-recognition-implementation-using-state-of-the-art-modern-artificial-intelligence-technique\n'], 'url_profile': 'https://github.com/eobi', 'info_list': ['2', 'Python', 'Updated Jul 20, 2020', 'Updated Jan 4, 2020', '1', 'Python', 'MIT license', 'Updated Dec 30, 2019', 'Updated Dec 30, 2019', 'Python', 'Updated May 7, 2020', '1', 'Python', 'Updated Dec 19, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Python', 'Updated Jan 1, 2020', 'Updated Jan 2, 2020', 'Java', 'Updated Jan 5, 2020']}","{'location': 'Cyprus, Nicosia', 'stats_list': [], 'contributions': '52 contributions\n        in the last year', 'description': ['Perceptron-Algorithms\nIntroduction to artificial intelligence.The perceptron algorithm, which is one of the introduction topics of artificial intelligence, was developed in Python Language.\n'], 'url_profile': 'https://github.com/RamazanBakir', 'info_list': ['2', 'Python', 'Updated Jul 20, 2020', 'Updated Jan 4, 2020', '1', 'Python', 'MIT license', 'Updated Dec 30, 2019', 'Updated Dec 30, 2019', 'Python', 'Updated May 7, 2020', '1', 'Python', 'Updated Dec 19, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Python', 'Updated Jan 1, 2020', 'Updated Jan 2, 2020', 'Java', 'Updated Jan 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['inmersivetech\nRepository of Inmersive tech as Virtual, augmented reality and artificial intelligence\n'], 'url_profile': 'https://github.com/camilomoal', 'info_list': ['2', 'Python', 'Updated Jul 20, 2020', 'Updated Jan 4, 2020', '1', 'Python', 'MIT license', 'Updated Dec 30, 2019', 'Updated Dec 30, 2019', 'Python', 'Updated May 7, 2020', '1', 'Python', 'Updated Dec 19, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Python', 'Updated Jan 1, 2020', 'Updated Jan 2, 2020', 'Java', 'Updated Jan 5, 2020']}","{'location': 'İstanbul', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': [""PACMAN\nAStar Search, Alpha-Beta Pruning, Minimax Algorithms, Depth-first Search, Breadth-first Search etc.\nThese AI algorithms' implementations on the Pacman game\n""], 'url_profile': 'https://github.com/senihcerit', 'info_list': ['2', 'Python', 'Updated Jul 20, 2020', 'Updated Jan 4, 2020', '1', 'Python', 'MIT license', 'Updated Dec 30, 2019', 'Updated Dec 30, 2019', 'Python', 'Updated May 7, 2020', '1', 'Python', 'Updated Dec 19, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Python', 'Updated Jan 1, 2020', 'Updated Jan 2, 2020', 'Java', 'Updated Jan 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '59 contributions\n        in the last year', 'description': ['SetupIAEnv\nInstall all packages required to perform machine learning and artificial intelligence. Works on Windows and Linux.\n'], 'url_profile': 'https://github.com/alexandreauda', 'info_list': ['2', 'Python', 'Updated Jul 20, 2020', 'Updated Jan 4, 2020', '1', 'Python', 'MIT license', 'Updated Dec 30, 2019', 'Updated Dec 30, 2019', 'Python', 'Updated May 7, 2020', '1', 'Python', 'Updated Dec 19, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Python', 'Updated Jan 1, 2020', 'Updated Jan 2, 2020', 'Java', 'Updated Jan 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['imperial_courseworks\n'], 'url_profile': 'https://github.com/arnoldcheung', 'info_list': ['2', 'Python', 'Updated Jul 20, 2020', 'Updated Jan 4, 2020', '1', 'Python', 'MIT license', 'Updated Dec 30, 2019', 'Updated Dec 30, 2019', 'Python', 'Updated May 7, 2020', '1', 'Python', 'Updated Dec 19, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Python', 'Updated Jan 1, 2020', 'Updated Jan 2, 2020', 'Java', 'Updated Jan 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '7 contributions\n        in the last year', 'description': ['Intro-to-AI-projects\nProjects done in CS188 at UC Berkeley(Intro to Artificial Intelligence)\n\nSearch\nGames\nReinforcement Learning\nGhostbusters(HMMs and BNs)\nMachinelearning\n\nSearch: In this project, your Pacman agent will find paths through his maze world, both to reach a particular location and to collect food efficiently. You will build general search algorithms and apply them to Pacman scenarios.\nGames: In this project, you will design agents for the classic version of Pacman, including ghosts. Along the way, you will implement both minimax and expectimax search and try your hand at evaluation function design.\nReinforcement Learning: In this project, you will implement value iteration and Q-learning. You will test your agents first on Gridworld (from class), then apply them to a simulated robot controller (Crawler) and Pacman.\nGhostbusters(HMMs and BNs): In this project, you will design Pacman agents that use sensors to locate and eat invisible ghosts. You’ll advance from locating single, stationary ghosts to hunting packs of multiple moving ghosts with ruthless efficiency.\nMachine Learning: This project will be an introduction to machine learning. In this project you will build a neural network to classify digits, and more!\n'], 'url_profile': 'https://github.com/ejee2020', 'info_list': ['2', 'Python', 'Updated Jul 20, 2020', 'Updated Jan 4, 2020', '1', 'Python', 'MIT license', 'Updated Dec 30, 2019', 'Updated Dec 30, 2019', 'Python', 'Updated May 7, 2020', '1', 'Python', 'Updated Dec 19, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Python', 'Updated Jan 1, 2020', 'Updated Jan 2, 2020', 'Java', 'Updated Jan 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['fiat-lux\nThis is my first GitHub repository. I am interested in artificial intelligence in content industry.\nWhat will happen if a story can be predicted?\n'], 'url_profile': 'https://github.com/huneyk', 'info_list': ['2', 'Python', 'Updated Jul 20, 2020', 'Updated Jan 4, 2020', '1', 'Python', 'MIT license', 'Updated Dec 30, 2019', 'Updated Dec 30, 2019', 'Python', 'Updated May 7, 2020', '1', 'Python', 'Updated Dec 19, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Python', 'Updated Jan 1, 2020', 'Updated Jan 2, 2020', 'Java', 'Updated Jan 5, 2020']}","{'location': 'Dhaka, Bangladesh', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/JeeSa', 'info_list': ['2', 'Python', 'Updated Jul 20, 2020', 'Updated Jan 4, 2020', '1', 'Python', 'MIT license', 'Updated Dec 30, 2019', 'Updated Dec 30, 2019', 'Python', 'Updated May 7, 2020', '1', 'Python', 'Updated Dec 19, 2020', 'Jupyter Notebook', 'Updated Dec 30, 2019', 'Python', 'Updated Jan 1, 2020', 'Updated Jan 2, 2020', 'Java', 'Updated Jan 5, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['MIT-6.034-AI-problem-sets\nMy solutions for MIT OCW 6.034 Artificial Intelligence assignments.\nLink to the online course:\nhttps://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-034-artificial-intelligence-fall-2010/assignments/\nMost of the code was provided in the course resources and only parts were supposed to be written by me. The majority of the solutions to these problem sets are in the files lab0.py, lab1.py etc.\n'], 'url_profile': 'https://github.com/oscar0325', 'info_list': ['Python', 'Updated Aug 22, 2017', 'Jupyter Notebook', 'Updated Mar 3, 2021', '1', 'Updated Jan 2, 2020', 'Java', 'Updated Dec 30, 2019', 'Updated Feb 3, 2020', 'HTML', 'Updated Apr 24, 2020', 'Python', 'Updated Jan 3, 2020', '1', 'Jupyter Notebook', 'Updated Jan 5, 2020', '1', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'BSD-2-Clause license', 'Updated Dec 31, 2019']}","{'location': 'Pakistan', 'stats_list': [], 'contributions': '253 contributions\n        in the last year', 'description': ['PIAIC\nLearning AI Course from PIAIC\n'], 'url_profile': 'https://github.com/waleedbutt98', 'info_list': ['Python', 'Updated Aug 22, 2017', 'Jupyter Notebook', 'Updated Mar 3, 2021', '1', 'Updated Jan 2, 2020', 'Java', 'Updated Dec 30, 2019', 'Updated Feb 3, 2020', 'HTML', 'Updated Apr 24, 2020', 'Python', 'Updated Jan 3, 2020', '1', 'Jupyter Notebook', 'Updated Jan 5, 2020', '1', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'BSD-2-Clause license', 'Updated Dec 31, 2019']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['momofboy\nI have studied artificial intelligence and advanced intelligence in my university for two semesters. And this very useful so I can pick up easily.\n'], 'url_profile': 'https://github.com/Deepi4179', 'info_list': ['Python', 'Updated Aug 22, 2017', 'Jupyter Notebook', 'Updated Mar 3, 2021', '1', 'Updated Jan 2, 2020', 'Java', 'Updated Dec 30, 2019', 'Updated Feb 3, 2020', 'HTML', 'Updated Apr 24, 2020', 'Python', 'Updated Jan 3, 2020', '1', 'Jupyter Notebook', 'Updated Jan 5, 2020', '1', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'BSD-2-Clause license', 'Updated Dec 31, 2019']}","{'location': 'Los Angeles, USA', 'stats_list': [], 'contributions': '54 contributions\n        in the last year', 'description': ['CSCI-561-AI\nContains my work for CSCI-561 (Artificial Intelligence) course assignments during fall 2019 at University of Southern California\n'], 'url_profile': 'https://github.com/RushabhK02', 'info_list': ['Python', 'Updated Aug 22, 2017', 'Jupyter Notebook', 'Updated Mar 3, 2021', '1', 'Updated Jan 2, 2020', 'Java', 'Updated Dec 30, 2019', 'Updated Feb 3, 2020', 'HTML', 'Updated Apr 24, 2020', 'Python', 'Updated Jan 3, 2020', '1', 'Jupyter Notebook', 'Updated Jan 5, 2020', '1', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'BSD-2-Clause license', 'Updated Dec 31, 2019']}","{'location': 'Yogyakarta', 'stats_list': [], 'contributions': '45 contributions\n        in the last year', 'description': ['blencong.io\nBlencongIO is an artificial intelligence-based image recognition application for the introduction of the atmosphere at the WAYANG KULIT show\n'], 'url_profile': 'https://github.com/helloaltop', 'info_list': ['Python', 'Updated Aug 22, 2017', 'Jupyter Notebook', 'Updated Mar 3, 2021', '1', 'Updated Jan 2, 2020', 'Java', 'Updated Dec 30, 2019', 'Updated Feb 3, 2020', 'HTML', 'Updated Apr 24, 2020', 'Python', 'Updated Jan 3, 2020', '1', 'Jupyter Notebook', 'Updated Jan 5, 2020', '1', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'BSD-2-Clause license', 'Updated Dec 31, 2019']}","{'location': 'NONE', 'stats_list': [], 'contributions': '38 contributions\n        in the last year', 'description': ['DS442\nCourse Website for DS/CMPSC 442 Artificial Intelligence course offered at Penn State University in Spring 2020\n'], 'url_profile': 'https://github.com/AmulyaYadav', 'info_list': ['Python', 'Updated Aug 22, 2017', 'Jupyter Notebook', 'Updated Mar 3, 2021', '1', 'Updated Jan 2, 2020', 'Java', 'Updated Dec 30, 2019', 'Updated Feb 3, 2020', 'HTML', 'Updated Apr 24, 2020', 'Python', 'Updated Jan 3, 2020', '1', 'Jupyter Notebook', 'Updated Jan 5, 2020', '1', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'BSD-2-Clause license', 'Updated Dec 31, 2019']}","{'location': 'Moscow, Russia', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['Paper: https://ieeexplore.ieee.org/document/8911054\n'], 'url_profile': 'https://github.com/SebyakinAndrei', 'info_list': ['Python', 'Updated Aug 22, 2017', 'Jupyter Notebook', 'Updated Mar 3, 2021', '1', 'Updated Jan 2, 2020', 'Java', 'Updated Dec 30, 2019', 'Updated Feb 3, 2020', 'HTML', 'Updated Apr 24, 2020', 'Python', 'Updated Jan 3, 2020', '1', 'Jupyter Notebook', 'Updated Jan 5, 2020', '1', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'BSD-2-Clause license', 'Updated Dec 31, 2019']}","{'location': 'Via Cossila 13, 10153 Turin, Italy', 'stats_list': [], 'contributions': '306 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/nopesir', 'info_list': ['Python', 'Updated Aug 22, 2017', 'Jupyter Notebook', 'Updated Mar 3, 2021', '1', 'Updated Jan 2, 2020', 'Java', 'Updated Dec 30, 2019', 'Updated Feb 3, 2020', 'HTML', 'Updated Apr 24, 2020', 'Python', 'Updated Jan 3, 2020', '1', 'Jupyter Notebook', 'Updated Jan 5, 2020', '1', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'BSD-2-Clause license', 'Updated Dec 31, 2019']}","{'location': 'Via Cossila 13, 10153 Turin, Italy', 'stats_list': [], 'contributions': '306 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/nopesir', 'info_list': ['Python', 'Updated Aug 22, 2017', 'Jupyter Notebook', 'Updated Mar 3, 2021', '1', 'Updated Jan 2, 2020', 'Java', 'Updated Dec 30, 2019', 'Updated Feb 3, 2020', 'HTML', 'Updated Apr 24, 2020', 'Python', 'Updated Jan 3, 2020', '1', 'Jupyter Notebook', 'Updated Jan 5, 2020', '1', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'BSD-2-Clause license', 'Updated Dec 31, 2019']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['AIoT-LPWAN\nArtificial Intelligence of Things project focused on Low Power Wide Area Networks to achieve more efficient and scalable subGHz 802.15.4g mesh networks.\n'], 'url_profile': 'https://github.com/SmartCityEdge', 'info_list': ['Python', 'Updated Aug 22, 2017', 'Jupyter Notebook', 'Updated Mar 3, 2021', '1', 'Updated Jan 2, 2020', 'Java', 'Updated Dec 30, 2019', 'Updated Feb 3, 2020', 'HTML', 'Updated Apr 24, 2020', 'Python', 'Updated Jan 3, 2020', '1', 'Jupyter Notebook', 'Updated Jan 5, 2020', '1', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'BSD-2-Clause license', 'Updated Dec 31, 2019']}"
"{'location': 'Via Cossila 13, 10153 Turin, Italy', 'stats_list': [], 'contributions': '306 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/nopesir', 'info_list': ['1', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Updated Jan 1, 2020', 'Updated Jan 3, 2020', '2', 'Java', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', '2', 'Jupyter Notebook', 'Updated Jan 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': [""deeplant.ai\nwe make innovation in the world of agriculture using artificial intelligence. We help increase agricultural production and find out various information about the state of plants in real time\n\nInspiration\nThe inspiration of deeplant.ai is to combine agriculture and technology using artificial intelligence. Where artificial intelligence in the form of machine learning is made with the help of python and tensorflow.\nWhat it does\ndeeplant.ai serves to provide information on plants or plantations in real time, and is able to detect pests or diseases in plants making it easier to overcome. This will help increase agricultural production in the future.\nHow I built it\nwe started by making a model using python and backend tensorflow with google colabs to training the data. Our data is taken secondary from several open sources, especially Kaggle. Then we save the model obtained in the form of .tflite file and then it is implemented in the Android application to detect diseases and do other things about agriculture\nChallenges I ran into\nthe challenges that we have encountered, especially on computers that lack support to develop applications and impact on slowing our performance and we found some error with code or application. then the data is a little difficult to find. Knowledge in several programming languages \u200b\u200bat once is still lacking. Changing from .flite to tensorflow.js is a bit complicated and also the application of tensorflow with the Internet of Things (IoT) has not been implemented due to the limitations of the above.\nAccomplishments that I'm proud of\nUntil now, we have successfully created a model and saved it as a .tflite file and implemented it in the application and have been able to apply it to tensorflow.js. besides that we try to make a smooth framework for deeplant.ai applications\nWhat I learned\nIn its application we learned a lot that tensorflow 2.0 is easier to use than version 1, and produces better output even though we have to adjust the changes and learn more, but we are happy with that. and we think tensorflow will be widely used to help many people in the future, especially the application of AI\nWhat's next for deeplant.ai\nThe next step, we will apply to tensorflow.js so that it is more flexible than the .tflite file, then we also intend to develop this application for the IoT. In addition to getting data directly and funding to overcome problems, we want to try to work with various parties including researchers, the government and its expectations with Google and Tensorflow\nBuilt With\nandroid-studio\ncss3\nhtml5\njava\njavascript\nnumpy\npandas\npython\n""], 'url_profile': 'https://github.com/linggaajiandika', 'info_list': ['1', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Updated Jan 1, 2020', 'Updated Jan 3, 2020', '2', 'Java', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', '2', 'Jupyter Notebook', 'Updated Jan 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/zeeshanhussainbhatti', 'info_list': ['1', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Updated Jan 1, 2020', 'Updated Jan 3, 2020', '2', 'Java', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', '2', 'Jupyter Notebook', 'Updated Jan 5, 2020']}","{'location': 'Bangalore', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['MobileChatBotFramework\nThe mobile chatbot  framework helps you find out all possible answers that your Bot can give for various formats of questions asked\nby your users. Aritificial Intelligence or training your Bot to understand different intents requires you to first understand how\nyour Bot currently behaves for various questions.\nThis framework can read the questions from a spreadsheet and run on a mobile emulator to test the Bot and records the answers given in an accurate manner.\n'], 'url_profile': 'https://github.com/seemacalibrecode', 'info_list': ['1', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Updated Jan 1, 2020', 'Updated Jan 3, 2020', '2', 'Java', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', '2', 'Jupyter Notebook', 'Updated Jan 5, 2020']}","{'location': 'Lagos, Nigeria', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': [""Supermarket-sales-Prediction\nA data analysis work with prediction on the dataset of a supermarket sales carried out by my fellow team members and I during the Nigerian-German Center(Implemented by GIZ) sponsored boot-camp on Data science and Artificial Intelligence training. The Dataset was gotten from (kaggle.com). The problem statement was pointed out to be  constant expiration of a supermarket's product caused by overstocking which frequently led to much losses. Hence, this led to the aim of this project which is to predict the quantity of goods to be purchased in each day of the first quarter of subsequent years. This is achieved by using the model that best yields approximately the same Quantities as in the Sales record/Dataset for each transaction in the first quarter of 2019.  The growth of supermarkets in most populated cities are increasing and market competitions are getting high hence the need for precision on the quantity of the individual product line to be stocked in First quarter of any year to avoid overstocking, expiration of goods and boost profit.\nFrom our analysis, Random Forest Regressor Model gave the most accurate result of all the other Models.\n""], 'url_profile': 'https://github.com/Christabel-Ajaero', 'info_list': ['1', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Updated Jan 1, 2020', 'Updated Jan 3, 2020', '2', 'Java', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', '2', 'Jupyter Notebook', 'Updated Jan 5, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['A-Simple-Chatbot-\nA chatbot (also known as a talkbot, chatterbot, Bot, IM bot, interactive agent, or Artificial Conversational Entity)The classic historic early chatbots are ELIZA (1966) and PARRY (1972).More recent notable programs include A.L.I.C.E., Jabberwacky and D.U.D.E (Agence Nationale de la Recherche and CNRS 2006). While ELIZA and PARRY were used exclusively to simulate typed conversation, many chatbots now include functional features such as games and web searching abilities. In 1984, a book called The Policeman\'s Beard is Half Constructed was published, allegedly written by the chatbot Racter (though the program as released would not have been capable of doing so).  One pertinent field of AI research is natural language processing. Usually, weak AI fields employ specialized software or programming languages created specifically for the narrow function required. For example, A.L.I.C.E. uses a markup language called AIML, which is specific to its function as a conversational agent, and has since been adopted by various other developers of, so called, Alicebots. Nevertheless, A.L.I.C.E. is still purely based on pattern matching techniques without any reasoning capabilities, the same technique ELIZA was using back in 1966. This is not strong AI, which would require sapience and logical reasoning abilities.  Jabberwacky learns new responses and context based on real-time user interactions, rather than being driven from a static database. Some more recent chatbots also combine real-time learning with evolutionary algorithms that optimise their ability to communicate based on each conversation held. Still, there is currently no general purpose conversational artificial intelligence, and some software developers focus on the practical aspect, information retrieval.  Chatbot competitions focus on the Turing test or more specific goals. Two such annual contests are the Loebner Prize and The Chatterbox Challenge (offline since 2015, materials can still be found from web archives).  According to Forrester (2015), AI will replace 16 percent of American jobs by the end of the decade.Chatbots have been used in applications such as customer service, sales and product education. However, a study conducted by Narrative Science in 2015 found that 80 percent of their respondents believe AI improves worker performance and creates jobs.[citation needed] is a computer program or an artificial intelligence which conducts a conversation via auditory or textual methods. Such programs are often designed to convincingly simulate how a human would behave as a conversational partner, thereby passing the Turing test. Chatbots are typically used in dialog systems for various practical purposes including customer service or information acquisition. Some chatterbots use sophisticated natural language processing systems, but many simpler systems scan for keywords within the input, then pull a reply with the most matching keywords, or the most similar wording pattern, from a database.  The term ""ChatterBot"" was originally coined by Michael Mauldin (creator of the first Verbot, Julia) in 1994 to describe these conversational programs.Today, most chatbots are either accessed via virtual assistants such as Google Assistant and Amazon Alexa, via messaging apps such as Facebook Messenger or WeChat, or via individual organizations\' apps and websites. Chatbots can be classified into usage categories such as conversational commerce (e-commerce via chat), analytics, communication, customer support, design, developer tools, education, entertainment, finance, food, games, health, HR, marketing, news, personal, productivity, shopping, social, sports, travel and utilities. Background\n'], 'url_profile': 'https://github.com/Yogapriya2512', 'info_list': ['1', 'Jupyter Notebook', 'Updated Jan 5, 2020', 'Updated Jan 1, 2020', 'Updated Jan 3, 2020', '2', 'Java', 'Updated Jan 2, 2020', 'Jupyter Notebook', 'Updated Jan 5, 2020', '2', 'Jupyter Notebook', 'Updated Jan 5, 2020']}",,,,
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Artificial Intelligence with Python Cookbook\n\nThis is the code repository for Artificial Intelligence with Python Cookbook, published by Packt.\nPractical recipes for next-generation deep learning and neural networks using TensorFlow and PyTorch\nWhat is this book about?\nWith artificial intelligence (AI) systems, we can develop goal-driven agents to automate problem-solving. This involves predicting and classifying the available data and training agents to execute tasks successfully. This book will help you to solve complex AI problems using practical recipes.\nThis book covers the following exciting features:\n\nImplement data preprocessing steps and optimize model hyperparameters\nWork with large amounts of data using distributed and parallel computing techniques\nGet to grips with representational learning from images using InfoGAN\nDelve into deep probabilistic modeling with a Bayesian network\nCreate your own artwork using adversarial neural networks\n\nIf you feel this book is for you, get your copy today!\n\nInstructions and Navigations\nAll of the code is organized into folders. For example, Chapter02.\nThe code will look like the following:\nfrom sklearn.datasets import fetch_openml\ndata = fetch_openml(data_id=42165, as_frame=True)\n\nFollowing is what you need for this book:\nThis AI book is for Python developers, data scientists, machine learning developers, and deep learning practitioners who want to learn how to build artificial intelligence solutions with easy-to-follow recipes. If you are looking for state-of-the-art solutions to perform different machine learning tasks in various use cases, this book is for you. Basic working knowledge of Python programming language and machine learning concepts will help you to work with the code.\nWith the following software and hardware list you can run all code files present in the book (Chapter 1-11).\nSoftware and Hardware List\n\n\n\nChapter\nSoftware required\nOS required\n\n\n\n\n1\nPython 3.6 or later\nWindows, Mac OS X, and Linux (Any)\n\n\n2\nTensorFlow 2.0 or later\nWindows, Mac OS X, and Linux (Any)\n\n\n3\nPyTorch 1.6 or later\nWindows, Mac OS X, and Linux (Any)\n\n\n4\nPandas 1.0 or later\nWindows, Mac OS X, and Linux (Any)\n\n\n5\nScikit-learn 0.22.0 or later\nWindows, Mac OS X, and Linux (Any)\n\n\n\nWe also provide a PDF file that has color images of the screenshots/diagrams used in this book. Click here to download it.\nRelated products\n\n\nHands-On Artificial Intelligence for Banking [Packt] [Amazon]\n\n\nArtificial Intelligence with Python - Second Edition [Packt] [Amazon]\n\n\nGet to Know the Author\nBen Auffarth\nis a full-stack data scientist with more than 15 years of work experience. With a background and Ph.D. in computational and cognitive neuroscience, he has designed and conducted wet lab experiments on cell cultures, analyzed experiments with terabytes of data, run brain models on IBM supercomputers with up to 64k cores, built production systems processing hundreds of thousands of transactions per day, and trained neural networks on millions of text documents. He resides in West London with his family, where you might find him in a playground with his young son. He co-founded and is the former president of Data Science Speakers, London.\nSuggestions and Feedback\nClick here if you have any feedback or suggestions.\n'], 'url_profile': 'https://github.com/PacktPublishing', 'info_list': ['32', 'Jupyter Notebook', 'MIT license', 'Updated Jan 21, 2021', '144', 'Jupyter Notebook', 'MIT license', 'Updated Mar 3, 2021', '2', 'Apache-2.0 license', 'Updated Mar 12, 2020', '4', 'Updated Jan 7, 2020', '5', 'HTML', 'Updated Mar 1, 2020', '4', 'Updated Jan 9, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Mar 7, 2020', 'Python', 'Updated Jan 6, 2020', 'Java', 'Updated Jan 13, 2020', 'HTML', 'Updated Jun 20, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '8,875 contributions\n        in the last year', 'description': ['\n#100DaysofMLCode\nTable of Contents\n1. Data Pre-processing\n\nImporting Libraries\nImporting Data sets\nHandling the missing data values\nEncoding categorical data\nSplit Data into Train data and Test data\nFeature Scaling\n\n2. Regression\n\nSimple Linear Regression\nMulti Linear Regression\nPolynomial Regression\nSupport Vector Regression\nDecision Tree Regression\nRandom Forest Regression\n\n3. Classification\n\nLogistic Regression\nK Nearest Neighbors Classification\nSupport Vector Machine\nKernel SVM\nNaive Bayes\nDecision Tree Classification\nRandom Forest Classification\n\n4. Clustering\n\nK-Means Clustering\nHierarchical Clustering\n\n5. Association Rule\n\nApriori\nEclat\n\n6. Reinforcement Learning\n\nUpper Confidence Bounds\nThompson Sampling\n\n7. Natural Language Processing \n\nAWS Comprehend\n\n8. Deep Learning\n\nArtificial Neural Networks (ANN)\n2. Convolutional Neural Networks (CNN)\n\n9. Dimensionality Reduction\n\nPrincipal Component Analysis\nLinear Discriminant Analysis\nKernel PCA\n\n10. Model Selection\n\nGrid Search\nK-fold Cross Validation\nXGBoost\n\n11. Data Visualization\n\nMatplotlib library in Python\nTableau\nPower BI\nGrafana\n\nLog of my Day-to-Day Activities\nTrack my daily activities here\nHow to Contribute\nThis is an open project and contribution in all forms are welcomed.\nPlease follow these Contribution Guidelines\nCode of Conduct\nAdhere to the GitHub specified community code.\nLicense\nCheck the official MIT License here.\n'], 'url_profile': 'https://github.com/NishkarshRaj', 'info_list': ['32', 'Jupyter Notebook', 'MIT license', 'Updated Jan 21, 2021', '144', 'Jupyter Notebook', 'MIT license', 'Updated Mar 3, 2021', '2', 'Apache-2.0 license', 'Updated Mar 12, 2020', '4', 'Updated Jan 7, 2020', '5', 'HTML', 'Updated Mar 1, 2020', '4', 'Updated Jan 9, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Mar 7, 2020', 'Python', 'Updated Jan 6, 2020', 'Java', 'Updated Jan 13, 2020', 'HTML', 'Updated Jun 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '81 contributions\n        in the last year', 'description': ['Welcome to GitHub Pages\nYou can use the editor on GitHub to maintain and preview the content for your website in Markdown files.\nWhenever you commit to this repository, GitHub Pages will run Jekyll to rebuild the pages in your site, from the content in your Markdown files.\nMarkdown\nMarkdown is a lightweight and easy-to-use syntax for styling your writing. It includes conventions for\nSyntax highlighted code block\n\n# Header 1\n## Header 2\n### Header 3\n\n- Bulleted\n- List\n\n1. Numbered\n2. List\n\n**Bold** and _Italic_ and `Code` text\n\n[Link](url) and ![Image](src)\nFor more details see GitHub Flavored Markdown.\nJekyll Themes\nYour Pages site will use the layout and styles from the Jekyll theme you have selected in your repository settings. The name of this theme is saved in the Jekyll _config.yml configuration file.\nSupport or Contact\nHaving trouble with Pages? Check out our documentation or contact support and we’ll help you sort it out.\n'], 'url_profile': 'https://github.com/uclaml', 'info_list': ['32', 'Jupyter Notebook', 'MIT license', 'Updated Jan 21, 2021', '144', 'Jupyter Notebook', 'MIT license', 'Updated Mar 3, 2021', '2', 'Apache-2.0 license', 'Updated Mar 12, 2020', '4', 'Updated Jan 7, 2020', '5', 'HTML', 'Updated Mar 1, 2020', '4', 'Updated Jan 9, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Mar 7, 2020', 'Python', 'Updated Jan 6, 2020', 'Java', 'Updated Jan 13, 2020', 'HTML', 'Updated Jun 20, 2020']}","{'location': 'Bangalore, India', 'stats_list': [], 'contributions': '11,676 contributions\n        in the last year', 'description': ['\nGenetic Algorithms and Genetic Programming in Computational Finance\nGenetic Data Analysis II: Methods for Discrete Population Genetic Data \nGenetic Programming: Theory and Practice II\nPattern Recognition Using Genetic Programming for Classification of Diabetes and Modulation Data\nFoundations of Genetic Programming\nGenetic Programming: An Introduction\n Genetic Programming: Theory and Practice XIV\nGenetic Programming IV\nGenetic Algorithms + DataStructures = Evolution Programs \nIntelligent Optimisation Techniques: Genetic Algorithms, Tabu Search, Simulated Annealing and Neural Networks\nAn Introduction to Machine Learning\nGenetic Programming: Theory and Practice XV\nApplied Genetic Programming and Machine Learning\n Genetic Programming: Theory and Practice VII\nA Field Guide to Genetic Programming\nBehavioral Program Synthesis with Genetic Programming\nLinear Genetic Programming\nData Mining Using Grammar Based Genetic Programming and Applications\n Automatic Program Repair Using Genetic Programming\n Genetic Programming in Mathematica\nGenetic Programming: Theory and Practice VIII\n Genetic Programming: Theory and Practice X\nGenetic Programming: Theory and Practice XIII\nLearning Basic Genetics with Interactive Computer Programs\nEvolving Artificial Neural Networks using Cartesian Genetic Programming\nCartesian Genetic Programming\n Genetic Programming: Theory and Practice XI\n Genetic and Evolutionary Computation – GECCO 2004\n\n'], 'url_profile': 'https://github.com/manjunath5496', 'info_list': ['32', 'Jupyter Notebook', 'MIT license', 'Updated Jan 21, 2021', '144', 'Jupyter Notebook', 'MIT license', 'Updated Mar 3, 2021', '2', 'Apache-2.0 license', 'Updated Mar 12, 2020', '4', 'Updated Jan 7, 2020', '5', 'HTML', 'Updated Mar 1, 2020', '4', 'Updated Jan 9, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Mar 7, 2020', 'Python', 'Updated Jan 6, 2020', 'Java', 'Updated Jan 13, 2020', 'HTML', 'Updated Jun 20, 2020']}","{'location': 'Sydney', 'stats_list': [], 'contributions': '43 contributions\n        in the last year', 'description': ['Artificial Intelligence COMP3411/COMP9814 20T0\nCourse Title\tArtificial Intelligence\nConvenor\tTatjana Zrimec\nLecturer\tTatjana Zrimec\n'], 'url_profile': 'https://github.com/brmuch', 'info_list': ['32', 'Jupyter Notebook', 'MIT license', 'Updated Jan 21, 2021', '144', 'Jupyter Notebook', 'MIT license', 'Updated Mar 3, 2021', '2', 'Apache-2.0 license', 'Updated Mar 12, 2020', '4', 'Updated Jan 7, 2020', '5', 'HTML', 'Updated Mar 1, 2020', '4', 'Updated Jan 9, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Mar 7, 2020', 'Python', 'Updated Jan 6, 2020', 'Java', 'Updated Jan 13, 2020', 'HTML', 'Updated Jun 20, 2020']}","{'location': 'Bangalore, India', 'stats_list': [], 'contributions': '11,676 contributions\n        in the last year', 'description': [""Python: \n\nMachine Learning in Python\nPython Programming: An Introduction to Computer Science\nHigh Performance Python\n Effective Python : 59 specific ways to write better Python\nAutomate the Boring Stuff with Python: Practical Programming for Total Beginners\nBlack Hat Python: Python Programming for Hackers and Pentesters\nCore Python Programming\nData Science from Scratch \nData Structures and Algorithms Using Python\nDeep Learning with Python\nDive into Python\nFluent Python\nHands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems\nHead First Python\nHow to Think Like a Computer Scientist: Learning with Python 3 Documentation\nInvent Your Own Computer Games with Python\nLearning Python\nLearn Python the Hard Way: A Very Simple Introduction to the Terrifyingly Beautiful World of Computers and Code \nNatural Language Processing with Python\nPython Crash Course: A Hands-On, Project-Based Introduction to Programming\n Programming Python \nPython Essential Reference \nPython for Kids: A Playful Introduction to Programming\nPython in a Nutshell\nPython Pocket Reference\nPython Cookbook\nPython Tricks: The Book\n Python Data Science Handbook\nPython for Data Analysis\nPython for Everybody: Exploring Data Using Python 3\nThe Hitchhiker's Guide to Python: Best Practices for Development\nThe Quick Python Book\nThink Python: How to Think Like a Computer Scientist\n Text Processing in Python\nViolent Python: A Cookbook for Hackers, Forensic Analysts, Penetration Testers and Security Engineers\nLearn Python in One Day and Learn It Well\n Web Scraping with Python\nIntroducing Python: Modern Computing in Simple Packages\nLean Python: Learn Just Enough Python to Build Useful Tools\nHadoop with Python\nTango with Django: A beginners guide to Python/Django\nProgramming Computer Vision with Python\n\n\nLisp:\n\nBuild Your Own Lisp\nA Practical Introduction to Fuzzy Logic using LISP\nAI Algorithms, Data Structures, and Idioms in Prolog, Lisp, and Java\nClojure Programming: Practical Lisp for the Java World\nCoders at Work: Reflections on the Craft of Programming\nCommon Lisp Recipes: A Problem-Solution Approach\n Common LISP: A Gentle Introduction to Symbolic Computation\nCommon Lisp the Language\nInterpreting LISP: Programming and Data Structures \nLet Over Lambda — 50 Years of Lisp\nLisp in Small Pieces\nLearn Lisp\nOn Lisp: Advanced Techniques for Common Lisp \nParadigms of AI Programming: Case Studies in Common Lisp\nPractical Common Lisp\nSuccessful Lisp: How to Understand and Use Common Lisp\nThe Clean Coder: A Code of Conduct for Professional Programmers\nThe Visual LISP Developers Bible\nLand of Lisp\nLisp Programming: Lecture Notes \nA Crash Course in LISP\nLISP-STAT: An Object-Oriented Environment for Statistical Computing and Dynamic Graphics\nLoving Common Lisp, or the Savvy Programmer's Secret Weapon\nANSI Common Lisp\nComputer Algebra with LISP and REDUCE: An Introduction to Computer-aided Pure Mathematics\nGetting Started in *Lisp\nHandbook of LISP Functions\nThe Evolution of Lisp\nAdventurer's Guide to Interleaf Lisp\nThe Art of Lisp Programming\nCommon LISP: An Interactive Approach\nSketchy LISP: An Introduction to Functional Programming in Scheme\nThe Programming Language LISP: Its Operation and Applications\n\n\nJava: \n\nHead First Java \nCore Java, Volume II — Advanced Features\nFundamentals of the Java Programming Language, Java SE 6\nFundamentals of Computer Science Using Java\nJava 9 Programming By Example \nJava for Absolute Beginners\nJAVA: A Beginner's Guide to Learning the Basics of Java Programming\nJAVA: Easy Java Programming for Beginners, Your Step-\nBy-Step Guide to Learning Java Programming \nJava For Beginners: A Simple Start To Java\nJava All-in-One For Dummies\nJava Programming Language Handbook \nJava: The Complete Reference\nLearning Java Functional Programming\nModern Java in Action\nThe Well-Grounded Java Developer\nJava in a Nutshell\nJava Generics and Collections\nSpring in Action\nThink Java: How to Think Like a Computer Scientist\nThinking in Java\nSpring Microservices in Action\nThe Elements of Java Style\nBeginning Programming with Java For Dummies\nEffective Java\nJava For Dummies\nJava Network Programming\nJava Puzzlers: Traps, Pitfalls, and Corner Cases\nJava SE 8 for the Really Impatient\nJava Threads\nThe Java Virtual Machine Specification (Java SE 8 Edition)\nMurach's Beginning Java 2\nJava Performance: The Definitive Guide\nConcurrent Programming in Java: Design Principles and Patterns\nJava Concurrency In Practice \nHigh-Performance Java Persistence\nJava: How To Program\nTeach Yourself JAVA in 21 Days\nJava 8 Lambdas \n\n\nProlog: \n\nAgent-Oriented Programming: From Prolog to Guarded Definite Clauses\nAn Introduction to Language Processing with Perl and Prolog\nLanguage Processing with Perl and Prolog: Theories, Implementation, and Application\nLearn Prolog Now!\nLogic Programming with Prolog\nNatural Language Processing for Prolog Programmers\n Problem Solving with Prolog\nProgramming in Prolog\nProlog Programming in Depth \nProlog Techniques\nProlog Versus You: An Introduction to Logic Programming\nProlog for Programmers\nThe Art of Prolog: Advanced Programming Techniques\nThinking as Computation: A First Course\nProlog by Example: How to Learn, Teach and Use It\nThe Practice of Prolog\nProlog Programming: A First Course\nProlog Basics\n6.1 Prolog++ Reference\nProlog Experiments in Discrete Mathematics, Logic, and Computability\nProlog and Natural-Language Analysis\nFrom Logic Programming to Prolog\nAdventure in Prolog\nPROLOG for Computer Science\nProlog Based Reasoning\nA High Performance Architecture for Prolog\nMicro-Prolog: Programming in Logic\n\n\nR Programming: \n\nEfficient R Programming: A Practical Guide to Smarter Programming\n Functional Programming in R: Advanced Statistical Programming for Data Science, Analysis and Finance\nLearn R for Applied Statistics: With Data Visualizations, Regressions, and Statistics\nMachine Learning with R\nHands-On Programming with R\nR For Dummies\n R in Action: Data analysis and graphics with R\nR Programming for Bioinformatics\nBeyond Spreadsheets with R \nR Programming Tutorial\nR Programming for Data Science\nStatistical Analysis with R\nThe Art of R Programming: A Tour of Statistical Software Design\nThe Book of R: A First Course in Programming and Statistics\nR Graphics\nLearning R Programming\nA Course in Statistics with R\nR Programming By Example\nMachine Learning with R Cookbook\nUsing R for Introductory Statistics\nSams Teach Yourself R in 24 Hours\nMetaprogramming in R: Advanced Statistical Programming\nfor Data Science, Analysis and Finance\nModeling and solving linear programming with R\nBusiness Analytics Using R - A Practical Approach\nData Manipulation with R\nIntroduction to Scientific Programming and Simulation Using R\nQuantitative Trading with R\nA Step-by-Step R Tutorial\nMastering Parallel Programming with R\nR for Stata Users\nR Markdown: The Definitive Guide\nR Programming and Its Applications in Financial Mathematics\nR Programming Succinctly\nSoftware for Data Analysis: Programming with R\nBeginning R: The Statistical Programming Language\nR for Data Science: Import, Tidy, Transform, Visualize, and Model Data\n\n""], 'url_profile': 'https://github.com/manjunath5496', 'info_list': ['32', 'Jupyter Notebook', 'MIT license', 'Updated Jan 21, 2021', '144', 'Jupyter Notebook', 'MIT license', 'Updated Mar 3, 2021', '2', 'Apache-2.0 license', 'Updated Mar 12, 2020', '4', 'Updated Jan 7, 2020', '5', 'HTML', 'Updated Mar 1, 2020', '4', 'Updated Jan 9, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Mar 7, 2020', 'Python', 'Updated Jan 6, 2020', 'Java', 'Updated Jan 13, 2020', 'HTML', 'Updated Jun 20, 2020']}","{'location': 'Florianópolis - Santa Catarina - Brasil', 'stats_list': [], 'contributions': '192 contributions\n        in the last year', 'description': ['\n\n\n\n\n\nArtificial Intelligence Methods and Techniques\nThis repository is under development and constantly being updated. Containing examples in Python of Artificial Intelligence applications, the Jupyter Nooteboks available aims to help mainly those who are starting in the AI area, with examples and links to reference materials.\nIf this repository helped you, give your star :)\nTable of Contents\n\nOverview\nHow To Use\nContributions\nLicense\n\nOverview\nThe examples available in this repository cover several processing steps, from pre-processing, processing and post-processing. The methods already available are described below.\nPre-Processing\n\n\n\nProcessing\n\nK-Means Clustering\nK-Nearest Neighbors\nSupport Vector Machine\n\nPos-Processing\n\n\n\nHow To Use\nYou can run the methods in two ways:\nRunning Locally\nClone this repository to your machine and add it to your jupyter notebook path:\ngit clone https://github.com/jefmenegazzo/Artificial-Intelligence-Methods-and-Techniques.git\nRunning Online\nYou can run online on Google Colab or Binder.\nContributions\nThe repository is open to contributions in documentation, error fixes, improvements and addition of new methods. Currently, the theoretical explanation of the methods is referenced through links in the Jupyter Notebook. However, contributions are welcome to add the theoretical explanation of the AI methods and techniques together with the source-code in Jupyter Notebook.\nLicense\nThe MIT License (MIT). Please see License File for more information.\n'], 'url_profile': 'https://github.com/jefmenegazzo', 'info_list': ['32', 'Jupyter Notebook', 'MIT license', 'Updated Jan 21, 2021', '144', 'Jupyter Notebook', 'MIT license', 'Updated Mar 3, 2021', '2', 'Apache-2.0 license', 'Updated Mar 12, 2020', '4', 'Updated Jan 7, 2020', '5', 'HTML', 'Updated Mar 1, 2020', '4', 'Updated Jan 9, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Mar 7, 2020', 'Python', 'Updated Jan 6, 2020', 'Java', 'Updated Jan 13, 2020', 'HTML', 'Updated Jun 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Artificial Intelligence USC CSI 561\n\nHW1: Find placement for officers on a grid world to collect maximum activity points. Simulated annealing search algorithm was used to find placements.\nHW2: Find next applicant to be chosen by an organization to maximize space efficiency. Min-Max algorithm was used to search in the tree generated by game playing between two competing organizations.\nHW3: Find optimal moves for a car in grid world given obstacles and rewards. Value iteration was used for this homework.\n\n'], 'url_profile': 'https://github.com/SapandeepDhaliwal', 'info_list': ['32', 'Jupyter Notebook', 'MIT license', 'Updated Jan 21, 2021', '144', 'Jupyter Notebook', 'MIT license', 'Updated Mar 3, 2021', '2', 'Apache-2.0 license', 'Updated Mar 12, 2020', '4', 'Updated Jan 7, 2020', '5', 'HTML', 'Updated Mar 1, 2020', '4', 'Updated Jan 9, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Mar 7, 2020', 'Python', 'Updated Jan 6, 2020', 'Java', 'Updated Jan 13, 2020', 'HTML', 'Updated Jun 20, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['AI programming assignment topics include\n\nstate space search\npropositional logic\nDavis-Putnam algorithm\ngame simulation machine learning\nsupervised machine learning\nNaive Bayes Classifier\ngradient descent\nnatural language processing\nexpression parse trees\n\n'], 'url_profile': 'https://github.com/jamezlim', 'info_list': ['32', 'Jupyter Notebook', 'MIT license', 'Updated Jan 21, 2021', '144', 'Jupyter Notebook', 'MIT license', 'Updated Mar 3, 2021', '2', 'Apache-2.0 license', 'Updated Mar 12, 2020', '4', 'Updated Jan 7, 2020', '5', 'HTML', 'Updated Mar 1, 2020', '4', 'Updated Jan 9, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Mar 7, 2020', 'Python', 'Updated Jan 6, 2020', 'Java', 'Updated Jan 13, 2020', 'HTML', 'Updated Jun 20, 2020']}","{'location': 'Gandhinagar', 'stats_list': [], 'contributions': '409 contributions\n        in the last year', 'description': ['Artificial intelligence\n'], 'url_profile': 'https://github.com/Vatsalparsaniya', 'info_list': ['32', 'Jupyter Notebook', 'MIT license', 'Updated Jan 21, 2021', '144', 'Jupyter Notebook', 'MIT license', 'Updated Mar 3, 2021', '2', 'Apache-2.0 license', 'Updated Mar 12, 2020', '4', 'Updated Jan 7, 2020', '5', 'HTML', 'Updated Mar 1, 2020', '4', 'Updated Jan 9, 2020', '2', 'Jupyter Notebook', 'MIT license', 'Updated Mar 7, 2020', 'Python', 'Updated Jan 6, 2020', 'Java', 'Updated Jan 13, 2020', 'HTML', 'Updated Jun 20, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/pandilwar605', 'info_list': ['Roff', 'Updated Jul 6, 2020', 'Jupyter Notebook', 'Updated Aug 26, 2020', '1', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 7, 2020', 'Python', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 13, 2020', 'Updated Jan 11, 2020', 'Java', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 10, 2020']}","{'location': 'California / Virginia ', 'stats_list': [], 'contributions': '83 contributions\n        in the last year', 'description': ['A.I.\nMultiple Linear Regression Lab:\nhttps://github.com/NatalieHaronitou/A.I.-/commit/837b5c31e118d7d76052b59b4546938d3387f93b\n\n'], 'url_profile': 'https://github.com/NataliaHaronitou', 'info_list': ['Roff', 'Updated Jul 6, 2020', 'Jupyter Notebook', 'Updated Aug 26, 2020', '1', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 7, 2020', 'Python', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 13, 2020', 'Updated Jan 11, 2020', 'Java', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 10, 2020']}","{'location': 'seoul, Korea', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['파이썬, AI(머신러닝, 딥러닝), opencv 프로젝트 및 코드\n'], 'url_profile': 'https://github.com/KimChungHee', 'info_list': ['Roff', 'Updated Jul 6, 2020', 'Jupyter Notebook', 'Updated Aug 26, 2020', '1', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 7, 2020', 'Python', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 13, 2020', 'Updated Jan 11, 2020', 'Java', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '141 contributions\n        in the last year', 'description': ['artificial-intelligence\nThis are some of the projects I worked on in my AI class\n'], 'url_profile': 'https://github.com/vh62', 'info_list': ['Roff', 'Updated Jul 6, 2020', 'Jupyter Notebook', 'Updated Aug 26, 2020', '1', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 7, 2020', 'Python', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 13, 2020', 'Updated Jan 11, 2020', 'Java', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 10, 2020']}","{'location': 'Pullman, WA, USA', 'stats_list': [], 'contributions': '75 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ReetBarik', 'info_list': ['Roff', 'Updated Jul 6, 2020', 'Jupyter Notebook', 'Updated Aug 26, 2020', '1', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 7, 2020', 'Python', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 13, 2020', 'Updated Jan 11, 2020', 'Java', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/aitorelvira', 'info_list': ['Roff', 'Updated Jul 6, 2020', 'Jupyter Notebook', 'Updated Aug 26, 2020', '1', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 7, 2020', 'Python', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 13, 2020', 'Updated Jan 11, 2020', 'Java', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 10, 2020']}","{'location': 'Ottawa, ON, CA', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mmheydari97', 'info_list': ['Roff', 'Updated Jul 6, 2020', 'Jupyter Notebook', 'Updated Aug 26, 2020', '1', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 7, 2020', 'Python', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 13, 2020', 'Updated Jan 11, 2020', 'Java', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 10, 2020']}","{'location': 'Hyderabad', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Artificial-Intelligence\nArtificial-Intelligence\n'], 'url_profile': 'https://github.com/MohammedHassaanAli', 'info_list': ['Roff', 'Updated Jul 6, 2020', 'Jupyter Notebook', 'Updated Aug 26, 2020', '1', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 7, 2020', 'Python', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 13, 2020', 'Updated Jan 11, 2020', 'Java', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/elif-caliskan', 'info_list': ['Roff', 'Updated Jul 6, 2020', 'Jupyter Notebook', 'Updated Aug 26, 2020', '1', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 7, 2020', 'Python', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 13, 2020', 'Updated Jan 11, 2020', 'Java', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 10, 2020']}","{'location': 'Munich, Germany', 'stats_list': [], 'contributions': '93 contributions\n        in the last year', 'description': ['Artificial_Intelligence\nThis project consists of topics related to:\n\nsearch problem\nconstraint satisfaction problem\n\n'], 'url_profile': 'https://github.com/sabean', 'info_list': ['Roff', 'Updated Jul 6, 2020', 'Jupyter Notebook', 'Updated Aug 26, 2020', '1', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 10, 2020', 'Python', 'Updated Jan 7, 2020', 'Python', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 13, 2020', 'Updated Jan 11, 2020', 'Java', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 10, 2020']}"
"{'location': 'Hyderabad', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Artificial-Intelligence\nArtificial-Intelligence\n'], 'url_profile': 'https://github.com/MohammedHassaanAli', 'info_list': ['Updated Jan 11, 2020', 'Python', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'Updated Nov 28, 2020', 'Python', 'Updated Feb 16, 2020', 'HTML', 'Updated Aug 28, 2020', 'C++', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Sep 21, 2020', 'Updated Feb 1, 2020', 'Updated Jan 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/rajesh-a-borade', 'info_list': ['Updated Jan 11, 2020', 'Python', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'Updated Nov 28, 2020', 'Python', 'Updated Feb 16, 2020', 'HTML', 'Updated Aug 28, 2020', 'C++', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Sep 21, 2020', 'Updated Feb 1, 2020', 'Updated Jan 8, 2020']}","{'location': 'Charlotte, NC', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': ['Artificial-Intelligence\n'], 'url_profile': 'https://github.com/ZTatman', 'info_list': ['Updated Jan 11, 2020', 'Python', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'Updated Nov 28, 2020', 'Python', 'Updated Feb 16, 2020', 'HTML', 'Updated Aug 28, 2020', 'C++', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Sep 21, 2020', 'Updated Feb 1, 2020', 'Updated Jan 8, 2020']}","{'location': 'China', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['AI\nArtificial Intelligence\n\n'], 'url_profile': 'https://github.com/DiegoJohnson', 'info_list': ['Updated Jan 11, 2020', 'Python', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'Updated Nov 28, 2020', 'Python', 'Updated Feb 16, 2020', 'HTML', 'Updated Aug 28, 2020', 'C++', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Sep 21, 2020', 'Updated Feb 1, 2020', 'Updated Jan 8, 2020']}","{'location': 'Oregon', 'stats_list': [], 'contributions': '50 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/NicholasFay-CIS', 'info_list': ['Updated Jan 11, 2020', 'Python', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'Updated Nov 28, 2020', 'Python', 'Updated Feb 16, 2020', 'HTML', 'Updated Aug 28, 2020', 'C++', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Sep 21, 2020', 'Updated Feb 1, 2020', 'Updated Jan 8, 2020']}","{'location': 'Finland', 'stats_list': [], 'contributions': '60 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/rabia174', 'info_list': ['Updated Jan 11, 2020', 'Python', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'Updated Nov 28, 2020', 'Python', 'Updated Feb 16, 2020', 'HTML', 'Updated Aug 28, 2020', 'C++', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Sep 21, 2020', 'Updated Feb 1, 2020', 'Updated Jan 8, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '36 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/akkirajput', 'info_list': ['Updated Jan 11, 2020', 'Python', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'Updated Nov 28, 2020', 'Python', 'Updated Feb 16, 2020', 'HTML', 'Updated Aug 28, 2020', 'C++', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Sep 21, 2020', 'Updated Feb 1, 2020', 'Updated Jan 8, 2020']}","{'location': 'Jyväskylä', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/MikkoAro', 'info_list': ['Updated Jan 11, 2020', 'Python', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'Updated Nov 28, 2020', 'Python', 'Updated Feb 16, 2020', 'HTML', 'Updated Aug 28, 2020', 'C++', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Sep 21, 2020', 'Updated Feb 1, 2020', 'Updated Jan 8, 2020']}","{'location': 'San Francisco, CA', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': ['Artificial Intelligence Fundamentals\nTask\nUnderstand the fundamentals of Artificial Intelligence.\nOutput\nThe repository contains work based on the topics below.\nCriteria\n\nIntroduction to AI\n\n\nSearch\n\n\nHeuristic and Adversarial Search\n\n\nConstant Satisfaction\n\n\nLogic\n\n\nPlanning\n\n\nProbabilistic Reasoning\n\n\nLearning\n\n\nNatural Language Processing\n\n\nPerception ? Advanced Topics\n\nTech Stack\n\nJava\n\n\nPython\n\n'], 'url_profile': 'https://github.com/MLBott', 'info_list': ['Updated Jan 11, 2020', 'Python', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'Updated Nov 28, 2020', 'Python', 'Updated Feb 16, 2020', 'HTML', 'Updated Aug 28, 2020', 'C++', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Sep 21, 2020', 'Updated Feb 1, 2020', 'Updated Jan 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '29 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Bruddock', 'info_list': ['Updated Jan 11, 2020', 'Python', 'Updated Jan 7, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', '1', 'Jupyter Notebook', 'Updated Nov 28, 2020', 'Python', 'Updated Feb 16, 2020', 'HTML', 'Updated Aug 28, 2020', 'C++', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Sep 21, 2020', 'Updated Feb 1, 2020', 'Updated Jan 8, 2020']}"
"{'location': 'Istanbul', 'stats_list': [], 'contributions': '14 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mzn', 'info_list': ['Updated Jan 9, 2020', 'Python', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 27, 2021', 'Updated Jan 6, 2020', 'Updated Jan 10, 2020', 'GPL-3.0 license', 'Updated Jan 8, 2020', 'Python', 'Updated Apr 5, 2020', 'Updated Jan 11, 2020', '1', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'C', 'Updated Mar 10, 2020']}","{'location': 'Athens, Greece', 'stats_list': [], 'contributions': '22 contributions\n        in the last year', 'description': ['Facebook NewsBot Instructions\nRequirements \n\nPycharm \nPython (only 3.7.x versions) \n\nThen follow this tutorial \nInstall this Packages : \n\npip3 install Flask  \npip3 install pymessenger  \npip3 install nltk \npip3 install numpy  \npip3 install tflearn  \npip3 install tensorflow==1.15.0 \n\n'], 'url_profile': 'https://github.com/tzortzis-kondylis', 'info_list': ['Updated Jan 9, 2020', 'Python', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 27, 2021', 'Updated Jan 6, 2020', 'Updated Jan 10, 2020', 'GPL-3.0 license', 'Updated Jan 8, 2020', 'Python', 'Updated Apr 5, 2020', 'Updated Jan 11, 2020', '1', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'C', 'Updated Mar 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '182 contributions\n        in the last year', 'description': ['Artificial Intelligence\nArtificial Intelligence Fundamental class at FTU\n'], 'url_profile': 'https://github.com/langsari', 'info_list': ['Updated Jan 9, 2020', 'Python', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 27, 2021', 'Updated Jan 6, 2020', 'Updated Jan 10, 2020', 'GPL-3.0 license', 'Updated Jan 8, 2020', 'Python', 'Updated Apr 5, 2020', 'Updated Jan 11, 2020', '1', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'C', 'Updated Mar 10, 2020']}","{'location': 'Chicago, IL', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Sheery96', 'info_list': ['Updated Jan 9, 2020', 'Python', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 27, 2021', 'Updated Jan 6, 2020', 'Updated Jan 10, 2020', 'GPL-3.0 license', 'Updated Jan 8, 2020', 'Python', 'Updated Apr 5, 2020', 'Updated Jan 11, 2020', '1', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'C', 'Updated Mar 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/GryffDerrick', 'info_list': ['Updated Jan 9, 2020', 'Python', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 27, 2021', 'Updated Jan 6, 2020', 'Updated Jan 10, 2020', 'GPL-3.0 license', 'Updated Jan 8, 2020', 'Python', 'Updated Apr 5, 2020', 'Updated Jan 11, 2020', '1', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'C', 'Updated Mar 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '28 contributions\n        in the last year', 'description': ['CS-6300-Artificial-Intelligence\nCS 6300 A.I. 2018 at University of Utah\nProject to create a working AI for Pacman\n'], 'url_profile': 'https://github.com/Justin-Ngo', 'info_list': ['Updated Jan 9, 2020', 'Python', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 27, 2021', 'Updated Jan 6, 2020', 'Updated Jan 10, 2020', 'GPL-3.0 license', 'Updated Jan 8, 2020', 'Python', 'Updated Apr 5, 2020', 'Updated Jan 11, 2020', '1', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'C', 'Updated Mar 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/cohenyu', 'info_list': ['Updated Jan 9, 2020', 'Python', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 27, 2021', 'Updated Jan 6, 2020', 'Updated Jan 10, 2020', 'GPL-3.0 license', 'Updated Jan 8, 2020', 'Python', 'Updated Apr 5, 2020', 'Updated Jan 11, 2020', '1', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'C', 'Updated Mar 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['PacMan-Artificial-Intelligence\n'], 'url_profile': 'https://github.com/K-V-S-Rachana', 'info_list': ['Updated Jan 9, 2020', 'Python', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 27, 2021', 'Updated Jan 6, 2020', 'Updated Jan 10, 2020', 'GPL-3.0 license', 'Updated Jan 8, 2020', 'Python', 'Updated Apr 5, 2020', 'Updated Jan 11, 2020', '1', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'C', 'Updated Mar 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['NTUA_Artificial_Intelligence\n""Artificial Intelligence"" course - ECE NTUA\n\nFind shortest path in a NxN grid with obstacles\nRecommendation System\n\n'], 'url_profile': 'https://github.com/maryka2', 'info_list': ['Updated Jan 9, 2020', 'Python', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 27, 2021', 'Updated Jan 6, 2020', 'Updated Jan 10, 2020', 'GPL-3.0 license', 'Updated Jan 8, 2020', 'Python', 'Updated Apr 5, 2020', 'Updated Jan 11, 2020', '1', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'C', 'Updated Mar 10, 2020']}","{'location': 'Toronto, ON, Canada', 'stats_list': [], 'contributions': '162 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/KojimaMcMaple', 'info_list': ['Updated Jan 9, 2020', 'Python', 'Updated Feb 6, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 27, 2021', 'Updated Jan 6, 2020', 'Updated Jan 10, 2020', 'GPL-3.0 license', 'Updated Jan 8, 2020', 'Python', 'Updated Apr 5, 2020', 'Updated Jan 11, 2020', '1', 'Jupyter Notebook', 'Updated Jan 10, 2020', 'C', 'Updated Mar 10, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '82 contributions\n        in the last year', 'description': [""Artificial-Intelligence-That-Can-See\nKeras, OpenAI Gym, DQNAgent, Deep reinforcement learning\nprerequisite: Python 3\ndeprecation errors are gym's fault\ninstersection of two lines\nhttps://stackoverflow.com/questions/22417842/how-do-i-find-the-intersection-of-two-line-segments\ninterseciton of line and circle\nhttps://stackoverflow.com/questions/30844482/what-is-most-efficient-way-to-find-the-intersection-of-a-line-and-a-circle-in-py\nHow to make a polygon\nhttps://shapely.readthedocs.io/en/stable/manual.html#polygons\n""], 'url_profile': 'https://github.com/McCannDahl', 'info_list': ['Python', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Python', 'Updated Jan 12, 2020', 'C#', 'Updated Jan 10, 2020', 'Updated Jan 6, 2020', 'MATLAB', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Jan 8, 2020']}","{'location': 'Santa Clara, California', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['Artificial Intelligence for Robotics - Udacity\nProgramming all the major systems of a robotic car. The Topics include Localization, Planning, Search, Track and Control.\nThis Repository includes my solutions to the quizzes and projects in the Udacity Artificial Intelligence for Robotics Course.\n'], 'url_profile': 'https://github.com/Gopsee', 'info_list': ['Python', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Python', 'Updated Jan 12, 2020', 'C#', 'Updated Jan 10, 2020', 'Updated Jan 6, 2020', 'MATLAB', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Jan 8, 2020']}","{'location': 'Pakistan', 'stats_list': [], 'contributions': '21 contributions\n        in the last year', 'description': ['Python-Basics-PIAIC-\nThis is the example of python basics learned in PIAIC\n'], 'url_profile': 'https://github.com/Talha089', 'info_list': ['Python', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Python', 'Updated Jan 12, 2020', 'C#', 'Updated Jan 10, 2020', 'Updated Jan 6, 2020', 'MATLAB', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Jan 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['Torcs-Python-Client-\nThis is python client of torcs bot\nRead torcs.pdf file and download torcs server and run this client and modify if you want to enhance its performance\n'], 'url_profile': 'https://github.com/Mian-Zaid', 'info_list': ['Python', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Python', 'Updated Jan 12, 2020', 'C#', 'Updated Jan 10, 2020', 'Updated Jan 6, 2020', 'MATLAB', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Jan 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/MichalPiechowiak', 'info_list': ['Python', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Python', 'Updated Jan 12, 2020', 'C#', 'Updated Jan 10, 2020', 'Updated Jan 6, 2020', 'MATLAB', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Jan 8, 2020']}","{'location': 'Greater Minneapolis St Paul Area', 'stats_list': [], 'contributions': '93 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/MorrisOmbiro', 'info_list': ['Python', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Python', 'Updated Jan 12, 2020', 'C#', 'Updated Jan 10, 2020', 'Updated Jan 6, 2020', 'MATLAB', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Jan 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Artificial-intelligence-project-using-CNN\n'], 'url_profile': 'https://github.com/thotanaveen044', 'info_list': ['Python', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Python', 'Updated Jan 12, 2020', 'C#', 'Updated Jan 10, 2020', 'Updated Jan 6, 2020', 'MATLAB', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Jan 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '163 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/jwmmy', 'info_list': ['Python', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Python', 'Updated Jan 12, 2020', 'C#', 'Updated Jan 10, 2020', 'Updated Jan 6, 2020', 'MATLAB', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Jan 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '62 contributions\n        in the last year', 'description': ['Problem sets of Artificial Intelligence course\n'], 'url_profile': 'https://github.com/blues-lead', 'info_list': ['Python', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Python', 'Updated Jan 12, 2020', 'C#', 'Updated Jan 10, 2020', 'Updated Jan 6, 2020', 'MATLAB', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Jan 8, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['AIBusiness\nPart 1 - Optimizing Business Processes\nCase Study: Optimizing the Flows in an E-Commerce Warehouse\nAI Solution: Q-Learning\nPart 2 - Minimizing Costs\nCase Study: Minimizing the Costs in Energy Consumption of a Data Center\nAI Solution: Deep Q-Learning\nPart 3 - Maximizing Revenues\nCase Study: Maximizing Revenue of an Online Retail Business\nAI Solution: Thompson Sampling\n'], 'url_profile': 'https://github.com/sebastianflaen', 'info_list': ['Python', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated May 3, 2020', 'Python', 'Updated Jan 12, 2020', 'C#', 'Updated Jan 10, 2020', 'Updated Jan 6, 2020', 'MATLAB', 'Updated Jan 8, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Feb 8, 2020', 'Python', 'Updated Jan 8, 2020']}"
"{'location': 'Orlando, FL', 'stats_list': [], 'contributions': '70 contributions\n        in the last year', 'description': ['CAP4630-Wocjan\nArtificial Intelligence class content\n'], 'url_profile': 'https://github.com/erick1439', 'info_list': ['Jupyter Notebook', 'Updated Apr 25, 2020', 'Python', 'Updated Jan 8, 2020', '2', 'Python', 'Updated Jan 14, 2020', '1', 'Python', 'Updated Jun 26, 2020', 'Rust', 'Updated Nov 8, 2020', '1', 'Python', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 11, 2020', '1', 'TeX', 'MIT license', 'Updated Sep 7, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Python', 'Updated Jan 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['AIBusiness\nPart 1 - Optimizing Business Processes\nCase Study: Optimizing the Flows in an E-Commerce Warehouse\nAI Solution: Q-Learning\nPart 2 - Minimizing Costs\nCase Study: Minimizing the Costs in Energy Consumption of a Data Center\nAI Solution: Deep Q-Learning\nPart 3 - Maximizing Revenues\nCase Study: Maximizing Revenue of an Online Retail Business\nAI Solution: Thompson Sampling\n'], 'url_profile': 'https://github.com/sebastianflaen', 'info_list': ['Jupyter Notebook', 'Updated Apr 25, 2020', 'Python', 'Updated Jan 8, 2020', '2', 'Python', 'Updated Jan 14, 2020', '1', 'Python', 'Updated Jun 26, 2020', 'Rust', 'Updated Nov 8, 2020', '1', 'Python', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 11, 2020', '1', 'TeX', 'MIT license', 'Updated Sep 7, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Python', 'Updated Jan 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '64 contributions\n        in the last year', 'description': ['\nAI agent plays and learns the game. In this project agents don\'t need training dataset to learn the game.\nFeatures\n\nDeveloped with python,\nOpen source project,\nOpen source libraries (pygame, tensorflow, numpy),\nFancy gui,\nIncludes art,\nEasy to understand,\nWorks on multiple platforms.\n\nSnake AI\nCurrently (January 2020) you should use python version 3.7.X. Tensorflow doesn\'t work at python version 3.8.X!\nYou can install needed libraries from shell with these lines.\npip install tensorflow\npip install numpy\npip install pygame\n\nAfter installing libraries type following lines to start your snake trainer.\n### STARTING THE PROGRAM ###\n> python main.py  # opens SnakeAI project\n     Select your choice\n     1. Load Agent     \n     2. Train Agent    \n     Choice: \n> 1/2             # type ""1"" to load agent or ""2"" to train new one\n\n### 1 - FOR LOADING AGENT ###\n     Select Agent ID: agent_\n> 5001            # type ""61"", ""401"" or ""5001"" to open saved agents\n\n### 2 - FOR TRAINING NEW AGENT ###\n     Game Limit: \n> 51              # type limit game number to train agent (start training with 50)\n\nGame GUI\n\nExamples\n\n\n\nGeneration 48\nGeneration 192\n\n\n\n\n\n\n\n\n\nArt\nThe famous painting ""Kaplumbağa Terbiyecisi"" (Turtle Trainer) of Mr. Osman Hamdi was used to add visual power to the project.\n\n'], 'url_profile': 'https://github.com/bilalguvenc', 'info_list': ['Jupyter Notebook', 'Updated Apr 25, 2020', 'Python', 'Updated Jan 8, 2020', '2', 'Python', 'Updated Jan 14, 2020', '1', 'Python', 'Updated Jun 26, 2020', 'Rust', 'Updated Nov 8, 2020', '1', 'Python', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 11, 2020', '1', 'TeX', 'MIT license', 'Updated Sep 7, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Python', 'Updated Jan 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['AIVIS\nArtificial Intelligence Virtual Interview System\n'], 'url_profile': 'https://github.com/monawin456', 'info_list': ['Jupyter Notebook', 'Updated Apr 25, 2020', 'Python', 'Updated Jan 8, 2020', '2', 'Python', 'Updated Jan 14, 2020', '1', 'Python', 'Updated Jun 26, 2020', 'Rust', 'Updated Nov 8, 2020', '1', 'Python', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 11, 2020', '1', 'TeX', 'MIT license', 'Updated Sep 7, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Python', 'Updated Jan 31, 2020']}","{'location': 'Netherlands', 'stats_list': [], 'contributions': '87 contributions\n        in the last year', 'description': ['RAISE\nRAISE is a neural network library writting in Rust from scratch.\nRAISE includes a seperate experimental autograd package called RAISE Graph which can be found here.\nInstallation\nRAISE needs to be installed with RAISE Graph on the same directory level.\n[dependencies]\nrand=""0.7.3""\nrand_distr = ""0.2.2""\ndyn-clone = ""1.0.1""\ninline-python = ""0.5.1""\nraise-graph = { path = ""../raise-graph"" }\nUsage\nFor an example how to use RAISE please see the examples folder.\nFor usable performance always build and run with the --release flag.\nMNIST\nmnist.rs downloads the mnist dataset and trains a simple neural network.\nThe accuracy resulting accuracy should be around 96%.\nInitializing a model works similarly  to PyTorch and TensorFlow:\nlet hidden_layer = 50;\n\nlet mut model = Sequential::new(vec![\n    Box::new(Linear::new([train_set.input_shape()[1], hidden_layer], ""relu"")),\n    Box::new(ReLU::new()),\n    Box::new(Linear::new([hidden_layer, train_set.target_shape()[1]], """")),\n]);\n\nlet mut loss_func = CrossEntropy::new();\nlet mut optimizer = SGD::new(0.05);\n\nlet batch_size = 64;\nTraining a model in RAISE only needs a couple of lines. No need to manually write a training loop. However, since everything is written in Rust you can easily take a look at how things work under the hood!\nlet (mean, std) = train_set.norm_input();\nvalid_set.norm_input_with(mean, std);\n\nlet train_loader = DataLoader::new(train_set, batch_size, true);\nlet valid_loader = DataLoader::new(valid_set, batch_size, false);\n\nfit(5, &mut model, &mut loss_func, &mut optimizer, &train_loader, &valid_loader);\nThis shoud result in an output similar this:\nEpoch 0: Train Accuracy: 0.852, Train Loss: 0.513, Valid Accuracy: 0.932, Valid Loss: 0.244, Elapsed Time: 4.80s\nEpoch 1: Train Accuracy: 0.925, Train Loss: 0.260, Valid Accuracy: 0.944, Valid Loss: 0.202, Elapsed Time: 4.67s\nEpoch 2: Train Accuracy: 0.942, Train Loss: 0.201, Valid Accuracy: 0.935, Valid Loss: 0.225, Elapsed Time: 4.73s\nEpoch 3: Train Accuracy: 0.952, Train Loss: 0.170, Valid Accuracy: 0.955, Valid Loss: 0.162, Elapsed Time: 4.69s\nEpoch 4: Train Accuracy: 0.956, Train Loss: 0.152, Valid Accuracy: 0.959, Valid Loss: 0.150, Elapsed Time: 4.68s\nContributing\nPull requests are welcome. For major changes, please open an issue first to discuss what you would like to change.\n'], 'url_profile': 'https://github.com/AlexanderKeijzer', 'info_list': ['Jupyter Notebook', 'Updated Apr 25, 2020', 'Python', 'Updated Jan 8, 2020', '2', 'Python', 'Updated Jan 14, 2020', '1', 'Python', 'Updated Jun 26, 2020', 'Rust', 'Updated Nov 8, 2020', '1', 'Python', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 11, 2020', '1', 'TeX', 'MIT license', 'Updated Sep 7, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Python', 'Updated Jan 31, 2020']}","{'location': 'Chicago, Illinois', 'stats_list': [], 'contributions': '71 contributions\n        in the last year', 'description': ['CS 480: Introduction to Artificial Intelligence - Fall 2019\nCourse Description\nIntroduction to computational methods for intelligent control of autonomous agents, and the use of programming paradigms that support development of flexible and reactive systems. These include heuristic search, knowledge representation, constraint satisfaction, probabilistic reasoning, decision-theoretic control, and sensor interpretation. Particular focus will be places on real-world application of the material.\n'], 'url_profile': 'https://github.com/VoraHarsh', 'info_list': ['Jupyter Notebook', 'Updated Apr 25, 2020', 'Python', 'Updated Jan 8, 2020', '2', 'Python', 'Updated Jan 14, 2020', '1', 'Python', 'Updated Jun 26, 2020', 'Rust', 'Updated Nov 8, 2020', '1', 'Python', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 11, 2020', '1', 'TeX', 'MIT license', 'Updated Sep 7, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Python', 'Updated Jan 31, 2020']}","{'location': 'Noida', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NEW LIST 2017 - 2020: Machine-Learning / Deep-Learning / AI -Tutorials\nHi - Thanks for dropping by!\n\nI will be updating this tutorials site on a daily basis adding all relevant topcis, including latest researches papers from internet such as arxiv.org, BIORXIV - Specifically Neuroscience to name a few. \n\nMore importantly the applications of ML/DL/AI into industry areas such as Transportation, Medicine/Healthcare etc. will be something I\'ll watch with keen interest and would love to share the same with you.\n\nFinally, it is YOUR help I will seek to make it more useful and less boring, so please do suggest/comment/contribute!\n\n\n\nIndex\n\n\ndeep-learning\n\nUBER | Pyro\nNetflix | VectorFlow\nPyTorch\ntensorflow\ntheano\nkeras\ncaffe\nTorch/Lua\nMXNET\n\n\n\nscikit-learn\n\n\nstatistical-inference-scipy\n\n\npandas\n\n\nmatplotlib\n\n\nnumpy\n\n\npython-data\n\n\nkaggle-and-business-analyses\n\n\nspark\n\n\nmapreduce-python\n\n\namazon web services\n\n\ncommand lines\n\n\nmisc\n\n\nnotebook-installation\n\n\nCurated list of Deep Learning / AI blogs\n\n\ncredits\n\n\ncontributing\n\n\ncontact-info\n\n\nlicense\n\n\ndeep-learning\nIPython Notebook(s) and other programming tools such as Torch/Lua/D lang in demonstrating deep learning functionality.\nuber-pyro-probabalistic-tutorials\n\n\n\nAdditional PyRo tutorials:\n\npyro-examples/full examples\npyro-examples/Variational Autoencoders\npyro-examples/Bayesian Regression\npyro-examples/Deep Markov Model\npyro-examples/AIR(Attend Infer Repeat)\npyro-examples/Semi-Supervised VE\npyro-examples/GMM\npyro-examples/Gaussian Process\npyro-examples/Bayesian Optimization\nFull Pyro Code\n\nnetflix-vectorflow-tutorials\n\n\n\n\nMNIST Example, running with Dlang\n\npytorch-tutorials\n\n\n\n\n\n\nLevel\nDescription\n\n\n\n\nBeginners/Zakizhou\nLearning the basics of PyTorch from Facebook.\n\n\nIntermedia/Quanvuong\nLearning the intermediate stuff about PyTorch of from Facebook.\n\n\nAdvanced/Chsasank\nLearning the advanced stuff about PyTorch of from Facebook.\n\n\nLearning PyTorch by Examples - Numpy, Tensors and Autograd\nAt its core, PyTorch provides two main features an n-dimensional Tensor, similar to numpy but can run on GPUs AND automatic differentiation for building and training neural networks.\n\n\nPyTorch - Getting to know autograd.Variable, Gradient, Neural Network\nHere we start with ultimate basics of Tensors, wrap a Tensor with Variable module, play with nn.Module and implement forward and backward function.\n\n\n\ntensor-flow-tutorials\n\n\n\n\nAdditional TensorFlow tutorials:\n\npkmital/tensorflow_tutorials\nnlintz/TensorFlow-Tutorials\nalrojo/tensorflow-tutorial\nBinRoot/TensorFlow-Book\n\n\n\n\nNotebook\nDescription\n\n\n\n\ntsf-basics\nLearn basic operations in TensorFlow, a library for various kinds of perceptual and language understanding tasks from Google.\n\n\ntsf-linear\nImplement linear regression in TensorFlow.\n\n\ntsf-logistic\nImplement logistic regression in TensorFlow.\n\n\ntsf-nn\nImplement nearest neighboars in TensorFlow.\n\n\ntsf-alex\nImplement AlexNet in TensorFlow.\n\n\ntsf-cnn\nImplement convolutional neural networks in TensorFlow.\n\n\ntsf-mlp\nImplement multilayer perceptrons in TensorFlow.\n\n\ntsf-rnn\nImplement recurrent neural networks in TensorFlow.\n\n\ntsf-gpu\nLearn about basic multi-GPU computation in TensorFlow.\n\n\ntsf-gviz\nLearn about graph visualization in TensorFlow.\n\n\ntsf-lviz\nLearn about loss visualization in TensorFlow.\n\n\n\ntensor-flow-exercises\n\n\n\nNotebook\nDescription\n\n\n\n\ntsf-not-mnist\nLearn simple data curation by creating a pickle with formatted datasets for training, development and testing in TensorFlow.\n\n\ntsf-fully-connected\nProgressively train deeper and more accurate models using logistic regression and neural networks in TensorFlow.\n\n\ntsf-regularization\nExplore regularization techniques by training fully connected networks to classify notMNIST characters in TensorFlow.\n\n\ntsf-convolutions\nCreate convolutional neural networks in TensorFlow.\n\n\ntsf-word2vec\nTrain a skip-gram model over Text8 data in TensorFlow.\n\n\ntsf-lstm\nTrain a LSTM character model over Text8 data in TensorFlow.\n\n\n\n\n\n\n\ntheano-tutorials\n\n\n\nNotebook\nDescription\n\n\n\n\ntheano-intro\nIntro to Theano, which allows you to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently. It can use GPUs and perform efficient symbolic differentiation.\n\n\ntheano-scan\nLearn scans, a mechanism to perform loops in a Theano graph.\n\n\ntheano-logistic\nImplement logistic regression in Theano.\n\n\ntheano-rnn\nImplement recurrent neural networks in Theano.\n\n\ntheano-mlp\nImplement multilayer perceptrons in Theano.\n\n\n\n\n\n\n\nkeras-tutorials\n\n\n\nNotebook\nDescription\n\n\n\n\nkeras\nKeras is an open source neural network library written in Python. It is capable of running on top of either Tensorflow or Theano.\n\n\nsetup\nLearn about the tutorial goals and how to set up your Keras environment.\n\n\nintro-deep-learning-ann\nGet an intro to deep learning with Keras and Artificial Neural Networks (ANN).\n\n\nPerceptrons and Adaline\nImplement Peceptron and adaptive linear neurons.\n\n\nMLP and MNIST Data\nClassifying handwritten digits,implement MLP, train and debug ANN\n\n\ntheano\nLearn about Theano by working with weights matrices and gradients.\n\n\nkeras-otto\nLearn about Keras by looking at the Kaggle Otto challenge.\n\n\nann-mnist\nReview a simple implementation of ANN for MNIST using Keras.\n\n\nconv-nets\nLearn about Convolutional Neural Networks (CNNs) with Keras.\n\n\nconv-net-1\nRecognize handwritten digits from MNIST using Keras - Part 1.\n\n\nconv-net-2\nRecognize handwritten digits from MNIST using Keras - Part 2.\n\n\nkeras-models\nUse pre-trained models such as VGG16, VGG19, ResNet50, and Inception v3 with Keras.\n\n\nauto-encoders\nLearn about Autoencoders with Keras.\n\n\nrnn-lstm\nLearn about Recurrent Neural Networks (RNNs) with Keras.\n\n\nlstm-sentence-gen\nLearn about RNNs using Long Short Term Memory (LSTM) networks with Keras.\n\n\nnlp-deep-learning\nLearn about NLP using ANN (Artificial Neural Networks.\n\n\nhyperparamter-tuning\nHyperparamters tuning using keras-wrapper.scikit-learn\n\n\n\ndeep-learning-misc\n\n\n\nNotebook\nDescription\n\n\n\n\ndeep-dream\nCaffe-based computer vision program which uses a convolutional neural network to find and enhance patterns in images.\n\n\n\n\n\n\n\nscikit-learn\nIPython Notebook(s) demonstrating scikit-learn functionality.\n\n\n\nNotebook\nDescription\n\n\n\n\nintro\nIntro notebook to scikit-learn.  Scikit-learn adds Python support for large, multi-dimensional arrays and matrices, along with a large library of high-level mathematical functions to operate on these arrays.\n\n\nknn\nImplement k-nearest neighbors in scikit-learn.\n\n\nlinear-reg\nImplement linear regression in scikit-learn.\n\n\nsvm\nImplement support vector machine classifiers with and without kernels in scikit-learn.\n\n\nrandom-forest\nImplement random forest classifiers and regressors in scikit-learn.\n\n\nk-means\nImplement k-means clustering in scikit-learn.\n\n\npca\nImplement principal component analysis in scikit-learn.\n\n\ngmm\nImplement Gaussian mixture models in scikit-learn.\n\n\nvalidation\nImplement validation and model selection in scikit-learn.\n\n\n\n\n\n\n\nstatistical-inference-scipy\nIPython Notebook(s) demonstrating statistical inference with SciPy functionality.\n\n\n\nNotebook\nDescription\n\n\n\n\nscipy\nSciPy is a collection of mathematical algorithms and convenience functions built on the Numpy extension of Python. It adds significant power to the interactive Python session by providing the user with high-level commands and classes for manipulating and visualizing data.\n\n\neffect-size\nExplore statistics that quantify effect size by analyzing the difference in height between men and women.  Uses data from the Behavioral Risk Factor Surveillance System (BRFSS) to estimate the mean and standard deviation of height for adult women and men in the United States.\n\n\nsampling\nExplore random sampling by analyzing the average weight of men and women in the United States using BRFSS data.\n\n\nhypothesis\nExplore hypothesis testing by analyzing the difference of first-born babies compared with others.\n\n\n\n\n\n\n\npandas\nIPython Notebook(s) demonstrating pandas functionality.\n\n\n\nNotebook\nDescription\n\n\n\n\npandas\nSoftware library written for data manipulation and analysis in Python. Offers data structures and operations for manipulating numerical tables and time series.\n\n\ngithub-data-wrangling\nLearn how to load, clean, merge, and feature engineer by analyzing GitHub data from the Viz repo.\n\n\nIntroduction-to-Pandas\nIntroduction to Pandas.\n\n\nIntroducing-Pandas-Objects\nLearn about Pandas objects.\n\n\nData Indexing and Selection\nLearn about data indexing and selection in Pandas.\n\n\nOperations-in-Pandas\nLearn about operating on data in Pandas.\n\n\nMissing-Values\nLearn about handling missing data in Pandas.\n\n\nHierarchical-Indexing\nLearn about hierarchical indexing in Pandas.\n\n\nConcat-And-Append\nLearn about combining datasets: concat and append in Pandas.\n\n\nMerge-and-Join\nLearn about combining datasets: merge and join in Pandas.\n\n\nAggregation-and-Grouping\nLearn about aggregation and grouping in Pandas.\n\n\nPivot-Tables\nLearn about pivot tables in Pandas.\n\n\nWorking-With-Strings\nLearn about vectorized string operations in Pandas.\n\n\nWorking-with-Time-Series\nLearn about working with time series in pandas.\n\n\nPerformance-Eval-and-Query\nLearn about high-performance Pandas: eval() and query() in Pandas.\n\n\n\n\n\n\n\nmatplotlib\nIPython Notebook(s) demonstrating matplotlib functionality.\n\n\n\nNotebook\nDescription\n\n\n\n\nmatplotlib\nPython 2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments across platforms.\n\n\nmatplotlib-applied\nApply matplotlib visualizations to Kaggle competitions for exploratory data analysis.  Learn how to create bar plots, histograms, subplot2grid, normalized plots, scatter plots, subplots, and kernel density estimation plots.\n\n\nIntroduction-To-Matplotlib\nIntroduction to Matplotlib.\n\n\nSimple-Line-Plots\nLearn about simple line plots in Matplotlib.\n\n\nSimple-Scatter-Plots\nLearn about simple scatter plots in Matplotlib.\n\n\nErrorbars.ipynb\nLearn about visualizing errors in Matplotlib.\n\n\nDensity-and-Contour-Plots\nLearn about density and contour plots in Matplotlib.\n\n\nHistograms-and-Binnings\nLearn about histograms, binnings, and density in Matplotlib.\n\n\nCustomizing-Legends\nLearn about customizing plot legends in Matplotlib.\n\n\nCustomizing-Colorbars\nLearn about customizing colorbars in Matplotlib.\n\n\nMultiple-Subplots\nLearn about multiple subplots in Matplotlib.\n\n\nText-and-Annotation\nLearn about text and annotation in Matplotlib.\n\n\nCustomizing-Ticks\nLearn about customizing ticks in Matplotlib.\n\n\nSettings-and-Stylesheets\nLearn about customizing Matplotlib: configurations and stylesheets.\n\n\nThree-Dimensional-Plotting\nLearn about three-dimensional plotting in Matplotlib.\n\n\nGeographic-Data-With-Basemap\nLearn about geographic data with basemap in Matplotlib.\n\n\nVisualization-With-Seaborn\nLearn about visualization with Seaborn.\n\n\n\n\n\n\n\nnumpy\nIPython Notebook(s) demonstrating NumPy functionality.\n\n\n\nNotebook\nDescription\n\n\n\n\nnumpy\nAdds Python support for large, multi-dimensional arrays and matrices, along with a large library of high-level mathematical functions to operate on these arrays.\n\n\nIntroduction-to-NumPy\nIntroduction to NumPy.\n\n\nUnderstanding-Data-Types\nLearn about data types in Python.\n\n\nThe-Basics-Of-NumPy-Arrays\nLearn about the basics of NumPy arrays.\n\n\nComputation-on-arrays-ufuncs\nLearn about computations on NumPy arrays: universal functions.\n\n\nComputation-on-arrays-aggregates\nLearn about aggregations: min, max, and everything in between in NumPy.\n\n\nComputation-on-arrays-broadcasting\nLearn about computation on arrays: broadcasting in NumPy.\n\n\nBoolean-Arrays-and-Masks\nLearn about comparisons, masks, and boolean logic in NumPy.\n\n\nFancy-Indexing\nLearn about fancy indexing in NumPy.\n\n\nSorting\nLearn about sorting arrays in NumPy.\n\n\nStructured-Data-NumPy\nLearn about structured data: NumPy\'s structured arrays.\n\n\n\n\n\n\n\npython-data\nIPython Notebook(s) demonstrating Python functionality geared towards data analysis.\n\n\n\nNotebook\nDescription\n\n\n\n\ndata structures\nLearn Python basics with tuples, lists, dicts, sets.\n\n\ndata structure utilities\nLearn Python operations such as slice, range, xrange, bisect, sort, sorted, reversed, enumerate, zip, list comprehensions.\n\n\nfunctions\nLearn about more advanced Python features: Functions as objects, lambda functions, closures, *args, **kwargs currying, generators, generator expressions, itertools.\n\n\ndatetime\nLearn how to work with Python dates and times: datetime, strftime, strptime, timedelta.\n\n\nlogging\nLearn about Python logging with RotatingFileHandler and TimedRotatingFileHandler.\n\n\npdb\nLearn how to debug in Python with the interactive source code debugger.\n\n\nunit tests\nLearn how to test in Python with Nose unit tests.\n\n\n\n\n\n\n\nkaggle-and-business-analyses\nIPython Notebook(s) used in kaggle competitions and business analyses.\n\n\n\nNotebook\nDescription\n\n\n\n\ntitanic\nPredict survival on the Titanic.  Learn data cleaning, exploratory data analysis, and machine learning.\n\n\nchurn-analysis\nPredict customer churn.  Exercise logistic regression, gradient boosting classifers, support vector machines, random forests, and k-nearest-neighbors.  Includes discussions of confusion matrices, ROC plots, feature importances, prediction probabilities, and calibration/descrimination.\n\n\n\n\n\n\n\nspark\nIPython Notebook(s) demonstrating spark and HDFS functionality.\n\n\n\nNotebook\nDescription\n\n\n\n\nspark\nIn-memory cluster computing framework, up to 100 times faster for certain applications and is well suited for machine learning algorithms.\n\n\nhdfs\nReliably stores very large files across machines in a large cluster.\n\n\n\n\n\n\n\nmapreduce-python\nIPython Notebook(s) demonstrating Hadoop MapReduce with mrjob functionality.\n\n\n\nNotebook\nDescription\n\n\n\n\nmapreduce-python\nRuns MapReduce jobs in Python, executing jobs locally or on Hadoop clusters. Demonstrates Hadoop Streaming in Python code with unit test and mrjob config file to analyze Amazon S3 bucket logs on Elastic MapReduce.  Disco is another python-based alternative.\n\n\n\n\n\n\n\naws\nIPython Notebook(s) demonstrating Amazon Web Services (AWS) and AWS tools functionality.\nAlso check out:\n\nSAWS: A Supercharged AWS command line interface (CLI).\nAwesome AWS: A curated list of libraries, open source repos, guides, blogs, and other resources.\n\n\n\n\nNotebook\nDescription\n\n\n\n\nboto\nOfficial AWS SDK for Python.\n\n\ns3cmd\nInteracts with S3 through the command line.\n\n\ns3distcp\nCombines smaller files and aggregates them together by taking in a pattern and target file.  S3DistCp can also be used to transfer large volumes of data from S3 to your Hadoop cluster.\n\n\ns3-parallel-put\nUploads multiple files to S3 in parallel.\n\n\nredshift\nActs as a fast data warehouse built on top of technology from massive parallel processing (MPP).\n\n\nkinesis\nStreams data in real time with the ability to process thousands of data streams per second.\n\n\nlambda\nRuns code in response to events, automatically managing compute resources.\n\n\n\n\n\n\n\ncommands\nIPython Notebook(s) demonstrating various command lines for Linux, Git, etc.\n\n\n\nNotebook\nDescription\n\n\n\n\nlinux\nUnix-like and mostly POSIX-compliant computer operating system.  Disk usage, splitting files, grep, sed, curl, viewing running processes, terminal syntax highlighting, and Vim.\n\n\nanaconda\nDistribution of the Python programming language for large-scale data processing, predictive analytics, and scientific computing, that aims to simplify package management and deployment.\n\n\nipython notebook\nWeb-based interactive computational environment where you can combine code execution, text, mathematics, plots and rich media into a single document.\n\n\ngit\nDistributed revision control system with an emphasis on speed, data integrity, and support for distributed, non-linear workflows.\n\n\nruby\nUsed to interact with the AWS command line and for Jekyll, a blog framework that can be hosted on GitHub Pages.\n\n\njekyll\nSimple, blog-aware, static site generator for personal, project, or organization sites.  Renders Markdown or Textile and Liquid templates, and produces a complete, static website ready to be served by Apache HTTP Server, Nginx or another web server.\n\n\npelican\nPython-based alternative to Jekyll.\n\n\ndjango\nHigh-level Python Web framework that encourages rapid development and clean, pragmatic design. It can be useful to share reports/analyses and for blogging. Lighter-weight alternatives include Pyramid, Flask, Tornado, and Bottle.\n\n\n\nmisc\nIPython Notebook(s) demonstrating miscellaneous functionality.\n\n\n\nNotebook\nDescription\n\n\n\n\nregex\nRegular expression cheat sheet useful in data wrangling.\n\n\nalgorithmia\nAlgorithmia is a marketplace for algorithms. This notebook showcases 4 different algorithms: Face Detection, Content Summarizer, Latent Dirichlet Allocation and Optical Character Recognition.\n\n\n\nnotebook-installation\nanaconda\nAnaconda is a free distribution of the Python programming language for large-scale data processing, predictive analytics, and scientific computing that aims to simplify package management and deployment.\nFollow instructions to install Anaconda or the more lightweight miniconda.\ndev-setup\nFor detailed instructions, scripts, and tools to set up your development environment for data analysis, check out the dev-setup repo.\nrunning-notebooks\nNote: If you intend to learn the hard way (preferred method)then I\'d strongly advice to write as much code as you can yourself and not just run pre-written code. If you still want to test it, then do the following:\nTo view interactive content or to modify elements within the IPython notebooks, you must first clone or download the repository then run the notebook.  More information on IPython Notebooks can be found here.\n$ git clone https://github.com/TarrySingh/Artificial-Intelligence-Deep-Learning-Machine-Learning-Tutorials.git\n$ cd Artificial-Intelligence-Deep-Learning-Machine-Learning-Tutorials\n$ jupyter notebook\n\nNotebooks tested with Python 2.7.x.(will soon be updated to Python 3.5+)\ncurated-list-of-deeplearning-blogs\n\nA Blog From a Human-engineer-being http://www.erogol.com/ (RSS)\nAakash Japi http://aakashjapi.com/ (RSS)\nAdit Deshpande https://adeshpande3.github.io/ (RSS)\nAdvanced Analytics & R http://advanceddataanalytics.net/ (RSS)\nAdventures in Data Land http://blog.smola.org (RSS)\nAgile Data Science http://blog.sense.io/ (RSS)\nAhmed El Deeb https://medium.com/@D33B (RSS)\nAirbnb Data blog http://nerds.airbnb.com/data/ (RSS)\nAlex Castrounis | InnoArchiTech http://www.innoarchitech.com/ (RSS)\nAlex Perrier http://alexperrier.github.io/ (RSS)\nAlgobeans | Data Analytics Tutorials & Experiments for the Layman https://algobeans.com (RSS)\nAmazon AWS AI Blog https://aws.amazon.com/blogs/ai/ (RSS)\nAnalytics Vidhya http://www.analyticsvidhya.com/blog/ (RSS)\nAnalytics and Visualization in Big Data @ Sicara https://blog.sicara.com (RSS)\nAndreas Müller http://peekaboo-vision.blogspot.com/ (RSS)\nAndrej Karpathy blog http://karpathy.github.io/ (RSS)\nAndrew Brooks http://brooksandrew.github.io/simpleblog/ (RSS)\nAndrey Kurenkov http://www.andreykurenkov.com/writing/ (RSS)\nAnton Lebedevich\'s Blog http://mabrek.github.io/ (RSS)\nArthur Juliani https://medium.com/@awjuliani (RSS)\nAudun M. Øygard http://www.auduno.com/ (RSS)\nAvi Singh https://avisingh599.github.io/ (RSS)\nBeautiful Data http://beautifuldata.net/ (RSS)\nBeckerfuffle http://mdbecker.github.io/ (RSS)\nBecoming A Data Scientist http://www.becomingadatascientist.com/ (RSS)\nBen Bolte\'s Blog http://benjaminbolte.com/ml/ (RSS)\nBen Frederickson http://www.benfrederickson.com/blog/ (RSS)\nBerkeley AI Research http://bair.berkeley.edu/blog/ (RSS)\nBig-Ish Data http://bigishdata.com/ (RSS)\nBlog on neural networks http://yerevann.github.io/ (RSS)\nBlogistic RegressionAbout Projects http://d10genes.github.io/blog/ (RSS)\nblogR | R tips and tricks from a scientist https://drsimonj.svbtle.com/ (RSS)\nBrain of mat kelcey http://matpalm.com/blog/ (RSS)\nBrilliantly wrong thoughts on science and programming https://arogozhnikov.github.io/ (RSS)\nBugra Akyildiz http://bugra.github.io/ (RSS)\nBuilding Babylon https://building-babylon.net/ (RSS)\nCarl Shan http://carlshan.com/ (RSS)\nChris Stucchio https://www.chrisstucchio.com/blog/index.html (RSS)\nChristophe Bourguignat https://medium.com/@chris_bour (RSS)\nChristopher Nguyen https://medium.com/@ctn (RSS)\nCloudera Data Science Posts http://blog.cloudera.com/blog/category/data-science/ (RSS)\ncolah\'s blog http://colah.github.io/archive.html (RSS)\nCortana Intelligence and Machine Learning Blog https://blogs.technet.microsoft.com/machinelearning/ (RSS)\nDaniel Forsyth http://www.danielforsyth.me/ (RSS)\nDaniel Homola http://danielhomola.com/category/blog/ (RSS)\nDaniel Nee http://danielnee.com (RSS)\nData Based Inventions http://datalab.lu/ (RSS)\nData Blogger https://www.data-blogger.com/ (RSS)\nData Labs http://blog.insightdatalabs.com/ (RSS)\nData Meets Media http://datameetsmedia.com/ (RSS)\nData Miners Blog http://blog.data-miners.com/ (RSS)\nData Mining Research http://www.dataminingblog.com/ (RSS)\nData Mining: Text Mining, Visualization and Social Media http://datamining.typepad.com/data_mining/ (RSS)\nData Piques http://blog.ethanrosenthal.com/ (RSS)\nData School http://www.dataschool.io/ (RSS)\nData Science 101 http://101.datascience.community/ (RSS)\nData Science @ Facebook https://research.facebook.com/blog/datascience/ (RSS)\nData Science Insights http://www.datasciencebowl.com/data-science-insights/ (RSS)\nData Science Tutorials https://codementor.io/data-science/tutorial (RSS)\nData Science Vademecum http://datasciencevademecum.wordpress.com/ (RSS)\nDataaspirant http://dataaspirant.com/ (RSS)\nDataclysm http://blog.okcupid.com/ (RSS)\nDataGenetics http://datagenetics.com/blog.html (RSS)\nDataiku https://www.dataiku.com/blog/ (RSS)\nDataKind http://www.datakind.org/blog (RSS)\nDataLook http://blog.datalook.io/ (RSS)\nDatanice https://datanice.wordpress.com/ (RSS)\nDataquest Blog https://www.dataquest.io/blog/ (RSS)\nDataRobot http://www.datarobot.com/blog/ (RSS)\nDatascope http://datascopeanalytics.com/blog (RSS)\nDatasFrame http://tomaugspurger.github.io/ (RSS)\nDavid Mimno http://www.mimno.org/ (RSS)\nDayne Batten http://daynebatten.com (RSS)\nDeep Learning http://deeplearning.net/blog/ (RSS)\nDeepdish http://deepdish.io/ (RSS)\nDelip Rao http://deliprao.com/ (RSS)\nDENNY\'S BLOG http://blog.dennybritz.com/ (RSS)\nDimensionless https://dimensionless.in/blog/ (RSS)\nDistill http://distill.pub/ (RSS)\nDistrict Data Labs http://districtdatalabs.silvrback.com/ (RSS)\nDiving into data https://blog.datadive.net/ (RSS)\nDomino Data Lab\'s blog http://blog.dominodatalab.com/ (RSS)\nDr. Randal S. Olson http://www.randalolson.com/blog/ (RSS)\nDrew Conway https://medium.com/@drewconway (RSS)\nDustin Tran http://dustintran.com/blog/ (RSS)\nEder Santana https://edersantana.github.io/blog.html (RSS)\nEdwin Chen http://blog.echen.me (RSS)\nEFavDB http://efavdb.com/ (RSS)\nEmilio Ferrara, Ph.D.  http://www.emilio.ferrara.name/ (RSS)\nEntrepreneurial Geekiness http://ianozsvald.com/ (RSS)\nEric Jonas http://ericjonas.com/archives.html (RSS)\nEric Siegel http://www.predictiveanalyticsworld.com/blog (RSS)\nErik Bern http://erikbern.com (RSS)\nERIN SHELLMAN http://www.erinshellman.com/ (RSS)\nEugenio Culurciello http://culurciello.github.io/ (RSS)\nFabian Pedregosa http://fa.bianp.net/ (RSS)\nFast Forward Labs http://blog.fastforwardlabs.com/ (RSS)\nFastML http://fastml.com/ (RSS)\nFlorian Hartl http://florianhartl.com/ (RSS)\nFlowingData http://flowingdata.com/ (RSS)\nFull Stack ML http://fullstackml.com/ (RSS)\nGAB41 http://www.lab41.org/gab41/ (RSS)\nGarbled Notes http://www.chioka.in/ (RSS)\nGreg Reda http://www.gregreda.com/blog/ (RSS)\nHyon S Chu https://medium.com/@adailyventure (RSS)\ni am trask http://iamtrask.github.io/ (RSS)\nI Quant NY http://iquantny.tumblr.com/ (RSS)\ninFERENCe http://www.inference.vc/ (RSS)\nInsight Data Science https://blog.insightdatascience.com/ (RSS)\nINSPIRATION INFORMATION http://myinspirationinformation.com/ (RSS)\nIra Korshunova http://irakorshunova.github.io/ (RSS)\nI’m a bandit https://blogs.princeton.edu/imabandit/ (RSS)\nJason Toy http://www.jtoy.net/ (RSS)\nJeremy D. Jackson, PhD http://www.jeremydjacksonphd.com/ (RSS)\nJesse Steinweg-Woods https://jessesw.com/ (RSS)\nJoe Cauteruccio http://www.joecjr.com/ (RSS)\nJohn Myles White http://www.johnmyleswhite.com/ (RSS)\nJohn\'s Soapbox http://joschu.github.io/ (RSS)\nJonas Degrave http://317070.github.io/ (RSS)\nJoy Of Data http://www.joyofdata.de/blog/ (RSS)\nJulia Evans http://jvns.ca/ (RSS)\nKDnuggets http://www.kdnuggets.com/ (RSS)\nKeeping Up With The Latest Techniques http://colinpriest.com/ (RSS)\nKenny Bastani http://www.kennybastani.com/ (RSS)\nKevin Davenport http://kldavenport.com/ (RSS)\nkevin frans http://kvfrans.com/ (RSS)\nkorbonits | Math ∩ Data http://korbonits.github.io/ (RSS)\nLarge Scale Machine Learning  http://bickson.blogspot.com/ (RSS)\nLATERAL BLOG https://blog.lateral.io/ (RSS)\nLazy Programmer http://lazyprogrammer.me/ (RSS)\nLearn Analytics Here https://learnanalyticshere.wordpress.com/ (RSS)\nLearnDataSci http://www.learndatasci.com/ (RSS)\nLearning With Data http://learningwithdata.com/ (RSS)\nLife, Language, Learning http://daoudclarke.github.io/ (RSS)\nLocke Data https://itsalocke.com/blog/ (RSS)\nLouis Dorard http://www.louisdorard.com/blog/ (RSS)\nM.E.Driscoll http://medriscoll.com/ (RSS)\nMachinalis http://www.machinalis.com/blog (RSS)\nMachine Learning (Theory) http://hunch.net/ (RSS)\nMachine Learning and Data Science http://alexhwoods.com/blog/ (RSS)\nMachine Learning https://charlesmartin14.wordpress.com/ (RSS)\nMachine Learning Mastery http://machinelearningmastery.com/blog/ (RSS)\nMachine Learning Blogs https://machinelearningblogs.com/ (RSS)\nMachine Learning, etc http://yaroslavvb.blogspot.com (RSS)\nMachine Learning, Maths and Physics https://mlopezm.wordpress.com/ (RSS)\nMachine Learning Flashcards https://machinelearningflashcards.com/ $10, but a nicely illustrated set of 300 flash cards\nMachined Learnings http://www.machinedlearnings.com/ (RSS)\nMAPPING BABEL https://jack-clark.net/ (RSS)\nMAPR Blog https://www.mapr.com/blog (RSS)\nMAREK REI http://www.marekrei.com/blog/ (RSS)\nMARGINALLY INTERESTING http://blog.mikiobraun.de/ (RSS)\nMath ∩ Programming http://jeremykun.com/ (RSS)\nMatthew Rocklin http://matthewrocklin.com/blog/ (RSS)\nMelody Wolk http://melodywolk.com/projects/ (RSS)\nMic Farris http://www.micfarris.com/ (RSS)\nMike Tyka http://mtyka.github.io/ (RSS)\nminimaxir | Max Woolf\'s Blog http://minimaxir.com/ (RSS)\nMirror Image https://mirror2image.wordpress.com/ (RSS)\nMitch Crowe http://www.dataphoric.com/ (RSS)\nMLWave http://mlwave.com/ (RSS)\nMLWhiz http://mlwhiz.com/ (RSS)\nModels are illuminating and wrong https://peadarcoyle.wordpress.com/ (RSS)\nMoody Rd http://blog.mrtz.org/ (RSS)\nMoonshots http://jxieeducation.com/ (RSS)\nMourad Mourafiq http://mourafiq.com/ (RSS)\nMy thoughts on Data science, predictive analytics, Python http://shahramabyari.com/ (RSS)\nNatural language processing blog http://nlpers.blogspot.fr/ (RSS)\nNeil Lawrence http://inverseprobability.com/blog.html (RSS)\nNLP and Deep Learning enthusiast http://camron.xyz/ (RSS)\nno free hunch http://blog.kaggle.com/ (RSS)\nNuit Blanche http://nuit-blanche.blogspot.com/ (RSS)\nNumber 2147483647 https://no2147483647.wordpress.com/ (RSS)\nOn Machine Intelligence https://aimatters.wordpress.com/ (RSS)\nOpiate for the masses Data is our religion. http://opiateforthemass.es/ (RSS)\np-value.info http://www.p-value.info/ (RSS)\nPete Warden\'s blog http://petewarden.com/ (RSS)\nPlotly Blog http://blog.plot.ly/ (RSS)\nProbably Overthinking It http://allendowney.blogspot.ca/ (RSS)\nProoffreader.com http://www.prooffreader.com (RSS)\nProoffreaderPlus http://prooffreaderplus.blogspot.ca/ (RSS)\nPublishable Stuff http://www.sumsar.net/ (RSS)\nPyImageSearch http://www.pyimagesearch.com/ (RSS)\nPythonic Perambulations https://jakevdp.github.io/ (RSS)\nquintuitive http://quintuitive.com/ (RSS)\nR and Data Mining https://rdatamining.wordpress.com/ (RSS)\nR-bloggers http://www.r-bloggers.com/ (RSS)\nR2RT http://r2rt.com/ (RSS)\nRamiro Gómez http://ramiro.org/notebooks/ (RSS)\nRandom notes on Computer Science, Mathematics and Software Engineering http://barmaley-exe.github.io/ (RSS)\nRandy Zwitch http://randyzwitch.com/ (RSS)\nRaRe Technologies http://rare-technologies.com/blog/ (RSS)\nRayli.Net http://rayli.net/blog/ (RSS)\nRevolutions http://blog.revolutionanalytics.com/ (RSS)\nRinu Boney http://rinuboney.github.io/ (RSS)\nRNDuja Blog http://rnduja.github.io/ (RSS)\nRobert Chang https://medium.com/@rchang (RSS)\nRocket-Powered Data Science http://rocketdatascience.org (RSS)\nSachin Joglekar\'s blog https://codesachin.wordpress.com/ (RSS)\nsamim https://medium.com/@samim (RSS)\nSean J. Taylor http://seanjtaylor.com/ (RSS)\nSebastian Raschka http://sebastianraschka.com/blog/index.html (RSS)\nSebastian Ruder http://sebastianruder.com/ (RSS)\nSebastian\'s slow blog http://www.nowozin.net/sebastian/blog/ (RSS)\nSFL Scientific Blog https://sflscientific.com/blog/ (RSS)\nShakir\'s Machine Learning Blog http://blog.shakirm.com/ (RSS)\nSimply Statistics http://simplystatistics.org (RSS)\nSpringboard Blog http://springboard.com/blog\nStartup.ML Blog http://startup.ml/blog (RSS)\nStatistical Modeling, Causal Inference, and Social Science http://andrewgelman.com/ (RSS)\nStigler Diet http://stiglerdiet.com/ (RSS)\nStitch Fix Tech Blog http://multithreaded.stitchfix.com/blog/ (RSS)\nStochastic R&D Notes http://arseny.info/ (RSS)\nStorytelling with Statistics on Quora http://datastories.quora.com/ (RSS)\nStreamHacker http://streamhacker.com/ (RSS)\nSubconscious Musings http://blogs.sas.com/content/subconsciousmusings/ (RSS)\nSwan Intelligence http://swanintelligence.com/ (RSS)\nTechnoCalifornia http://technocalifornia.blogspot.se/ (RSS)\nTEXT ANALYSIS BLOG | AYLIEN http://blog.aylien.com/ (RSS)\nThe Angry Statistician http://angrystatistician.blogspot.com/ (RSS)\nThe Clever Machine https://theclevermachine.wordpress.com/ (RSS)\nThe Data Camp Blog https://www.datacamp.com/community/blog (RSS)\nThe Data Incubator http://blog.thedataincubator.com/ (RSS)\nThe Data Science Lab https://datasciencelab.wordpress.com/ (RSS)\nTHE ETZ-FILES http://alexanderetz.com/ (RSS)\nThe Science of Data http://www.martingoodson.com (RSS)\nThe Shape of Data https://shapeofdata.wordpress.com (RSS)\nThe unofficial Google data science Blog http://www.unofficialgoogledatascience.com/ (RSS)\nTim Dettmers http://timdettmers.com/ (RSS)\nTombone\'s Computer Vision Blog http://www.computervisionblog.com/ (RSS)\nTommy Blanchard http://tommyblanchard.com/category/projects (RSS)\nTrevor Stephens http://trevorstephens.com/ (RSS)\nTrey Causey http://treycausey.com/ (RSS)\nUW Data Science Blog http://datasciencedegree.wisconsin.edu/blog/ (RSS)\nWellecks http://wellecks.wordpress.com/ (RSS)\nWes McKinney http://wesmckinney.com/archives.html (RSS)\nWhile My MCMC Gently Samples http://twiecki.github.io/ (RSS)\nWildML http://www.wildml.com/ (RSS)\nWill do stuff for stuff http://rinzewind.org/blog-en (RSS)\nWill wolf http://willwolf.io/ (RSS)\nWILL\'S NOISE http://www.willmcginnis.com/ (RSS)\nWilliam Lyon http://www.lyonwj.com/ (RSS)\nWin-Vector Blog http://www.win-vector.com/blog/ (RSS)\nYanir Seroussi http://yanirseroussi.com/ (RSS)\nZac Stewart http://zacstewart.com/ (RSS)\nŷhat http://blog.yhat.com/ (RSS)\nℚuantitative √ourney http://outlace.com/ (RSS)\n大トロ http://blog.otoro.net/ (RSS)\n\ncredits\n\nPython for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython by Wes McKinney\nPyCon 2015 Scikit-learn Tutorial by Jake VanderPlas\nPython Data Science Handbook by Jake VanderPlas\nParallel Machine Learning with scikit-learn and IPython by Olivier Grisel\nStatistical Interference Using Computational Methods in Python by Allen Downey\nTensorFlow Examples by Aymeric Damien\nTensorFlow Tutorials by Parag K Mital\nTensorFlow Tutorials by Nathan Lintz\nTensorFlow Tutorials by Alexander R Johansen\nTensorFlow Book by Nishant Shukla\nSummer School 2015 by mila-udem\nKeras tutorials by Valerio Maggio\nKaggle\nYhat Blog\n\ncontributing\nContributions are welcome!  For bug reports or requests please submit an issue.\ncontact-info\nFeel free to contact me to discuss any issues, questions, or comments.\n\nEmail: tarry.singh@gmail.com\nTwitter: @tarrysingh\nGitHub: tarrysingh\nLinkedIn: Tarry Singh\nWebsite: tarrysingh.com\nMedium: tarry@Medium\nQuora : Answers from Tarry on Quora\n\nlicense\nThis repository contains a variety of content; some developed by Tarry Singh and some from third-parties and a lot will be maintained by me. The third-party content is distributed under the license provided by those parties.\nThe content was originally developed by Donne Martin is distributed under the following license. I will be maintaining and revamping it by adding PyTorch, Torch/Lua, MXNET and much more:\nI am providing code and resources in this repository to you under an open source license.\nCopyright 2017 Tarry Singh\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\n'], 'url_profile': 'https://github.com/PIYUSH0812', 'info_list': ['Jupyter Notebook', 'Updated Apr 25, 2020', 'Python', 'Updated Jan 8, 2020', '2', 'Python', 'Updated Jan 14, 2020', '1', 'Python', 'Updated Jun 26, 2020', 'Rust', 'Updated Nov 8, 2020', '1', 'Python', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 11, 2020', '1', 'TeX', 'MIT license', 'Updated Sep 7, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Python', 'Updated Jan 31, 2020']}","{'location': 'Nüremberg - Germany', 'stats_list': [], 'contributions': '407 contributions\n        in the last year', 'description': ['Artificial Intelligence in Industry 4.0\n'], 'url_profile': 'https://github.com/jupiterbak', 'info_list': ['Jupyter Notebook', 'Updated Apr 25, 2020', 'Python', 'Updated Jan 8, 2020', '2', 'Python', 'Updated Jan 14, 2020', '1', 'Python', 'Updated Jun 26, 2020', 'Rust', 'Updated Nov 8, 2020', '1', 'Python', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 11, 2020', '1', 'TeX', 'MIT license', 'Updated Sep 7, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Python', 'Updated Jan 31, 2020']}","{'location': 'Malaysia', 'stats_list': [], 'contributions': '927 contributions\n        in the last year', 'description': ['DAT263x\nIntroduction to Artificial Intelligence (AI)\n'], 'url_profile': 'https://github.com/dennislamcv1', 'info_list': ['Jupyter Notebook', 'Updated Apr 25, 2020', 'Python', 'Updated Jan 8, 2020', '2', 'Python', 'Updated Jan 14, 2020', '1', 'Python', 'Updated Jun 26, 2020', 'Rust', 'Updated Nov 8, 2020', '1', 'Python', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 11, 2020', '1', 'TeX', 'MIT license', 'Updated Sep 7, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Python', 'Updated Jan 31, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '116 contributions\n        in the last year', 'description': ['CIS365-Project1\nCIS365-Artificial Intelligence-Project 1-Capture The Flag(Pacman)\n'], 'url_profile': 'https://github.com/JosefHartsough', 'info_list': ['Jupyter Notebook', 'Updated Apr 25, 2020', 'Python', 'Updated Jan 8, 2020', '2', 'Python', 'Updated Jan 14, 2020', '1', 'Python', 'Updated Jun 26, 2020', 'Rust', 'Updated Nov 8, 2020', '1', 'Python', 'Updated Jan 9, 2020', 'Python', 'Updated Jan 11, 2020', '1', 'TeX', 'MIT license', 'Updated Sep 7, 2020', 'Jupyter Notebook', 'Updated Mar 29, 2020', 'Python', 'Updated Jan 31, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['Artificial Intelligence learns to play Kariba!\nPython implementation of Multiple-Observer Information Set Monte Carlo Tree Search (MOISMCTS) for the card game Kariba, as described in this blog.\nGetting Started\nFirst, install required dependencies (either conda or pip)\n    conda install numpy\n    conda install jupyter\n    conda install tqdm\nClone this repo\n    git clone https://github.com/KnurpsBram/AI_plays_kariba.git\n    cd AI_plays_kariba \nTo play a game of Kariba against an AI using MOISMCTS, open interactive_game.ipynb. From the command line:\n    cd src\n    jupyter notebook\n\nMOISMCTS keeps track of a separate tree per player. A node in this tree is not a state, but an \'information set\'; the set of all states the game could be in given the information that the player has. A player cannot observe the opponent\'s hand, but it can reason about which hands are either in the deck or in the opponents hand. These cards are in the \'jungle\', the union of the deck and the opponent\'s hand(s). The tree distinguishes between post-action-nodes and neutral-nodes. We only keep track of the amount of simulated wins from post-action-nodes onwards, because only for those nodes do we need to compute the upper confidence bound (UCB) (which requires the amount of wins).\n>>> from kariba_moismcts import Kariba, moismcts, Simulators, Node\n\n... kariba = Kariba()\n... event = kariba.random_card_draw() # actions and card draws are both \'events\', represented as a dictionary\n... kariba.apply_event(event)\n... root_state = kariba\n... best_action = moismcts(root_state, n=100)\n\n... print(kariba)\n... print(event)\n... print(best_action)\n100%|██████████| 100/100 [00:00<00:00, 130.70it/s]\n\n-------------------------\nturn: player0\ndeck:\n[8 7 8 8 8 7 7 6]\nfield:\n[0 0 0 0 0 0 0 0]\nhands:\nplayer0 [0 1 0 0 0 1 1 2]\nplayer1 [0 0 0 0 0 0 0 0]\n-------------------------\n\n{\'kind\': \'deck_draw\', \'who\': \'player0\', \'cards\': array([0, 1, 0, 0, 0, 1, 1, 2])}\n{\'kind\': \'action\', \'who\': \'player0\', \'cards\': array([0, 0, 0, 0, 0, 1, 0, 0])}\n\n# what happens inside the moismcts function\n>>> simulators = Simulators(root_state)\n\n... simulators.apply_event(simulators.select_action())\n... simulators.next_turn()\n... simulators.apply_event(simulators.random_card_draw()) # draw cards for player0\n\n... print(""Complete information of the state:"")\n... print(simulators.game)\n\n... print(""Partial information available to player0:"")\n... print(Node(simulators.game, ""player0""))\n\n... print(""Partial information available to player1:"")\n... print(Node(simulators.game, ""player1""))\nComplete information of the state:\n-------------------------\nturn: player1\ndeck:\n[7 7 8 8 6 7 7 4]\nfield:\n[0 0 0 0 0 0 1 0]\nhands:\nplayer0 [0 1 0 0 0 1 0 2]\nplayer1 [1 0 0 0 2 0 0 2]\n-------------------------\n\nPartial information available to player0:\n+------------------------\nneutral_node\nself: player0\nturn: player1\nn: 0\njungle:\n[8 7 8 8 8 7 7 6]\nfield:\n[0 0 0 0 0 0 1 0]\nhand:\n[0 1 0 0 0 1 0 2]\n\nPartial information available to player1:\n+------------------------\nneutral_node\nself: player1\nturn: player1\nn: 0\njungle:\n[7 8 8 8 6 8 7 6]\nfield:\n[0 0 0 0 0 0 1 0]\nhand:\n[1 0 0 0 2 0 0 2]\n\n>>> # run n simulations\n... n = 6\n\n... simulators = Simulators(root_state)\n... for _ in range(n):\n...     while not simulators.game.is_final:\n...         simulators.apply_event(simulators.random_card_draw()) # give cards to the player whose turn it is, at the very first turn, this should not do anything\n...         simulators.apply_event(simulators.select_action()) # the player whose turn it is may select the action, apply the action to the game and update both the players\' trees\n...         simulators.next_turn()\n...     winner = simulators.game.leading_player\n...     simulators.backpropagate(winner)\n...     simulators.reset_game()\n    \n... print(""Tree of player0 after ""+str(n)+"" simulations:"")\n... print(simulators.tree_dict[""player0""])\n\n... print(""Tree of player1 after ""+str(n)+"" simulations:"")\n... print(simulators.tree_dict[""player1""])\nTree of player0 after 6 simulations:\n+------------------------\nneutral_node\nself: player0\nturn: player1\nn: 6\njungle:\n[8 7 8 8 8 7 7 6]\nfield:\n[0 0 0 0 0 0 1 0]\nhand:\n[0 1 0 0 0 1 0 2]\n    +------------------------\n    neutral_node\n    self: player0\n    turn: player1\n    n: 2\n    jungle:\n    [8 7 8 8 7 7 7 6]\n    field:\n    [0 0 0 0 1 0 1 0]\n    hand:\n    [0 1 0 0 0 1 0 2]\n        +------------------------\n        neutral_node\n        self: player0\n        turn: player0\n        n: 1\n        jungle:\n        [7 7 8 8 7 7 7 6]\n        field:\n        [0 0 0 0 1 0 1 0]\n        hand:\n        [1 1 0 0 0 1 0 2]\n        \n    +------------------------\n    neutral_node\n    self: player0\n    turn: player1\n    n: 1\n    jungle:\n    [7 7 8 8 8 7 7 6]\n    field:\n    [1 0 0 0 0 0 1 0]\n    hand:\n    [0 1 0 0 0 1 0 2]\n    \n    +------------------------\n    neutral_node\n    self: player0\n    turn: player1\n    n: 1\n    jungle:\n    [8 7 8 8 8 7 7 4]\n    field:\n    [0 0 0 0 0 0 1 2]\n    hand:\n    [0 1 0 0 0 1 0 2]\n    \n    +------------------------\n    neutral_node\n    self: player0\n    turn: player1\n    n: 1\n    jungle:\n    [8 7 8 8 6 7 7 6]\n    field:\n    [0 0 0 0 2 0 1 0]\n    hand:\n    [0 1 0 0 0 1 0 2]\n    \n    +------------------------\n    neutral_node\n    self: player0\n    turn: player1\n    n: 1\n    jungle:\n    [8 7 8 8 8 7 7 5]\n    field:\n    [0 0 0 0 0 0 1 1]\n    hand:\n    [0 1 0 0 0 1 0 2]\n    \nTree of player1 after 6 simulations:\n+------------------------\nneutral_node\nself: player1\nturn: player1\nn: 6\njungle:\n[7 8 8 8 6 8 7 6]\nfield:\n[0 0 0 0 0 0 1 0]\nhand:\n[1 0 0 0 2 0 0 2]\n    +------------------------\n    post_action_node\n    self: player1\n    turn: player1\n    n: 2\n    w: 2\n    jungle:\n    [7 8 8 8 6 8 7 6]\n    field:\n    [0 0 0 0 1 0 1 0]\n    hand:\n    [1 0 0 0 1 0 0 2]\n        +------------------------\n        neutral_node\n        self: player1\n        turn: player0\n        n: 1\n        jungle:\n        [7 8 8 8 6 8 7 4]\n        field:\n        [0 0 0 0 1 0 1 2]\n        hand:\n        [1 0 0 0 1 0 0 2]\n        \n    +------------------------\n    post_action_node\n    self: player1\n    turn: player1\n    n: 1\n    w: 0\n    jungle:\n    [7 8 8 8 6 8 7 6]\n    field:\n    [1 0 0 0 0 0 1 0]\n    hand:\n    [0 0 0 0 2 0 0 2]\n    \n    +------------------------\n    post_action_node\n    self: player1\n    turn: player1\n    n: 1\n    w: 0\n    jungle:\n    [7 8 8 8 6 8 7 6]\n    field:\n    [0 0 0 0 0 0 1 2]\n    hand:\n    [1 0 0 0 2 0 0 0]\n    \n    +------------------------\n    post_action_node\n    self: player1\n    turn: player1\n    n: 1\n    w: 1\n    jungle:\n    [7 8 8 8 6 8 7 6]\n    field:\n    [0 0 0 0 2 0 1 0]\n    hand:\n    [1 0 0 0 0 0 0 2]\n    \n    +------------------------\n    post_action_node\n    self: player1\n    turn: player1\n    n: 1\n    w: 0\n    jungle:\n    [7 8 8 8 6 8 7 6]\n    field:\n    [0 0 0 0 0 0 1 1]\n    hand:\n    [1 0 0 0 2 0 0 1]\n\n'], 'url_profile': 'https://github.com/KnurpsBram', 'info_list': ['1', 'Python', 'Updated Feb 24, 2020', 'Java', 'Updated Jan 8, 2020', 'Python', 'MIT license', 'Updated Feb 18, 2020', 'Updated Jan 7, 2020', 'Python', 'Updated Jul 30, 2020', 'Python', 'MIT license', 'Updated Jan 12, 2020', '1', 'Python', 'Updated Jan 8, 2020', 'ShaderLab', 'Updated Jan 12, 2021', '2', 'Python', 'MIT license', 'Updated Dec 22, 2020', 'Python', 'Updated Jan 10, 2020']}","{'location': 'Irvine, California', 'stats_list': [], 'contributions': '27 contributions\n        in the last year', 'description': ['SudokuSolver\nConstraint Propagation based Sudoku Solver.\nCourse Project for CS271P Intro to Artificial Intelligence, UC Irvine, Fall 2019.\nDeveloped by Rishabh Saxena & Chaitanya Ujeniya\n'], 'url_profile': 'https://github.com/rsaxena07', 'info_list': ['1', 'Python', 'Updated Feb 24, 2020', 'Java', 'Updated Jan 8, 2020', 'Python', 'MIT license', 'Updated Feb 18, 2020', 'Updated Jan 7, 2020', 'Python', 'Updated Jul 30, 2020', 'Python', 'MIT license', 'Updated Jan 12, 2020', '1', 'Python', 'Updated Jan 8, 2020', 'ShaderLab', 'Updated Jan 12, 2021', '2', 'Python', 'MIT license', 'Updated Dec 22, 2020', 'Python', 'Updated Jan 10, 2020']}","{'location': 'Dallas, TX', 'stats_list': [], 'contributions': '356 contributions\n        in the last year', 'description': [""AI Guide for Fantasy Premier League\nArtificial Intelligence guide for preparing my team for Fantasy Premier League.\n\n\nFantasy Premier League\nA.K.A FPL in short, it is a game in which participants assemble an imaginary team of real life footballers for a gameweek in the English Premier League and score points based on those players' actual statistical performance or their perceived contribution on the field of play. It requires predicting and foreseeing the future of the players' performance.\nThe fpl_AI Bot\n\nIt gives suggestions based on insights on what steps to take to maximize the FPL team points.\nIt makes the suggestions for player transfer if needed based on:\n\nupcoming fixtures\nplayers' form\nteam budget\n\n\nIt sends the suggestions or advice automatically via email a few hours before the FPL Gameweek deadline.\n\nWorkflow\nFetch and organize data\n\nScrape data\n\nParse and store all the data\n\n\n\ncd src/scraping\npython global_scraper.py\npython teams_scraper.py ${TEAM_ID}\n\n\nUpdate existing data with new data\n\ncd src\nrm -rf raw_data/2019-20/players\nmv scraping/data/2019-20/players raw_data/2019-20/ -f\nmv scraping/data/2019-20/players_raw.csv raw_data/2019-20/ -f\nmv scraping/data/2019-20/fixtures.csv raw_data/ -f\nmv scraping/data/team_data/my_team.json raw_data/ -f\nmv scraping/data/gameweeks.json raw_data/ -f\nrm -rf scraping/data\n\n\nClean and preprocess the data\n\npython data_cleaner.py\npython data_maker.py\n\n\nRun the bot\n\ncd src\npython ai.py\n\n\nNotify the user\n\ncd src\npython ai.py\n\nLinks\n\nAutomating GitHub Workflow\nGitHub scheduled events\n\nData\nKudos to vaastav for the data\nConcept\nKudos to ravgeetdhillon for the concept\n""], 'url_profile': 'https://github.com/likarajo', 'info_list': ['1', 'Python', 'Updated Feb 24, 2020', 'Java', 'Updated Jan 8, 2020', 'Python', 'MIT license', 'Updated Feb 18, 2020', 'Updated Jan 7, 2020', 'Python', 'Updated Jul 30, 2020', 'Python', 'MIT license', 'Updated Jan 12, 2020', '1', 'Python', 'Updated Jan 8, 2020', 'ShaderLab', 'Updated Jan 12, 2021', '2', 'Python', 'MIT license', 'Updated Dec 22, 2020', 'Python', 'Updated Jan 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mg065', 'info_list': ['1', 'Python', 'Updated Feb 24, 2020', 'Java', 'Updated Jan 8, 2020', 'Python', 'MIT license', 'Updated Feb 18, 2020', 'Updated Jan 7, 2020', 'Python', 'Updated Jul 30, 2020', 'Python', 'MIT license', 'Updated Jan 12, 2020', '1', 'Python', 'Updated Jan 8, 2020', 'ShaderLab', 'Updated Jan 12, 2021', '2', 'Python', 'MIT license', 'Updated Dec 22, 2020', 'Python', 'Updated Jan 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '193 contributions\n        in the last year', 'description': ['Owari\nPurpose:\nThe goal of this assignment is to gain an understanding of game playing, the minimax algorithm, and alpha-beta pruning.\nBackground:\nOwari is a two-player, alternating-move, zero-sum game of complete information. Each player (called SOUTH and NORTH) has six pits lined up horizontally in front of him or her, and a ""goal"" pit on the side to the player’s right. The board will be represented as a picture:\n3 3 3 3 3 3\n0                   0\n3 3 3 3 3 3\nHere, SOUTH’s pits are at the bottom and goal pit at the right, while NORTH’s pits are at the top and goal pit at the left. The picture above shows the starting configuration for every game: each pit has 3 stones in it, except for the goal pits which are empty.\nTo make a move, a player chooses one of the six pits on his or her side of the board (the chosen pit must have stones in it) and redistributes (or ""sows"") the stones one-by-one going counterclockwise around the board, starting with the pit following the one picked. The opponent\'s goal pit, if reached, is skipped. For the purposes of this implementation, the pits on SOUTH\'s side are numbered 0 to 5, with pit 6 being the SOUTH\'s goal pit, while NORTH \'s pits are numbered 7 to 12 from right to left (i.e., continuing counter-clockwise) and NORTH\'s goal pit is pit number 13. If SOUTH moves first and chooses to move from pit number 4, the resulting position would be:\n3 3 3 3 3 4\n0                   1\n3 3 3 3 0 4\nCapturing: If the last stone of a player\'s move falls into an empty pit on the moving player\'s side of the board, then any stones in the pit opposite to it are captured and placed in the moving player\'s goal pit. For instance, if it were SOUTH’s turn in the position shown above, SOUTH could choose to move from pit number 1, and the resulting configuration would be:\n3 3 3 3 0 4\n0                   4\n3 0 4 4 1 4\nNote that all of the stones from pit number 8 have been captured and placed in SOUTH’s pit.\nEnding the game: The game is over when either player empties all six pits on his or her side of the board. The other player then takes all of the remaining stones from his or her own side, and places them in his or her goal pit. Players then count the stones in their goal pits. The player with the most stones is the winner.\nRequirements:\nFor this assignment, you are required to write a program that plays a competent game of Owari against an interactive opponent.  You may assume that the computer is always the SOUTH player and the interactive opponent is always the NORTH player. The main components of your program include the following:\n\nGetWhoMovesFirst: This routine will interactively prompt the human opponent (NORTH) to select whether he or she wants to move first or second.\nGetHumanPlayerMove:  This routine will prompt the human opponent (NORTH) to specify the pit from which he or she wants to move stones (7, 8, 9, 10, 11, or 12).  If the specified pit is empty, your program should display an appropriate error message and redisplay the prompt.  Repeat the process until the user selects a pit that is not empty, and then move the stones from that pit to other pits as described above, skipping the opponent’s goal pit if necessary.\nGenerateComputerPlayerMove:  This routine will use the minimax algorithm with alpha-beta pruning to determine the optimal next move for the computer player.  The algorithm used by this routine is relatively simple:\na)\tGiven the current state, generate all possible successor states by trying all legal computer moves. (A maximum of six moves are possible.)\nb)\tSelect the move that leads to the successor state that is most advantageous for the computer player.\nOf course, the tricky part for the computer player comes from evaluating each possible successor state.  The computer proceeds by assuming that the human player will always pick the move that leads to a successor state that is most favorable to him or her.  Unfortunately, the size of the search space for Owari is large enough to make exhaustive search of the game space prohibitively costly.  For this reason, your computer move generator should keep track of the current search depth, and should stop generating new successor states when it reaches a predefined depth limit.  It should then call a static evaluation function to estimate the value of the current state, and return the estimated value to the calling routine.\n\n'], 'url_profile': 'https://github.com/Nat101', 'info_list': ['1', 'Python', 'Updated Feb 24, 2020', 'Java', 'Updated Jan 8, 2020', 'Python', 'MIT license', 'Updated Feb 18, 2020', 'Updated Jan 7, 2020', 'Python', 'Updated Jul 30, 2020', 'Python', 'MIT license', 'Updated Jan 12, 2020', '1', 'Python', 'Updated Jan 8, 2020', 'ShaderLab', 'Updated Jan 12, 2021', '2', 'Python', 'MIT license', 'Updated Dec 22, 2020', 'Python', 'Updated Jan 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['Machine Learning with AWS\nThis course is the right place to start if you are interested in learning useful artificial intelligence and machine learning skills using Amazon Web Services, the most popular and powerful cloud platform. You will learn in detail how to use AWS to transform your projects into apps that work at high speed and are highly scalable. From natural language processing (NLP) applications, such as language translation,\nunderstanding news articles and other text sources, to creating chatbots with both voice and text interfaces, to processing huge numbers of images fast, and creating machine learning models, you will learn all that there is to know about using AWS to your advantage. By the end of this course, you will have the skills to efficiently use AWS in your machine learning and artificial intelligence projects..\nWhat you will learn\n\nGet up and running with machine learning on the AWS platform\nAnalyze unstructured text using AI and Amazon Comprehend\nCreate a chatbot and interact with it using speech and text input\nRetrieve external data via your chatbot\nDevelop a natural language interface\nApply AI to images and videos with Amazon Rekognition\n\nHardware Requirement\nFor an optimal student experience, we recommend the following hardware configuration:\n\nProcessor: Intel Core i5 or equivalent\nMemory: 4GB RAM\nStorage: 35GB available space\n\nSoftware Requirement\nYou’ll also need the following software installed in advance:\n\nOS: Windows 7 SP1 64-bit, Windows 8.1 64-bit or Windows 10 64-bit\nBrowser: Google Chrome, Latest Version\nAn AWS account\n\n'], 'url_profile': 'https://github.com/elephantscale', 'info_list': ['1', 'Python', 'Updated Feb 24, 2020', 'Java', 'Updated Jan 8, 2020', 'Python', 'MIT license', 'Updated Feb 18, 2020', 'Updated Jan 7, 2020', 'Python', 'Updated Jul 30, 2020', 'Python', 'MIT license', 'Updated Jan 12, 2020', '1', 'Python', 'Updated Jan 8, 2020', 'ShaderLab', 'Updated Jan 12, 2021', '2', 'Python', 'MIT license', 'Updated Dec 22, 2020', 'Python', 'Updated Jan 10, 2020']}","{'location': 'Tempe, Arizona, USA', 'stats_list': [], 'contributions': '38 contributions\n        in the last year', 'description': [""arijit7978-Artificial-Intelligence-CSE-471--PacMan\nArizona State University\nCSE 471 Intro To Artificial Intelligence\nFall 2019\nGrade A\nIn this project, several techniques of Artificial Intelligence such as Searching, Adversarial Behaviour, Deep Reinforcement Learning, Neural Network etc are implemented to help the pacman agent to maximize its expected utility.\nProject 0: Python Autograder Tutorial\nProject 1: Searching - DFS, BFS, UCS, Greedy Search, A* Search, etc.\nProject 2: MultiAgent Pacman - minimax, alpha-beta pruning, expectimax, etc.\nProject 3: Reinforcement Learning - MDP, value iteration, q-learning, epsilon-greedy, approximate q-learning, etc.\nProject 4: Ghostbusters - HMM, Particle Filtering, Bayes' Nets, Deep Reinforcement Learning, etc.\nRefer project specification file under each project for more details.\nProgramming Language - Python 3.7\n""], 'url_profile': 'https://github.com/arijit7978', 'info_list': ['1', 'Python', 'Updated Feb 24, 2020', 'Java', 'Updated Jan 8, 2020', 'Python', 'MIT license', 'Updated Feb 18, 2020', 'Updated Jan 7, 2020', 'Python', 'Updated Jul 30, 2020', 'Python', 'MIT license', 'Updated Jan 12, 2020', '1', 'Python', 'Updated Jan 8, 2020', 'ShaderLab', 'Updated Jan 12, 2021', '2', 'Python', 'MIT license', 'Updated Dec 22, 2020', 'Python', 'Updated Jan 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '57 contributions\n        in the last year', 'description': ['Unity The hague university artificial intelligence Gold Quest\nThe hague university - game minor - artificial intelligence Gold Quest - A little student projec.\nAn Tic Tac Toe made in Unity with an unbeatable ai.\nScreenshots\n\n\n'], 'url_profile': 'https://github.com/nietjoost', 'info_list': ['1', 'Python', 'Updated Feb 24, 2020', 'Java', 'Updated Jan 8, 2020', 'Python', 'MIT license', 'Updated Feb 18, 2020', 'Updated Jan 7, 2020', 'Python', 'Updated Jul 30, 2020', 'Python', 'MIT license', 'Updated Jan 12, 2020', '1', 'Python', 'Updated Jan 8, 2020', 'ShaderLab', 'Updated Jan 12, 2021', '2', 'Python', 'MIT license', 'Updated Dec 22, 2020', 'Python', 'Updated Jan 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '208 contributions\n        in the last year', 'description': ['\nProgramming Artificial Intelligence Utilities is a package that aims to make\nartificial intelligence and machine learning programming easier through\nabstractions of extensive APIs, research paper implementations, and data\nmanipulation.\nPackage Features\n\nAnalytics\n\nPlotting of data through embedding algorithms, such as Isomap and TSNE\n\n\nAudio\n\nRecording and playing\nVolume, speed, and pitch manipulation\nTrimming and Splitting\nSpectrogram, Fbanks, and MFCC creation\nAudio file conversions\n\n\nImage\n\nSimplified OpenCV Interface\n\n\nAutoencoder\n\nTrainer and Predictor\nTrainer with extra decoder\nVAE Trainer\n\n\nEvolution Algorithm\n\nOne dimensional evolution algorithm\nHyperparameter tuner\n\n\nGAN\n\nGAN Trainer\nGANI Trainer (GAN which takes provided Inputs)\nCycle GAN Trainer\nPredictors\n\n\nNeural Network\n\nTrainer and Predictor\nDense layers that combine batch norm\nConvolution layers that combine batch norm, max pooling, upsampling, and transposing\n\n\nReinforcement\n\nOpenAI Gym wrapper\nMulti-agent adverserial environment\nGreedy, ascetic, and stochastic policies\nNoise policies\nExponential, linear, and constant decay\nNormal memory and efficient time distributed memory (for stacked states)\nAgents\n\nQAgent: Q-learning with a table\nDQNAgent Q-learning with a neural network model\nPGAgent: State to action neural network model (Actor) trained with\npolicy gradients\nDDPGAgent: State to continous action space neural network model trained\nwith deterministic policy gradients\n\n\n\n\nReinforcement Agents\n\nDQNPGAgent: Combination of a DQN and PG agent into one agent\nA2CAgent: Advantage Actor Critic agent\nPPOAgent: Proximal Policy Optimization agent\nTD3Agent: Twin Delayed DDPG Agent\nPGCAgent: Continuous variant of PGAgent\nA2CCAgent: Continuous variant of A2CAgent\n\n\n\n'], 'url_profile': 'https://github.com/Tiger767', 'info_list': ['1', 'Python', 'Updated Feb 24, 2020', 'Java', 'Updated Jan 8, 2020', 'Python', 'MIT license', 'Updated Feb 18, 2020', 'Updated Jan 7, 2020', 'Python', 'Updated Jul 30, 2020', 'Python', 'MIT license', 'Updated Jan 12, 2020', '1', 'Python', 'Updated Jan 8, 2020', 'ShaderLab', 'Updated Jan 12, 2021', '2', 'Python', 'MIT license', 'Updated Dec 22, 2020', 'Python', 'Updated Jan 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/dipakbarua', 'info_list': ['1', 'Python', 'Updated Feb 24, 2020', 'Java', 'Updated Jan 8, 2020', 'Python', 'MIT license', 'Updated Feb 18, 2020', 'Updated Jan 7, 2020', 'Python', 'Updated Jul 30, 2020', 'Python', 'MIT license', 'Updated Jan 12, 2020', '1', 'Python', 'Updated Jan 8, 2020', 'ShaderLab', 'Updated Jan 12, 2021', '2', 'Python', 'MIT license', 'Updated Dec 22, 2020', 'Python', 'Updated Jan 10, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Mustymustapha12', 'info_list': ['Updated Jan 7, 2020', 'Python', 'Updated Jan 11, 2020', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 16, 2020', 'Updated Jan 6, 2020', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 27, 2020', 'Updated Jan 8, 2020', 'Python', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ddarrah17', 'info_list': ['Updated Jan 7, 2020', 'Python', 'Updated Jan 11, 2020', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 16, 2020', 'Updated Jan 6, 2020', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 27, 2020', 'Updated Jan 8, 2020', 'Python', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020']}","{'location': 'Ireland', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['Intro-to-AI\nFiles from my Intro to Artificial Intelligence module\n3 word doc on questions. No coding assignments\n'], 'url_profile': 'https://github.com/patricklowe', 'info_list': ['Updated Jan 7, 2020', 'Python', 'Updated Jan 11, 2020', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 16, 2020', 'Updated Jan 6, 2020', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 27, 2020', 'Updated Jan 8, 2020', 'Python', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020']}","{'location': 'Bucharest', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Madaist', 'info_list': ['Updated Jan 7, 2020', 'Python', 'Updated Jan 11, 2020', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 16, 2020', 'Updated Jan 6, 2020', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 27, 2020', 'Updated Jan 8, 2020', 'Python', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['aigarbagefinder Βρίσκουμε σκουπίδια με Τεχνητή Νοημοσύνη\nΟμάδα: Κυνηγοί Σκουπιδιών\nΒρίσκουμε σκουπίδια χρησιμοποιώντας ΤΝ (Τεχνητή νοημοσύνη - Artificial Intelligence)\nΣκοπός του έργου είναι να φτιαχτεί ένα σύστημα το οποίο θα μπορεί να αναγνωρίζει σκουπίδια σε ένα χώρο και να ειδοποιεί όταν ο αριθμός των σκουπιδιών είναι μεγάλος. Για αυτό το σκοπό θα χρησιμοποιήσουμε την πλατφόρμα maixduino (https://wiki.sipeed.com/en/maix/board/maixduino.html https://www.seeedstudio.com/Sipeed-Maixduino-Kit-for-RISC-V-AI-IoT-p-4047.html), η οποία μας παρέχει τη δυνατότητα να τρέχουμε εφαρμογές τεχνητής νοημοσύνης με χαμηλό κόστος (<40 ευρώ) για την βασική πλακέτα. Θα προσπαθήσουμε να εκπαιδεύσουμε το σύστημά μας να αναγνωρίζει σκουπίδια χρησιμοποιώντας τη πλατφόρμα tensorflow και στη συνέχεια να μεταφέρουμε το μοντέλο μας στην πλακέτα maixduino, χρησιμοποιώντας micropython. Η πλακέτα maixduino περιλαμβάνει κάμερα και TFT οθόνη που θα μας επιτρέψει να πάρουμε εικόνες από την περιοχή που έχει σκουπίδια, να βρούμε τα σκουπίδια και να τα απεικονίσουμε στην TFT οθόνη. Η πλατφόρμα μας θα μπορούσε να βρίσκεται και σε ένα απλό ρομποτικό όχημα που να κινείται για να κάνει ανίχνευση σκουπιδιών σε μια περιοχή.\nΛίστα Υλικών\n\n\n\nΑ/Α\nΥλικό\n\n\n\n\n1\nmaixduino board με οθόνη και κάμερα\n\n\n\n'], 'url_profile': 'https://github.com/CRPKalamata', 'info_list': ['Updated Jan 7, 2020', 'Python', 'Updated Jan 11, 2020', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 16, 2020', 'Updated Jan 6, 2020', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 27, 2020', 'Updated Jan 8, 2020', 'Python', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""\nCognitive Science, Artificial Intelligence and Education\nATTENTION:\nDue to the storm and the potential dangers associated with it, the workshop Minds and Machines is cancelled for today and will start on Tuesday 11 February 2020.\nPurpose of the Workshop\nLearning science as an interdisciplinary subject requires integrating distinct fields and skillsets. In particular, Cognitive Science, which studies and models the human mind, and Artificial Intelligence, which seeks to generate intelligent behavior in machines, share deep theoretical and practical concerns in the domains of education and learning which make interdisciplinary research that spans these two disciplines highly relevant. First, AI is more cognitive than appears at first glance. At the heart of the current AI revolution is a massive transfer of knowledge from humans to machines, in the form of learning from human-labeled and human-structured data. Creating and curating appropriate datasets for training AI systems requires a deep understanding of human-like knowledge representations and the subtleties of converting abstract human knowledge (e.g., what concept or skill a test question assesses) into a machine readable form. Second, AI systems most often have humans as users, as in the case of adaptive learning or assessment, requiring the AI system to maintain human interpretability. Interpretable AI requires the decisions, recommendations and advice delivered to provide sensible interpretations that can be understood by various stakeholders (such as educators, researchers or students), which imposes interesting constraints on learning methodologies for autonomous systems. Finally, Cognitive Science provides proof of concept demonstrations of learned behavior that provide next-generation targets for what AI might achieve. In this workshop we explore these themes through lectures, tutorials, and collaborative projects to enable students to participate in this exciting interdisciplinary research frontier.\nBy the end of this workshop, students will have gained both conceptual knowledge and practical experience in using advanced machine learning (ML) methods applied to educational settings, domains, and datasets. ML topics include: deep learning, reinforcement learning, and natural language processing; with applications to cognitive modeling and recommender systems in the educational domain.\nWhen and Where\n10 Feb 2020 - 14 Feb 2020, University of Luxembourg, Belval Campus. More details soon.\nMost of the workshop will take place in the Learning Hub 1.01 room, in the first floor of the Luxembourg Learning Centre.\nWorkshop Structure\nThe workshop will span 1 full week (5 days), fulltime.\nWe will interleave lectures, tutorials and team-project work throughout the day.\nAt the end of the workshop students present their project.\nInstructors (alphabetic):\n\nPedro Cardoso-Leite\nDominic Mussack\nSiwen Guo\nConstantin Rothkopf\nChristoph Schommer\nPaul Schrater\nMorteza Ansarinia (Teaching Assistant)\n\nStudents / Audience\nThe workshop is destined in priority for PhD students from the Unviersity of Luxembourg but is open to anyone for free. Students need to apply (send CV, current project description, recommendations) and are selected by the instructors.\nECTS\nPhD students from the UL may earn 2 ECTS if they\n\nregister on moodle (link will be provided soon)\nattend all lectures\ncomplete project\n\nOther people may participate to the workshop as well (e.g., Master students) but they won't be able to earn ECTS.\nPre-requisites\nSkills:\n\n\nProgramming: Participants musk know programming, ideally in Python, in order to complete the projects and earn ECTS. Participants without programming skills may still participate but might not earn ECTS.\n\n\nMath: Background knowledge in math (Linear Algebra, Probability theory) is desirable.\n\n\nEnglish: All courses will be taught in English.\n\n\nReading list/Preparation for the workshop:\n\nLinear Algebra and Learning from Data by Gilbert Strang\nEdu Data / Learning Analytics review papers (TBD)\nAI / CogSci review papers (TBD)\n\nSetup\nBring your own laptop. If you don't have a laptop you may borrow one from the Luxembourg Learning Centre.\nIn this workshop we will mostly use Python3 and Pytorch.\nWorkshop Program\nThe workshop will take place in\nDay 1:  General Intro\nCANCELLED\nDay 2: Introduction\n\n\n\nTime\nTopic\n\n\n\n\n09:00-09:30\nWelcoming remarks (Cardoso-Leite)\n\n\n09:30-10:30\nEducation Intro\t(Cardoso-Leite)\n\n\n10:30-11:30\nCogSci Intro (Schrater)\n\n\n11:30-12:15\nAI intro (Schommer)\n\n\n13:15-16:00\nAI Methods / Deep Learning intro (Mussack)\n\n\n16:00-18:30\nGroup work\n\n\n\nDay 3: Recommender Systems\n\n\n\nTime\nTopic\n\n\n\n\n08:00-10:30\nNLP & Sentiment Analysis in RecSys (Guo)\n\n\n10:30-12:30\nIRT, Deep Learning for structured data RecSys, Pytorch factorization recommender (Cardoso-Leite; Schrater)\n\n\n14:00-16:00\nNLP-Paper discussion (Schommer)\n\n\n16:00-18:00\nGroup work\n\n\n\nDay 4: Cognitive Modeling, RL\n\n\n\nTime\nTopic\n\n\n\n\n08:00-10:00\nRL intro (Schrater; Rothkopf)\n\n\n10:00-12:00\nTutorial\n\n\n13:00-14:00\nEcological behavior, looking:  RL analysis (Rothkopf)\n\n\n14:00-16:00\nReading and disucssions\n\n\n16:00-18:00\nGroup work\n\n\n\nDay 5: Project Presenations\n\n\n\nTime\nTopic\n\n\n\n\n08:00-10:00\nGroup presenations\n\n\n10:00-12:00\nRoundtable discussion\n\n\n14:00-18:00\nGroup work\n\n\n\nHow to apply\nParticipation is free but places are limited and will be filled on a continuous basis. Therefore, if you are interested, apply as soon as possible by sending an email with the following information:\n\na short cv (in particular explain if you have programming experience)\na short motivation letter (who are you? how would you benefit from this workshop?)\n\nApplications should be send by email to contact@xcit.org\nDeadline: 4 February 2020\n""], 'url_profile': 'https://github.com/xcit-lab', 'info_list': ['Updated Jan 7, 2020', 'Python', 'Updated Jan 11, 2020', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 16, 2020', 'Updated Jan 6, 2020', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 27, 2020', 'Updated Jan 8, 2020', 'Python', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '119 contributions\n        in the last year', 'description': ['Taipei-QA-Bot\nDeep Learning and Its Applications & Advanced Artificial Intelligence Final Project\n以Bert實作Taipei QA Bot資料集\n'], 'url_profile': 'https://github.com/seanbbear', 'info_list': ['Updated Jan 7, 2020', 'Python', 'Updated Jan 11, 2020', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 16, 2020', 'Updated Jan 6, 2020', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 27, 2020', 'Updated Jan 8, 2020', 'Python', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['SELENA\nA concept of operations for an autonomous artificial intelligence\nThese are the specs for an autonomous Artificial Intelligence. I am looking for people to join the project.\n'], 'url_profile': 'https://github.com/twilightravengames', 'info_list': ['Updated Jan 7, 2020', 'Python', 'Updated Jan 11, 2020', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 16, 2020', 'Updated Jan 6, 2020', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 27, 2020', 'Updated Jan 8, 2020', 'Python', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020']}","{'location': 'New York metropolitan area', 'stats_list': [], 'contributions': '223 contributions\n        in the last year', 'description': [""The following files are my programming assignments that I had to write as part of my 'Introduction Into Artificial Intelligence' class (CS370)\nThey use no external libraries (such as Tensorflow or PyTorch) and only require it to be ran on Python 3 (preferably Python 3.7.4)\n""], 'url_profile': 'https://github.com/digitalcircuits', 'info_list': ['Updated Jan 7, 2020', 'Python', 'Updated Jan 11, 2020', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 16, 2020', 'Updated Jan 6, 2020', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 27, 2020', 'Updated Jan 8, 2020', 'Python', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020']}","{'location': 'Turin, Italy', 'stats_list': [], 'contributions': '267 contributions\n        in the last year', 'description': ['ML-AI_HW3\nHomework 3 of 3 of Machine Learning and Artificial Intelligence @ Polito\n'], 'url_profile': 'https://github.com/Vaelthur', 'info_list': ['Updated Jan 7, 2020', 'Python', 'Updated Jan 11, 2020', 'Updated Jan 6, 2020', 'Python', 'Updated Jan 16, 2020', 'Updated Jan 6, 2020', 'Updated Feb 10, 2020', 'Python', 'Updated Feb 27, 2020', 'Updated Jan 8, 2020', 'Python', 'Updated Jan 6, 2020', 'Jupyter Notebook', 'Updated Jan 7, 2020']}"
"{'location': 'Rayadurg', 'stats_list': [], 'contributions': '117 contributions\n        in the last year', 'description': ['Batch No - B7\nStudent Informative Voice Based Chat Bot System\nProject Overview\n\nChat bot is an automation system and different formats of chat bots are command-line, graphical, web application, and voice based. Chat bots typically provide a text-based user interface, allowing the user to type commands and receive text as well as text to speech response. The functionality of a Chat bot works only on the existing commands. Chat bots  usually remember previous commands in order to provide functionality\n\nWhat Will I Learn?\n\nDevelop a RESTful web application using the Python framework Flask\nImplementing OAuth authentication.\nImplementing CRUD (create, read, update and delete) operations.\nHow to handle chatterbot module in python\n\nSkills used for this project\n\nNatural language processing tool kit\nPattern Matching\nPython –flask\nSQL alchemy\n\nPreRequisites\n\nPython ~3.7\nSQL alchemy\nChatterBot\n\nHow to Run\n\n\nInstall Python ~ 3.7\n\n\nClone this repo\n\n\nUnzip the file\n\n\ngo student_chatbot folder\n\n\ninstall all requirements using pip\n\n\n$ pip install -r requirements.txt\n\n\nCreate a database and tables\n\n$ Python database_setup.py\n\n\nPopulate the database with some initial data\n\n$ Python input_data.py\n\n\nLaunch application\n\n$ Python app.py\n\n\nOpen the browser and go to http://localhost:5000\n\n'], 'url_profile': 'https://github.com/satheesh22g', 'info_list': ['4', 'HTML', 'Updated Feb 4, 2021', 'HTML', 'Updated Jan 18, 2020', '3', 'Updated Jan 6, 2020', 'MIT license', 'Updated Jan 12, 2020', 'Java', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Updated Jan 11, 2020', 'PHP', 'Updated Apr 7, 2019', 'Python', 'MIT license', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 21, 2020']}","{'location': 'Montreal', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ndebray', 'info_list': ['4', 'HTML', 'Updated Feb 4, 2021', 'HTML', 'Updated Jan 18, 2020', '3', 'Updated Jan 6, 2020', 'MIT license', 'Updated Jan 12, 2020', 'Java', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Updated Jan 11, 2020', 'PHP', 'Updated Apr 7, 2019', 'Python', 'MIT license', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 21, 2020']}","{'location': 'Bangalore, India', 'stats_list': [], 'contributions': '11,676 contributions\n        in the last year', 'description': [""\nRanking Fraud Detection for Mobile Apps: A Holistic View\nSecuring the Deep Fraud Detector in Large-Scale E-Commerce Platform via Adversarial Machine Learning Approach\nGraph-Based User Behavior Modeling: From Prediction to Fraud Detection\nExposing Search and Advertisement Abuse Tactics and Infrastructure of Technical Support Scammers\nHiDDen: Hierarchical Dense Subgraph Detection with Application to Financial Fraud Detection\nREV2: Fraudulent User Prediction in Rating Platforms\nFraud Detection with Density Estimation Trees\nAI Technologies to Defeat Identity Theft Vulnerabilities\nA Machine-Learned Proactive Moderation System for Auction Fraud Detection\nRealtime Constrained Cycle Detection in Large Dynamic Graphs\nUsing Co-Visitation Networks For Detecting Large Scale Online Display Advertising Exchange Fraud\nCrowd Fraud Detection in Internet Advertising\nLarge Graph Mining: Patterns, Cascades, Fraud Detection, and Algorithms \nOnline Modeling of Proactive Moderation System for Auction Fraud Detection\nUsing Relational Knowledge Discovery to Prevent Securities Fraud\nNetProbe: A Fast and Scalable System for Fraud Detection in Online Auction Networks\nIdentifying Anomalies in Graph Streams Using Change Detection\nDetecting Fraud in Health Insurance Data: Learning to Model Incomplete Benford's Law Distributions\nKey Player Identification in Underground Forums over Attributed Heterogeneous Information Network Embedding Framework\nFraud Detection by Generating Positive Samples for Classification from\nUnlabeled Data\nPD-FDS: Purchase Density based Online Credit Card Fraud Detection System\nImproving Card Fraud Detection through Suspicious Pattern Discovery\nA graph-based, semi-supervised, credit card fraud detection system\nA Pattern Discovery Approach to Retail Fraud Detection\nFRAUDAR: Bounding Graph Fraud in the Face of Camouflage\nFARE: Schema-Agnostic Anomaly Detection in Social Event Logs\nImproving Credit Card Fraud Detection with Calibrated Probabilities\nCredit Card Fraud Detection Using Meta-Learning: Issues and Initial Results\nRobust System for Identifying Procurement Fraud\nBIRDNEST: Bayesian Inference for Ratings-Fraud Detection\nDocument Classification and Visualisation to Support the Investigation of Suspected Fraud\nDetecting Fraudulent Personalities in Networks of Online Auctioneers\nFrauDetector: A Graph-Mining-based Framework for Fraudulent Phone Call Detection \nToward Scalable Learning with Non-uniform Class and Cost Distributions: A Case Study in Credit Card Fraud Detection\nToward An Intelligent Agent for Fraud Detection — The CFE Agent\nUncovering Download Fraud Activities in Mobile App Markets\nNo Place to Hide: Catching Fraudulent Entities in Tensors\nDetection of money laundering groups using supervised learning in networks\nCall-based Fraud Detection in Mobile Communication Networks using a Hierarchical Regime-Switching Model\nImpression Allocation for Combating Fraud in E-commerce Via Deep\nReinforcement Learning with Action Norm Penalty\nOnline Reputation Fraud Campaign Detection in User Ratings\nCatch the Black Sheep: Unified Framework for Shilling Attack Detection\nBased on Fraudulent Action Propagation\nUtility-Based Fraud Detection\nGraph Analysis for Detecting Fraud, Waste, and Abuse in Healthcare Data\nAnomaly, Event, and Fraud Detection in Large Network Datasets\nFraudulent Support Telephone Number Identification Based on Co-occurrence Information on theWeb\n\n""], 'url_profile': 'https://github.com/manjunath5496', 'info_list': ['4', 'HTML', 'Updated Feb 4, 2021', 'HTML', 'Updated Jan 18, 2020', '3', 'Updated Jan 6, 2020', 'MIT license', 'Updated Jan 12, 2020', 'Java', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Updated Jan 11, 2020', 'PHP', 'Updated Apr 7, 2019', 'Python', 'MIT license', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['KI_UniFrankfurt\nFiles for Course ""Artificial Intelligence (KI-b)"" at Goethe Uni Frankfurt\n'], 'url_profile': 'https://github.com/anselm-ug', 'info_list': ['4', 'HTML', 'Updated Feb 4, 2021', 'HTML', 'Updated Jan 18, 2020', '3', 'Updated Jan 6, 2020', 'MIT license', 'Updated Jan 12, 2020', 'Java', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Updated Jan 11, 2020', 'PHP', 'Updated Apr 7, 2019', 'Python', 'MIT license', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 21, 2020']}","{'location': 'Bologna, Italy', 'stats_list': [], 'contributions': '212 contributions\n        in the last year', 'description': [""A.I.DA\nProject work for Foundations of Artificial Intelligence - MSc Computer Engineering (University of Bologna)\nNamed after Giuseppe Verdi's opera, A.I.DA is a Java-based genetic algorithm that produces monophonic music.\nAmong other parameters, the user can choose the length of the samples, the mapping from genotype to phenotype by means of a scale (i.e. C Major, etc.), and the note range.\nThe best results were obtained with a Pentatonic C Major scale, 16 notes per sample, and 6 as the note range.\nIn /report there is a PowerPoint with audio samples, an explanation of what happens in A.I.DA, the UML diagram and the references. There is also a PDF version of the presentation.\nMore results can be found in /results.\nThis work has been inspired by Prof. John Biles, whose work on music can be found here.\nThis paper in particular has been very useful: Biles, John. (1994). GenJam: A Genetic Algorithm for Generating Jazz Solos.\n""], 'url_profile': 'https://github.com/ilceltico', 'info_list': ['4', 'HTML', 'Updated Feb 4, 2021', 'HTML', 'Updated Jan 18, 2020', '3', 'Updated Jan 6, 2020', 'MIT license', 'Updated Jan 12, 2020', 'Java', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Updated Jan 11, 2020', 'PHP', 'Updated Apr 7, 2019', 'Python', 'MIT license', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '177 contributions\n        in the last year', 'description': ['10316-materials-design-ai\nDTU Course 10316 - Material design with machine learning and artificial intelligence\n'], 'url_profile': 'https://github.com/Strandgaard96', 'info_list': ['4', 'HTML', 'Updated Feb 4, 2021', 'HTML', 'Updated Jan 18, 2020', '3', 'Updated Jan 6, 2020', 'MIT license', 'Updated Jan 12, 2020', 'Java', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Updated Jan 11, 2020', 'PHP', 'Updated Apr 7, 2019', 'Python', 'MIT license', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['RuleBasedQuestionGeneration\nGeneration of questions based on given context. No artificial intelligence, just rules.\n'], 'url_profile': 'https://github.com/dimitrije-zucc', 'info_list': ['4', 'HTML', 'Updated Feb 4, 2021', 'HTML', 'Updated Jan 18, 2020', '3', 'Updated Jan 6, 2020', 'MIT license', 'Updated Jan 12, 2020', 'Java', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Updated Jan 11, 2020', 'PHP', 'Updated Apr 7, 2019', 'Python', 'MIT license', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 21, 2020']}","{'location': 'Depok', 'stats_list': [], 'contributions': '159 contributions\n        in the last year', 'description': ['ai-kepribadian\nArtificial intelligence (AI) berbasis web PHP untuk memperkirakan kepribadian pengguna berdasarkan survey.\n'], 'url_profile': 'https://github.com/VClude', 'info_list': ['4', 'HTML', 'Updated Feb 4, 2021', 'HTML', 'Updated Jan 18, 2020', '3', 'Updated Jan 6, 2020', 'MIT license', 'Updated Jan 12, 2020', 'Java', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Updated Jan 11, 2020', 'PHP', 'Updated Apr 7, 2019', 'Python', 'MIT license', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': ['Programmentwurf Sternensammler\nA* Suchverfahren\nZiel\nEs soll ein Programm erstellt werden, welches ein Labyrinth aus gegebenen Informationen aus verschiedenen csv Dateien generiert und dies dann anhand einer gegebener Start und Ziel Koordinate  sowie Energiemenge löst.  Der A-Stern Algorithmus soll hierzu verwendet werden.\nParameter und Variablen\nHier werden die wichtigsten Parameter und deren Sinn erklärt.\nLabyrinth: 2 Dimensionales Array der Größe 10 mal 10 indem der Inhalt des Labyrinth gespeichert wird. Alle Felder des Arrays werden bei Initialisierung auf 0 für Leer gesetzt. Das Labyrinth besitzt aber auch  wie Sterne und Energie die eingesammelt werden können und Mauern welches die Bewegung einschränken\nMauern: Mauern werden zwischen 2 Felder gesetzt und verhindern das man von dem einen Feld auf  das andere kommt. Die Position der Mauern im Labyrinth wird aus einer csv Datei eingelesen.\nSterne: Sterne werden auf Felder gesetzt und geben 2 Punkte wenn sie durch das betreten eines Feldes eingesammelt werden. Die Position der Sterne im Labyrinth wird aus einer csv Datei eingelesen.\nEnergie: Energie wird auf Felder gesetzt und gibt 5 Energiepunkte wenn sie durch das betreten eines Feldes eingesammelt wird. Wenn die Gesamtenergie aufgebraucht ist und das Ziel noch nicht erreicht so ist das Programm gescheitert. Die Position der Energie im Labyrinth wird aus einer csv Datei eingelesen. Die Anfangsenergie wird im CLI Betrieb al Parameter übergeben oder bei der GUI  in einem Feld eingegeben.\nStart und Ziel Koordinate: Start und Ziel Koordinaten werden im CLI Betrieb als Parameter übergeben. Im GUI Betrieb in einem Feld eingegeben. Die Eingabe ist in beiden Fällen ein String welches aus x und y Koordinate besteht getrennt von einem Komma bsp. „3, 2“ Koordinate x = 3 y = 2.\nFeldererklärung\n\n0 = Leeres Feld\n1 = Feld mit angrenzender Mauer\n2 = Feld mit Energie\n3 = Feld mit Energie und angrenzender Mauer\n4 = Feld mit Stern\n5 = Feld mit Stern und angrenzender Mauer\n\nEs ist nicht möglich von einem Feld mit angrenzender  Mauer (1) auf ein anderes Feld mit angrenzender Mauer zu gehen (1, 3, 5).   Wenn man ein Feld mit Energy (2, 3) oder Stern (4, 5) betritt so wird dies um den Standardwert des Feldes reduziert (-2 bei Energie oder -4 bei Stern) dadurch wird diese Feld dann zu einem Leeren Felod oder einem mit Mauer (0, 1) und der Stern oder die Energie wird eingesammelt\nUmsetzung\nDas Programm wurde als Python Script implementiert welches in 2 verschiedenen Modi verwendet werden kann. Eine Command Line Interface (CLI) welches das Ergebnis für ein Pfad durch das  Labyrinth liefert. Sowie eine Grafische Benutzeroberfläche wo man mehrere verschiedene Labyrinthe mit Unterschiedlichen Start und Ziel Koordinaten laufen lassen kann.\nDie Ausgabe ist in beiden Versionen der Pfad, die anzahl Gesammelter Sterne Mal 2 genommen, die übrig gebiebene Energiemenge und ob man es bis zum Ziel geschaft hatt mit der Energie.\nFür die umsetztung wurden einige Biblotheken von Python verwendet. Diese Sind zum verwenden von der Anwendung nötig.\n\nPandas: Bibliothek für die Verwaltung von Daten\nNumpy: Bibliothek für den Umgang mit Vektoren und Matrizen\nTkinter:  Bibliothek um GUIs in Python zu Erstellen\nSys\nArgparse:  Bibliothek um die CLI interface zu erstellen\n\nVerwendung mit Beispiele\nCommand Line Interface (CLI)\nDie CLI bestitzt 2 Hauptoptionen man kann die Hilfe aufrufen oder die Eigenliche Anwendung Verwenden. Die Hilfe wird mit dem nachfolgendem Befehl aufgerufen.\nfoo@bar:~/Astart-DHBW-KI$ python3 A-Star-Search.py -h\nusage: A-Star-Search.py [-h] [-s START] [-g GOAL] [-se STARTENERGY]\n                          [-w WALLS] [-st STARS] [-e ENERGY]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -s START, --start START\n                        Startvalue as: ""x,y""\n  -g GOAL, --goal GOAL  Goalvalue as: ""x,y""\n  -se STARTENERGY, --startenergy STARTENERGY\n                        Energy as number\n  -w WALLS, --walls WALLS\n                        Path to wall csv\n  -st STARS, --stars STARS\n                        Path to star csv\n  -e ENERGY, --energy ENERGY\n                        Path to energy csv\n\nHier sieht man auch die Parameter die Benötigt werden um die Anwendung im CLI Betrieb auszuführen. Es werden alle Parameter Benötigt sowie im folgendem Beispiel gezeigt wird. Wenn nicht alle Parameter Gegeben sind wird die GUI Version aufgerufen.\nfoo@bar:~/Astart-DHBW-KI$ python3 A-Star-Search.py -s ""0,0"" -g ""9,9"" -se 15 -w ""CSV-Data/S_A01_Mauer.csv"" -st ""CSV-Data/S_A01_Stern.csv"" -e ""CSV-Data/S_A01_Energie.csv""\nStart CMD\n### Enter start as: x,y | default 0, 0\n### Enter goal as: x,y | default 9, 9\n### Enter energy as number default 5\n15\n### 1) Generate Clean Maze ###\n[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n### 2) Get Wall data from csv if empty uses example csv ###\n### 3) Set Wall Data in maze ###\n### 4) Get Energy data from csv if empty uses example csv###\n### 5) Set Star Data in maze ###\n### 6) Get Star data from csv if empty uses example csv###\n### 7) Set Star Data in maze ###\n[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 4. 0. 0. 0. 3. 1. 0. 0.]\n [0. 1. 1. 1. 0. 4. 1. 1. 0. 0.]\n [0. 1. 1. 1. 0. 0. 0. 0. 4. 0.]\n [0. 0. 4. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 4. 0. 0. 0.]\n [0. 0. 0. 4. 0. 0. 1. 1. 0. 0.]\n [0. 0. 0. 0. 0. 0. 1. 1. 0. 4.]\n [0. 0. 2. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 4. 0. 0. 0. 0. 0.]]\nstar\nstar\n### 8) A* was successful ###\n### 9) Print Path ###\n[(0, 0), (1, 1), (1, 2), (1, 3), (2, 4), (3, 5), (4, 6), (5, 7), (6, 8), (7, 9), (8, 9), (9, 9)]\n### 10) Print energy ###\n3\n### 11) print stars ###\n4\n\nProcess finished with exit code 0\n\nGraphical User Interface (GUI)\nUm die Grafische Benutzeroberfläche aufzurufen wird der Script ohne Parameter ausgeführt.\nfoo@bar:~/Astart-DHBW-KI$ python3 A-Star-Search.py \nDann können die Start und Zielwerte sowie Anfangsenergie in der GUI angegeben werden und die Dateien für die Sterne, Energie und Mauern können über den Dateibrowser ausgewählt werden.\n\n'], 'url_profile': 'https://github.com/skywalkeretw', 'info_list': ['4', 'HTML', 'Updated Feb 4, 2021', 'HTML', 'Updated Jan 18, 2020', '3', 'Updated Jan 6, 2020', 'MIT license', 'Updated Jan 12, 2020', 'Java', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Updated Jan 11, 2020', 'PHP', 'Updated Apr 7, 2019', 'Python', 'MIT license', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 21, 2020']}","{'location': 'Kongens Lyngby', 'stats_list': [], 'contributions': '68 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/augustsemrau', 'info_list': ['4', 'HTML', 'Updated Feb 4, 2021', 'HTML', 'Updated Jan 18, 2020', '3', 'Updated Jan 6, 2020', 'MIT license', 'Updated Jan 12, 2020', 'Java', 'Updated Jan 27, 2020', '1', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Updated Jan 11, 2020', 'PHP', 'Updated Apr 7, 2019', 'Python', 'MIT license', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 21, 2020']}"
"{'location': 'The Internet', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/MrDataScience', 'info_list': ['Python', 'MIT license', 'Updated Jan 16, 2020', 'R', 'Updated Jan 21, 2020', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 10, 2020', 'C#', 'Updated Apr 17, 2020', '1', 'Updated Jan 6, 2020', 'Swift', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Updated Jan 12, 2020', '1', 'Python', 'Updated Jan 10, 2020']}","{'location': 'Kongens Lyngby', 'stats_list': [], 'contributions': '68 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/augustsemrau', 'info_list': ['Python', 'MIT license', 'Updated Jan 16, 2020', 'R', 'Updated Jan 21, 2020', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 10, 2020', 'C#', 'Updated Apr 17, 2020', '1', 'Updated Jan 6, 2020', 'Swift', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Updated Jan 12, 2020', '1', 'Python', 'Updated Jan 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '46 contributions\n        in the last year', 'description': ['Information-Theoretic XAI\nThis repository contains links to publications on information-theoretic methods which could be used for Explainable Artificial Intelligence (XAI).\nVariational Information Bottleneck\n\nKolchinsky, Artemy, Brendan D. Tracey, and Steven Van Kuyk. ""Caveats for information bottleneck in deterministic scenarios."" (ICLR 2019) [PDF]\nSaxe, Andrew M., Yamini Bansal, Joel Dapello, Madhu Advani, Artemy Kolchinsky, Brendan D. Tracey, and David D. Cox. ""On the information bottleneck theory of deep learning."" (ICLR 2019) [PDF]\nBang, Seojin, Pengtao Xie, Wei Wu, and Eric Xing. ""Explaining a black-box using Deep Variational Information Bottleneck Approach."" arXiv preprint arXiv:1902.06918 (2019) [PDF]\nLi, Xiang Lisa, and Jason Eisner. ""Specializing Word Embeddings (for Parsing) by Information Bottleneck."" (EMNLP 2019) [PDF]\nDai, Bin, Chen Zhu, and David Wipf. ""Compressing neural networks using the variational information bottleneck."" Proceedings of the 35th International Conference on Machine Learning (ICML 2018) [PDF]\nAlemi, Alexander A., Ian Fischer, and Joshua V. Dillon. ""Uncertainty in the variational information bottleneck."" arXiv preprint arXiv:1807.00906 (2018) [PDF]\nAlemi, Alexander A., Ian Fischer, Joshua V. Dillon, and Kevin Murphy. ""Deep variational information bottleneck."" (ICLR 2017) [PDF]\n\nMutual Information Maximization\n\nKong, Lingpeng, Cyprien de Masson d\'Autume, Wang Ling, Lei Yu, Zihang Dai, and Dani Yogatama. ""A Mutual Information Maximization Perspective of Language Representation Learning."" (ICLR 2020) [PDF]\nSong, Jiaming, and Stefano Ermon. ""Understanding the Limitations of Variational Mutual Information Estimators."" (ICLR 2020) [PDF]\nPoole, Ben, Sherjil Ozair, Aaron Van Den Oord, Alex Alemi, and George Tucker. ""On Variational Bounds of Mutual Information."" In the 36th International Conference on Machine Learning, pp. 5171-5180. (ICML 2019) [PDF]\nTschannen, Michael, Josip Djolonga, Paul K. Rubenstein, Sylvain Gelly, and Mario Lucic. ""On mutual information maximization for representation learning."" arXiv preprint arXiv:1907.13625 (2019) [PDF]\nGabrié, Marylou, Andre Manoel, Clément Luneau, Nicolas Macris, Florent Krzakala, and Lenka Zdeborová. ""Entropy and mutual information in models of deep neural networks."" In Advances in Neural Information Processing Systems, pp. 1821-1831. (NIPS 2018) [PDF]\n\n'], 'url_profile': 'https://github.com/nuaaxc', 'info_list': ['Python', 'MIT license', 'Updated Jan 16, 2020', 'R', 'Updated Jan 21, 2020', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 10, 2020', 'C#', 'Updated Apr 17, 2020', '1', 'Updated Jan 6, 2020', 'Swift', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Updated Jan 12, 2020', '1', 'Python', 'Updated Jan 10, 2020']}","{'location': 'Montréal, QC, Canada', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['Assignments  for ""INF8215: Methods and Algorithms for Artificial Intelligence""\nDescription\nThese are the 3 assignments for the INF8215: Methods and Algorithms for Artificial Intelligence, taught by Prof. DANIEL ALOISE. The content of this course is very similar and inspired from the content of the CS 188: Introduction to Artificial Intelligence course at UC Berkeley.\nThis is a joint work with Théo Moins and Amine Bellahsen.\nAssignment 1 : search methods [Python]\nImplemented methods: search trees, A* search algorithm and Variable Neighborhood Search (VNS).\nStatement in English\nSolution in French.\nAssignment 2: Prolog, logic programming and constraint programming [Prolog&MiniZinc]\nUsing constraint programming to solve two combinatorial problems and logic programming to extract information.\nStatement in English\nSolution:\nExercise 1\nExercise 2\nExercise 3\nExercise 4\nReport in French.\nAssignment 3: Machine Learning [Python]\nImplementation of softmax regression, data preprocessing and InclassKaggle challenge.\nStatement in English\nSolution in French.\n'], 'url_profile': 'https://github.com/Sanaelotfi', 'info_list': ['Python', 'MIT license', 'Updated Jan 16, 2020', 'R', 'Updated Jan 21, 2020', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 10, 2020', 'C#', 'Updated Apr 17, 2020', '1', 'Updated Jan 6, 2020', 'Swift', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Updated Jan 12, 2020', '1', 'Python', 'Updated Jan 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/liamshawcarleton', 'info_list': ['Python', 'MIT license', 'Updated Jan 16, 2020', 'R', 'Updated Jan 21, 2020', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 10, 2020', 'C#', 'Updated Apr 17, 2020', '1', 'Updated Jan 6, 2020', 'Swift', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Updated Jan 12, 2020', '1', 'Python', 'Updated Jan 10, 2020']}","{'location': 'Bangalore, India', 'stats_list': [], 'contributions': '11,676 contributions\n        in the last year', 'description': [""\nMerge Sort Algorithm\nA Concurrency-Optimal Binary Search Tree\nLRC: Dependency-Aware Cache Management for Data Analytics Clusters\nLLVM: A Compilation Framework for Lifelong Program Analysis and Transformation\nSCTP in Go\nThe Little Manual of API Design\nNo Silver Bullet − Essence and Accident in Software Engineering\nCalvin: Fast Distributed Transactions for Partitioned Database Systems\nConcurrent Data Structures\nUnderstanding Real-World Concurrency Bugs in Go\nAn Approach to Improve the Performance of Insertion Sort Algorithm\nAn Overview of Approaches Used In Focused Crawlers\nSmartcrawler: a two-stage crawler for efficient search result \nAn O(1) algorithm for implementing the LFU cache eviction scheme\nLarge Scale Unit Testing for Go Programming Language Packages\nProceedings of FAST' 03: 2nd USENIX Conference on File and Storage Technologies\nAmazon Aurora: Design Considerations for High Throughput Cloud-Native Relational Databases\nView-Centric Performance Optimization for Database-Backed Web Applications\nThe Science of Software and System Design\nSkip Lists: A Probabilistic Alternative to Balanced Trees\nProgramming Paradigms for Dummies: What Every Programmer Should Know\nGoogle's DeepWeb Crawl\n\n""], 'url_profile': 'https://github.com/manjunath5496', 'info_list': ['Python', 'MIT license', 'Updated Jan 16, 2020', 'R', 'Updated Jan 21, 2020', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 10, 2020', 'C#', 'Updated Apr 17, 2020', '1', 'Updated Jan 6, 2020', 'Swift', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Updated Jan 12, 2020', '1', 'Python', 'Updated Jan 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/guillemgbt', 'info_list': ['Python', 'MIT license', 'Updated Jan 16, 2020', 'R', 'Updated Jan 21, 2020', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 10, 2020', 'C#', 'Updated Apr 17, 2020', '1', 'Updated Jan 6, 2020', 'Swift', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Updated Jan 12, 2020', '1', 'Python', 'Updated Jan 10, 2020']}","{'location': 'Singapore', 'stats_list': [], 'contributions': '453 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/DouMaokang', 'info_list': ['Python', 'MIT license', 'Updated Jan 16, 2020', 'R', 'Updated Jan 21, 2020', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 10, 2020', 'C#', 'Updated Apr 17, 2020', '1', 'Updated Jan 6, 2020', 'Swift', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Updated Jan 12, 2020', '1', 'Python', 'Updated Jan 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Stephoni_Lee\nA.A - Accountancy/Accounting, A.S-T - Business Administration, B.S - Biochemistry/Biomedical Engineering (IP), Data Science/Machine Learning/Artificial Intelligence at Lamda School (2020)\n'], 'url_profile': 'https://github.com/stephonilee19', 'info_list': ['Python', 'MIT license', 'Updated Jan 16, 2020', 'R', 'Updated Jan 21, 2020', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 10, 2020', 'C#', 'Updated Apr 17, 2020', '1', 'Updated Jan 6, 2020', 'Swift', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Updated Jan 12, 2020', '1', 'Python', 'Updated Jan 10, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['AI Projects\nA few Artificial Intelligence based assignments completed during the first semester of my senior year. Class was taugh by Professor Richard Burns, burns@wcupa.edu\n'], 'url_profile': 'https://github.com/ryanowings', 'info_list': ['Python', 'MIT license', 'Updated Jan 16, 2020', 'R', 'Updated Jan 21, 2020', 'Updated Jan 12, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 10, 2020', 'C#', 'Updated Apr 17, 2020', '1', 'Updated Jan 6, 2020', 'Swift', 'Updated Feb 14, 2020', 'Jupyter Notebook', 'Updated Jan 9, 2020', 'Updated Jan 12, 2020', '1', 'Python', 'Updated Jan 10, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Iroh-1', 'info_list': ['Updated Jan 7, 2020', 'Updated Jan 9, 2020', 'Python', 'Updated Aug 31, 2020', 'C', 'GPL-3.0 license', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'HTML', 'GPL-3.0 license', 'Updated Jan 15, 2020', 'Python', 'MIT license', 'Updated Jan 9, 2020', '1', 'Objective-C', 'BSD-3-Clause license', 'Updated Nov 24, 2020', 'Python', 'Updated Jan 7, 2020', 'Shell', 'GPL-3.0 license', 'Updated Jan 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/alexisarekion', 'info_list': ['Updated Jan 7, 2020', 'Updated Jan 9, 2020', 'Python', 'Updated Aug 31, 2020', 'C', 'GPL-3.0 license', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'HTML', 'GPL-3.0 license', 'Updated Jan 15, 2020', 'Python', 'MIT license', 'Updated Jan 9, 2020', '1', 'Objective-C', 'BSD-3-Clause license', 'Updated Nov 24, 2020', 'Python', 'Updated Jan 7, 2020', 'Shell', 'GPL-3.0 license', 'Updated Jan 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '135 contributions\n        in the last year', 'description': ['Pathfinder\nDescription:\nPathfinder is a python visualizer program that shows how the shortest path algorithm finds the appropriate route given a starting and ending point in the grid. Users can also draw boundaries on the grid and the algorithm will adapt to it accordingly. In each step, the algorithm looks at four postions from the current position: top, bottom, right and left to find the shortest path. Python Pygame is used to build this application.\nDemo:\n\nCredits\nText input module imported from: https://github.com/Nearoo/pygame-text-input\nContributor(s):\nMd. Abrar Labib: https://github.com/abrar-labib\n'], 'url_profile': 'https://github.com/Saquibirtiza', 'info_list': ['Updated Jan 7, 2020', 'Updated Jan 9, 2020', 'Python', 'Updated Aug 31, 2020', 'C', 'GPL-3.0 license', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'HTML', 'GPL-3.0 license', 'Updated Jan 15, 2020', 'Python', 'MIT license', 'Updated Jan 9, 2020', '1', 'Objective-C', 'BSD-3-Clause license', 'Updated Nov 24, 2020', 'Python', 'Updated Jan 7, 2020', 'Shell', 'GPL-3.0 license', 'Updated Jan 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['8puzzle\nSolving the 8puzzle game using BFS and A* search algorithm and compare their effectivenes. Group project for Artificial Intelligence, UOWM Semester 2019-2020\n'], 'url_profile': 'https://github.com/katerinatheo', 'info_list': ['Updated Jan 7, 2020', 'Updated Jan 9, 2020', 'Python', 'Updated Aug 31, 2020', 'C', 'GPL-3.0 license', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'HTML', 'GPL-3.0 license', 'Updated Jan 15, 2020', 'Python', 'MIT license', 'Updated Jan 9, 2020', '1', 'Objective-C', 'BSD-3-Clause license', 'Updated Nov 24, 2020', 'Python', 'Updated Jan 7, 2020', 'Shell', 'GPL-3.0 license', 'Updated Jan 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/jmstomber', 'info_list': ['Updated Jan 7, 2020', 'Updated Jan 9, 2020', 'Python', 'Updated Aug 31, 2020', 'C', 'GPL-3.0 license', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'HTML', 'GPL-3.0 license', 'Updated Jan 15, 2020', 'Python', 'MIT license', 'Updated Jan 9, 2020', '1', 'Objective-C', 'BSD-3-Clause license', 'Updated Nov 24, 2020', 'Python', 'Updated Jan 7, 2020', 'Shell', 'GPL-3.0 license', 'Updated Jan 9, 2020']}","{'location': 'Ottawa, Ontario, Canada', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['ARIA\nA prototype for granting agencies to be able to use Artificial Intelligence and Machine Learning to pre-score applications based on set program eligibility criteria.\nNot Production, Experimental Only\nBuilt as a project for the Canadian School of Public Service, Digital Academy (Cohort II)\nDemo at https://xerotalent.github.io/ARIA/\n'], 'url_profile': 'https://github.com/xerotalent', 'info_list': ['Updated Jan 7, 2020', 'Updated Jan 9, 2020', 'Python', 'Updated Aug 31, 2020', 'C', 'GPL-3.0 license', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'HTML', 'GPL-3.0 license', 'Updated Jan 15, 2020', 'Python', 'MIT license', 'Updated Jan 9, 2020', '1', 'Objective-C', 'BSD-3-Clause license', 'Updated Nov 24, 2020', 'Python', 'Updated Jan 7, 2020', 'Shell', 'GPL-3.0 license', 'Updated Jan 9, 2020']}","{'location': 'Mumbai, India', 'stats_list': [], 'contributions': '325 contributions\n        in the last year', 'description': ['Reinforcement learning on Breakout and Pong\nCourse Project for AI-ML(CS-337), Autumn 2019, IIT Bombay.\nThis project explores the following reinforcement learning algorithms and performs a comparative analysis on the game Breakout and Pong:\n\nDeep Q-learning\nAdvantage Actor-Critic Learning\nProximal policy optimization\nTrust Region Policy Optimization\n\nPlease visit https://github.com/ajd12342/exploring-reinforcement-learning/tree/master/Pong/videos and https://github.com/ajd12342/exploring-reinforcement-learning/tree/master/Breakout/videos for demo videos of the AI playing Breakout and Pong after training is complete.\n'], 'url_profile': 'https://github.com/ajd12342', 'info_list': ['Updated Jan 7, 2020', 'Updated Jan 9, 2020', 'Python', 'Updated Aug 31, 2020', 'C', 'GPL-3.0 license', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'HTML', 'GPL-3.0 license', 'Updated Jan 15, 2020', 'Python', 'MIT license', 'Updated Jan 9, 2020', '1', 'Objective-C', 'BSD-3-Clause license', 'Updated Nov 24, 2020', 'Python', 'Updated Jan 7, 2020', 'Shell', 'GPL-3.0 license', 'Updated Jan 9, 2020']}","{'location': 'Japan', 'stats_list': [], 'contributions': '77 contributions\n        in the last year', 'description': ['A.I.Segmentation version 1.4.1\nThe Xcode project of A.I.Segmentation version 1.4.1.\n\nYou can download the latest A.I.Segmentation from HERE. (The source code is not public)\nNOTE: This is the source code of the old version, known bugs that were unknown at its release are left unchanged.\n\n\n'], 'url_profile': 'https://github.com/tkshirakawa', 'info_list': ['Updated Jan 7, 2020', 'Updated Jan 9, 2020', 'Python', 'Updated Aug 31, 2020', 'C', 'GPL-3.0 license', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'HTML', 'GPL-3.0 license', 'Updated Jan 15, 2020', 'Python', 'MIT license', 'Updated Jan 9, 2020', '1', 'Objective-C', 'BSD-3-Clause license', 'Updated Nov 24, 2020', 'Python', 'Updated Jan 7, 2020', 'Shell', 'GPL-3.0 license', 'Updated Jan 9, 2020']}","{'location': 'Colorado', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['Othello-AI\nWorked with a classmate to develop an Othello AI for our final project for the Artificial Intelligence course. We utilized the Monte Carlo search tree algorithm to accomplish this. The AI was programmed in Python.\n'], 'url_profile': 'https://github.com/MiguelAnGuerrero', 'info_list': ['Updated Jan 7, 2020', 'Updated Jan 9, 2020', 'Python', 'Updated Aug 31, 2020', 'C', 'GPL-3.0 license', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'HTML', 'GPL-3.0 license', 'Updated Jan 15, 2020', 'Python', 'MIT license', 'Updated Jan 9, 2020', '1', 'Objective-C', 'BSD-3-Clause license', 'Updated Nov 24, 2020', 'Python', 'Updated Jan 7, 2020', 'Shell', 'GPL-3.0 license', 'Updated Jan 9, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['AL_F\nSystem ready, alienated Artificial Intelligence command center. The language syntax is similar to C++, although it is a custom language operating as a interpretation mechanism for a highly complex artificial intelligence from a supposed planetary constellation approximated 50 light years from our solar system. To understand the systematic representation of the core integration of the code is interpretative to the command sequence of plain-text to computer Kernel Grade Assembly debugging of changes to the computer system. The code it self is not compatible with current system, yet is achieved change in the registers behind the scenes of the Graphical User Interface, as it is with in the Verbose consistence of the main frame of a computer record.  Each command starts with AL_FIN as follows (WHAT2FIND)::PLAINTEXT::CLASSINHEIRANCE; So a command such as a desire to inject a CMD prompt into a foreign computer system as follows  AL_FIN(WORKSTATIONNAME)::INJECT@(cmd.exe) -^ LOCALIZEDHOST#[%HOST%];  So as presented this command is suppose to find the workstation which is the name of the computer system than as follows the host is into a localized host pool that is part of GLOBALHOST domain.\n'], 'url_profile': 'https://github.com/DragonStar666', 'info_list': ['Updated Jan 7, 2020', 'Updated Jan 9, 2020', 'Python', 'Updated Aug 31, 2020', 'C', 'GPL-3.0 license', 'Updated Jan 5, 2020', 'Jupyter Notebook', 'Updated Feb 7, 2020', 'HTML', 'GPL-3.0 license', 'Updated Jan 15, 2020', 'Python', 'MIT license', 'Updated Jan 9, 2020', '1', 'Objective-C', 'BSD-3-Clause license', 'Updated Nov 24, 2020', 'Python', 'Updated Jan 7, 2020', 'Shell', 'GPL-3.0 license', 'Updated Jan 9, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Teach-a-Neural-Network-to-Read-Handwriting\nNeural networks and deep learning are two success stories in modern artificial intelligence. They’ve led to major advances in image recognition, automatic text generation, and even in self-driving cars. To get involved with this exciting field, you should start with a manageable dataset. The MNIST Handwritten Digit Classification Challenge is the classic entry point. Image data is generally harder to work with than “flat” relational data. The MNIST data is beginner-friendly and is small enough to fit on one computer. Handwriting recognition will challenge you, but it doesn’t need high computational power. Build a neural network from scratch that solves the MNIST challenge with high accuracy. Data Sources MNIST (http://yann.lecun.com/exdb/mnist/) – MNIST is a modified subset of two datasets collected by the U.S. National Institute of Standards and Technology. It contains 70,000 labeled images of handwritten digits.\n'], 'url_profile': 'https://github.com/Adwayt-APN', 'info_list': ['1', 'Jupyter Notebook', 'Updated Jan 20, 2020']}",,,,,,,,,
"{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""The Applied Artificial Intelligence Workshop\n\n\n\n\nThis is the repository for The Applied Artificial Intelligence Workshop, published by Packt. It contains all the supporting project files necessary to work through the course from start to finish.\nRequirements and Setup\n\nTo get started with the project files, you'll need to:\n\nSetup The Jupyter Notebook\n\nAbout The Applied Artificial Intelligence Workshop\nThe Applied Artificial Intelligence Workshop teaches you the ins and outs of machine learning and neural networks from the ground up, using real-world examples. You'll learn to develop AI and ML models using Python, starting with using the minmax algorithm and alpha-beta pruning to create your first game, and ending with classifying images using neural networks.\nWhat you will learn\n\nCreate your first AI game in Python with the minimax algorithm\nImplement regression techniques to simplify real-world data\nExperiment with classification techniques to label real-world data\nPerform predictive analysis in Python using decision trees and random forests\nUse clustering algorithms to group data without manual support\nUnderstand how to use neural networks to process and classify labeled images\n\nRelated Workshops\nIf you've found this repository useful, you might want to check out some of our other workshop titles:\n\nThe Natural Language Processing Workshop\nThe Computer Vision Workshop\nThe Machine Learning Workshop\n\n""], 'url_profile': 'https://github.com/PacktWorkshops', 'info_list': ['5', 'Jupyter Notebook', 'MIT license', 'Updated Jan 20, 2021', '17', 'Smalltalk', 'Updated Jul 11, 2020', '10', 'HTML', 'Updated Jan 16, 2020', '1', 'Python', 'Updated Feb 15, 2021', '3', 'Jupyter Notebook', 'Updated Jul 31, 2020', 'Java', 'Updated Jan 17, 2020', 'Python', 'MIT license', 'Updated Jan 19, 2020', 'Python', 'Updated Feb 17, 2020', 'Updated Jan 19, 2020', '12', 'CC0-1.0 license', 'Updated Jul 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': [""Apress Source Code\nThis repository accompanies Agile Artificial Intelligence in Pharo by Alexandre Bergel (Apress, 2020).\n\nThe book provides a complete implementation of a number of complex algorithms. Download the files as a zip using the green button, or clone the repository to your machine using Git.\nHow to load it?\nThe provided code was designed for Pharo and it works Pharo 8 and Pharo 9. Open a playground and execute the instructions:\nMetacello new\n    baseline: 'AgileArtificialIntelligence';\n    repository: 'github://Apress/agile-ai-in-pharo/src';\n    load.\nContent\nThe repository provides the complete implementation of:\n\nNeural network library\nMatrix library\nGenetic algorithm\nZoomorphic creature\nNEAT neuroevolution algorithm\nMario-like game\n\nFurthermore, all the scripts and code snippets are provided in the scripts folder.\nReleases\nRelease v1.0 corresponds to the code in the published book, without corrections or updates.\nContributions\nSee the file Contributing.md for more information on how you can contribute to this repository.\n""], 'url_profile': 'https://github.com/Apress', 'info_list': ['5', 'Jupyter Notebook', 'MIT license', 'Updated Jan 20, 2021', '17', 'Smalltalk', 'Updated Jul 11, 2020', '10', 'HTML', 'Updated Jan 16, 2020', '1', 'Python', 'Updated Feb 15, 2021', '3', 'Jupyter Notebook', 'Updated Jul 31, 2020', 'Java', 'Updated Jan 17, 2020', 'Python', 'MIT license', 'Updated Jan 19, 2020', 'Python', 'Updated Feb 17, 2020', 'Updated Jan 19, 2020', '12', 'CC0-1.0 license', 'Updated Jul 12, 2020']}","{'location': 'Los Angeles', 'stats_list': [], 'contributions': '96 contributions\n        in the last year', 'description': [""Artificial Intelligence for Trading Nanodegree\n\nSource\nWorldQuant, a global quantitative asset management firm, in partnership with global online learning company Udacity announces the launch of a new Artificial Intelligence for Trading Nanodegree program. Students enrolled in the program will analyze real data and build financial models by learning the basics of quantitative trading, as well as how to analyze alternative data and use machine learning to generate trading signals.\nUdacity and WorldQuant have collaborated with top industry professionals with prior experience at leading financial institutions to ensure students are exposed to the latest AI applications in trading and quantitative finance. By learning from industry experts, students will advance their finance knowledge, build a strong portfolio of real-world projects and learn to generate trading signals using natural language processing, recurrent neural networks and random forests. Graduates will gain the quantitative skills currently in demand across multiple functions and roles at hedge funds, investment banks and fintech startups.\nThe program consists of two three-month terms. In the first term, students learn the basics of quantitative analysis, covering data processing, trading signal generation and portfolio management. The second term is focused on AI Algorithms for Trading, where students work with alternative data and use machine learning to generate trading signals and run backtests to evaluate signals\nNanodegree Program Information\nThis nanodegree program is comprised of 8 courses and 8 projects, which are described in detail below.\nBuilding a project is one of the best ways to demonstrate the skills we've learned around the mastery of\nquantitative finance.\n\nProject 1: Trading with Momentum\nProject 2: Breakout Strategy\nProject 3: Smart Beta and Portfolio Optimization\nProject 4: Alpha Research and Factor Modeling\nProject 5: NLP on Financial Statements (generate Alpha Factors from 10-k)\nProject 6: Sentiment Analysis with Neural Networks (LSTM)\nProject 7: Combining Signals for Enhancing Alphas (using Machine Learning)\nProject 8: Backtesting (Barra data)\n\nProject 1: Trading with Momentum\nIn this project, you will learn to implement a trading strategy on your own, and test to see if it has the potential to be profitable. You will be supplied with a universe of stocks and time range. You will also be provided with a textual description of how to generate a trading signal based on a momentum indicator. You will then compute the signal for the time range given and apply it to the dataset to produce projected returns. Finally, you will perform a statistical test on the mean of the returns to conclude if there is alpha in the signal. For the dataset, we'll be using the end of day from Quotemedia.\nProject 2: Breakout Strategy\nIn this project, you will implement the breakout strategy. You'll find and remove any outliers. You'll test to see if it has the potential to be profitable using a Histogram and P-Value. For the dataset, we'll be using the end of day from Quotemedia.\nIn this project, you will code and evaluate a breakout signal. You will run statistical tests to test for normality\nand to find alpha. You will also learn to find outliers and evaluate the effect that filtered outliers could have\non your trading signal. You will run various scenarios of your model with or without the outliers and decide\nif the outliers should be kept or not. You'll test to see if it has the potential to be profitable using a Histogram and P-Value. For the dataset, we'll be using the end of day from Quotemedia.\nProject 3: Smart Beta and Portfolio Optimization\nIn this project, you will build a smart beta portfolio and compare it to a benchmark index. To find out how well the smart beta portfolio did, you’ll calculate the tracking error against the index. You’ll then build a portfolio by using quadratic programming to optimize the weights. Your code will rebalance this portfolio and calculate turn over to evaluate the performance. You’ll use this metric to find the optimal rebalancing Frequency. For the dataset, we'll be using the end of day from Quotemedia.\nProject 4: Multi-Factor Model\nIn this project, you will build a statistical risk model using PCA. You’ll use this model to build a portfolio along with 5 alpha factors. You’ll create these factors, then evaluate them using factor-weighted returns, quantile analysis, sharpe ratio, and turnover analysis. At the end of the project, you’ll optimize the portfolio using the risk model and factors using multiple optimization formulations. For the dataset, we'll be using the end of day from Quotemedia and sector data from Sharadar.\nProject 5: NLP on Financial Statements\nIn this project, you will apply Natural Language Processing (NLP) on corporate filings, such as 10Q and 10K\nstatements, from cleaning data and text processing, to feature extraction and modeling. You will utilize\nbag-of-words and TF-IDF to generate company-specific sentiments. Based on the sentiments, you will decide\nwhich company to invest in, and the optimal time to buy or sell. You will use NLP generate an alpha factor.For the dataset, we'll be using the end of day from Quotemedia and Loughran-McDonald sentiment word lists.\nProject 6: Sentiment Analysis with Neural Networks\nIn this project, you'll build your own deep learning model to classify the sentiment of messages from StockTwits, a social network for investors and traders. Your model will be able to predict if any particular message is positive or negative. From this, you'll be able to generate a signal of the public sentiment for various ticker symbols.\n\nYou will construct and train LSTM networks for sentiment classification. You will run backtests and apply the models to news data for signal generation\n\nProject 7: Combining Signals for Enhanced Alphas\nIn this project, you'll combine signals on a random forest for enhanced alpha. While implementing this, you'll have to solve the problem of overlapping samples. For the dataset, we'll be using the end of day from Quotemedia and sector data from Sharadar.\nProject 8: Backtesting\nIn this project, you will build a fairly realistic backtester that uses the Barra data. The backtester will perform\nportfolio optimization that includes transaction costs, and you'll implement it with computational efficiency\nin mind, to allow for a reasonably fast backtest. You'll also use performance attribution to identify the major\ndrivers of your portfolio's profit-and-loss (PnL). You will have the option to modify and customize the\nbacktest as well.\n""], 'url_profile': 'https://github.com/tatwan', 'info_list': ['5', 'Jupyter Notebook', 'MIT license', 'Updated Jan 20, 2021', '17', 'Smalltalk', 'Updated Jul 11, 2020', '10', 'HTML', 'Updated Jan 16, 2020', '1', 'Python', 'Updated Feb 15, 2021', '3', 'Jupyter Notebook', 'Updated Jul 31, 2020', 'Java', 'Updated Jan 17, 2020', 'Python', 'MIT license', 'Updated Jan 19, 2020', 'Python', 'Updated Feb 17, 2020', 'Updated Jan 19, 2020', '12', 'CC0-1.0 license', 'Updated Jul 12, 2020']}","{'location': 'NTNU', 'stats_list': [], 'contributions': '211 contributions\n        in the last year', 'description': ['AI-programming\nRepository for the subject IT3105 - Artificial Intelligence Programming, NTNU\n'], 'url_profile': 'https://github.com/bendiknordeng', 'info_list': ['5', 'Jupyter Notebook', 'MIT license', 'Updated Jan 20, 2021', '17', 'Smalltalk', 'Updated Jul 11, 2020', '10', 'HTML', 'Updated Jan 16, 2020', '1', 'Python', 'Updated Feb 15, 2021', '3', 'Jupyter Notebook', 'Updated Jul 31, 2020', 'Java', 'Updated Jan 17, 2020', 'Python', 'MIT license', 'Updated Jan 19, 2020', 'Python', 'Updated Feb 17, 2020', 'Updated Jan 19, 2020', '12', 'CC0-1.0 license', 'Updated Jul 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '40 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/faiqueali017', 'info_list': ['5', 'Jupyter Notebook', 'MIT license', 'Updated Jan 20, 2021', '17', 'Smalltalk', 'Updated Jul 11, 2020', '10', 'HTML', 'Updated Jan 16, 2020', '1', 'Python', 'Updated Feb 15, 2021', '3', 'Jupyter Notebook', 'Updated Jul 31, 2020', 'Java', 'Updated Jan 17, 2020', 'Python', 'MIT license', 'Updated Jan 19, 2020', 'Python', 'Updated Feb 17, 2020', 'Updated Jan 19, 2020', '12', 'CC0-1.0 license', 'Updated Jul 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['ArtificialIntelligenceExercises\nThis repository holds some implementations of algorithms we learned about in the course aboute Artificial Intelligence.\nThe first package contains different approaches to solve an Eight Puzzle, namely Breadth-First-Search, Depth-First-Search, iterative deepening and A*-Search.\nThe other package uses some test data sets and implements the kNearesNeighbour algorithm with crossvalidation.\n'], 'url_profile': 'https://github.com/MilenaEisemann', 'info_list': ['5', 'Jupyter Notebook', 'MIT license', 'Updated Jan 20, 2021', '17', 'Smalltalk', 'Updated Jul 11, 2020', '10', 'HTML', 'Updated Jan 16, 2020', '1', 'Python', 'Updated Feb 15, 2021', '3', 'Jupyter Notebook', 'Updated Jul 31, 2020', 'Java', 'Updated Jan 17, 2020', 'Python', 'MIT license', 'Updated Jan 19, 2020', 'Python', 'Updated Feb 17, 2020', 'Updated Jan 19, 2020', '12', 'CC0-1.0 license', 'Updated Jul 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Artificial Intelligence Engines\nComputer code collated from various sources for use with the book:\nArtificial Intelligence Engines: A Tutorial Introduction to the Mathematics of Deep Learning\nby James V Stone\nThis file is: https://github.com/jgvfwstone/ArtificialIntelligenceEngines\nNote that the book is principally about the mathematics of deep learning.\nThis repository is intended to provide \'taster\' code, rather than an exercise in how to program deep learning networks. Because this code has been collated from different sources, the coding style varies between examples.\nEach example has been reproduced with permission from the author.\nDownloading Single Files\nGithub normally insists you download the whole repository.\nHowever, to download a single file\n\ngo to the file so you can see it in the github user interface\nclick on the RAW button in the upper right\nuse the browser ""save as ..."" menu to save the file to your computer.\n\nHow To ...\nThere is a README file within each directory.\nSystem Requirements\nEach example has been tested on a mac (System Version:\tOS X 10.11.3), MacBook Air 1.6GHz.\nPython examples have been tested using the Spyder (3.3.2) python application with Python 3.7.0.\n'], 'url_profile': 'https://github.com/PlanetG3', 'info_list': ['5', 'Jupyter Notebook', 'MIT license', 'Updated Jan 20, 2021', '17', 'Smalltalk', 'Updated Jul 11, 2020', '10', 'HTML', 'Updated Jan 16, 2020', '1', 'Python', 'Updated Feb 15, 2021', '3', 'Jupyter Notebook', 'Updated Jul 31, 2020', 'Java', 'Updated Jan 17, 2020', 'Python', 'MIT license', 'Updated Jan 19, 2020', 'Python', 'Updated Feb 17, 2020', 'Updated Jan 19, 2020', '12', 'CC0-1.0 license', 'Updated Jul 12, 2020']}","{'location': 'Seattle, Washington', 'stats_list': [], 'contributions': '68 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ryan-moll', 'info_list': ['5', 'Jupyter Notebook', 'MIT license', 'Updated Jan 20, 2021', '17', 'Smalltalk', 'Updated Jul 11, 2020', '10', 'HTML', 'Updated Jan 16, 2020', '1', 'Python', 'Updated Feb 15, 2021', '3', 'Jupyter Notebook', 'Updated Jul 31, 2020', 'Java', 'Updated Jan 17, 2020', 'Python', 'MIT license', 'Updated Jan 19, 2020', 'Python', 'Updated Feb 17, 2020', 'Updated Jan 19, 2020', '12', 'CC0-1.0 license', 'Updated Jul 12, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['Artificial-Intelligence\n'], 'url_profile': 'https://github.com/peppermintcoding', 'info_list': ['5', 'Jupyter Notebook', 'MIT license', 'Updated Jan 20, 2021', '17', 'Smalltalk', 'Updated Jul 11, 2020', '10', 'HTML', 'Updated Jan 16, 2020', '1', 'Python', 'Updated Feb 15, 2021', '3', 'Jupyter Notebook', 'Updated Jul 31, 2020', 'Java', 'Updated Jan 17, 2020', 'Python', 'MIT license', 'Updated Jan 19, 2020', 'Python', 'Updated Feb 17, 2020', 'Updated Jan 19, 2020', '12', 'CC0-1.0 license', 'Updated Jul 12, 2020']}","{'location': 'India', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['Awesome AI Labs \nA curated list of Artificial Intelligence Labs doing cutting edge research.\nContributions welcome.\n\n\nStanford AI Lab\n\n\n\nNeuro AI Lab\n \n\n\nStanford NLP\n \n\n\nStanford AIMI\n\n\n\nHuman Centered AI\n\n\n\nEthics of AI UoT\n\n\n\nPSU Crowd AI Lab PennState\n\n\n\nCenter For Brains Mind and Machines\n \n\n\nCMU AI\n\n\nComputational Cognitive Neuroscience Lab\n\n\n\nComputational Visual Neuroscience Lab\n\n\n\nCenter for Computational Brain Research\n\n\nFlowers Project-Team at INRIA\n \n\n\nArtificial Intelligence Laboratory,  Linz Institute of Technology\n\n\nKording Lab\n \n\n\nWicklow AI in Medicine Research Institute\n \n\n\nVan Djik Lab\n\n\nThe Open Cognition Project\n\n\nThe Alan Turing Institute\n \n\n\nVector Institute for AI\n \n\n\nMila\n \n\n\nQUVA Deep Vision Lab\n \n\n\nNASA Quantum AI Lab\n\n\nAllen NLP\n \n\n\nHugging Face\n \n\n\nIDSIA\n \n\n\nIDIAP\n \n\n\nCriteo AI Lab\n \n\n\nWadhwani AI\n \n\n\nFacebook AI Research\n \n\n\nPeoples AI Research\n\n\n\nGoogle Deepmind\n \n\n\nGoogle Brain\n\n\nOpenAI\n \n\n\nUber AI\n \n\n\nSalesforce Research - Einstein\n \n\n\nLyft Level 5\n \n\n\nMicrosoft Research\n \n\n\nAdobe Research\n \n\n\nMIT-IBM Watson AI LAB\n\n\n\nSiemens AI Lab\n\n\n'], 'url_profile': 'https://github.com/sairampillai', 'info_list': ['5', 'Jupyter Notebook', 'MIT license', 'Updated Jan 20, 2021', '17', 'Smalltalk', 'Updated Jul 11, 2020', '10', 'HTML', 'Updated Jan 16, 2020', '1', 'Python', 'Updated Feb 15, 2021', '3', 'Jupyter Notebook', 'Updated Jul 31, 2020', 'Java', 'Updated Jan 17, 2020', 'Python', 'MIT license', 'Updated Jan 19, 2020', 'Python', 'Updated Feb 17, 2020', 'Updated Jan 19, 2020', '12', 'CC0-1.0 license', 'Updated Jul 12, 2020']}"
"{'location': 'Missouri', 'stats_list': [], 'contributions': '141 contributions\n        in the last year', 'description': ['Artificial Intelligence\nCourse: https://github.com/badriadhikari/AI-2020spring\nDescription:\n\nThis course provides an introduction to artificial intelligence (AI). The list of topics may include artificial neural networks, search, planning, knowledge-based reasoning, probabilistic inference, machine learning, natural language processing, and practical applications.\n\nTopics:\n\nThe sequences of topics will be as follows:\n\n\nUse Python, Numpy and Keras to design, train, and evaluate basic feed-forward neural networks\nLearn an overview of artificial intelligence principles and approaches\nLearn a basic understanding of the building blocks of AI as presented in terms of intelligent agents\nSelect and evaluate various searching algorithms\nUnderstand some of the problems and ideas in the field of natural language processing, perception, and robotics\nLearn the philosophical foundations of AI and the future of AI\nImplement various AI algorithms such as DFS, BFS, etc.\n\nAssignments:\n\nData Analysis & Preparation\nModel Selection & Evaluation\nFeature Importance and Reduction\nAddress Peer-Review (Evaluation)\nFinal Assembly\n\n'], 'url_profile': 'https://github.com/zegster', 'info_list': ['Jupyter Notebook', 'Updated May 7, 2020', 'Python', 'Updated Mar 15, 2020', 'MIT license', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Updated Mar 9, 2020', 'Updated Jan 16, 2020', 'C++', 'Updated May 24, 2020', 'Python', 'Updated Feb 21, 2020', 'Updated Jan 16, 2020', 'JavaScript', 'Updated Feb 6, 2020']}","{'location': 'Portland, OR', 'stats_list': [], 'contributions': '166 contributions\n        in the last year', 'description': ['artificial_intelligence\nprojects for ai course\n'], 'url_profile': 'https://github.com/charboltron', 'info_list': ['Jupyter Notebook', 'Updated May 7, 2020', 'Python', 'Updated Mar 15, 2020', 'MIT license', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Updated Mar 9, 2020', 'Updated Jan 16, 2020', 'C++', 'Updated May 24, 2020', 'Python', 'Updated Feb 21, 2020', 'Updated Jan 16, 2020', 'JavaScript', 'Updated Feb 6, 2020']}","{'location': 'Portland', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Artificial-Intelligence\nThis project is a web application that uses IBM Watson technology for image classification and render plotly js dashboard on the form of gauge chart\n'], 'url_profile': 'https://github.com/moadtahri', 'info_list': ['Jupyter Notebook', 'Updated May 7, 2020', 'Python', 'Updated Mar 15, 2020', 'MIT license', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Updated Mar 9, 2020', 'Updated Jan 16, 2020', 'C++', 'Updated May 24, 2020', 'Python', 'Updated Feb 21, 2020', 'Updated Jan 16, 2020', 'JavaScript', 'Updated Feb 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['Artificial_Intelligence\ngraphs, heuristics, constraint solvers, game trees, genetic algorithms, learning, natural language processing, agents\n'], 'url_profile': 'https://github.com/kazuyachue', 'info_list': ['Jupyter Notebook', 'Updated May 7, 2020', 'Python', 'Updated Mar 15, 2020', 'MIT license', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Updated Mar 9, 2020', 'Updated Jan 16, 2020', 'C++', 'Updated May 24, 2020', 'Python', 'Updated Feb 21, 2020', 'Updated Jan 16, 2020', 'JavaScript', 'Updated Feb 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '143 contributions\n        in the last year', 'description': ['Artificial-Intelligence\nBIT Notes for Artificial Intelligence\nBIM Notes for Artificial Intelligence\n'], 'url_profile': 'https://github.com/MohanBhandari', 'info_list': ['Jupyter Notebook', 'Updated May 7, 2020', 'Python', 'Updated Mar 15, 2020', 'MIT license', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Updated Mar 9, 2020', 'Updated Jan 16, 2020', 'C++', 'Updated May 24, 2020', 'Python', 'Updated Feb 21, 2020', 'Updated Jan 16, 2020', 'JavaScript', 'Updated Feb 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/gGrobinson', 'info_list': ['Jupyter Notebook', 'Updated May 7, 2020', 'Python', 'Updated Mar 15, 2020', 'MIT license', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Updated Mar 9, 2020', 'Updated Jan 16, 2020', 'C++', 'Updated May 24, 2020', 'Python', 'Updated Feb 21, 2020', 'Updated Jan 16, 2020', 'JavaScript', 'Updated Feb 6, 2020']}","{'location': 'Orange, CA', 'stats_list': [], 'contributions': '175 contributions\n        in the last year', 'description': ['CPSC_390\nArtificial Intelligence\n'], 'url_profile': 'https://github.com/TobyJChappell', 'info_list': ['Jupyter Notebook', 'Updated May 7, 2020', 'Python', 'Updated Mar 15, 2020', 'MIT license', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Updated Mar 9, 2020', 'Updated Jan 16, 2020', 'C++', 'Updated May 24, 2020', 'Python', 'Updated Feb 21, 2020', 'Updated Jan 16, 2020', 'JavaScript', 'Updated Feb 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '79 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Kronemeyer', 'info_list': ['Jupyter Notebook', 'Updated May 7, 2020', 'Python', 'Updated Mar 15, 2020', 'MIT license', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Updated Mar 9, 2020', 'Updated Jan 16, 2020', 'C++', 'Updated May 24, 2020', 'Python', 'Updated Feb 21, 2020', 'Updated Jan 16, 2020', 'JavaScript', 'Updated Feb 6, 2020']}","{'location': 'Canada', 'stats_list': [], 'contributions': '15 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/haroldvelasquez', 'info_list': ['Jupyter Notebook', 'Updated May 7, 2020', 'Python', 'Updated Mar 15, 2020', 'MIT license', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Updated Mar 9, 2020', 'Updated Jan 16, 2020', 'C++', 'Updated May 24, 2020', 'Python', 'Updated Feb 21, 2020', 'Updated Jan 16, 2020', 'JavaScript', 'Updated Feb 6, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['iot_eCare_ai\nPanoramica\nIl progetto consiste in una piattaforma software a supporto della erogazione un servizio innovativo basato su tecnologie biometriche e domotica che supportano la persona (utenti anziani) a prendersi cura di se stessa, attraverso il monitoraggio costante di tutti i parametri vitali, la stimola a seguire uno stile di vita sano e offre un controllo da parte di una centrale operativa 24h per la sicurezza e la gestione delle emergenze. In caso di valori devianti rispetto alla serie storica, la piattaforma di Artificial Intelligence (AI) fa scattare automaticamente delle azioni (da una videochiamata sulla telecamera posizionata in casa dell’utente, al controllo della posizione sull’orologio intelligente che sarà dato in dotazione agli utenti, fino all’intervento di un operatore socio-assistenziale). Si tratta di un progetto di Internet of Things (Internet delle cose – IoT) e Intelligenza Artificiale con tecnologie biometriche applicato al settore dell’assistenza sociale agli anziani. Non esiste un’applicazione simile a livello europeo in corso d’uso che abbracci tutta la catena del valore dalla rilevazione del dato all’intervento sul campo.\nInstallazione\nSeguire le istruzioni di installazione presenti nel file ""Manuale_Installazione_AI.pdf""\nContributi\nSe vuoi contribuire a iot_eCare, leggi il file CONTRIBUTING.md, e restuiscilo firmato\nLicenza\nQuesto progetto è distribuito sotto EUPL License. Vedi il file LICENSE per le informazioni sulla licenza: https://github.com/igcomsrl/iot_eCare_ai/blob/master/LICENSE\n'], 'url_profile': 'https://github.com/igcomsrl', 'info_list': ['Jupyter Notebook', 'Updated May 7, 2020', 'Python', 'Updated Mar 15, 2020', 'MIT license', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated Jan 18, 2020', 'Updated Mar 9, 2020', 'Updated Jan 16, 2020', 'C++', 'Updated May 24, 2020', 'Python', 'Updated Feb 21, 2020', 'Updated Jan 16, 2020', 'JavaScript', 'Updated Feb 6, 2020']}"
"{'location': 'Singapore', 'stats_list': [], 'contributions': '3 contributions\n        in the last year', 'description': ['CS3243 Artificial Intelligence\nNational University of Singapore\n'], 'url_profile': 'https://github.com/Olivvvia', 'info_list': ['1', 'Updated Jan 18, 2020', 'Python', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Nov 11, 2020', 'Python', 'Updated Dec 16, 2020', 'MIT license', 'Updated Mar 27, 2020', 'Python', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 13, 2020', 'HTML', 'Updated Jan 13, 2020', 'C++', 'Updated Jan 19, 2020', 'C#', 'MIT license', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ParthKulkarni', 'info_list': ['1', 'Updated Jan 18, 2020', 'Python', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Nov 11, 2020', 'Python', 'Updated Dec 16, 2020', 'MIT license', 'Updated Mar 27, 2020', 'Python', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 13, 2020', 'HTML', 'Updated Jan 13, 2020', 'C++', 'Updated Jan 19, 2020', 'C#', 'MIT license', 'Updated Jan 27, 2020']}","{'location': 'Broomfield, Colorado', 'stats_list': [], 'contributions': '53 contributions\n        in the last year', 'description': ['CSCI-3202-Artificial-Intelligence\nA repository full of my work from AI (Artifical Intelligence), CSCI 3202 at CU Boulder. Spring 2020.\nDISCLAIMER: Please do not use work in dishonest way! This serves as a place to show the work I have completed in the past.\n'], 'url_profile': 'https://github.com/MuntahaPasha', 'info_list': ['1', 'Updated Jan 18, 2020', 'Python', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Nov 11, 2020', 'Python', 'Updated Dec 16, 2020', 'MIT license', 'Updated Mar 27, 2020', 'Python', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 13, 2020', 'HTML', 'Updated Jan 13, 2020', 'C++', 'Updated Jan 19, 2020', 'C#', 'MIT license', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '35 contributions\n        in the last year', 'description': [""CS: AI\nRepository for work done in PSU'S CS Artificial Intelligence Course.\nHomework 1.\nThe goal of Homework 1 was to create an implementation to compare the BFS and\nA* algorithm.\nHomework 2.\nThe goal of Homework 2 is to create a solution to the 8 Queens problem using\na Genetic Algorithm (GA). In this GA, the mutation is to swap two elements at\nrandom until generationally, a solution had been found. More results can be\nfound in the writeup attached.\nFinal\nAs the COVID-19 Pandemic set in, I wanted to focus more on enhancing previous\nstudied material. Thus, I worked on improving the third homework assignment:\nRobby the Robot. Robby is a robot that utilizes a Q-Learning Matrix to identify\nwhat the correct action is to do in a hypothetical n x n map where the actions\nthe robot can perform is to move left, right, up, down, or clear the vacuum.\nThe robot must not crash into a wall, pick up dirt in a square that is clean,\nor not clean in a square that is dirty.\nThe final involved tweaking variables common to the Q-Learning rewards\nalgorithm, and finding what the sweetspot of combinations are and how they\naffect the efficiency of Robby the Robot.\n""], 'url_profile': 'https://github.com/blzzrd', 'info_list': ['1', 'Updated Jan 18, 2020', 'Python', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Nov 11, 2020', 'Python', 'Updated Dec 16, 2020', 'MIT license', 'Updated Mar 27, 2020', 'Python', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 13, 2020', 'HTML', 'Updated Jan 13, 2020', 'C++', 'Updated Jan 19, 2020', 'C#', 'MIT license', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['CAP6635-Artificial-Intelligence\n2020 Spring\n'], 'url_profile': 'https://github.com/AdamCorbinFAUPhD', 'info_list': ['1', 'Updated Jan 18, 2020', 'Python', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Nov 11, 2020', 'Python', 'Updated Dec 16, 2020', 'MIT license', 'Updated Mar 27, 2020', 'Python', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 13, 2020', 'HTML', 'Updated Jan 13, 2020', 'C++', 'Updated Jan 19, 2020', 'C#', 'MIT license', 'Updated Jan 27, 2020']}","{'location': 'Seoul, Korea', 'stats_list': [], 'contributions': '34 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/hyunas1996', 'info_list': ['1', 'Updated Jan 18, 2020', 'Python', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Nov 11, 2020', 'Python', 'Updated Dec 16, 2020', 'MIT license', 'Updated Mar 27, 2020', 'Python', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 13, 2020', 'HTML', 'Updated Jan 13, 2020', 'C++', 'Updated Jan 19, 2020', 'C#', 'MIT license', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '67 contributions\n        in the last year', 'description': ['README\n\u200b\t\t包括课件、实验和作业相应答案。不过我们这一届的考试特别难（比作业难了好多），千万不要以作业为准来复习。\n'], 'url_profile': 'https://github.com/hit-thusz-RookieCJ', 'info_list': ['1', 'Updated Jan 18, 2020', 'Python', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Nov 11, 2020', 'Python', 'Updated Dec 16, 2020', 'MIT license', 'Updated Mar 27, 2020', 'Python', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 13, 2020', 'HTML', 'Updated Jan 13, 2020', 'C++', 'Updated Jan 19, 2020', 'C#', 'MIT license', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['CarImageArtificialIntelligence\n'], 'url_profile': 'https://github.com/balrajendran', 'info_list': ['1', 'Updated Jan 18, 2020', 'Python', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Nov 11, 2020', 'Python', 'Updated Dec 16, 2020', 'MIT license', 'Updated Mar 27, 2020', 'Python', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 13, 2020', 'HTML', 'Updated Jan 13, 2020', 'C++', 'Updated Jan 19, 2020', 'C#', 'MIT license', 'Updated Jan 27, 2020']}","{'location': 'Dhaka, Bangladesh', 'stats_list': [], 'contributions': '1,281 contributions\n        in the last year', 'description': ['Checkers_Game-Artificial-Intelligence\nA console based Checkers Game that I made during the Artificial Intelligence course in my university, played\nbetween the user/player and AI that uses heuristics/Alpha-Beta pruning.\n'], 'url_profile': 'https://github.com/sakiib', 'info_list': ['1', 'Updated Jan 18, 2020', 'Python', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Nov 11, 2020', 'Python', 'Updated Dec 16, 2020', 'MIT license', 'Updated Mar 27, 2020', 'Python', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 13, 2020', 'HTML', 'Updated Jan 13, 2020', 'C++', 'Updated Jan 19, 2020', 'C#', 'MIT license', 'Updated Jan 27, 2020']}","{'location': 'Tijuana, Baja California, Mexico', 'stats_list': [], 'contributions': '39 contributions\n        in the last year', 'description': ['Unity Artificial Intelligence\nArtificial Intelligence examples and progress.\n'], 'url_profile': 'https://github.com/JCharlieDev', 'info_list': ['1', 'Updated Jan 18, 2020', 'Python', 'Updated Feb 18, 2020', 'Jupyter Notebook', 'Updated Nov 11, 2020', 'Python', 'Updated Dec 16, 2020', 'MIT license', 'Updated Mar 27, 2020', 'Python', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 13, 2020', 'HTML', 'Updated Jan 13, 2020', 'C++', 'Updated Jan 19, 2020', 'C#', 'MIT license', 'Updated Jan 27, 2020']}"
"{'location': 'Dhaka, Bangladesh', 'stats_list': [], 'contributions': '1,281 contributions\n        in the last year', 'description': ['datingAI\nA dating/matching program that I made during the Artificial Intelligence course in my university, which uses a logic base to give the  compatibility score for man/woman looking for woman/man respectively according to their given preference & information.\n'], 'url_profile': 'https://github.com/sakiib', 'info_list': ['Prolog', 'Updated Jan 19, 2020', 'HTML', 'Updated Jan 13, 2020', 'C++', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Sep 6, 2020', 'Python', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 14, 2020', 'HTML', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Python', 'Updated Jan 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '11 contributions\n        in the last year', 'description': ['CarImageArtificialIntelligence\n'], 'url_profile': 'https://github.com/balrajendran', 'info_list': ['Prolog', 'Updated Jan 19, 2020', 'HTML', 'Updated Jan 13, 2020', 'C++', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Sep 6, 2020', 'Python', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 14, 2020', 'HTML', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Python', 'Updated Jan 13, 2020']}","{'location': 'Dhaka, Bangladesh', 'stats_list': [], 'contributions': '1,281 contributions\n        in the last year', 'description': ['Checkers_Game-Artificial-Intelligence\nA console based Checkers Game that I made during the Artificial Intelligence course in my university, played\nbetween the user/player and AI that uses heuristics/Alpha-Beta pruning.\n'], 'url_profile': 'https://github.com/sakiib', 'info_list': ['Prolog', 'Updated Jan 19, 2020', 'HTML', 'Updated Jan 13, 2020', 'C++', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Sep 6, 2020', 'Python', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 14, 2020', 'HTML', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Python', 'Updated Jan 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/YulanJS', 'info_list': ['Prolog', 'Updated Jan 19, 2020', 'HTML', 'Updated Jan 13, 2020', 'C++', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Sep 6, 2020', 'Python', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 14, 2020', 'HTML', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Python', 'Updated Jan 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1,268 contributions\n        in the last year', 'description': ['Notas de Aula / Anotacoes gerais\nDetecção de Anomalia:\nhttps://towardsdatascience.com/best-clustering-algorithms-for-anomaly-detection-d5b7412537c8\nhttps://medium.com/@elutins/dbscan-what-is-it-when-to-use-it-how-to-use-it-8bd506293818\n\nipykernel - Alterar Kernel no Jupyter\nconda install jupyter\nconda install nb_conda\nconda install ipykernel\npython -m ipykernel install --user --name mykernel\njupyter kernelspec remove mykernel\n\nPythonAnyWhere Deploy\nhttps://blog.pythonanywhere.com/169/\nhttps://blog.pythonanywhere.com/121/\n\nSome Data related Posts\nPandas where loc mask good examples:\nhttps://kanoki.org/2019/07/17/pandas-how-to-replace-values-based-on-conditions/\nhttps://amitkushwaha.co.in/data-visualization-part-1.html\nhttps://www.kaggle.com/masumrumi/a-statistical-analysis-ml-workflow-of-titanic\nhttps://blog.minitab.com/blog/understanding-statistics/understanding-qualitative-quantitative-attribute-discrete-and-continuous-data-types\n\nSELENIUM\nAdicionar executavel geckodriever ao path:\nexport PATH=$PATH:/root/pasta.\nhttps://medium.com/@cagriaydogdu2334/3d-visualization-of-k-means-clustering-47d3d3e82117\nhttps://www.bigendiandata.com/2017-04-18-Jupyter_Customer360/\nhttp://scraping.pro/recaptcha-solve-selenium-python/\nhttps://www.howtoforge.com/tutorial/tesseract-ocr-installation-and-usage-on-ubuntu-16-04/\n\n'], 'url_profile': 'https://github.com/havyx', 'info_list': ['Prolog', 'Updated Jan 19, 2020', 'HTML', 'Updated Jan 13, 2020', 'C++', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Sep 6, 2020', 'Python', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 14, 2020', 'HTML', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Python', 'Updated Jan 13, 2020']}","{'location': 'Bloomington, Indiana', 'stats_list': [], 'contributions': '191 contributions\n        in the last year', 'description': ['Assignment 1 - Searching\nPart 1 : The Luddy Puzzle\nFormulation:\nThe game board consists of 4 * 4 grid with 15 tiles numbered from 0\nto 15 in a randomized way. In each turn, the player can slide a tile into an adjacent empty space. Our job is to find a set of sliding moves of the blank tile which converts the initial board to the goal state.\nAbstraction\nIntitial State:\nRandom arrangement of tiles from 0-15, 0 numbered tile being a blank tile\nGoal State:\nTiles arranged in an ascending order from 1-15  with 0 at the last position of the grid.\nState Space:\nState of all objects are stored in the fringe. Each state object contains, the heuristic value of that node and an evaluation value i.e g(s) + h(s),  the path to the current state from the initial node.\nSuccessor function\nSuccessor function will give us the next possible states that can be generated by moving the blank tile. In this assignment, we have 3 Successor functions depending on the variant the users passes as an input\n\nOriginal: This Successor function generates successors of its neighboring tiles in all the direction of the current state, Up, down , left, bottom. We use check valid-index method to determine whether node exists specially in the case of edge nodes.\nCircular: In this variant, the successor function is defined same as original variant considering circular moves constraint.\nLuddy: Defined 8 moves with respect to the luddy constraint and successor can be found based on this moves.\n\nHeuristic Function:\nThis heuristic function returns the number of misplaced tiles of current state. That is, it checks every tile of every game board to see if it is in goal-state. We first move the empty tile in all possible direction in the current state and calculate the h-score which is nothing but number of misplaced tiles. Then g-score is evaluated as number of nodes traversed from the start state to the current state. Example\nAdmissibility of the heuristic:\nWe can say that the heuristic function is admissible because we are always underestimating the cost to reach the goal state, i.e. we cannot reach the goal state in fewer number of moves than the number of misplaced tiles in the board. Hence, each tile which is not in place must be moved at least once, this shows the admissibility.\nAlgorithm Used\nThe code was implemented using A* algorithm (Search 3 Algorithm)\nCode Flow\nThe code takes two inputs one is board and other the variant. Then the code takes the initial-board as its first state as a start stage and finds the empty tile that is tile numbered 0. Depending on the variant the start state calculates all of its successor. Once the successors are calculated, it is added it to the closed set. Then for every successor we calculate the heuristic function and adds it to the priority queue. The node having the least heuristic value is returned or popped out of the queue. This process is repeated until we get the final state.\nApproach & Problem Faced\nThe given code snippet was working only for original variant. Our next task was to make it work for two new variants circular and luddy respectively. Original variant was not giving a best solution as it did not take into account the visited state because of this some boards were running into an infinite loop.\nWe first of all implemented the visited state logic to remove the states that were already visited and then thought of implementing heuristic. There was some confusion in choosing heuristic, so this link quite helped me. https://cs.stackexchange.com/questions/37795/why-is-manhattan-distance-a-better-heuristic-for-15-puzzle-than-number-of-til\nPseudo Code for Heuristic: Number of Misplaced Tiles.\nfor i in the state:\nif state[i] is not equal to i+1\n#this will ensure all the nodes are in ascending order\nincrement count\nreturn count\n#count will only increment if the two adjacent tiles are not in the ascending order\nThe next step was to implement priority queue to get the least value of the heuristic. Some state space was quite huge because of which our A* algorithm was not fast enough and consumed lots of time. To handle this case, I first of all thought may be using different heuristic could solve my problem at ease or perhaps it is due to heuristic I am getting large number of state space and traversing in the same loop. I tried using manhattan distance heuristic but was in the same loop hole of infinite loop and timeout error. Also read about IDA * search algorithm as it is the fastest solution to get the output. I tried implementing IDA * search but because the implementation was too complicated was not able to do so.\nPart 2 - Road trip!\nI) a description of how you formulated the search problem, including precisely defning the state space, the successor function, the edge weights, the goal state, and (if applicable) the heuristic function(s) you designed, including an argument for why they are admissible;\nFormulation: The user will give a to and from city for which we have to find a route which will be optimal according to the constraint given by the user.\n1.Initial State: All the segment containing start city\n2.Goal State: The Current hop made has the End city (Example: if my end city is B and my current hop is B to C then its my                    goal state)\n3.Successor Function and edge weight and heuristic: According to the constraint the cost of every succesor is returned\n-distance - haversine distance for next city in the hop and destination i.e the end_city is calculated and added to the         existing cost\n-segments - for every hop segment++\n-time - distance given in the data divided by speed limit given in the data i.e the road-segments.txt and added to the          existing cost\n-mpg - calculated using the formula in the Instructions for ""V"" in the formula used the speed limit for that hop and added      to the existing cost\n4.Admissibality: As we are haversine distance it will always give optimal solution\nII) a brief description of how your search algorithm works\nList of Functions and Code Flow\n\nMain function\n-we take command line arguments in variables\n-we check if the given constraint is correct and the given cities name are in the data set\n-call parse_document function\n-call A star function\nparse_document()\n-parse road-segments.txt in gv_rd_seg_lst list structure\n-parse city-gps.txt in gv_cty_gps_lst list structure\nUsing haversine function of Python directly\n-to calculate the GPS distance between two location which will be used in\nA* search heuristic for distance constraint\ndist_calc()\n-it passes values to haversine_distance()\n-the values are extracted from gv_cty_gps_lst - latitude and longitude of current and destination city\ncost_calc()\n-for every next hop possible we need to calculate cost of that hop which is done in this function\na_star_srch()\n-we chack if initial state is same as goal state or not\n-if not we initialize a priority queue\nwe use priority queue and the priority queue uses the cost to pop the next segment with lowest cost calculated in cost_clac\n-Queue is initialized with the initial segments based on start city\n-the while loop runs till queue is empty in this case printing ""INF"" or till we reach goal state\n-we pop the value with the lowest cost an duse it as the current state\n-successors for this current state are calculated if the current state is not equal to goal state\n-for every successor we calculate the next cost, distance, time, total gas gallon, segments\n-we append the route as well over here for every successor\n-when the goal state is found we print the segment, distance travelled, time required, total gas gallons, and path\n\nIII)discussion of any problems you faced, any assumptions,simplications, and/or design decisions.\n1.initially both files were parsed in list structure for time optimization it was changed to dictionary after comparison of      run time list structure took less time\n2.bidirectional case handling : we switched the city names  where needed in the successor states\nCities which didn\'t have GPS details case handling : in dist_calc() if we do not find details in the given data we return 0.\nif the returned value from dist_calc is 0 we consider the distance                                        given for that segment in the dataset\n3.the haversine module is not available on SICE server - added a function for it haversine_distance()\n4.For mpg ""V"" in the formula given assumed the speed limit for that hop\n5.total gas gallons = (hop distance/mpg for the hop)\nPart 3 - Choosing a team\nProble Statement\nChoose a team of robots that has the greatest possible skill\nGiven Code :\nThe sceleton code used a greedy approach to choose the team of robots. Firstly the list of robots is picked in decreasing order of skill per unit cost. But the solution was not giving the team of robots in whole number.\nSo to give the solution in more optimized way we reversed the robots in the sorted list and removed the logic for fraction solution in the Else part of If Statement. This gave me the answer for the team of robots in whole number(1.0000).\ndef approx_solve(people, budget): \n\nThis function solves the code using Greedy Search\nWhich may or may not be optimal for every scenario\n\nProblem Faced :\nWe tried using multiple datasets for the above approch but the code didn\'t gave the optimal solution for all the case. This was because the code considers the perfect solution is the one that it finds at that moment.\nTo solve this we thought of using the Knapsack algorithm which is similar used to solve similar problems as the problem statement.\nThen we started implementing the 0/1 Knapsack problem with the Dynamic programming approach. Though if using the matrix technique approach for calculating the max cost and perfect team of robots. But as the values for cost and skills were in the floating point we used naive way of taking the round of that number and solving it. But this didn\'t gave the optimal solution for the robots who were having cost/skill relatively similar values.\nFinal Approach :\nKnapsack Branch and Bound Function \n\ndef approx_solve_branchandbound(people, budget):\n\n\n\nAlgorith Used : 0/1 Knapsack using Branch and Bound\nAs the problem statement is based on solving combinatory optimization 0/1 knapsack algorithm provides optimized solution for choosing robots based on the budget given.\nThis algorithm solved both the problems faced in 2 earlier approaches and gave the optimum solution for every single dataset and case. The backtracking solution helps in ignoring the infeasible solutions.\n\n\nFlow :\nCreated 2 classes for storing the Robots details and Robots State in the node of tree\nCode starts with reading the file and storing the data in the dictionary.\nSorted all the robot items in the descending order of ratio of cost per skills\nInitialize the max_skill =0 and also Added a dummy node of Decision tree and added it to Dequeue.\nAfter that untill the deque is not empty we deque one by one element and find profit of next node. If the profit is more then we update the max_skill. Then calculate Upper Bound and if bound > max_skill then add node to the dequeue.\nConsider the case when next node is not considered but we have to add the next level to the dequeue without updating the max_skill.\n\n\nClass to calculate the upper bound for element\n\ndef calculate_bound(u, capacity, item_count, items):\n\n'], 'url_profile': 'https://github.com/Kaustubh-DB', 'info_list': ['Prolog', 'Updated Jan 19, 2020', 'HTML', 'Updated Jan 13, 2020', 'C++', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Sep 6, 2020', 'Python', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 14, 2020', 'HTML', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Python', 'Updated Jan 13, 2020']}","{'location': 'Pabna, Bangladesh', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['Artificial-Intelligence-and-Robotics-Lab\n'], 'url_profile': 'https://github.com/6shihab', 'info_list': ['Prolog', 'Updated Jan 19, 2020', 'HTML', 'Updated Jan 13, 2020', 'C++', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Sep 6, 2020', 'Python', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 14, 2020', 'HTML', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Python', 'Updated Jan 13, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['IF.03.01-11 Basic Web Techniques - Dynamic Css 3 Features\nThis coding assignment shall make you practice the dynamic css3 features. Make sure that you read the section Required Tasks  in CodingAssignment.md carefully and to complete all the tasks listed there.\n'], 'url_profile': 'https://github.com/zzArchive-if-03-01-C-fall-2019', 'info_list': ['Prolog', 'Updated Jan 19, 2020', 'HTML', 'Updated Jan 13, 2020', 'C++', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Sep 6, 2020', 'Python', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 14, 2020', 'HTML', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Python', 'Updated Jan 13, 2020']}","{'location': 'Orlando, Florida', 'stats_list': [], 'contributions': '97 contributions\n        in the last year', 'description': [""CAP4630-Artificial-Intelligence-UCF\nAbdool Shakur's Repo for the UCF course CAP4630 aka Artificial Intelligence and Machine Learning.\n""], 'url_profile': 'https://github.com/salamshakur', 'info_list': ['Prolog', 'Updated Jan 19, 2020', 'HTML', 'Updated Jan 13, 2020', 'C++', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Sep 6, 2020', 'Python', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 14, 2020', 'HTML', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Python', 'Updated Jan 13, 2020']}","{'location': 'Türkiye', 'stats_list': [], 'contributions': '124 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/mustafakoca99', 'info_list': ['Prolog', 'Updated Jan 19, 2020', 'HTML', 'Updated Jan 13, 2020', 'C++', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Sep 6, 2020', 'Python', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 14, 2020', 'HTML', 'Updated Jan 30, 2020', 'Jupyter Notebook', 'Updated Apr 25, 2020', 'Python', 'Updated Jan 13, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['yemolysis\nARTIFICIAL INTELLIGENCE (AI)\n'], 'url_profile': 'https://github.com/Yemolysis', 'info_list': ['Apache-2.0 license', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 23, 2020', 'Updated Jan 16, 2020', 'Updated Jan 15, 2020', 'Updated Jan 17, 2020', 'Updated Jan 17, 2020', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 29, 2020', 'Python', 'Updated Oct 25, 2020', 'Updated Jan 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2,734 contributions\n        in the last year', 'description': ['AI Challenges\nResearching narrow artificial intelligence.\nContents\n\nImage Classification\nImage Synthesis\nLanguage Model\nSpeech Recognition\nSpeech Synthesis\n\nReferences\n\nhttps://en.wikipedia.org/wiki/Weak_AI\n\nLicense\nMIT licensed\n'], 'url_profile': 'https://github.com/wurde', 'info_list': ['Apache-2.0 license', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 23, 2020', 'Updated Jan 16, 2020', 'Updated Jan 15, 2020', 'Updated Jan 17, 2020', 'Updated Jan 17, 2020', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 29, 2020', 'Python', 'Updated Oct 25, 2020', 'Updated Jan 18, 2020']}","{'location': 'Seoul, Korea', 'stats_list': [], 'contributions': '93 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/guiyomj', 'info_list': ['Apache-2.0 license', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 23, 2020', 'Updated Jan 16, 2020', 'Updated Jan 15, 2020', 'Updated Jan 17, 2020', 'Updated Jan 17, 2020', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 29, 2020', 'Python', 'Updated Oct 25, 2020', 'Updated Jan 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '50 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/venkatesh405', 'info_list': ['Apache-2.0 license', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 23, 2020', 'Updated Jan 16, 2020', 'Updated Jan 15, 2020', 'Updated Jan 17, 2020', 'Updated Jan 17, 2020', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 29, 2020', 'Python', 'Updated Oct 25, 2020', 'Updated Jan 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '110 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Arbert407', 'info_list': ['Apache-2.0 license', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 23, 2020', 'Updated Jan 16, 2020', 'Updated Jan 15, 2020', 'Updated Jan 17, 2020', 'Updated Jan 17, 2020', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 29, 2020', 'Python', 'Updated Oct 25, 2020', 'Updated Jan 18, 2020']}","{'location': 'Fairfax, VA ', 'stats_list': [], 'contributions': '442 contributions\n        in the last year', 'description': ['Slides of presentations I have done at various meetups\n'], 'url_profile': 'https://github.com/vineetk1', 'info_list': ['Apache-2.0 license', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 23, 2020', 'Updated Jan 16, 2020', 'Updated Jan 15, 2020', 'Updated Jan 17, 2020', 'Updated Jan 17, 2020', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 29, 2020', 'Python', 'Updated Oct 25, 2020', 'Updated Jan 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Welcome to GitHub Pages\nYou can use the editor on GitHub to maintain and preview the content for your website in Markdown files.\nWhenever you commit to this repository, GitHub Pages will run Jekyll to rebuild the pages in your site, from the content in your Markdown files.\nMarkdown\nMarkdown is a lightweight and easy-to-use syntax for styling your writing. It includes conventions for\nSyntax highlighted code block\n\n# Header 1\n## Header 2\n### Header 3\n\n- Bulleted\n- List\n\n1. Numbered\n2. List\n\n**Bold** and _Italic_ and `Code` text\n\n[Link](url) and ![Image](src)\nFor more details see GitHub Flavored Markdown.\nJekyll Themes\nYour Pages site will use the layout and styles from the Jekyll theme you have selected in your repository settings. The name of this theme is saved in the Jekyll _config.yml configuration file.\nSupport or Contact\nHaving trouble with Pages? Check out our documentation or contact support and we’ll help you sort it out.\n'], 'url_profile': 'https://github.com/adityajadhav3', 'info_list': ['Apache-2.0 license', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 23, 2020', 'Updated Jan 16, 2020', 'Updated Jan 15, 2020', 'Updated Jan 17, 2020', 'Updated Jan 17, 2020', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 29, 2020', 'Python', 'Updated Oct 25, 2020', 'Updated Jan 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Adsanvar', 'info_list': ['Apache-2.0 license', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 23, 2020', 'Updated Jan 16, 2020', 'Updated Jan 15, 2020', 'Updated Jan 17, 2020', 'Updated Jan 17, 2020', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 29, 2020', 'Python', 'Updated Oct 25, 2020', 'Updated Jan 18, 2020']}","{'location': 'Toronto, Canada', 'stats_list': [], 'contributions': '602 contributions\n        in the last year', 'description': ['intro-artif-intel\nOld Artificial Intelligence coursework using search algorithms on search spaces.\nReferences\nSonya Allin / Bahar Aameri, University of Toronto Winter 2020 \n'], 'url_profile': 'https://github.com/22victoryy', 'info_list': ['Apache-2.0 license', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 23, 2020', 'Updated Jan 16, 2020', 'Updated Jan 15, 2020', 'Updated Jan 17, 2020', 'Updated Jan 17, 2020', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 29, 2020', 'Python', 'Updated Oct 25, 2020', 'Updated Jan 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['aicodes\nArtificial intelligence works\n'], 'url_profile': 'https://github.com/kenbravo2017', 'info_list': ['Apache-2.0 license', 'Updated Jan 13, 2020', 'Jupyter Notebook', 'MIT license', 'Updated Jan 23, 2020', 'Updated Jan 16, 2020', 'Updated Jan 15, 2020', 'Updated Jan 17, 2020', 'Updated Jan 17, 2020', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 29, 2020', 'Python', 'Updated Oct 25, 2020', 'Updated Jan 18, 2020']}"
"{'location': 'Bloomington, Indiana', 'stats_list': [], 'contributions': '191 contributions\n        in the last year', 'description': ['a0\nPart 1: Finding Your Way\nPython program for Finding Your Way from a given location to the final location.\nThis program demonstrates search algorithm named Breadth First Search. It basically finds for best possible shortest path for a given set of input map. The set of components for the program are: 1] The map given in a ‘.Txt’ File. 2] Certain blocked positions are marked by ‘&’. 3] The starting position marked by #. 4] A end position or the target position marked by @.\nSolution of Finding Your Way problem we check the possible valid paths that is, all the (‘.‘) positions, in the same row and in the same column, starting with the ‘#’, and moving onto the next valid adjacent nodes, considering & as a block-way and thereby restricting its path.\nState of states will be all the possible moves from # point to @. Initial state will be the starting location of traversal(#). Successor function Succ:S will be all the valid positions(“.”) in the same row and column for that particular state.\nObservations and Experiment,\nAt the first point, Initial node is inserted into the fringe, and then popped out(Queue approach) and its valid successor is being appended to the fringe. And this task is being carried out until we reach our goal state. At Initial stage, main difficulty that I found going through this code was it was going in infinite loop as it was searching for all possible moves, and also considered the recurring moves. Hence, to reduce the number of states I used visited_node list, thereby adding visited states into fringe. Also to get all the possible directions(N,S,W,E), I passed *move(present) and *curr_move(successor) to see the comparison between row or column values, for instance if curr_move’s row valueis 1 less than move’s value then it is travelling North so used literal ‘N’ and thereby appended it in the fringe along with the cost of moving to its successor node that is 1. This loop was carried until we reached our goal state thereby increasing its distance by one while moving to its successor and adding path of its traversal. Also if solution is not found, and is going into infinite loop then it will return Inf.\nPart 2: Hide and Seek\nPython program for arranging K friends, such that no two friends can see one another.\nThis program demonstrates search algorithm named Depth First Search. The set of components for the program are: 1] The map given in a ‘.Txt’ File. 2] Certain blocked positions that are marked by ‘&’.\nSolution of arranging is that we iterate from left to right and top to bottom over the map to get the first empty position. So now is the time where we can place the 1st Friend at that point. The Initial State will be defined by placing the first friend and moving this state to the fringe. Successor function Succ:S will be all the valid position that the second friend could be placed with a condition that it can’t be placed in the same row, column respective to the friend. New Friend can only be placed after the block between the same row and column. Check if new friend is visible on the east, west, south and north from this side walk, if the friend is not visible in the same path, then place that friend on the board and push it on the fringe. Check this loop till all the friends are placed and no more fringe items are left. That will be our goal state.\nObservation and Experiment,\nAt the initial stage, I was stuck with a problem of exploring all the possible states including the one that is visited recursively for the same row and column, I thereby reduced the number of states to explore by adding traversed states list in the fringe. I made a try to use recursive method in a way to avoid using fringe but was not able to do so. Hence maximizing the time complexity of the code. The code can be made more optimizable by breaking the loop for traversal if we are not able to find the friend’s placement in the same row or column after the first iteration and was not able to achieve that goal. I will in fact try to learn more about recursive functionality and avoiding unnecessary loop traversals in the coming weeks and try to implement it in the next assignment making the code optimum as possible.\n'], 'url_profile': 'https://github.com/Kaustubh-DB', 'info_list': ['Python', 'Updated Jan 13, 2020', '1', 'Python', 'Updated Feb 24, 2020', '1', 'Jupyter Notebook', 'Updated Jun 14, 2020', 'MIT license', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'PHP', 'MIT license', 'Updated Jan 14, 2020', '3', 'Updated Mar 6, 2020', 'Python', 'Updated Dec 17, 2020']}","{'location': 'Taiwan', 'stats_list': [], 'contributions': '4,192 contributions\n        in the last year', 'description': ['Data Science and Artificial Intelligence Note\nPython\n\nBasic\nData Wrangling\n\nMachine Learning\n\nBasic\nVectors, Matrices, And Arrays\nPreprocessing Structured Data\nPreprocessing Images\nPreprocessing Text\n\nDeep Learning\nReference\nhttps://chrisalbon.com/?fbclid=IwAR0DqM9JoqeDV2JVeLX7Jr3k5SsCax3J2BgDSjDpddI9UnmR8LH1WRs27hU\n'], 'url_profile': 'https://github.com/Offliners', 'info_list': ['Python', 'Updated Jan 13, 2020', '1', 'Python', 'Updated Feb 24, 2020', '1', 'Jupyter Notebook', 'Updated Jun 14, 2020', 'MIT license', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'PHP', 'MIT license', 'Updated Jan 14, 2020', '3', 'Updated Mar 6, 2020', 'Python', 'Updated Dec 17, 2020']}","{'location': 'Torino', 'stats_list': [], 'contributions': '129 contributions\n        in the last year', 'description': ['Landmark Recognition\nLandmark Recognition is a large-scale classification task\nwhich is raising the interest of machine learners from all\nover the world for its peculiar challenging aspects. A huge\namount of landmarks have been captured and gathered in\nthe Google Landmark Dataset. This extreme classification\nscenario is combined with the necessity to confirm the correctness\nof the obtained prediction, since the test set contains\n’tricky’ samples.\nTo do so, a retrieval mechanism\nbased on Deep Local Features is implemented. This solution\nprovides a full pipeline composed by three main stages:\n\nA pre-processing and filtering step to\ndeal with constraints imposed by both time and computational\nresources available;\nThe classification phase has been performed with a ResNet50 model exploiting transfer\nlearning from ImageNet dataset;\nDELF module has been adapted to our specific application with a thresholdbased\ndecision system for an efficient verification of the predictions.\n\nIn the following paper the results will be analyzed.\n'], 'url_profile': 'https://github.com/iliodipietro', 'info_list': ['Python', 'Updated Jan 13, 2020', '1', 'Python', 'Updated Feb 24, 2020', '1', 'Jupyter Notebook', 'Updated Jun 14, 2020', 'MIT license', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'PHP', 'MIT license', 'Updated Jan 14, 2020', '3', 'Updated Mar 6, 2020', 'Python', 'Updated Dec 17, 2020']}","{'location': 'Montreal', 'stats_list': [], 'contributions': '2,614 contributions\n        in the last year', 'description': ['maiart\n\nMaster in Art - Artificial Intelligence\n\nX\nx 200117 1528 - Receive variations\n--@STCGoal TouchDesigner and Arduino Talks to each other\n\nx/x__arduino_td__200117/README.md\n\n09\n\nIntegrating another channel to control rotation of the noize : FAILED - Can not connect param, sucks I know !!\n\n10\n\nINtegrating RENDER\n\n'], 'url_profile': 'https://github.com/jgwill', 'info_list': ['Python', 'Updated Jan 13, 2020', '1', 'Python', 'Updated Feb 24, 2020', '1', 'Jupyter Notebook', 'Updated Jun 14, 2020', 'MIT license', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'PHP', 'MIT license', 'Updated Jan 14, 2020', '3', 'Updated Mar 6, 2020', 'Python', 'Updated Dec 17, 2020']}","{'location': 'Vienna, Austria', 'stats_list': [], 'contributions': '2 contributions\n        in the last year', 'description': ['ARTICLE\nARTificial IntelligenCe powered Literature sEarch\nhttps://github.com/tyiannak/pyScholar/blob/master/pyScholar.py\n'], 'url_profile': 'https://github.com/TheCabbageBaggage', 'info_list': ['Python', 'Updated Jan 13, 2020', '1', 'Python', 'Updated Feb 24, 2020', '1', 'Jupyter Notebook', 'Updated Jun 14, 2020', 'MIT license', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'PHP', 'MIT license', 'Updated Jan 14, 2020', '3', 'Updated Mar 6, 2020', 'Python', 'Updated Dec 17, 2020']}","{'location': 'Zurich, Switzerland', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/tucaman', 'info_list': ['Python', 'Updated Jan 13, 2020', '1', 'Python', 'Updated Feb 24, 2020', '1', 'Jupyter Notebook', 'Updated Jun 14, 2020', 'MIT license', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'PHP', 'MIT license', 'Updated Jan 14, 2020', '3', 'Updated Mar 6, 2020', 'Python', 'Updated Dec 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': [""AIProject_FraudDetection\nProject on Artificial Intelligence Course. A model tailored to the exact business that detect fraud users.\nRead more in the report\nGetting Started\nYou don't need to clone this project to run it! However, we are not able to provide you the full data due to the NDA.\nThe part of the code that you can run is marked with --REPRODUCIBLE PART--. Thus you can check how the saved models perform on the test data.\nPrerequisites\nRead more in requirements.txt\nRunning the code\nFor accessing the saved models and test data, click here. You need to save it in your Drive in order that Colab could access the files.\nAfter that you should mount your drive in Colab(cell in our notebook with code as below) and run imports(next cell).\nYou can scroll through the non-reproducible part and still see the code results.\nfrom google.colab import drive\ndrive.mount('/content/drive/')\n\nBuilt With\n\nGoogle Colab - Jupyter notebook environment that runs in the cloud\n\nAuthors\n\nSofiya Hevorhyan - SofiyaHevorhyan\nOlya Zubyk - olyazub\n\nAcknowledgments\n\nBase of this project was developed during the course of internship at Corevalue Inc.\nSpecial thanks to our mentor, Olena Domanska\n\n""], 'url_profile': 'https://github.com/SofiyaHevorhyan', 'info_list': ['Python', 'Updated Jan 13, 2020', '1', 'Python', 'Updated Feb 24, 2020', '1', 'Jupyter Notebook', 'Updated Jun 14, 2020', 'MIT license', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'PHP', 'MIT license', 'Updated Jan 14, 2020', '3', 'Updated Mar 6, 2020', 'Python', 'Updated Dec 17, 2020']}","{'location': 'Royaume uni des pays bastille', 'stats_list': [], 'contributions': '19,375 contributions\n        in the last year', 'description': [""What is QuanticoDB ?\nQuanticoDB is a fast, simple ( 4 command ), Fully Encrypted NoSQL DB + A.I. with auto training, can talk and learn from: Google Prediction - Microsoft Cognitive - IBM Watson with one simple language.\nrequire 'Quantico.php';\n\nQuantico\\DB::in();  // --- insert data\nQuantico\\DB::ver(); // --- verify data\nQuantico\\DB::del(); // --- delete data\nQuantico\\DB::out(); // --- extract data\n\n// *******************\n// ** Best Practice **\n// *******************\n\nuse Quantico as Q;\n\nQ\\DB::in();  // ---------- insert data\nQ\\DB::ver(); // ---------- verify data\nQ\\DB::del(); // ---------- delete data\nQ\\DB::out(); // ---------- extract data\nRequirements\nQuanticoDB requires Linux + PHP 5.6 or greater.\n\n\nLicense\nQuanticoDB is released under the MIT license.\n\n\nContributing\nArtificial Intelligence & language translations are welcome.\n\n\nDocumentation\nI'm working to Wiki ... keep watching QuanticoDB.\n\n""], 'url_profile': 'https://github.com/nondejus', 'info_list': ['Python', 'Updated Jan 13, 2020', '1', 'Python', 'Updated Feb 24, 2020', '1', 'Jupyter Notebook', 'Updated Jun 14, 2020', 'MIT license', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'PHP', 'MIT license', 'Updated Jan 14, 2020', '3', 'Updated Mar 6, 2020', 'Python', 'Updated Dec 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '91 contributions\n        in the last year', 'description': ['AI/ML Learning Path\nA list of (mostly free) ressources to learn Artificial Intelligence and Machine Learning.\nUniversity Courses\nThe following courses have their slide, notes, homework, and/or videos available online:\n\nCS188: Intro to AI, based on Artificial Intelligence: A Modern Approach\nCS229: Machine Learning\nCS230: Deep Learning\nCS231n: Convolutional Neural Networks for Visual Recognition\nCS224n: Natural Language Processing with Deep Learning\nCS285: Deep Reinforcement Learning\nCS246: Mining of Massive Datasets\nStat212b: Topics Course on Deep Learning\n\nOnline Courses\n\nFast.ai\nOpenAI Spinning Up\n\nBooks\n\n🎁 Free The Hundred-Page Machine Learning Book\nHands-On Machine Learning  with Scikit-Learn & TensorFlow, the associated Github repository has tutorials as Jupyter notebooks.\nDeep Learning with Python, by the creator of Keras\n\nTextbooks\nMathematical background for machine learning (but you probably want to look elsewhere if you are not used to maths):\n\n🎁 Free Mathematics for Machine Learning\n\nTo have a general tour of techniques in artificial intelligence read AIMA:\n\nArtificial Intelligence: A Modern Approach\n\nUse these textbooks to get a grasp of machine and statistical learning:\n\n🎁 Free Foundations of Data Science\n🎁 Free A Course in Machine Learning\n🎁 Free Elements of Statistical Learning\n🎁 Free Pattern Recognition and Machine Learning\nMachine Learning: A Probabilistic Perspective\nLearning from Data, starts to be old and is not enough by itself but can help understand the very basics of the VC\n\nLearn deep learning with these:\n\n🎁 Free Neural Networks and Deep Learning\n🎁 Free Deep Learning\n\nIf you want to learn reinforcement learning, this one is the reference:\n\n🎁 Free Reinforcement Learning, 2nd Edition\n\nFor data mining specific methods:\n\n🎁 Free Mining of Massive Datasets\n\nPapers\nTo get into Deep Reinforcement Learning, this review is nice:\n\nAn Introduction to Deep Reinforcement Learning\n\nUse the Arxiv Sanity Preserver to find new papers to read.\nInteresting Blog Posts\n\nDeep Learning: Our Miraculous Year 1990-1991\nBetter Language Models\nand Their Implications\nEmergent Tool Use from\nMulti-Agent Interaction\n\nFrameworks/Libraries\nnumpy and pandas are essential, numpy is basically everything math-related whereas pandas is all about manipulating data.\n\nnumpy\npandas\n\nscikit-learn is part of the bigger scipy framework but works as a standalone. It contains a lot of non-deep learning machine learning algorithm and many preprocessing methods. Perfect for learning linear/logistic regression, SVMs, or tree-based methods.\n\nscikit-learn\n\nPyTorch and TensorFlow are pretty much competitors, chose one and implement everything with it. Keep a distant eye on the other.\n\nPyTorch\nTensorFlow\n\nKeras is a layer of abstraction on top of TensorFlow, CNTK or Theano (which it uses as a backend).\n\nKeras\n\n'], 'url_profile': 'https://github.com/Qu3tzal', 'info_list': ['Python', 'Updated Jan 13, 2020', '1', 'Python', 'Updated Feb 24, 2020', '1', 'Jupyter Notebook', 'Updated Jun 14, 2020', 'MIT license', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'PHP', 'MIT license', 'Updated Jan 14, 2020', '3', 'Updated Mar 6, 2020', 'Python', 'Updated Dec 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '267 contributions\n        in the last year', 'description': ['AI For Connect 4\n\nA Connect 4 AI which uses the minimax algorithm and alpha beta pruning to search for the possible best move. A transposition table is incorporated storing previous calculations and iterative deepening is used to make the AI search progressively deeper as the transposition table becomes larger. The pygame module is used for the GUI in which the player can compete with the AI on a 6 x 7 board. The difficulty of the AI can be adjusted by changing the time available to the AI.\n\nUsage\nTo run the script vs AI:\n$ python Connect4_AI.py\nSee the section on adjustable parameters to change behaviour of the AI\nThe transposition and zobrists tables are already present with game data.\nTo reset the tables:\n$ python Cache_Init.py\nor, delete the cachetable.pickle and zobtable.pickle files.\nRunning the AI script will automatically check for these files and run the Cache_Init.py script if the files are not found.\nTo run the 2 player script:\n$ python Connect4_Basic.py\nDependencies\nThe following external modules are used\n\npygame\nnumpy\n\nCommands for installation of pygame 2.0 and numpy\n$ pip install pygame\n$ pip install numpy\nSearching algorithms\nMinimax\n\nThis AI uses the minimax algorithm to search for all possible board outcomes and returns the best move which will result in the best scoring board position if the AI always plays the move maximising its chance of winning and the opponent player always plays the move minimising the AI’s chance of winning. The minimax will go down each possible board position and recursively score that board position using the minimax algorithm at a lower depth, until all board positions and their scores up to a given depth are calculated. The AI will then return the best board position if it is the AI’s move, and return the worst board position if it is the player’s move. The algorithm will therefore finally return the best move to play at the current moment to maximise the score of the eventual board position if the player plays the best moves as expected.\n\nAlpha Beta Pruning\n\nThe minimax algorithm, when used naively, is slow and inefficient as it explores all possible outcomes up to certain depth, meaning with each depth the search time increases 7x. Alpha beta pruning significantly reduces the search time by eliminating branches of the search that will never be explored as it is undesirable for either the AI, or the player, and so the minimax will never have returned a final value from that branch. The nature of the algorithm means if the best move is found early in the search, more branches can be disregarded. Therefore the searching algorithm is altered to search moves form the centre column first, then spread the search outwards to neighbouring columns, as the best move is more likely to be nearer to the centre.\n\nScoring Function\nBasic Scoring System\n\nThe AI uses a scoring function to assess board positions in order to implement the minimax algorithm. After testing different values, the following were found to work the best:\n\n3 AI and 1 EMPTY  ------>  +5\n2 AI and 2 EMPTY  ------>  +2\n\n3 PLAYER and 1 EMPTY  -->  -5\n2 PLAYER and 2 EMPTY  -->  -2\n\n\nAlongside this basic scoring system, the odd-even strategy was implemented to more accurately calculate the value of a given position so the AI can improve its decision making.\n\nOdd-Even Strategy\n\nThe odd-even strategy dictates that is is favourable for the odd player to have a potential win with an empty position which completes the win on an odd row, with the opposite true for the even player. The odd player is the player that starts first and vice versa for the even player. This is because when the board becomes full the other player will be forced to play their coin in the column that will give the opponent the win. Therefore the lower down the empty space is, the better. The power the strategy means it must be scored highly:\n\nOdd-Even for AI  ------->  Distance from top * 100\nOdd-Even for PLAYER  --->  Distance from top * -100\n\nWinning Move\n\nThe winning move must be given ultimate value as it is the aim of the game.\n\nWinning Move  ---------->  +1000000\nLoosing Move  ---------->  -1000000\n\n\nThe AI can be optimised by anticipating a win when the board is in a position in which any one player can win the game in the next move. This effectively means that a depth 4 AI can peak into what will happen at depth 5, and can prevent a loss or push to a win.\n\nAnticiapted Win  ------->  +100000\nAnticiapted Loss  ------>  -100000\n\n\nThis value is 10x less than the value give to a win, because is must be much greater than all other board positions, however a win/loss is still prioritised over an imminent win/loss as the AI must not opt for a potential win and loose the game as a result.\n\nFurther Optimisations\nTransposition table\n\nThe transposition table uses zobrist hashing to store hash values of board positions calculated during the game to save the AI from starting from scratch every time it is called to calculate the best move. The transposition table stores the hash of the board, the score of the board, and the calculation depth that has resulted in the score. The transposition table is referred to every time the minimax algorithm is assessing a board, and if the board hash is in the table and the calculation depth is greater than or equal to the depth of calculation the minimax algorithm must perform, the score stored in the table is used and the calculation doesn’t need to be done. If the table does not contain a hash, or the table contains a hash with a lower corresponding depth than the one calculated, the entry in the table is (over)written as the new calculation. The calculations made in previous games are saved in the transposition table so the minimax can build upon the previous calculations instead of repeating them in future games.\n\nIterative Deepening\n\nUsing the introduction of the transposition table, iterative deepening can be implemented. Instead of limiting the depth of the search, the time available to the AI can be the limiting factor. The AI will iterative increase the depth of the search until the time available is up, at which point the AI is terminated and the move returned by the highest depth that could be calculated in the given time is used as the final move.\n\nAdjustable Parameters\nThe time available to the AI in seconds can be adjusted to any integer value. A higher time value means the AI will achieve a higher depth of calculation.\n# maximum seconds AI can take\nAI_TIME = 6\nThis can be switched to true to train the AI by making it playing itself. This will improve the AI by increasing the number or boards stored in the transposition table.\nAI_VS_AI = False\nThe play order can be adjusted to give the first move to the PLAYER or to the AI. The order of play is randomised by default.\nPLAY_ORDER = [PLAYER, AI]\n\n# PLAY_ORDER.reverse()\nrandom.shuffle(PLAY_ORDER)\n'], 'url_profile': 'https://github.com/dhruvnps', 'info_list': ['Python', 'Updated Jan 13, 2020', '1', 'Python', 'Updated Feb 24, 2020', '1', 'Jupyter Notebook', 'Updated Jun 14, 2020', 'MIT license', 'Updated Feb 29, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Updated Jan 16, 2020', 'Jupyter Notebook', 'Updated Apr 26, 2020', 'PHP', 'MIT license', 'Updated Jan 14, 2020', '3', 'Updated Mar 6, 2020', 'Python', 'Updated Dec 17, 2020']}"
"{'location': 'Cleveland, OH', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/annasedla', 'info_list': ['R', 'Updated Jan 15, 2020', 'Python', 'Updated Jan 19, 2020', 'C++', 'Updated Feb 13, 2020', 'Python', 'Updated Jan 19, 2020', 'Java', 'Updated Jan 29, 2020', 'Python', 'Updated Jan 15, 2020', '1', 'C#', 'Updated Jan 16, 2020', '1', 'Python', 'Updated Jan 31, 2020', 'Java', 'Updated Jan 19, 2020', 'C++', 'Updated Jan 19, 2020']}","{'location': 'İstanbul', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/simalcubuk', 'info_list': ['R', 'Updated Jan 15, 2020', 'Python', 'Updated Jan 19, 2020', 'C++', 'Updated Feb 13, 2020', 'Python', 'Updated Jan 19, 2020', 'Java', 'Updated Jan 29, 2020', 'Python', 'Updated Jan 15, 2020', '1', 'C#', 'Updated Jan 16, 2020', '1', 'Python', 'Updated Jan 31, 2020', 'Java', 'Updated Jan 19, 2020', 'C++', 'Updated Jan 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['Minimax_Checkers\n\n\nMinimax based artificial intelligence that plays checkers programmed in C++ and C\n\n\nVariable depth of future moves the minimax algorithm will analyze, anything higher than 8 runs slowly\n\n\nAlpha beta pruning is used to optimize runtime when searching ahead a large number of moves\n\n\nGame is displayed on a 32x32 LED matrix controlled by an Arduino Mega\n\n\nAlso contains Player vs. Player gamemode\n\n\nFull project report for Software Engineering 101 class included in repository\n\n\nCreated alongside Connor Byers, Nick Makharinets, and Andrew Wang\n\n\n'], 'url_profile': 'https://github.com/n-faria', 'info_list': ['R', 'Updated Jan 15, 2020', 'Python', 'Updated Jan 19, 2020', 'C++', 'Updated Feb 13, 2020', 'Python', 'Updated Jan 19, 2020', 'Java', 'Updated Jan 29, 2020', 'Python', 'Updated Jan 15, 2020', '1', 'C#', 'Updated Jan 16, 2020', '1', 'Python', 'Updated Jan 31, 2020', 'Java', 'Updated Jan 19, 2020', 'C++', 'Updated Jan 19, 2020']}","{'location': 'İstanbul', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/simalcubuk', 'info_list': ['R', 'Updated Jan 15, 2020', 'Python', 'Updated Jan 19, 2020', 'C++', 'Updated Feb 13, 2020', 'Python', 'Updated Jan 19, 2020', 'Java', 'Updated Jan 29, 2020', 'Python', 'Updated Jan 15, 2020', '1', 'C#', 'Updated Jan 16, 2020', '1', 'Python', 'Updated Jan 31, 2020', 'Java', 'Updated Jan 19, 2020', 'C++', 'Updated Jan 19, 2020']}","{'location': 'Poznań', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/aleksykrolczyk', 'info_list': ['R', 'Updated Jan 15, 2020', 'Python', 'Updated Jan 19, 2020', 'C++', 'Updated Feb 13, 2020', 'Python', 'Updated Jan 19, 2020', 'Java', 'Updated Jan 29, 2020', 'Python', 'Updated Jan 15, 2020', '1', 'C#', 'Updated Jan 16, 2020', '1', 'Python', 'Updated Jan 31, 2020', 'Java', 'Updated Jan 19, 2020', 'C++', 'Updated Jan 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/federica-collab', 'info_list': ['R', 'Updated Jan 15, 2020', 'Python', 'Updated Jan 19, 2020', 'C++', 'Updated Feb 13, 2020', 'Python', 'Updated Jan 19, 2020', 'Java', 'Updated Jan 29, 2020', 'Python', 'Updated Jan 15, 2020', '1', 'C#', 'Updated Jan 16, 2020', '1', 'Python', 'Updated Jan 31, 2020', 'Java', 'Updated Jan 19, 2020', 'C++', 'Updated Jan 19, 2020']}","{'location': 'Jakarta, Indonesia', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['Bang Bang\nBang Bang adalah sebuah game FPS yang dibuat menggunakan Unity. Game ini memiliki genre Survival, dimana Player harus bertahan hidup dengan cara membunuh musuh-musuh yang akan respawn terus-menerus. Bang Bang juga menggunakan sistem skor, dimana setiap saat Player membunuh Enemy, maka skor Player akan bertambah sesuai dengan jenis musuh yang dibunuh.\nOur Team\n\n\n2101628783 -\nKristi Handayani -\nhttps://www.linkedin.com/in/kristi-handayani-9a11a5178\n\n\n2101651786 -\nRio Rafelino -\nhttps://www.linkedin.com/in/rio-rafelino-91a30a18b\n\n\n2101665450 -\nHans Michael -\nhttps://www.linkedin.com/in/hansmichaels/\n\n\n2101659025 -\nAnton -\nhttps://www.linkedin.com/in/anton-anton-5951b01a0\n\n\n2101627465 -\nIvan Andi -\nhttps://www.linkedin.com/in/ivan-andi-10a940183/\n\n\n'], 'url_profile': 'https://github.com/hansmichaels', 'info_list': ['R', 'Updated Jan 15, 2020', 'Python', 'Updated Jan 19, 2020', 'C++', 'Updated Feb 13, 2020', 'Python', 'Updated Jan 19, 2020', 'Java', 'Updated Jan 29, 2020', 'Python', 'Updated Jan 15, 2020', '1', 'C#', 'Updated Jan 16, 2020', '1', 'Python', 'Updated Jan 31, 2020', 'Java', 'Updated Jan 19, 2020', 'C++', 'Updated Jan 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '13 contributions\n        in the last year', 'description': ['Matt Tetreau\nTim Van Dyke\nChristian Lundy\nJiri Hoffmann\n'], 'url_profile': 'https://github.com/tetreaum', 'info_list': ['R', 'Updated Jan 15, 2020', 'Python', 'Updated Jan 19, 2020', 'C++', 'Updated Feb 13, 2020', 'Python', 'Updated Jan 19, 2020', 'Java', 'Updated Jan 29, 2020', 'Python', 'Updated Jan 15, 2020', '1', 'C#', 'Updated Jan 16, 2020', '1', 'Python', 'Updated Jan 31, 2020', 'Java', 'Updated Jan 19, 2020', 'C++', 'Updated Jan 19, 2020']}","{'location': 'Los Angeles', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': ['CSCI-561\nAssignments for Artificial Intelligence course- Fall 2019\n'], 'url_profile': 'https://github.com/nidhi-86', 'info_list': ['R', 'Updated Jan 15, 2020', 'Python', 'Updated Jan 19, 2020', 'C++', 'Updated Feb 13, 2020', 'Python', 'Updated Jan 19, 2020', 'Java', 'Updated Jan 29, 2020', 'Python', 'Updated Jan 15, 2020', '1', 'C#', 'Updated Jan 16, 2020', '1', 'Python', 'Updated Jan 31, 2020', 'Java', 'Updated Jan 19, 2020', 'C++', 'Updated Jan 19, 2020']}","{'location': 'Orange County, California', 'stats_list': [], 'contributions': '129 contributions\n        in the last year', 'description': ['Sudoku-Game-Solver\nProject for CS271P Intro to Artificial Intelligence\n'], 'url_profile': 'https://github.com/yulinzhang0822', 'info_list': ['R', 'Updated Jan 15, 2020', 'Python', 'Updated Jan 19, 2020', 'C++', 'Updated Feb 13, 2020', 'Python', 'Updated Jan 19, 2020', 'Java', 'Updated Jan 29, 2020', 'Python', 'Updated Jan 15, 2020', '1', 'C#', 'Updated Jan 16, 2020', '1', 'Python', 'Updated Jan 31, 2020', 'Java', 'Updated Jan 19, 2020', 'C++', 'Updated Jan 19, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '195 contributions\n        in the last year', 'description': ['ai-projects-uni\nArtificial Intelligence - projects for University Course\nThis project contains my projects, which were creted for certain University courses.\nIntro to AI\nThis course was as an intoductory subject for AI and ML. We have been learning there about Machine Learning algoroithms and implementing some of them in Jupyter Notebook]\nArtificial Life with Cognitive Science\nThis course teached us about basics of simulation and evolutionary algorithms\n'], 'url_profile': 'https://github.com/trebacz626', 'info_list': ['Jupyter Notebook', 'Updated Mar 4, 2021', 'Python', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'Updated Jul 2, 2020', '1', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Updated Jan 14, 2020', 'Python', 'Updated Jun 14, 2020', 'Java', 'Updated Feb 3, 2020', '2', 'HTML', 'Updated Oct 4, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020']}","{'location': 'Bryan, Texas', 'stats_list': [], 'contributions': '88 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/achaar', 'info_list': ['Jupyter Notebook', 'Updated Mar 4, 2021', 'Python', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'Updated Jul 2, 2020', '1', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Updated Jan 14, 2020', 'Python', 'Updated Jun 14, 2020', 'Java', 'Updated Feb 3, 2020', '2', 'HTML', 'Updated Oct 4, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020']}","{'location': 'İstanbul', 'stats_list': [], 'contributions': '18 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/simalcubuk', 'info_list': ['Jupyter Notebook', 'Updated Mar 4, 2021', 'Python', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'Updated Jul 2, 2020', '1', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Updated Jan 14, 2020', 'Python', 'Updated Jun 14, 2020', 'Java', 'Updated Feb 3, 2020', '2', 'HTML', 'Updated Oct 4, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '615 contributions\n        in the last year', 'description': ['AI-Accessed-Excercise\nAssessed Exercise for Artificial Intelligence (H)\n'], 'url_profile': 'https://github.com/iShauny', 'info_list': ['Jupyter Notebook', 'Updated Mar 4, 2021', 'Python', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'Updated Jul 2, 2020', '1', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Updated Jan 14, 2020', 'Python', 'Updated Jun 14, 2020', 'Java', 'Updated Feb 3, 2020', '2', 'HTML', 'Updated Oct 4, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020']}","{'location': 'New York, NY', 'stats_list': [], 'contributions': '24 contributions\n        in the last year', 'description': ['Music-AI-LSTM\nArtificial Intelligence for Music Generation, Columbia University\n\n\n\n\n\n\n\n\n\n\n\n'], 'url_profile': 'https://github.com/skandupmanyu', 'info_list': ['Jupyter Notebook', 'Updated Mar 4, 2021', 'Python', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'Updated Jul 2, 2020', '1', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Updated Jan 14, 2020', 'Python', 'Updated Jun 14, 2020', 'Java', 'Updated Feb 3, 2020', '2', 'HTML', 'Updated Oct 4, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/gurmeetsdigitalworld', 'info_list': ['Jupyter Notebook', 'Updated Mar 4, 2021', 'Python', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'Updated Jul 2, 2020', '1', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Updated Jan 14, 2020', 'Python', 'Updated Jun 14, 2020', 'Java', 'Updated Feb 3, 2020', '2', 'HTML', 'Updated Oct 4, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020']}","{'location': 'Torino', 'stats_list': [], 'contributions': '129 contributions\n        in the last year', 'description': ['K-Nearest Neighbors (KNN) and Support Vector Machines (SVMs)\nK-Nearest Neighbors (KNN) and Support Vector Machines (SVMs) are a set of learning methods used for\nregression and classification (also density estimation in the case of KNN). The aim of this experience is to apply\nboth KNN and SVM to the same dataset, analysing a set of parameters (k in the case of KNN and C, Gamma for\nSVM) in order to achieve the best accuracy on the validation set and then trying to predict correctly the labels\non the test set. At the end, K-fold Cross-Validation will be applied to improve the quality of the classification.\n'], 'url_profile': 'https://github.com/iliodipietro', 'info_list': ['Jupyter Notebook', 'Updated Mar 4, 2021', 'Python', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'Updated Jul 2, 2020', '1', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Updated Jan 14, 2020', 'Python', 'Updated Jun 14, 2020', 'Java', 'Updated Feb 3, 2020', '2', 'HTML', 'Updated Oct 4, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '4 contributions\n        in the last year', 'description': ['CSCI 446 Artificial intelligence assignement 1\nAssignement goal\nThe goal of this assignment was to generate a map randomly: a set of points on a plane connected to each other. Then, to color each point so the connected points do not have the same color. The four-color theorem states that for a planar a graph, such a coloring is alway possible with at most 4 colors.\nAbstract\nIn order to solve the planar graph coloring problem, a CSP, we implemented five different algorithms on a randomly generated graph. The first 3 algorithms build off of each other. All three of the backtracking algorithms reliably give a correct graph coloring. But, the difference comes in the resources required for each. Simple backtracking is horribly expensive because it has no checks and balances. This is where forward checking improves the algorithm, but in this application, it does very little good after about 50 regions. Implementing arc consistency provides a huge leap in performance and resources needed. Performing much better than initially anticipated. Even up to 100 regions, the algorithm swiftly delivers the correct coloring. Genetic is a very interesting approach but seems unadapted for this problem. The algorithm will not provide consistent assignment of colors for a graph with size 30, this is due to the unguided nature of the search, which is driven by fitness, but occurs with random mutations. Simulated annealing performs much better as the mutations are not random but based on a min conflict heuristic. The space is also explored more thoroughly because of the possibility of accepting a worse state. This minimizes the local optima problem.\nScreenshots\n\n\n'], 'url_profile': 'https://github.com/olimar718', 'info_list': ['Jupyter Notebook', 'Updated Mar 4, 2021', 'Python', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'Updated Jul 2, 2020', '1', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Updated Jan 14, 2020', 'Python', 'Updated Jun 14, 2020', 'Java', 'Updated Feb 3, 2020', '2', 'HTML', 'Updated Oct 4, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': 'NONE', 'description': ['NONE'], 'url_profile': 'https://github.com/AIDA-UIUC', 'info_list': ['Jupyter Notebook', 'Updated Mar 4, 2021', 'Python', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'Updated Jul 2, 2020', '1', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Updated Jan 14, 2020', 'Python', 'Updated Jun 14, 2020', 'Java', 'Updated Feb 3, 2020', '2', 'HTML', 'Updated Oct 4, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '30 contributions\n        in the last year', 'description': ['AITND-P1-TRADING-WITH-MOMENTUM\nThis is the first project in the nanodegree, artificial intelligence for trading\nThe aim is to implement a trading strategy and test to see if has the potential to be profitable. Supplied are a universe of stocks and time range.\nThe project requires to implement some functions:\nResample Adjusted Prices\nThe trading signal for this project isn\'t based on daily prices, and therefore a resample of the daily adjusted closing prices into monthly buckets is performed\nCompute Log Returns\nCompute the log returns (Rt) from prices (Pt) for each ticker and date\nRt = log e(Pt) - log e(Pt-1)\nShift Returns\nImplement a shift_return function to shift the log returns to the previous or future returns in the time series. The parameter shift_n is 2 and returns is the following and shifts the stocks in the future, if it\'s a negative number, the stock are shift in the past\nGenerate Trading signal\nA trading signal is a sequence of trading actions, or results that can be used to take trading actions. A common form is to produce a ""long"" and ""short"" portfolio of stocks on each date (e.g. end of each month, or whatever frequency you desire to trade at). This signal can be interpreted as rebalancing your portfolio on each of those dates, entering long (""buy"") and short (""sell"") positions as indicated.\nWe tried to implement the following:\nFor each month-end observation period, rank the stocks by previous returns, from the highest to the lowest. Select the top performing stocks for the long portfolio, and the bottom performing stocks for the short portfolio.\nImplementing the get_top_n function is to get the top performing stock for each month.\nProjected Returns\nHere we check the trading signal if it has the potential to become profitable.\nTherefore we compute the net returns that this portfolio would return. For simplicity, we\'ll assume every stock gets an equal dollar amount of investment. This makes it easier to compute a portfolio\'s returns as the simple arithmetic average of the individual stock returns.\nImplement the portfolio_returns function to compute the expected portfolio returns. Using df_long to indicate which stocks to long and df_short to indicate which stocks to short, calculate the returns using lookahead_returns.\nT-TEST\nThe null hypothesis ( 𝐻0 ) that the actual mean can return from the signal is zero. We  performed a one-sample, one-sided t-test on the observed mean return, to see if it can reject 𝐻0.\nFirst compute the t-statistic and then find it\'s corresponding p-value. The p-value will indicate the profitability of observing a t-statistic equally or more extreme than the one that is observed in the null hypothesis were true.\nA small p-value means that the chance of observing the t-statistic we observed under the null hypothesis is small, and thus casts doubt on the null hypothesis. It\'s good practice to set a desired level of significance or alpha ( 𝛼 ) before computing the p-value, and then reject the null hypothesis if  𝑝 < 𝛼.\nFor this project, we\'ll use  𝛼=0.05 , since it\'s a common value to use\nRESULT\nIn our test the alpha was set as 0.05, the result above show a value (0.073359) > 0.05, therefore we cannot reject the null hypothesis and moreover we can conclude that the strategy doesn\'t contains an alpha.\n'], 'url_profile': 'https://github.com/CDA70', 'info_list': ['Jupyter Notebook', 'Updated Mar 4, 2021', 'Python', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'Updated Jul 2, 2020', '1', 'Jupyter Notebook', 'Updated Jan 29, 2020', 'Updated Jan 14, 2020', 'Python', 'Updated Jun 14, 2020', 'Java', 'Updated Feb 3, 2020', '2', 'HTML', 'Updated Oct 4, 2020', 'Jupyter Notebook', 'Updated Jun 4, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '142 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/RUGMJ7443', 'info_list': ['JavaScript', 'Updated Jan 18, 2020', 'Python', 'Updated Jan 19, 2020', 'MIT license', 'Updated Jan 18, 2020', 'Common Lisp', 'Updated Jan 18, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'Updated Apr 6, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'C++', 'Updated May 28, 2020', 'Python', 'Updated Nov 29, 2019']}","{'location': 'Berkeley, CA', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ericjortiz', 'info_list': ['JavaScript', 'Updated Jan 18, 2020', 'Python', 'Updated Jan 19, 2020', 'MIT license', 'Updated Jan 18, 2020', 'Common Lisp', 'Updated Jan 18, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'Updated Apr 6, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'C++', 'Updated May 28, 2020', 'Python', 'Updated Nov 29, 2019']}","{'location': 'Bangkok, Thailand', 'stats_list': [], 'contributions': '135 contributions\n        in the last year', 'description': ['Udacity Build a Sudoku Solver\nFirst Project from the Udacity Artificial Intelligence Nanodegree\nSynopsis\nIn this project, students will extend the Sudoku-solving agent developed in the classroom lectures to solve diagonal Sudoku puzzles. A diagonal Sudoku puzzle is identical to traditional Sudoku puzzles with the added constraint that the boxes on the two main diagonals of the board must also contain the digits 1-9 in each cell (just like the rows, columns, and 3x3 blocks).\nInstructions\nFollow the instructions in the classroom lesson to install and configure the AIND Anaconda environment. That environment includes several important packages that are used for the project.\n'], 'url_profile': 'https://github.com/AekachaiTang', 'info_list': ['JavaScript', 'Updated Jan 18, 2020', 'Python', 'Updated Jan 19, 2020', 'MIT license', 'Updated Jan 18, 2020', 'Common Lisp', 'Updated Jan 18, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'Updated Apr 6, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'C++', 'Updated May 28, 2020', 'Python', 'Updated Nov 29, 2019']}","{'location': 'Kansas City, MO', 'stats_list': [], 'contributions': '2,340 contributions\n        in the last year', 'description': ['Paradigms of Artificial Intelligence Programming\nby Peter Norvig\n\nJust some notes I took while reading the book for tinkering along with his examples.\nThis will not be useful to anyone; just something I want to keep in case I revisit the chapters in the future.\n'], 'url_profile': 'https://github.com/Fedreg', 'info_list': ['JavaScript', 'Updated Jan 18, 2020', 'Python', 'Updated Jan 19, 2020', 'MIT license', 'Updated Jan 18, 2020', 'Common Lisp', 'Updated Jan 18, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'Updated Apr 6, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'C++', 'Updated May 28, 2020', 'Python', 'Updated Nov 29, 2019']}","{'location': 'Berkeley, CA', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/ericjortiz', 'info_list': ['JavaScript', 'Updated Jan 18, 2020', 'Python', 'Updated Jan 19, 2020', 'MIT license', 'Updated Jan 18, 2020', 'Common Lisp', 'Updated Jan 18, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'Updated Apr 6, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'C++', 'Updated May 28, 2020', 'Python', 'Updated Nov 29, 2019']}","{'location': 'Reutlingen, Germany', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['brain\nB.R.A.I.N (Best Reutlingen Artificial Intelligence Network)\n'], 'url_profile': 'https://github.com/rtlion', 'info_list': ['JavaScript', 'Updated Jan 18, 2020', 'Python', 'Updated Jan 19, 2020', 'MIT license', 'Updated Jan 18, 2020', 'Common Lisp', 'Updated Jan 18, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'Updated Apr 6, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'C++', 'Updated May 28, 2020', 'Python', 'Updated Nov 29, 2019']}","{'location': 'NONE', 'stats_list': [], 'contributions': '20 contributions\n        in the last year', 'description': ['ECSE526\nAI for the course ECSE-526 Artificial Intelligence\n'], 'url_profile': 'https://github.com/acoulombe', 'info_list': ['JavaScript', 'Updated Jan 18, 2020', 'Python', 'Updated Jan 19, 2020', 'MIT license', 'Updated Jan 18, 2020', 'Common Lisp', 'Updated Jan 18, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'Updated Apr 6, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'C++', 'Updated May 28, 2020', 'Python', 'Updated Nov 29, 2019']}","{'location': 'Beijing, China', 'stats_list': [], 'contributions': '218 contributions\n        in the last year', 'description': ['Fundamentals_of_AI\nCourse projects and reports for Fundamentals_of_AI by Prof. @ZhangChangshui and Prof. @JiangRui from Department of Automation in Tsinghua University. Project details in respective folders.\n'], 'url_profile': 'https://github.com/Iceblaze9527', 'info_list': ['JavaScript', 'Updated Jan 18, 2020', 'Python', 'Updated Jan 19, 2020', 'MIT license', 'Updated Jan 18, 2020', 'Common Lisp', 'Updated Jan 18, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'Updated Apr 6, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'C++', 'Updated May 28, 2020', 'Python', 'Updated Nov 29, 2019']}","{'location': 'NONE', 'stats_list': [], 'contributions': '171 contributions\n        in the last year', 'description': ['AI4Games\nImplementations of various Artificial Intelligence Behaviours for videogames in C++\n'], 'url_profile': 'https://github.com/MarcosJLR', 'info_list': ['JavaScript', 'Updated Jan 18, 2020', 'Python', 'Updated Jan 19, 2020', 'MIT license', 'Updated Jan 18, 2020', 'Common Lisp', 'Updated Jan 18, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'Updated Apr 6, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'C++', 'Updated May 28, 2020', 'Python', 'Updated Nov 29, 2019']}","{'location': 'Montreal, Quebec', 'stats_list': [], 'contributions': '283 contributions\n        in the last year', 'description': ['SentimentClassifier\nCOMP 472 Project 2\n'], 'url_profile': 'https://github.com/SashSubba', 'info_list': ['JavaScript', 'Updated Jan 18, 2020', 'Python', 'Updated Jan 19, 2020', 'MIT license', 'Updated Jan 18, 2020', 'Common Lisp', 'Updated Jan 18, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'Updated Apr 6, 2020', 'Jupyter Notebook', 'Updated Jan 15, 2020', 'C++', 'Updated May 28, 2020', 'Python', 'Updated Nov 29, 2019']}"
"{'location': 'Portugal', 'stats_list': [], 'contributions': '1,398 contributions\n        in the last year', 'description': ['IART\n2019/2020 - 3rd Year, 2nd Semester\nCourse: Inteligência Artificial | Artificial Intelligence\nProjects developed by:\n\nMartim Silva\nLuís Ramos\nFrancisco Gonçalves \n\n\n\n\nProject Number\nProject Name\nDescription\n\n\n\n\n1\nSelf Driving Rides\nPython implementation of optimization algorithms (solution to google hashcode 2018)\n\n\n2\nFootball Predictions\nMachine Learning and Python to predict the outcome of football matches\n\n\n\nDisclaimer - This repository was created for educational purposes and we do not take any responsibility for anything related to its content. You are free to use any code or algorithm you find, but do so at your own risk.\n'], 'url_profile': 'https://github.com/motapinto', 'info_list': ['Python', 'Updated Jul 11, 2020', '1', 'Python', 'Updated Dec 19, 2020', 'Jupyter Notebook', 'Updated Jun 21, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'MIT license', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 13, 2020', 'Python', 'Updated Dec 4, 2020', 'ShaderLab', 'MIT license', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020']}","{'location': 'Waterloo, Ontario', 'stats_list': [], 'contributions': '328 contributions\n        in the last year', 'description': [""Synviz\n\nDevpost submission\nSynviz is an IoT device that uses state of the art artificial intelligence to decode text from the movement of a speaker's mouth.\nInspiration\nThere were two primary sources of inspiration. The first one was a paper published by University of Oxford researchers, who proposed a state of the art deep learning pipeline to extract spoken language from video. The paper can be found here. The repo for the model used as a base template can be found here.\nThe second source of inspiration is an existing product on the market, Focals by North. Focals are smart glasses that aim to put the important parts of your life right in front of you through a projected heads up display. We thought it would be a great idea to build onto a platform like this through adding a camera and using artificial intelligence to gain valuable insights about what you see, which in our case, is deciphering speech from visual input.\nPipeline Overview\n\n\nThe user presses the button on the glasses to start a recording\nThe user clicks the button again to stop recording\nThe data is passed to a Google Cloud Platform bucket as an mp4 file\nSimultaneously, the glasses ping the Flask backend server to let it know there's something to be processed\nThe backend downloads the video file\nThe backend runs the video through a Haar Cascade classifier to detect a face\nThe video is cropped so that it tracks the mouth of the speaker\nThe cropped video is fed through a transformer network to get a transcript\nThe backend passes the transcript and file URL to the frontend through a socket\nThe frontend displays the transcript it got from the backend, and also allows playback of the mp4 file found on Google Cloud Platform\n\n\nUse Cases\n\nFor individuals who are hard-of hearing or deaf\nNoisy environments where automatic speech recognition is difficult\nCombined with speech recognition for ultra-accurate, real-time transcripts\nLanguage learners who want a transcript or translation\n\nSocial Impact\nThis hack can help in situations where communication is difficult. One of the most promising use cases is when\nthis technology is combined with automatic speech recognition. All-in-one solutions for real-time transcription and translation\nare becoming more viable as our technology progresses.\nThis proof-of-concept is another key piece that would\nimprove human computer interaction.\nNext Steps\nWith stronger on-board battery, 5G network connection, and a computationally stronger compute server, we believe it will be possible to achieve near real-time transcription from a video feed that can be implemented on an existing platform like North's Focals to deliver a promising business appeal.\nThe Team\n\n\nWaleed Ahmed - PM, Backend (Flask) & Cloud (GCP)\nSinclair Hudson - Hardware (Raspberry Pi) & Computer Vision (OpenCV)\nMartin Ethier - Deep Learning (TensorFlow)\nWilliam Lu - Frontend (React)\n\n""], 'url_profile': 'https://github.com/w29ahmed', 'info_list': ['Python', 'Updated Jul 11, 2020', '1', 'Python', 'Updated Dec 19, 2020', 'Jupyter Notebook', 'Updated Jun 21, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'MIT license', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 13, 2020', 'Python', 'Updated Dec 4, 2020', 'ShaderLab', 'MIT license', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020']}","{'location': 'Guntur', 'stats_list': [], 'contributions': '150 contributions\n        in the last year', 'description': ['Introduction-to-TensorFlow-for-Artificial-Intelligence-Machine-Learning-and-Deep-Learning-\nLink to course: https://www.coursera.org/learn/introduction-tensorflow/home/welcome\nThis is a 4 week course where you will be introduced to Keras and use it to implement different models and learn about different callbacks.\nLearnt the following concepts:\nWeek1: Working with Google Colab,Introduction to Keras that uses Tensorflow as backend\nWeek2: Working with MNIST Dataset using ANNs,using callbacks to control training of neural network\nWeek3: Working with MNIST Dataset using CNNs\nWeek4: Working with complex Datasets\n'], 'url_profile': 'https://github.com/sonusajid004', 'info_list': ['Python', 'Updated Jul 11, 2020', '1', 'Python', 'Updated Dec 19, 2020', 'Jupyter Notebook', 'Updated Jun 21, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'MIT license', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 13, 2020', 'Python', 'Updated Dec 4, 2020', 'ShaderLab', 'MIT license', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/u8karshgupta', 'info_list': ['Python', 'Updated Jul 11, 2020', '1', 'Python', 'Updated Dec 19, 2020', 'Jupyter Notebook', 'Updated Jun 21, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'MIT license', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 13, 2020', 'Python', 'Updated Dec 4, 2020', 'ShaderLab', 'MIT license', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020']}","{'location': 'Aveiro, Portugal', 'stats_list': [], 'contributions': '213 contributions\n        in the last year', 'description': [""iia-ia-bomberman\nBomberman clone for AI teaching\n\nHow to install\nMake sure you are running Python 3.5.\n$ pip install -r requirements.txt\nTip: you might want to create a virtualenv first\nHow to play\nopen 3 terminals:\n$ python3 server.py\n$ python3 viewer.py\n$ python3 client.py\nto play using the sample client make sure the client pygame hidden window has focus\nKeys\nDirections: arrows\nA: 'a' - detonates (only after picking up the detonator powerup)\nB: 'b' - drops bomb\nDebug Installation\nMake sure pygame is properly installed:\npython -m pygame.examples.aliens\nTested on:\n\nUbuntu 18.04\nOSX 10.14.6\nWindows 10.0.18362\n\nDevelopers:\n\nRenato Valente\nJacinto Luf\n\n""], 'url_profile': 'https://github.com/renatovalente5', 'info_list': ['Python', 'Updated Jul 11, 2020', '1', 'Python', 'Updated Dec 19, 2020', 'Jupyter Notebook', 'Updated Jun 21, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'MIT license', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 13, 2020', 'Python', 'Updated Dec 4, 2020', 'ShaderLab', 'MIT license', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020']}","{'location': 'Tangerang, Banten, INDONESIA', 'stats_list': [], 'contributions': '72 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/NicholasDominic', 'info_list': ['Python', 'Updated Jul 11, 2020', '1', 'Python', 'Updated Dec 19, 2020', 'Jupyter Notebook', 'Updated Jun 21, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'MIT license', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 13, 2020', 'Python', 'Updated Dec 4, 2020', 'ShaderLab', 'MIT license', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020']}","{'location': 'Bucharest', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Madaist', 'info_list': ['Python', 'Updated Jul 11, 2020', '1', 'Python', 'Updated Dec 19, 2020', 'Jupyter Notebook', 'Updated Jun 21, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'MIT license', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 13, 2020', 'Python', 'Updated Dec 4, 2020', 'ShaderLab', 'MIT license', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '112 contributions\n        in the last year', 'description': ['intro-to-ai\n[2019 Fall] course materials for FDU DATA130008: Introduction to Artificial Intelligence 人工智能\nUnfortunately no course website is available for this semester. There is an old one for 2019 Spring, where you can find lecture slides, videos, codes, etc.\nTextbook: Stuart J. Russell, Peter Norvig (2009) Artificial Intelligence A Modern Approach, 3rd Edition.2009, Prentice Hall\n'], 'url_profile': 'https://github.com/sgallon-rin', 'info_list': ['Python', 'Updated Jul 11, 2020', '1', 'Python', 'Updated Dec 19, 2020', 'Jupyter Notebook', 'Updated Jun 21, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'MIT license', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 13, 2020', 'Python', 'Updated Dec 4, 2020', 'ShaderLab', 'MIT license', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '219 contributions\n        in the last year', 'description': ['UnityZombies\nProject for passing the subject ""Fundamentals of Artificial Intelligence"" at the Jagiellonian University\n'], 'url_profile': 'https://github.com/Deusald', 'info_list': ['Python', 'Updated Jul 11, 2020', '1', 'Python', 'Updated Dec 19, 2020', 'Jupyter Notebook', 'Updated Jun 21, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'MIT license', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 13, 2020', 'Python', 'Updated Dec 4, 2020', 'ShaderLab', 'MIT license', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020']}","{'location': 'Varanasi, India', 'stats_list': [], 'contributions': '828 contributions\n        in the last year', 'description': ['CSE-241N-AI-Lab-Codes\nThis repository contains the Lab codes of the course Artificial Intelligence (CSE-241) at IIT BHU Varanasi - Even Semester 2019-20.\n\nGuided By: Dr. Anil Kumar Singh, Associate Professor, CSE, IIT (BHU) Varanasi.\n\nContents:\n\nAssignment 0: Tools Installation.\nAssignment 1: Sudoku Solver.\nAssignment 2: Linear Regression.\nAssignment 3: Logistic Regression.\nAssignment 4: K Means Clustering.\nAssignment 5: HMM Viterbi.\n\n'], 'url_profile': 'https://github.com/krashish8', 'info_list': ['Python', 'Updated Jul 11, 2020', '1', 'Python', 'Updated Dec 19, 2020', 'Jupyter Notebook', 'Updated Jun 21, 2020', 'Python', 'Updated Jan 19, 2020', 'Python', 'MIT license', 'Updated Jan 14, 2020', 'Python', 'Updated Jan 17, 2020', 'Python', 'Updated Jan 13, 2020', 'Python', 'Updated Dec 4, 2020', 'ShaderLab', 'MIT license', 'Updated Jan 26, 2020', 'Jupyter Notebook', 'Updated Mar 18, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/JeongbokSon', 'info_list': ['Updated Apr 1, 2020', 'Updated Jan 13, 2020', 'Java', 'Updated Jan 13, 2020', 'Java', 'GPL-3.0 license', 'Updated Dec 14, 2020', 'Java', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jul 2, 2020', 'MIT license', 'Updated Jan 15, 2021', '1', 'Java', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated May 22, 2020', 'Python', 'Updated Jan 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/MEHEDl', 'info_list': ['Updated Apr 1, 2020', 'Updated Jan 13, 2020', 'Java', 'Updated Jan 13, 2020', 'Java', 'GPL-3.0 license', 'Updated Dec 14, 2020', 'Java', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jul 2, 2020', 'MIT license', 'Updated Jan 15, 2021', '1', 'Java', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated May 22, 2020', 'Python', 'Updated Jan 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '1,079 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/aridavis', 'info_list': ['Updated Apr 1, 2020', 'Updated Jan 13, 2020', 'Java', 'Updated Jan 13, 2020', 'Java', 'GPL-3.0 license', 'Updated Dec 14, 2020', 'Java', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jul 2, 2020', 'MIT license', 'Updated Jan 15, 2021', '1', 'Java', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated May 22, 2020', 'Python', 'Updated Jan 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '12 contributions\n        in the last year', 'description': [""ai-uoi-coursework\nExercise for artificial intelligence course at cse-uoi, spring of 2019.\nWhole project is written in Java.\n\nCollaborators\n\n@DimitrisSintos\n@Billy54\n\n\nContents\n\nAI_part1 : a project that simulates a robot movements inside a labyrinth.\nThe highlighted algorithms are Uniform Cost Search and A-Star.\neuretic.pdf explains the choice behind the heuristic function on the A-Star Algorithm.\nAI_part2 : a two player game, where the opponent is the computer.\nAlgorithm MiniMax is used to calculate the computer's actions.\n\nComments\n\nyou can find the requirements at exercise-instructions(greek).pdf\n\nTO-DO LIST_\n\n translate requirements and other pdf files to english\n clean code\n\n""], 'url_profile': 'https://github.com/ThThoma', 'info_list': ['Updated Apr 1, 2020', 'Updated Jan 13, 2020', 'Java', 'Updated Jan 13, 2020', 'Java', 'GPL-3.0 license', 'Updated Dec 14, 2020', 'Java', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jul 2, 2020', 'MIT license', 'Updated Jan 15, 2021', '1', 'Java', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated May 22, 2020', 'Python', 'Updated Jan 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '19 contributions\n        in the last year', 'description': ['sudoku\nsolve sudoku game using artificial intelligence (Backtracking algorithm , Forward checking , Simulated Annealing)\n\n'], 'url_profile': 'https://github.com/an9080', 'info_list': ['Updated Apr 1, 2020', 'Updated Jan 13, 2020', 'Java', 'Updated Jan 13, 2020', 'Java', 'GPL-3.0 license', 'Updated Dec 14, 2020', 'Java', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jul 2, 2020', 'MIT license', 'Updated Jan 15, 2021', '1', 'Java', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated May 22, 2020', 'Python', 'Updated Jan 17, 2020']}","{'location': 'Tangerang, Banten, INDONESIA', 'stats_list': [], 'contributions': '72 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/NicholasDominic', 'info_list': ['Updated Apr 1, 2020', 'Updated Jan 13, 2020', 'Java', 'Updated Jan 13, 2020', 'Java', 'GPL-3.0 license', 'Updated Dec 14, 2020', 'Java', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jul 2, 2020', 'MIT license', 'Updated Jan 15, 2021', '1', 'Java', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated May 22, 2020', 'Python', 'Updated Jan 17, 2020']}","{'location': 'Bologna, Italy', 'stats_list': [], 'contributions': '552 contributions\n        in the last year', 'description': ['uni-notes\nA collection of notes in markdown from my Master in Artificial Intelligence:\nhttps://www.unibo.it/it\n'], 'url_profile': 'https://github.com/nihil21', 'info_list': ['Updated Apr 1, 2020', 'Updated Jan 13, 2020', 'Java', 'Updated Jan 13, 2020', 'Java', 'GPL-3.0 license', 'Updated Dec 14, 2020', 'Java', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jul 2, 2020', 'MIT license', 'Updated Jan 15, 2021', '1', 'Java', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated May 22, 2020', 'Python', 'Updated Jan 17, 2020']}","{'location': 'Gujarat', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/prachi-ag', 'info_list': ['Updated Apr 1, 2020', 'Updated Jan 13, 2020', 'Java', 'Updated Jan 13, 2020', 'Java', 'GPL-3.0 license', 'Updated Dec 14, 2020', 'Java', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jul 2, 2020', 'MIT license', 'Updated Jan 15, 2021', '1', 'Java', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated May 22, 2020', 'Python', 'Updated Jan 17, 2020']}","{'location': 'Bangalore, Karnataka', 'stats_list': [], 'contributions': '41 contributions\n        in the last year', 'description': ['Data Science Projects\nData Science Projects on Artificial Intelligence, Python, R , SAS, Adv. EXCEL, IBM SPSS, IBM Modeller etc.,\nRapid miner Projects will be updated soon....\nAdvanced Excel\n'], 'url_profile': 'https://github.com/sanjaytallolli', 'info_list': ['Updated Apr 1, 2020', 'Updated Jan 13, 2020', 'Java', 'Updated Jan 13, 2020', 'Java', 'GPL-3.0 license', 'Updated Dec 14, 2020', 'Java', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jul 2, 2020', 'MIT license', 'Updated Jan 15, 2021', '1', 'Java', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated May 22, 2020', 'Python', 'Updated Jan 17, 2020']}","{'location': 'Tangerang, Banten, INDONESIA', 'stats_list': [], 'contributions': '72 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/NicholasDominic', 'info_list': ['Updated Apr 1, 2020', 'Updated Jan 13, 2020', 'Java', 'Updated Jan 13, 2020', 'Java', 'GPL-3.0 license', 'Updated Dec 14, 2020', 'Java', 'Updated Jan 19, 2020', 'Jupyter Notebook', 'Updated Jul 2, 2020', 'MIT license', 'Updated Jan 15, 2021', '1', 'Java', 'Updated Jan 15, 2020', 'Jupyter Notebook', 'Updated May 22, 2020', 'Python', 'Updated Jan 17, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '277 contributions\n        in the last year', 'description': ['02445_Statistical_evaluation_of_AI\nScripts used in the course 02445 Statistical evaluation of Artificial Intelligence\n'], 'url_profile': 'https://github.com/s183920', 'info_list': ['HTML', 'Updated Feb 12, 2020', 'JavaScript', 'MIT license', 'Updated Sep 11, 2020', 'Python', 'Updated Dec 22, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Python', 'Updated Apr 12, 2020', 'Python', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Feb 14, 2021', 'Makefile', 'Updated Jan 17, 2020', 'JavaScript', 'Updated Jan 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '67 contributions\n        in the last year', 'description': [""\nLive Subtitles\n\nDisplay what people are saying as they are talking in real time!\nA quick app built by David Shen, Roger Wang, Jerry Han, and Alex Sun.\nCreated using React, Tensorflow.js, and Chrome's speech-to-text API.\n\n \n\nTry It Yourself\n\n\n\n\n\n\nSee Full Video Demonstration!\n\n\n\n\n\n\n\n\nGetting Started\n\nAll the code required to get started\nImages of what it should look like\n\nClone\n\nClone this repo to your local machine\ngit clone https://github.com/rogerwangcs/ar-dialogue-subtitles.git\n\nSetup\nInstall dependencies\nyarn install or npm install\nInstall dependencies\nyarn install or npm install\nRun\nyarn start or npm npm start\nEnjoy!\n\nContributors\n\nRoger Wang\nDavid Shen\nJerry Han\nAlexander Sun\n\n\nLicense\n\nMIT license\n\n""], 'url_profile': 'https://github.com/heladegachi', 'info_list': ['HTML', 'Updated Feb 12, 2020', 'JavaScript', 'MIT license', 'Updated Sep 11, 2020', 'Python', 'Updated Dec 22, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Python', 'Updated Apr 12, 2020', 'Python', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Feb 14, 2021', 'Makefile', 'Updated Jan 17, 2020', 'JavaScript', 'Updated Jan 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '49 contributions\n        in the last year', 'description': [""fake-news-detection\nIntro\nThe goal of this repository is to build a machine learning model which is able to detect fake news by estimating the relative perspective (stance) of two pieces of text relative to a topic or claim.\nInput:\nA headline and a body text - either from the same news article or from two different articles.\nOutput:\nClassify the stance of the body text relative to the claim made in the headline into one of four categories:\n\nAgrees: The body text agrees with the headline.\nDisagrees: The body text disagrees with the headline.\nDiscusses: The body text discuss the same topic as the headline, but does not take a position.\nUnrelated: The body text discusses a different topic than the headline.\n\nEvaluation\nPerformance is measured based on a weighted, two-level scoring system:\n\nLevel 1: Classify headline and body text as related or unrelated: 25% score weighting.\nLevel 2: Classify related pairs as agrees, disagrees, or discusses: 75% score weighting.\n\nRationale: The related/unrelated classification task is expected to be much easier and is less relevant for detecting fake news, so it is given less weight in the evaluation metric. The Stance Detection task (classify as agrees, disagrees or discuss) is both more difficult and more relevant to fake news detection, so is to be given much more weight in the evaluation metric.\nConcretely, if a [HEADLINE, BODY TEXT] pair in the test set has the target label unrelated, evaluation score will be incremented by 0.25 if it labels the pair as unrelated.\nIf the [HEADLINE, BODY TEXT] test pair is related, the evaluation score will be incremented by 0.25 if it labels the pair as any of the three classes: agrees, disagrees, or discusses.\nThe evaluation score will so be incremented by an additional 0.75 for each related pair if gets the relationship right by labeling the pair with the single correct class: agrees, disagrees, or discusses.\nData\nTraining sets: Pairs of headline and body text with the appropriate class label for each.\n\ntrain_bodies.csv - shape (1683, 2): contains the article body column ('Body ID') with corresponding body text of article ('article Body') column.\ntrain_stances.csv - shape (49952, 3): contains article headline ('Headline') for pairs of article body ('Body ID'), and labeled stance ('Stance') columns.\n\nTest sets: Pairs of headline and body text without class labels used to evaluate systems.\n\ntest_bodies.csv - shape (905, 2): contains the article body column ('Body ID') with corresponding body text of article ('article Body') column.\ntest_stances_unlabeled.csv - shape (25414, 3): contains article headline ('Headline') for pairs of article body ('Body ID') (no Stance column).\n\nData source: The data is derived from the Emergent Dataset created by Craig Silverman. For more information, visit Fake news challenge.\nModels\nI implemented 3 methods:\n1. Perceptron\n(44 features)\n\nScore on the development set (Hold-out split): 3307.25 out of 4448.5 (74.3%).\nScore on the test set: 8461.75 out of 11651.25 (72.6%).\n\n2. Softmax linear model with additional tf-idf features\n(44 features + tf-idf features)\n\nScore on the development set (Hold-out split): 3857.75 out of 4448.5 (86.7%).\nScore on the test set: 8963.25 out of 11651.25 (76.9%)\n\n3. Feedforward neural network with additional tf-idf features\n(44 features + tf-idf features)\n\nScore on the development set (Hold-out split): 3907.25 out of 4448.5 (87.8%).\nScores on the test set: 8984.25 out of 11651.25\t(77.1%).\n\nUsage\npython main.py -method [method's name]\n\nmethod's name: choose one method for training: perceptron, softmax_linear_model (softmax linear model with additional tf-idf features), or feedforward_NN (feedforward neural network with additional tf-idf features).\nReference\nData source and evaluation: Fake news challenge\nDate uploaded: Jan 11, 2020\nDate updated: Dec 22, 2020\n""], 'url_profile': 'https://github.com/huyenkn', 'info_list': ['HTML', 'Updated Feb 12, 2020', 'JavaScript', 'MIT license', 'Updated Sep 11, 2020', 'Python', 'Updated Dec 22, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Python', 'Updated Apr 12, 2020', 'Python', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Feb 14, 2021', 'Makefile', 'Updated Jan 17, 2020', 'JavaScript', 'Updated Jan 21, 2020']}","{'location': 'Canada', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['This file is used for the machine learning and artificial intelligence. In recent decades, the artificial intelligence has been widely used in many fields.\nHowever, the advantages of the technology have not been fully understood.\n'], 'url_profile': 'https://github.com/Datalearn168', 'info_list': ['HTML', 'Updated Feb 12, 2020', 'JavaScript', 'MIT license', 'Updated Sep 11, 2020', 'Python', 'Updated Dec 22, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Python', 'Updated Apr 12, 2020', 'Python', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Feb 14, 2021', 'Makefile', 'Updated Jan 17, 2020', 'JavaScript', 'Updated Jan 21, 2020']}","{'location': 'Alexandria, Egypt', 'stats_list': [], 'contributions': '69 contributions\n        in the last year', 'description': ['Deft-Eval\nThis is an implementation of definition evaluation project as a class project within the Artificial Intelligence class.\nMotivation\nThis project aimed to learn more about the classification algorithms such as naive byes, word2vec tool in classification and familiarity with machine learning in general.\n'], 'url_profile': 'https://github.com/ahmedfawzy98', 'info_list': ['HTML', 'Updated Feb 12, 2020', 'JavaScript', 'MIT license', 'Updated Sep 11, 2020', 'Python', 'Updated Dec 22, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Python', 'Updated Apr 12, 2020', 'Python', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Feb 14, 2021', 'Makefile', 'Updated Jan 17, 2020', 'JavaScript', 'Updated Jan 21, 2020']}","{'location': 'Montreal, QC, Canada', 'stats_list': [], 'contributions': '580 contributions\n        in the last year', 'description': ['https://github.com/razine-bensari/IndonesianDotPuzzle\nIndonesian Dot Puzzle\nlink of repo above\nTo run the dfs, you need to have python 3.7.x\nsimple go to run the main.py file\npython3 main.py (in the terminal)\n'], 'url_profile': 'https://github.com/razine-bensari', 'info_list': ['HTML', 'Updated Feb 12, 2020', 'JavaScript', 'MIT license', 'Updated Sep 11, 2020', 'Python', 'Updated Dec 22, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Python', 'Updated Apr 12, 2020', 'Python', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Feb 14, 2021', 'Makefile', 'Updated Jan 17, 2020', 'JavaScript', 'Updated Jan 21, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '119 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/bentrey', 'info_list': ['HTML', 'Updated Feb 12, 2020', 'JavaScript', 'MIT license', 'Updated Sep 11, 2020', 'Python', 'Updated Dec 22, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Python', 'Updated Apr 12, 2020', 'Python', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Feb 14, 2021', 'Makefile', 'Updated Jan 17, 2020', 'JavaScript', 'Updated Jan 21, 2020']}","{'location': 'Pemba', 'stats_list': [], 'contributions': '564 contributions\n        in the last year', 'description': ['FEVER NOTIFICATION SYSTEM USING EMAIL AND SMS\nThe purpose and aim of this application is to actualise the ability of a system to connect a fever status detection AI system to a computer/mobile client application and use clients SMS/notification systems. Below are some of the components in its implementation.\nDevelopmental enviromental installation\nDo not use ordinary python. Rather use condo enviroment for ANN and ML\n--------------------------------\nconda activate atslearning\n\npython serverMaker1.py\npython serverMaker2.py\npython serverMaker3.py\n\n---------------------------\npip install twilio\npip install flask\n\npython server.py\npython request.py\n----------------------------------\n\nDevices and Components\n\nSensors and PC server\nServer and Website-UI\nServer, Tablet-UI and Phone-UI\nSMS Python/NodeJS\nEmail Python/Nodejs\n\nUser Interface UX\nHere are screens shots from the application accuracy plots and User experiece interfaces\n\n\n\n\n\n'], 'url_profile': 'https://github.com/LINOSNCHENA', 'info_list': ['HTML', 'Updated Feb 12, 2020', 'JavaScript', 'MIT license', 'Updated Sep 11, 2020', 'Python', 'Updated Dec 22, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Python', 'Updated Apr 12, 2020', 'Python', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Feb 14, 2021', 'Makefile', 'Updated Jan 17, 2020', 'JavaScript', 'Updated Jan 21, 2020']}","{'location': 'Toronto, Canada', 'stats_list': [], 'contributions': '102 contributions\n        in the last year', 'description': ['sick-tac-toe\n2 player iOS game with Artificial Intelligence: Player vs AI, Swift 3\n\n\n\n\n\n\n'], 'url_profile': 'https://github.com/Lizz1102', 'info_list': ['HTML', 'Updated Feb 12, 2020', 'JavaScript', 'MIT license', 'Updated Sep 11, 2020', 'Python', 'Updated Dec 22, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Python', 'Updated Apr 12, 2020', 'Python', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Feb 14, 2021', 'Makefile', 'Updated Jan 17, 2020', 'JavaScript', 'Updated Jan 21, 2020']}","{'location': 'Porto, Portugal', 'stats_list': [], 'contributions': '1,250 contributions\n        in the last year', 'description': ['FEUP-AIAD\nProjects developed for the AIAD (Distributed Artificial Intelligence Agents) course unit.\nContext\n\nDate: 4th Year, 1st Semester, 2019/2020\nTopic:  Exercises\nCourse: Agentes de Inteligência Artificial Distribuidos (AIAD) | Distributed Artificial Intelligence Agents\nCourse Link: https://sigarra.up.pt/feup/pt/UCURR_GERAL.FICHA_UC_VIEW?pv_ocorrencia_id=436453\n\nDisclaimer\nThis repository, and every other FEUP-COURSE* repos on GitHub correspond to school projects from the respective COURSE. The code on this repo is intended for educational purposes. I do not take any responsibility, liability or whateverity over any code faults, inconsistency or anything else. If you intend on copying most or parts of the code for your school projects, keep in mind that this repo is public, and that your professor might search the web for similar project solutions or whatnot and choose to fail you for copying.\n(Credit to miguelpduarte for the original README layout, adapted by me)\n'], 'url_profile': 'https://github.com/xRuiAlves', 'info_list': ['HTML', 'Updated Feb 12, 2020', 'JavaScript', 'MIT license', 'Updated Sep 11, 2020', 'Python', 'Updated Dec 22, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Python', 'Updated Apr 12, 2020', 'Python', 'Updated Mar 1, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Feb 14, 2021', 'Makefile', 'Updated Jan 17, 2020', 'JavaScript', 'Updated Jan 21, 2020']}"
"{'location': 'Hoboken, NJ', 'stats_list': [], 'contributions': '572 contributions\n        in the last year', 'description': ['Tic-tac-toe with AI - CLI\nAbout\nA CLI version of Tic-tac-toe with 0-, 1-, and 2-player modes:\n\nA 0-player game has two computer players playing against each other with no interaction from the user.\nA 1-player game has a human playing against a computer.\nA 2-player game has two human players.\n\nInitial Setup\nThis project is supported by Bundler and includes a Gemfile.\nRun bundle install before running the app.\nPlaying The Game\nStart the game by entering bin/tictactoe into the command line, then follow the instructions in the console\n'], 'url_profile': 'https://github.com/grangerl330', 'info_list': ['Ruby', 'Updated Jan 17, 2020', '1', 'MIT license', 'Updated Jan 14, 2020', '1', 'TypeScript', 'MIT license', 'Updated Dec 27, 2020', 'JavaScript', 'Updated Jan 21, 2020', 'C++', 'GPL-3.0 license', 'Updated Jan 18, 2020', 'Python', 'GPL-3.0 license', 'Updated Jan 18, 2020', 'Python', 'Updated Apr 28, 2020', 'Java', 'Updated Jan 13, 2020', 'HTML', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 27, 2020']}","{'location': 'Porto, Portugal', 'stats_list': [], 'contributions': '51 contributions\n        in the last year', 'description': [""aima-cpp\nC++ implementation of algorithms from Russell And Norvig's Artificial Intelligence - A Modern Approach 3rd Edition. You can use this in conjunction with a course on AI, or for study on your own.\n""], 'url_profile': 'https://github.com/ei06125', 'info_list': ['Ruby', 'Updated Jan 17, 2020', '1', 'MIT license', 'Updated Jan 14, 2020', '1', 'TypeScript', 'MIT license', 'Updated Dec 27, 2020', 'JavaScript', 'Updated Jan 21, 2020', 'C++', 'GPL-3.0 license', 'Updated Jan 18, 2020', 'Python', 'GPL-3.0 license', 'Updated Jan 18, 2020', 'Python', 'Updated Apr 28, 2020', 'Java', 'Updated Jan 13, 2020', 'HTML', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 27, 2020']}","{'location': 'Sri Lanka', 'stats_list': [], 'contributions': '141 contributions\n        in the last year', 'description': ['\nBL Studio\nBL Studio is an open-source collection of tools for generating music using Artificial Intelligence. Learn More...\n'], 'url_profile': 'https://github.com/Buddhilive', 'info_list': ['Ruby', 'Updated Jan 17, 2020', '1', 'MIT license', 'Updated Jan 14, 2020', '1', 'TypeScript', 'MIT license', 'Updated Dec 27, 2020', 'JavaScript', 'Updated Jan 21, 2020', 'C++', 'GPL-3.0 license', 'Updated Jan 18, 2020', 'Python', 'GPL-3.0 license', 'Updated Jan 18, 2020', 'Python', 'Updated Apr 28, 2020', 'Java', 'Updated Jan 13, 2020', 'HTML', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 27, 2020']}","{'location': 'Porto, Portugal', 'stats_list': [], 'contributions': '1,250 contributions\n        in the last year', 'description': ['FEUP-AIAD\nProjects developed for the AIAD (Distributed Artificial Intelligence Agents) course unit.\nContext\n\nDate: 4th Year, 1st Semester, 2019/2020\nTopic:  Exercises\nCourse: Agentes de Inteligência Artificial Distribuidos (AIAD) | Distributed Artificial Intelligence Agents\nCourse Link: https://sigarra.up.pt/feup/pt/UCURR_GERAL.FICHA_UC_VIEW?pv_ocorrencia_id=436453\n\nDisclaimer\nThis repository, and every other FEUP-COURSE* repos on GitHub correspond to school projects from the respective COURSE. The code on this repo is intended for educational purposes. I do not take any responsibility, liability or whateverity over any code faults, inconsistency or anything else. If you intend on copying most or parts of the code for your school projects, keep in mind that this repo is public, and that your professor might search the web for similar project solutions or whatnot and choose to fail you for copying.\n(Credit to miguelpduarte for the original README layout, adapted by me)\n'], 'url_profile': 'https://github.com/xRuiAlves', 'info_list': ['Ruby', 'Updated Jan 17, 2020', '1', 'MIT license', 'Updated Jan 14, 2020', '1', 'TypeScript', 'MIT license', 'Updated Dec 27, 2020', 'JavaScript', 'Updated Jan 21, 2020', 'C++', 'GPL-3.0 license', 'Updated Jan 18, 2020', 'Python', 'GPL-3.0 license', 'Updated Jan 18, 2020', 'Python', 'Updated Apr 28, 2020', 'Java', 'Updated Jan 13, 2020', 'HTML', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 27, 2020']}","{'location': 'Poland', 'stats_list': [], 'contributions': '1 contribution\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/karol57', 'info_list': ['Ruby', 'Updated Jan 17, 2020', '1', 'MIT license', 'Updated Jan 14, 2020', '1', 'TypeScript', 'MIT license', 'Updated Dec 27, 2020', 'JavaScript', 'Updated Jan 21, 2020', 'C++', 'GPL-3.0 license', 'Updated Jan 18, 2020', 'Python', 'GPL-3.0 license', 'Updated Jan 18, 2020', 'Python', 'Updated Apr 28, 2020', 'Java', 'Updated Jan 13, 2020', 'HTML', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 27, 2020']}","{'location': '24 Rue Pasteur, 94270 Le Kremlin-Bicêtre', 'stats_list': [], 'contributions': '259 contributions\n        in the last year', 'description': ['AIA_gomoku_2019\n3rd year artificial intelligence project in Python in which the goal is to implement a Gomoku Narabe game bot/ai.\nIt is a 2-player game that is played on a 19x19 game board by default (20x20 on ""Piskvork""). Each player plays a stone at his/her turn, and the game ends as soon as one has a 5 stones in a row (vertically, horizontally or diagonaly) and thus wins.\nThe bot is compliant with the communication protocol and can be upload on ""Piskvork"" (windows Gomoku software plateform).\nThe program is built using the Min-max method and can be played against a human and ai player. Other than on Piskvork it can also be manually tested on Linux/Macos with commands sequences detailed below.\nUSAGE :\n./pbrain-minMax.py\n\nCOMMANDS :\nSTART [size >= 20] - Select board sizes.\n\nBEGIN - To start the game.\n\nTURN [X],[Y] - The parameters are coordinate of the opponent\'s move. All coordinates are numbered from zero.\n\nBOARD - This command imposes entirely new playing field. It is suitable for continuation of an opened match or for undo/redo user commands.\nAfter this command the data forming the playing field are send. Every line is in the form: [X],[Y],[field]\nwhere [X] and [Y] are coordinates and [field] is either number 1 (own stone) or number 2 (opponent\'s stone) or number 3      (only if continuous game is enabled, stone is part of winning line or is forbidden according to renju rules).\nThen Data are ended by DONE command.\n\nINFO [key] [value] - Informations about the current game (time remaining in the game, time remaining for each moves...) :\n\nThe key can be:\ntimeout_turn  - time limit for each move (milliseconds, 0=play as fast as possible)\ntimeout_match - time limit of a whole match (milliseconds, 0=no limit)\nmax_memory    - memory limit (bytes, 0=no limit)\ntime_left     - remaining time limit of a whole match (milliseconds)\ngame_type     - 0=opponent is human, 1=opponent is brain, 2=tournament, 3=network tournament\nrule          - bitmask or sum of 1=exactly five in a row win, 2=continuous game, 4=renju\nevaluate      - coordinates X,Y representing current position of the mouse cursor\nfolder        - folder for persistent files\n\nEND - To end the current game.\n\nABOUT - Informations about the current player such as author, country, www, email etc...\n\nMore detailed commands can be found here : https://svn.code.sf.net/p/piskvork/code/trunk/source/doc/protocl2en.htm\n\n\n'], 'url_profile': 'https://github.com/WoshiWoshu', 'info_list': ['Ruby', 'Updated Jan 17, 2020', '1', 'MIT license', 'Updated Jan 14, 2020', '1', 'TypeScript', 'MIT license', 'Updated Dec 27, 2020', 'JavaScript', 'Updated Jan 21, 2020', 'C++', 'GPL-3.0 license', 'Updated Jan 18, 2020', 'Python', 'GPL-3.0 license', 'Updated Jan 18, 2020', 'Python', 'Updated Apr 28, 2020', 'Java', 'Updated Jan 13, 2020', 'HTML', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 27, 2020']}","{'location': 'Toronto, Canada', 'stats_list': [], 'contributions': '23 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/tonyliu6ix', 'info_list': ['Ruby', 'Updated Jan 17, 2020', '1', 'MIT license', 'Updated Jan 14, 2020', '1', 'TypeScript', 'MIT license', 'Updated Dec 27, 2020', 'JavaScript', 'Updated Jan 21, 2020', 'C++', 'GPL-3.0 license', 'Updated Jan 18, 2020', 'Python', 'GPL-3.0 license', 'Updated Jan 18, 2020', 'Python', 'Updated Apr 28, 2020', 'Java', 'Updated Jan 13, 2020', 'HTML', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '33 contributions\n        in the last year', 'description': ['8puzzle_AI_Assignment\nProgramming Assignment for module CE213 Artificial Intelligence. The task is to build a program to solve the 8-puzzle\n'], 'url_profile': 'https://github.com/MarcosLaydner', 'info_list': ['Ruby', 'Updated Jan 17, 2020', '1', 'MIT license', 'Updated Jan 14, 2020', '1', 'TypeScript', 'MIT license', 'Updated Dec 27, 2020', 'JavaScript', 'Updated Jan 21, 2020', 'C++', 'GPL-3.0 license', 'Updated Jan 18, 2020', 'Python', 'GPL-3.0 license', 'Updated Jan 18, 2020', 'Python', 'Updated Apr 28, 2020', 'Java', 'Updated Jan 13, 2020', 'HTML', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '5 contributions\n        in the last year', 'description': [""image_classification-project\nThis repository contains the files for my final project for Udacity's Nanogree program- Artificial Intelligence with Python.\n""], 'url_profile': 'https://github.com/Phylliac', 'info_list': ['Ruby', 'Updated Jan 17, 2020', '1', 'MIT license', 'Updated Jan 14, 2020', '1', 'TypeScript', 'MIT license', 'Updated Dec 27, 2020', 'JavaScript', 'Updated Jan 21, 2020', 'C++', 'GPL-3.0 license', 'Updated Jan 18, 2020', 'Python', 'GPL-3.0 license', 'Updated Jan 18, 2020', 'Python', 'Updated Apr 28, 2020', 'Java', 'Updated Jan 13, 2020', 'HTML', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 27, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '73 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/Lexden12', 'info_list': ['Ruby', 'Updated Jan 17, 2020', '1', 'MIT license', 'Updated Jan 14, 2020', '1', 'TypeScript', 'MIT license', 'Updated Dec 27, 2020', 'JavaScript', 'Updated Jan 21, 2020', 'C++', 'GPL-3.0 license', 'Updated Jan 18, 2020', 'Python', 'GPL-3.0 license', 'Updated Jan 18, 2020', 'Python', 'Updated Apr 28, 2020', 'Java', 'Updated Jan 13, 2020', 'HTML', 'Updated Jan 13, 2020', 'Python', 'Updated Jan 27, 2020']}"
"{'location': 'PH', 'stats_list': [], 'contributions': '2,275 contributions\n        in the last year', 'description': ['Artificial Intelligence: N-Queens Puzzle\nA java program that solves the n-queens puzzle using Hill Climbing and Random Restart algorithm in Artificial Intelligence.\nProblem\nN-Queens is a famous computer science problem. The goal is to place “N” Number of queens on an “N x N” sized chess board such that no queen is under attack by another queen.\nBelow, you can see one possible solution to the N-queens problem for N = 4.\n\nNo two queens are on the same row, column, or diagonal.\nSolution\nResources\n'], 'url_profile': 'https://github.com/shaniadicen', 'info_list': ['Java', 'Updated Jan 20, 2020', 'Updated Jan 19, 2020', 'Python', 'GPL-3.0 license', 'Updated Aug 31, 2020', 'Python', 'Updated Jan 16, 2020', 'Python', 'Updated Apr 24, 2020', 'MIT license', 'Updated Jan 19, 2020', 'HTML', 'Updated Jan 16, 2020', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['hello-datascience\nArtificial Intelligence,Machine Learning,Deep Learning,Data Analysis,Data Mining,Big Data,R Programming,Python For Data Science.\nI am a Freelance Programmer now Learning Data Science.\n'], 'url_profile': 'https://github.com/seeniappan', 'info_list': ['Java', 'Updated Jan 20, 2020', 'Updated Jan 19, 2020', 'Python', 'GPL-3.0 license', 'Updated Aug 31, 2020', 'Python', 'Updated Jan 16, 2020', 'Python', 'Updated Apr 24, 2020', 'MIT license', 'Updated Jan 19, 2020', 'HTML', 'Updated Jan 16, 2020', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 19, 2020']}","{'location': 'Magdeburg, Germany', 'stats_list': [], 'contributions': '251 contributions\n        in the last year', 'description': ['Shortest-Path-Finding-Algorithm\nThis is an implementation of A-Star Algorithm to find the shortest path of two provided nodes. This is widely used algorithm for Artificial Intelligence in Games and in general.\n'], 'url_profile': 'https://github.com/JalajVora', 'info_list': ['Java', 'Updated Jan 20, 2020', 'Updated Jan 19, 2020', 'Python', 'GPL-3.0 license', 'Updated Aug 31, 2020', 'Python', 'Updated Jan 16, 2020', 'Python', 'Updated Apr 24, 2020', 'MIT license', 'Updated Jan 19, 2020', 'HTML', 'Updated Jan 16, 2020', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 19, 2020']}","{'location': 'Berkeley, California', 'stats_list': [], 'contributions': '645 contributions\n        in the last year', 'description': ['Tic-Tac-Toe\nA Python application that allows users to play games of Tic-Tac-Toe against a minimax-based artificial intelligence.\n# How to use:\n- In terminal, navigate to the project folder and run the application using the command line command:\n\tpython __init__.py \n- Then, indicate which player starts first 0 = user, 1 = AI\n\nInformation\nThis application is a work-in-progress and will be constantly updated at varying intervals of time. This project was created as a means for me to learn more about the minimax descision rule and its use cases for creating artificial intelligences that will always tie or win solved games.\nRoom for improvement\n\nAdd to the user interface, allowing users to click buttons to indicate which player starts first (player or ai).\nImplement a ""restart"" feature that allows users to play multiple games without having to restart the program.\n\nScreenshots\n\nNotes\nWhen running the game using a system running macOS, the final moves that cause an end state (win/lose/tie) will not be drawn. This is not the case with systems running Windows 10, and I am currently unsure of the issue.\n'], 'url_profile': 'https://github.com/mattau13', 'info_list': ['Java', 'Updated Jan 20, 2020', 'Updated Jan 19, 2020', 'Python', 'GPL-3.0 license', 'Updated Aug 31, 2020', 'Python', 'Updated Jan 16, 2020', 'Python', 'Updated Apr 24, 2020', 'MIT license', 'Updated Jan 19, 2020', 'HTML', 'Updated Jan 16, 2020', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '619 contributions\n        in the last year', 'description': ['\nteX-annotator for F-RCNN\nA PDF documents annotator, based on lateX files downloaded from arXiv.org, which outputs annotated documents\' pages used for a Faster RCNN trainining.\n1. Repo contents\nIn this repo I present a PDF annotator, used to obtains data input for F-RCNN training tasks.\nThe PDF annotator has the aim to detect:\n\nTitles\nFigures\nLists\nTables\n\nThe teX-annotator uses two type of files: a PDF file and its related lateX source code. It works thanks to the\ninformations retrived from two parsing tasks: the first is done with tex_parser.py, a python script here\npresented, that analyzes all the .tex files linked with its PDF file; then, the second parsing task is done\nusing PDFMIner, a very useful tool which collects information from each PDF\nline. Once the parsing is finished, the knowledge obtained from those two steps is merged in order to match lateX and PDF\nPDF ""objects"" (titles, figures, tables and lists) and save their xy coordinates from bbox PDFMiners elements\'\nproperty. So, each object of each page of each PDF file saved in the PDF_files directory is identified and located\nthanks to its coordinates: a list is created with all these obejct. A detected object is uniquely represented with\nthese values memorized into a list:\ndetected_object = [page, x_min, x_max, y_min, y_max, object_category]\n\npage: The object page\nx_min, y_max: the bottom left bounding box point\nx_max, y_max: the top right bounding box point\nobject_category: the object category\n\nThese are used for annotations: a summary images_annotations.csv file, from which is obtained a .txt file\nused to indicate to the Faster RCNN the training images. The test images are produced during the main program runs.\nPDF files and their source files (lateX) have been downloaded from arXiv.org; there were not\ndocument layout distinctions in downloading files.\n2. Project structure\nThe project is organized as follows:\nmain.py\n\nIt\'s the main, and does the following operations:\n\nIt generates as many .png files as many pdf pages for each downloaded paper.\nIt does parsing tasks and retrievs objects coordinates.\nIt divides train and test images, listed inside PNG_files dir, parsing TEX and PDF files, which are stored in\nPDF_files and TEX_files directories respectively. The 90% of pdf files will generates train images,\nthe rest 10% the test images ones.\nIt generates annotations_images.csv and annotated_train_images.txt files; this last one will be given in input\nto the frcnn.\n\nThe main.py can be launched from shell. There are two optioned commands:\npython3 main.py --help\nusage: main.py [-h] [--csv_file_path CSV_FILE_PATH]\n               [--annotations ANNOTATIONS]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --csv_file_path CSV_FILE_PATH\n                        Type the .csv file name to convert. By default is:\n                        images_annotations. The converter then will generate\n                        test_annotations_images.txt and\n                        train_annotations_images.txt .\n  --annotations ANNOTATIONS\n                        Choose if generate annotated images where: red=\n                        titles; green= figures; blu= lists; aqua green=\n                        tables; yellow= text; typing yes or no.\n\nThe most of the work is done by PDF_parser.py, which calls tex_parser.py and optionally the\nimages_annotator.py. images_annotator.py highlights different objects categories whit different colors:\n\nRED -------------> titles\nGREEN ---------> images\nBLUE ------------>  lists\nTURQUOISE ---> tables\nYELLOW --------> text NOT USED IN FRCNN.\n\nWhen the main finishes, all is set-up for start with the frcnn training.\nNB: the graphic pages annotations (specifying --annotations=yes) could be slow since also text is annotated.\n3. Download files from arXiv.org\nThe PDF and teX files are downloaded thanks to the arXiv_download_script.py. This script has to be launched before\nthe main one because it creates PDF_files and TEX_files populating them (unless you don\'t have >10K pdf and related\nteX).\narXiv_download_script.py can be launched from bash; there are some optional commands:\npython3 arXiv_download_script.py --h\nusage: arXiv_download_script.py [-h] [--year YEAR] [--month MONTH]\n                                [--counter COUNTER] [--max_items MAX_ITEMS]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --year YEAR           Choose the year from which you want to start\n                        downloading papers from arXIv. Default 20.\n  --month MONTH         Choose the month, once you have choseh the year, from\n                        which you want to start downloading papers from arXIv.\n                        Default 1(janaury)\n  --counter COUNTER     Choose the starting file counter. With 0 you will\n                        download all files from the year and the month\n                        specified.Default 0.\n  --max_items MAX_ITEMS\n                        Chose how many files you will download\n\n\nThese commands take into accounts how papers are saved on arXiv.org.\n4. F-RCNN\nThe annotated images serve as F-RCNN input data; the goal of this project is to test how much correctly the net can\ndetects titles, figures, lists and tables. For this task I\'ve chosen the F-RCNN net; further informations about it\nare available on R-CNN, Fast R-CNN, Faster R-CNN, YOLO — Object Detection Algorithms.\nI\'ve cloned the repo from this git repo keras-frcnn and the I\'ve followed\nthis guide for implementation.\nStart with training is very easy: open the shell, go to /DDM_Project/venv/frcnn/ and start!\ncd frcnn\npython3 train_frcnn.py -o simple -p annotated_train_images.txt\n\nThis command will start training which will generate an h5 model, that is the input for test:\npython3 test_frcnn.py -p ../png_files/test_images/\n\nThe test procedure will outputs the annotated test images basing on the training task results.\n5. Test evaluation\nTest evaluation is available. After test task, a .txt file will be generated with the result images: predicted_test_images.txt.\nThis file contains all the details about all the instances detected during the test (obviously concerning the test set).\nFurthermore, it\'s important to generate a similar file which constitutes the Ground Truth. Such a file has to be created\nafter the FRCNN test phase, running the test_images_annotator.py script.\npython3 test_images_annotator.py\n\nThis script will generate the annotated_test_images.txt and the parse_error_test_files.txt, which will contains the paper name\nthat have been badly elaborated (parsing errors); this file is very important because, before the test evaluation, it is crucial\nto remove all the lines which refers to the erroneous papers in the annotated_test_images.txt: such an operation insures\nan unique correspondence between the GT and the predictions (annotated_test_images.txt and predicted_test_images.txt).\nOnce you have done this operation, you can evaluate your FRCNN predictions simply running the evaluate.py script:\nNB: It is very important that you sort the predicted_test_images.txt and the annotated_test_images.txt\nrunning this command:\nsort -u -o <file_to_sort> <file_where_output_sort_operation>\n\nThen, run:\npython3 evaluate.py\n\nThis script will generate:\n\na log with all the GT papers and another with all the PREDICTION papers; these logs file\ncould be useful if correspondence problems occur.\na test_results.txt file which contains all the information about the test evaluation of all the single papers.\nThis file is so constructed:\n\nPRED and GT different pages lists\nPrecision\nRecall\nTrue Positives, False Positives, False Negatives\nF1 score\n\n\n\nAll these statistics are evaluated varying a threshold value used in the Intersection Over Union\n(IoU) algorithm used for predicted papers instances classification. The used thresholds are:\n[0.1, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50, 0.55, 0.60, 0.65, 0.70, 0.75, 0.80].\nAn F1-PRECISION-RECALL plot it is shown after the computation, giving an idea of the test accuracy.\n6. Requirements\nThe dipendencies required for this project are listed inside requirements.txt; you should simply open a shell and\nuse pip3 running:\npip3 install -r requirements.txt\n\nFor this project I used Python 3.7; I recommend to use PyCharm.\n7: Numbers, examples, results\nI would like to share with you some numbers:\n\n10.200 pdf files processed (papers and various scientific articles) downloaded from arXIv.\n\n9 180 for training set\n1 020 for test set\n\n\n204 658 relative papers source files downloaded from arXiv\n187 485 png files generated, one image for one paper page\n466452 instances (titles, images, lists and tables) found and analyzed with frcnn.\n\n8. References\nThis project has been inspired by PubLayNet, a project where PDF taken from the PubMed Central dataset\n(over 360 thousands of articles!) are annotated with theirs relative XML files. PubLayNet paper is\navailable here.\nHere an example of the PubLayNet paper annotated using tex-annotator!\n\n'], 'url_profile': 'https://github.com/pisalore', 'info_list': ['Java', 'Updated Jan 20, 2020', 'Updated Jan 19, 2020', 'Python', 'GPL-3.0 license', 'Updated Aug 31, 2020', 'Python', 'Updated Jan 16, 2020', 'Python', 'Updated Apr 24, 2020', 'MIT license', 'Updated Jan 19, 2020', 'HTML', 'Updated Jan 16, 2020', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 19, 2020']}","{'location': 'Gurgaon, Haryana, India', 'stats_list': [], 'contributions': '25 contributions\n        in the last year', 'description': ['Welcome to GitHub Pages\nYou can use the editor on GitHub to maintain and preview the content for your website in Markdown files.\nWhenever you commit to this repository, GitHub Pages will run Jekyll to rebuild the pages in your site, from the content in your Markdown files.\nMarkdown\nMarkdown is a lightweight and easy-to-use syntax for styling your writing. It includes conventions for\nSyntax highlighted code block\n\n# Header 1\n## Header 2\n### Header 3\n\n- Bulleted\n- List\n\n1. Numbered\n2. List\n\n**Bold** and _Italic_ and `Code` text\n\n[Link](url) and ![Image](src)\nFor more details see GitHub Flavored Markdown.\nJekyll Themes\nYour Pages site will use the layout and styles from the Jekyll theme you have selected in your repository settings. The name of this theme is saved in the Jekyll _config.yml configuration file.\nSupport or Contact\nHaving trouble with Pages? Check out our documentation or contact support and we’ll help you sort it out.\n'], 'url_profile': 'https://github.com/thumbarnirmal', 'info_list': ['Java', 'Updated Jan 20, 2020', 'Updated Jan 19, 2020', 'Python', 'GPL-3.0 license', 'Updated Aug 31, 2020', 'Python', 'Updated Jan 16, 2020', 'Python', 'Updated Apr 24, 2020', 'MIT license', 'Updated Jan 19, 2020', 'HTML', 'Updated Jan 16, 2020', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 19, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Artificial Intelligence Website\nThis is the website I created for the Independent Study assignment. This website was made with a parralax effect. Keep scrolling and enjoy! Download instructions are below.\nDownloading the Website\'s Contents\nThe first step is to download the website. Click the green ""Clone or download"" button, and download the ZIP.\nExtracting the Contents and Wiewing the Site\nThe next step is to extract the contents of the folder. After this is done, navigate to the folder with the uncompressed files (the non-ZIP folder) and open it up. Open the ""AI_and_Robotics_main.htm file. Enjoy!\n'], 'url_profile': 'https://github.com/sanjram', 'info_list': ['Java', 'Updated Jan 20, 2020', 'Updated Jan 19, 2020', 'Python', 'GPL-3.0 license', 'Updated Aug 31, 2020', 'Python', 'Updated Jan 16, 2020', 'Python', 'Updated Apr 24, 2020', 'MIT license', 'Updated Jan 19, 2020', 'HTML', 'Updated Jan 16, 2020', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 19, 2020']}","{'location': 'Dhanbad', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/rackson3861', 'info_list': ['Java', 'Updated Jan 20, 2020', 'Updated Jan 19, 2020', 'Python', 'GPL-3.0 license', 'Updated Aug 31, 2020', 'Python', 'Updated Jan 16, 2020', 'Python', 'Updated Apr 24, 2020', 'MIT license', 'Updated Jan 19, 2020', 'HTML', 'Updated Jan 16, 2020', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 19, 2020']}","{'location': 'Las Palmas de Gran Canaria', 'stats_list': [], 'contributions': '1,485 contributions\n        in the last year', 'description': ['Análisis de Sentimientos mediante Redes neuronales\n\nUniversidad Politécnica\nMáster Universitario en Inteligencia Artificial\nWeb Science\nEnero 2020\n\nAutores\n\nAntonio Sejas Mustafá\nElena Saa Noblejas\n\nINTRODUCCIÓN\nEl objetivo de esta práctica es realizar una aplicación implementando alguno de los métodos vistos en clase. Por tanto tuvimos que elegir entre Sistema de recomendación, Clasificación de documentos siguiendo Topic Models, Reconocimiento de Entidades o Análisis de Sentimiento.\nNosotros decidimos desarrollar este último proyecto. De este modo, nuestro objetivo es realizar una aplicación que dado un texto sea capaz de identificar si caracter positivo o negativo.\nMás concretamente hemos decidido trabajar sobre un dataset ya conocido. El dataset de reviews de películas de IMDB. En el siguiente apartado ampliamos la información sobre el dataset y comentamos dónde está disponible para su descarga.\nNuestros textos, como ya hemos comentado son reviews de películas, y el sentimiento será si una review le ha gustado a un usuario o no.\n¿Pero cómo vamos a evaluar nuestra aplicación?\nUn texto puede estar lleno de ambiguedad, incluso una misma review puede tener comentarios pariales de caracter positivo y otras críticas negativas. Nosotros vamos a ignorar estas situaciones y seguiremos el gold standard marcado por el artículo descrito en Maas, A. L. et. al. 2011 [1]\nEn este artículo se describe que las reviews positivas son aquellas que tengan una nota de 7 estrellas o más. Y de forma equivalente las negativas son las que tengan asociada una valoración de 0 a 4 estrellas. De esta forma no se tienen en cuenta los textos más ambiguos o indecisos, las reseñas Neutrales.\nEl Dataset de IMDB\nEste dataset ha sido realizado por los investigadores de Stanford autores del artículo original [1] .\nEl dataset original está disponible en: http://ai.stanford.edu/~amaas/data/sentiment/\nEl dataset cuenta con 50.000 reviews, que ellos utilizaron 25.000 para entrenamiento y 25.000 para testing. Nosotros para reducir complejidad solo usaremos una mitad, que hemos tratado previamente para eliminar caracteres raros y poner los textos en minúsculas.\nAdemás este dataset cuenta con un porcentaje equilibrado de reseñas. Siendo la mitad positivas y la otra mitad negativas.\nCada review tiene una etiqueta que lo categoriza de positiva o negativamente.\nTecnologías utilizadas\nA continuación describimos la metodología que hemos seguido para el analizador de sentimientos. Cada uno de estos puntos corresopnde con una sección del código.\nLimpieza del dataset\nEl primer paso es analizar y hacer un tratamiento de los textos del dataset. En el área de procesamiento del lenguaje natural hay una gran cantidad de alternativas y posibilidades. Es posible realizar distintas representación de los textos del corpus, y una gran variedad de extracción de características.\nEl planteamiento del analizador de sentimientos puede verse de alguna forma con un clasificador de textos, en el que se intenta clasificar un text (review) como positivo o negativo.\nPor este motivo todos los métodos de representación utiizados en PLN son válidos. Algunos de estos modelos son: bag of words, vector space model, tf-idf, topic model.\nNosotros hemos decidido obtener una bolsa de palabras (bag of words), teniendo en cuenta la frecuencia relativa con respecto a la otra clase. La idea es similar a un TF-IDF pero a nivel de clase. Esta representación nos permitirá darle más peso a las palabras más polarizadas.\nPor mantener determinar un límite en esta práctica no aplicaremos ningún procedimiento lematización ni stemming. Tampoco utilizaremos n-gramas. Únicamente eliminaremos las palabras vacías, stopwords, para reducir el ruido de los datos. La tokenización utilizada consiste en convertir las palabas en índices de un array. Todo este tipo de técnicas las hemos visto en clase y también se describen en más detalle en el libro""Natural Language Processing in Action"" [3]\nEntrenamiento\nDe forma similar a la limipieza del dataset y la extracción de variables predictoras, en el entrenamiento podemos utilizar prácticamente cualquier algoritmo de clasificación. Desde un Naive Bayes, Support Vector Machine, árboles de decisión o redes neuronales son algunas de las opciones más utilizadas.\nNosotros al no haber cursado ninguna asignatura de redes neuronales, hemos decidido utilizar una red neuronal, en concreto un Perceptrón multicapa.\nLa primera capa, capa de entrada, tendrá tantos nodos como palabras haya en nuestro vocabulario. La segunda capa tendrá 30 nodos, de forma experimental hemos observado un buen comportamiento con 10 a 30 nodos.\nPor último la capa de salida tendrá un solo nodo que dará un valor comprendido entre 0 y 1. Cuanto más cerca del 1 , más positiva se considerará la reseña. Un valor cercano al 0.5 se considerará la reseña ""neutral"".\nValidación\nPor último, nosotros hemos preferido evaluar la precisión de nuestro algoritmo utilizando un dropout 70/30 por sencillez de implementación. Una solución más profesional requeriría utilizar métodos de validación más sofistiados como un k-fold.\nAdemás hemos observado que incluso usando la mitad del dataset de entrenamiento, obtenemos valores muy cercanos al SVM descrito en el artículo [1]. En el artículo se alcanzan precisiones de entorno al 0.88, mientras que como veremos nuestra red se queda en 0.86 debido a falta de reducción de ruido comentada anteriormente.\nExtra\nDe forma adicional, hemos creado una celda con una caja de texto para que se pueda comprobar el fucionamiento con textos fuera del dataset. Esta caja de texto está identificada bajo el título ""Inserta un texto para probar el analizador de sentimientos"". Hay que escribir un texto y ejecutar esa celda y la siguiente para ver los resultados.\n\nEl código está autocontenido en este Jupyter Notebook. El cual está disponible online: https://colab.research.google.com/drive/11ZvUGrctfSbuTqa_tk3J-SNkEolqmZbK\nAdemás el código fuente y el dataset están disponibles en Github: https://github.com/sejas/muia-imdb-sentiment-analysis\n\nReferencias\n\n\nMaas, A. L., Daly, R. E., Pham, P. T., Huang, D., Ng, A. Y., & Potts, C. (2011, June). Learning word vectors for sentiment analysis. In Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies-volume 1 (pp. 142-150). Association for Computational Linguistics. Descargar Artículo\n\n\nHochreiter, Sepp & Schmidhuber, Jürgen. (1997). Long Short-term Memory. Neural computation. 9. 1735-80. 10.1162/neco.1997.9.8.1735.\n\n\nLane, H., Howard, C., & Hapke, H. M. (2019). Natural Language Processing in Action: Understanding, Analyzing, and Generating Text with Python. Manning Publications Company.\n\n\nCARGA DEL DATASET\n# Importar librerías\nimport sys\nimport pandas as pd\nimport numpy as np\nfrom collections import Counter\n# Carga de los datos en un dataframe\ndf = pd.read_csv(\'imdb.csv\', index_col=0)\ndf.head()\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nlabels\nreviews\n\n\n\n\n0\nPOSITIVE\nbromwell high is a cartoon comedy . it ran at ...\n\n\n1\nNEGATIVE\nstory of a man who has unnatural feelings for ...\n\n\n2\nPOSITIVE\nhomelessness  or houselessness as george carli...\n\n\n3\nNEGATIVE\nairport    starts as a brand new luxury    pla...\n\n\n4\nPOSITIVE\nbrilliant over  acting by lesley ann warren . ...\n\n\n\n\nlen(df)\n25000\n\nLos datos de imdb.csv han sido preprocesados y el contenido está preparado para contener solo caracteres en minúsculas. Esto es para simplificar el la identificación de las palabras, independientemente de cómo hayan sido escritas.\nANÁLISIS Y TRATAMIENTO PREVIO DEL DATASET\nUtilizando tres objetos Counter podemos calcular la frequencia absoluta para cada tipod e clase, positiva y negativa y un tercer counter para la contabilizar la frecuencia total de cada palabra en el corpus.\npositive_freq = Counter()\nnegative_freq = Counter()\ntotal_freq = Counter()\nAdemás de contabilizar la frecuencia de cada palabra en cada clase y en total, aprovechamos para eliminar las palabras vacías previamente conocidas y facilitadas por sklearn.\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\ncustom_stopwords = stop_words.ENGLISH_STOP_WORDS.union([\'br\', \'.\'])\ndef remove_stopwords(text):\n  return [word for word in text.split(\' \') if word not in custom_stopwords]\nfor _, (label, review) in df[df[\'labels\']==\'POSITIVE\'].iterrows():\n  positive_freq += Counter(remove_stopwords(review))\nfor _, (label, review) in df[df[\'labels\']==\'NEGATIVE\'].iterrows():\n  negative_freq += Counter(remove_stopwords(review))\n\ntotal_freq = positive_freq + negative_freq\n\nExtraemos las palabras de las reseñas positivas y negativas ordenándolas de más a menos comunes.\npositive_freq.most_common(20)\n[(\'\', 550468),\n (\'s\', 33815),\n (\'film\', 20937),\n (\'movie\', 19074),\n (\'t\', 13720),\n (\'like\', 9038),\n (\'good\', 7720),\n (\'just\', 7152),\n (\'story\', 6780),\n (\'time\', 6515),\n (\'great\', 6419),\n (\'really\', 5476),\n (\'people\', 4479),\n (\'best\', 4319),\n (\'love\', 4301),\n (\'life\', 4199),\n (\'way\', 4036),\n (\'films\', 3813),\n (\'think\', 3655),\n (\'movies\', 3586)]\n\nnegative_freq.most_common(20)\n[(\'\', 561462),\n (\'s\', 31546),\n (\'movie\', 24965),\n (\'t\', 20361),\n (\'film\', 19218),\n (\'like\', 11238),\n (\'just\', 10619),\n (\'good\', 7423),\n (\'bad\', 7401),\n (\'really\', 6262),\n (\'time\', 6209),\n (\'don\', 5336),\n (\'story\', 5208),\n (\'people\', 4806),\n (\'make\', 4722),\n (\'plot\', 4154),\n (\'movies\', 4080),\n (\'acting\', 4056),\n (\'way\', 3989),\n (\'think\', 3643)]\n\nAunque hayamos quitado las palabras vacías de un diccionario, hay un gran número de palabras vacías intrínsecas a nuestro dominio. En nuestro caso estas palabras que no aportan valor a la hora de distinguir entre una polarización positivia o negativa deberían ser consideradas como palabras vacías. Un ejemplo de estas palabras son muchas de las que aparecen en las listas de arriba. film, movie, acting y muchos nombres de actores y películas.\nA continuación calculamos el ratio de las palabras positivas entre las negativas, esto nos indicará si una palabra es muy positiva, neutra o nada positiva.\nLa forma de calcular este ratio de frecuencia es:\nnúmero de usos positivos / (número de usos negativos+1)\nSe le añade +1 al denominador para no dividir entre 0.\nMIN_FREQ = 200\npositive_negative_prop = Counter()\n\nfor word,freq in list(total_freq.most_common()):\n    if(freq > MIN_FREQ):\n        proportion = positive_freq[word] / float(negative_freq[word]+1)\n        positive_negative_prop[word] = proportion\nExaminamos el ratio de algunas palabras:\ndef check_words(words_list):\n  for word_to_check in words_list:\n    print(""Word \'%s\' = %s""%(word_to_check, positive_negative_prop[word_to_check]))\ncheck_words([\'film\', \'fantastic\', \'bad\'])\nWord \'film\' = 1.089390707112753\nWord \'fantastic\' = 4.503448275862069\nWord \'bad\' = 0.2576330721426641\n\nComo podemos ver, las palabras positivas tendrán valores muy altos. (>1)\nLas palabras neutrales que aparecen en reviews positivas o negativas, tendrán valores muy cercanos a 1. (Equilibradas)\nY las palabras negativas estarán muy próximas a 0.\nfor word,ratio in positive_negative_prop.most_common():\n    positive_negative_prop[word] = np.log(ratio)\nUna forma sencilla de normalizar estos valores y conseguir que las palabras neutrales estén en torno al 0 en vez de entorno al 1, es usando la función logaritmo.\nA continuación comprobamos las mismas palabras anterioremente comprobadas y observamos los nuevos valores normalizados.\ncheck_words([\'film\', \'fantastic\', \'bad\'])\nWord \'film\' = 0.08561855565085673\nWord \'fantastic\' = 1.5048433868558566\nWord \'bad\' = -1.3562189073456823\n\nArriba vemos que film, apenas aporta un valor discriminatorio.\nA continuación vemos la lista de palabras más polarizadas y sus nuevos valores.\npositive_negative_prop.most_common(20)\n[(\'victoria\', 2.681021528714291),\n (\'captures\', 2.038619547159581),\n (\'wonderfully\', 2.0218960560332353),\n (\'powell\', 1.978345424808467),\n (\'refreshing\', 1.8551812956655511),\n (\'delightful\', 1.8002701588959635),\n (\'beautifully\', 1.7626953362841438),\n (\'underrated\', 1.7197859696029656),\n (\'superb\', 1.7091514458966952),\n (\'welles\', 1.667706820558076),\n (\'sinatra\', 1.6389967146756448),\n (\'touching\', 1.637217476541176),\n (\'stewart\', 1.611998733295774),\n (\'brilliantly\', 1.5950491749820008),\n (\'friendship\', 1.5677652160335325),\n (\'wonderful\', 1.5645425925262093),\n (\'magnificent\', 1.54663701119507),\n (\'finest\', 1.546259010812569),\n (\'jackie\', 1.5439233053234738),\n (\'freedom\', 1.5091151908062312)]\n\nlist(reversed(positive_negative_prop.most_common()))[0:20]\n[(\'unfunny\', -2.6922395950755678),\n (\'waste\', -2.6193845640165536),\n (\'pointless\', -2.4553061800117097),\n (\'redeeming\', -2.3682390632154826),\n (\'lousy\', -2.307572634505085),\n (\'worst\', -2.286987896180378),\n (\'laughable\', -2.264363880173848),\n (\'awful\', -2.227194247027435),\n (\'poorly\', -2.2207550747464135),\n (\'sucks\', -1.987068221548821),\n (\'lame\', -1.981767458946166),\n (\'insult\', -1.978345424808467),\n (\'horrible\', -1.9102590939512902),\n (\'amateurish\', -1.9095425048844386),\n (\'pathetic\', -1.9003933102308506),\n (\'wasted\', -1.8382794848629478),\n (\'crap\', -1.8281271133989299),\n (\'tedious\', -1.802454758344803),\n (\'dreadful\', -1.7725281073001673),\n (\'badly\', -1.753626599532611)]\n\nLa aparición de ""Victoria"" parece indicar que sus películas tienen asociadas muy buenas críticas. Pero sabemos que en inglés hace referencia a un nombre propio, por lo que  para mejorar nuestra predicción habría considerar los nombres propios como stopwords.\npd.DataFrame(positive_negative_prop.most_common()).plot.hist(bins=50)\n<matplotlib.axes._subplots.AxesSubplot at 0x7ffb79d86fd0>\n\n\nEste histograma nos enseña la polaridad de las palabras en todo el corpus. Podemos observar que sigue una distribución normal con media en torno al 0. Es decir, la mayoría de las palabras están categorizadas como neutrales. Esto es ruido en nuestro clasificador. Esto se podría solucionar teniendo en cuenta aquellas palabas que aporten un valor discriminatorio mayor de |0.5|\nGENERANDO NUESTRO VOCABULARIO\nA continuación para ""tokenizar"" nuestros textos y convertirlos en vector de palabras, vamos a crear un vocabulario que será la entrada de nuestra red neuronal.\nvocab = set(total_freq.keys())\nvocab_size = len(vocab)\nprint(vocab_size)\n73759\n\nword2index = {}\nfor i,word in enumerate(vocab):  \n    word2index[word] = i\nword2index\n{\'\': 0,\n \'pork\': 1,\n \'cancer\': 2,\n \'hypermacho\': 3,\n \'beam\': 4,\n \'didja\': 5,\n \'sires\': 6,\n \'colonised\': 7,\n \'jest\': 8,\n \'fem\': 9,\n \'mitochondrial\': 10,\n \'azuma\': 11,\n \'stunk\': 12,\n \'attracting\': 13,\n \'cathernine\': 14,\n \'ventricle\': 15,\n \'ding\': 16,\n \'religous\': 17,\n \'training\': 18,\n \'cranks\': 19,\n \'hobbs\': 20,\n \'novac\': 21,\n \'millennia\': 22,\n \'zinn\': 23,\n \'sacrilage\': 24,\n \'mistry\': 25,\n \'sensualists\': 26,\n \'giff\': 27,\n \'bungling\': 28,\n \'raechel\': 29,\n \'swedes\': 30,\n \'miffed\': 31,\n \'ultimate\': 32,\n \'dought\': 33,\n \'plagiaristic\': 34,\n \'limned\': 35,\n \'jee\': 36,\n \'aracnophobia\': 37,\n \'centerpiece\': 38,\n \'unfaithal\': 39,\n \'knievel\': 40,\n \'ecstacy\': 41,\n \'trudged\': 42,\n \'alun\': 43,\n \'habituation\': 44,\n \'cannibalism\': 45,\n \'alarmist\': 46,\n \'looney\': 47,\n \'sudser\': 48,\n \'min\': 49,\n \'michelle\': 50,\n \'winninger\': 51,\n \'deployment\': 52,\n \'menzel\': 53,\n \'demonstrative\': 54,\n \'overpowered\': 55,\n \'seema\': 56,\n \'psychotics\': 57,\n \'coughthe\': 58,\n \'rollin\': 59,\n \'interferring\': 60,\n \'shimbei\': 61,\n \'orientated\': 62,\n \'traumatized\': 63,\n \'meriwether\': 64,\n \'kind\': 65,\n \'gruff\': 66,\n \'palsey\': 67,\n \'substories\': 68,\n \'acquittal\': 69,\n \'movecheck\': 70,\n \'compromised\': 71,\n \'zarustica\': 72,\n \'maadri\': 73,\n \'kaiser\': 74,\n \'budgetary\': 75,\n \'mt\': 76,\n \'factors\': 77,\n \'goulding\': 78,\n \'transposing\': 79,\n \'chineese\': 80,\n \'herbal\': 81,\n \'orkly\': 82,\n \'murderer\': 83,\n \'stephan\': 84,\n \'tage\': 85,\n \'forefathers\': 86,\n \'plays\': 87,\n \'dysfunction\': 88,\n \'gramophone\': 89,\n \'pendleton\': 90,\n \'juxtapositions\': 91,\n \'upto\': 92,\n \'excitement\': 93,\n \'ruphert\': 94,\n \'ultimo\': 95,\n \'mallorquins\': 96,\n \'lunacy\': 97,\n \'pratfalls\': 98,\n \'skyraiders\': 99,\n \'varela\': 100,\n \'rexes\': 101,\n \'mattresses\': 102,\n \'shvollenpecker\': 103,\n \'oversexed\': 104,\n \'taiwanese\': 105,\n \'toyota\': 106,\n \'neds\': 107,\n \'sugarman\': 108,\n \'facebuster\': 109,\n \'doel\': 110,\n \'veal\': 111,\n \'druidic\': 112,\n \'wary\': 113,\n \'extravaganzas\': 114,\n \'spiteful\': 115,\n \'sublime\': 116,\n \'nyfd\': 117,\n \'enthuses\': 118,\n \'wheaton\': 119,\n \'pharmaceutical\': 120,\n \'fulfill\': 121,\n \'innocence\': 122,\n \'undertake\': 123,\n \'infantile\': 124,\n \'crapfest\': 125,\n \'nec\': 126,\n \'shroyer\': 127,\n \'flour\': 128,\n \'valseuses\': 129,\n \'text\': 130,\n \'breasted\': 131,\n \'tachigui\': 132,\n \'additives\': 133,\n \'vanlint\': 134,\n \'mcphillip\': 135,\n \'impersonated\': 136,\n \'fictionalization\': 137,\n \'hitler\': 138,\n \'burry\': 139,\n \'curses\': 140,\n \'worn\': 141,\n \'thirbly\': 142,\n \'spitted\': 143,\n \'calhoun\': 144,\n \'hoyden\': 145,\n \'peculiarities\': 146,\n \'crops\': 147,\n \'blinding\': 148,\n \'gossemar\': 149,\n \'genghis\': 150,\n \'dusting\': 151,\n \'mausoleum\': 152,\n \'braincell\': 153,\n \'carrer\': 154,\n \'thumper\': 155,\n \'wale\': 156,\n \'beresford\': 157,\n \'coleman\': 158,\n \'deix\': 159,\n \'porkys\': 160,\n \'weasel\': 161,\n \'norton\': 162,\n \'garmes\': 163,\n \'croquet\': 164,\n \'aristocats\': 165,\n \'cigliutti\': 166,\n \'amore\': 167,\n \'casket\': 168,\n \'pending\': 169,\n \'mutated\': 170,\n \'probate\': 171,\n \'favourable\': 172,\n \'grandeurs\': 173,\n \'cavelleri\': 174,\n \'exasperated\': 175,\n \'kak\': 176,\n \'conflictive\': 177,\n \'paradoxically\': 178,\n \'aamir\': 179,\n \'aauugghh\': 180,\n \'onhand\': 181,\n \'deshimaru\': 182,\n \'strolls\': 183,\n \'grete\': 184,\n \'sickroom\': 185,\n \'clouded\': 186,\n \'baguettes\': 187,\n \'unabsorbing\': 188,\n \'sarajevo\': 189,\n \'sulk\': 190,\n \'chart\': 191,\n \'explore\': 192,\n \'permitted\': 193,\n \'malkovichian\': 194,\n \'whys\': 195,\n \'schlitz\': 196,\n \'disingenuous\': 197,\n \'hustle\': 198,\n \'immortel\': 199,\n \'insightfully\': 200,\n \'workforces\': 201,\n \'lyndon\': 202,\n \'aden\': 203,\n \'dunham\': 204,\n \'disbelieving\': 205,\n \'dunbar\': 206,\n \'segal\': 207,\n \'laroche\': 208,\n \'shakespearian\': 209,\n \'peasant\': 210,\n \'retention\': 211,\n \'concerted\': 212,\n \'serve\': 213,\n \'getz\': 214,\n \'discos\': 215,\n \'fused\': 216,\n \'looong\': 217,\n \'deceiving\': 218,\n \'ancients\': 219,\n \'brigadier\': 220,\n \'sistahs\': 221,\n \'violin\': 222,\n \'unengineered\': 223,\n \'deranged\': 224,\n \'lachlin\': 225,\n \'veoh\': 226,\n \'clung\': 227,\n \'ran\': 228,\n \'swabby\': 229,\n \'rataud\': 230,\n \'endearment\': 231,\n \'comity\': 232,\n \'bookend\': 233,\n \'waaaaaayyyy\': 234,\n \'siren\': 235,\n \'misleads\': 236,\n \'alrite\': 237,\n \'examination\': 238,\n \'panned\': 239,\n \'themsleves\': 240,\n \'wandered\': 241,\n \'simper\': 242,\n \'pliers\': 243,\n \'rump\': 244,\n \'cripplingly\': 245,\n \'scrawl\': 246,\n \'lewinski\': 247,\n \'gearheads\': 248,\n \'ktla\': 249,\n \'ambience\': 250,\n \'dozens\': 251,\n \'presumes\': 252,\n \'awards\': 253,\n \'surpressors\': 254,\n \'edits\': 255,\n \'difficulties\': 256,\n \'remar\': 257,\n \'wheelchairs\': 258,\n \'fiascos\': 259,\n \'claimed\': 260,\n \'waldeman\': 261,\n \'dangles\': 262,\n \'aloud\': 263,\n \'luncheon\': 264,\n \'cliffhangers\': 265,\n \'reminding\': 266,\n \'protected\': 267,\n \'serafinowicz\': 268,\n \'sorrell\': 269,\n \'bused\': 270,\n \'vulnerability\': 271,\n \'kaleidoscope\': 272,\n \'lizard\': 273,\n \'plateful\': 274,\n \'subbed\': 275,\n \'mpkdh\': 276,\n \'majkowski\': 277,\n \'eroticism\': 278,\n \'latecomers\': 279,\n \'outreach\': 280,\n \'visualizes\': 281,\n \'ramotswe\': 282,\n \'scientific\': 283,\n \'mcgaw\': 284,\n \'zb\': 285,\n \'mole\': 286,\n \'macho\': 287,\n \'uninstructive\': 288,\n \'resourceful\': 289,\n \'pumba\': 290,\n \'soleil\': 291,\n \'whopper\': 292,\n \'adhering\': 293,\n \'slobber\': 294,\n \'ai\': 295,\n \'lifelike\': 296,\n \'finisher\': 297,\n \'eponymous\': 298,\n \'shoudln\': 299,\n \'oyl\': 300,\n \'carrefour\': 301,\n \'argonne\': 302,\n \'golovanov\': 303,\n \'gunmen\': 304,\n \'palestinians\': 305,\n \'precocious\': 306,\n \'teapot\': 307,\n \'somtimes\': 308,\n \'aiden\': 309,\n \'curmudgeon\': 310,\n \'opting\': 311,\n \'imagery\': 312,\n \'stitches\': 313,\n \'irresistibly\': 314,\n \'ezra\': 315,\n \'hypesters\': 316,\n \'spritely\': 317,\n \'honeymooners\': 318,\n \'mined\': 319,\n \'muggings\': 320,\n \'fallow\': 321,\n \'grimm\': 322,\n \'fiddler\': 323,\n \'daneille\': 324,\n \'carelessness\': 325,\n \'braveheart\': 326,\n \'cahoots\': 327,\n \'reflexivity\': 328,\n \'agekudos\': 329,\n \'abdu\': 330,\n \'tick\': 331,\n \'kindling\': 332,\n \'flowed\': 333,\n \'terrifically\': 334,\n \'montegna\': 335,\n \'rest\': 336,\n \'unperceptive\': 337,\n \'fannin\': 338,\n \'hindersome\': 339,\n \'monique\': 340,\n \'einstein\': 341,\n \'lea\': 342,\n \'portrayed\': 343,\n \'garrett\': 344,\n \'arcaica\': 345,\n \'parlor\': 346,\n \'blight\': 347,\n \'abusing\': 348,\n \'gainful\': 349,\n \'infects\': 350,\n \'twiggy\': 351,\n \'storszek\': 352,\n \'tediousness\': 353,\n \'tigerland\': 354,\n \'spirited\': 355,\n \'skipping\': 356,\n \'gills\': 357,\n \'barrels\': 358,\n \'soni\': 359,\n \'guanajuato\': 360,\n \'burkhalter\': 361,\n \'ingela\': 362,\n \'emulations\': 363,\n \'estefan\': 364,\n \'adlai\': 365,\n \'trainor\': 366,\n \'attraction\': 367,\n \'adma\': 368,\n \'flippantly\': 369,\n \'irritated\': 370,\n \'pendant\': 371,\n \'annoyed\': 372,\n \'storaro\': 373,\n \'az\': 374,\n \'punters\': 375,\n \'radical\': 376,\n \'unresponsive\': 377,\n \'printer\': 378,\n \'hmmmmmmmm\': 379,\n \'portrayer\': 380,\n \'gained\': 381,\n \'lars\': 382,\n \'willed\': 383,\n \'appreciation\': 384,\n \'herilhy\': 385,\n \'campy\': 386,\n \'fahrenheit\': 387,\n \'rodrix\': 388,\n \'nordham\': 389,\n \'underfoot\': 390,\n \'woolgathering\': 391,\n \'bs\': 392,\n \'aldonova\': 393,\n \'elequence\': 394,\n \'suspending\': 395,\n \'incubates\': 396,\n \'sans\': 397,\n \'misfire\': 398,\n \'reassuring\': 399,\n \'jerri\': 400,\n \'rework\': 401,\n \'utilities\': 402,\n \'handlers\': 403,\n \'margineanus\': 404,\n \'cos\': 405,\n \'masters\': 406,\n \'widened\': 407,\n \'excuse\': 408,\n \'pinkish\': 409,\n \'split\': 410,\n \'kewl\': 411,\n \'attract\': 412,\n \'wavy\': 413,\n \'alda\': 414,\n \'recognizable\': 415,\n \'whip\': 416,\n \'securing\': 417,\n \'insular\': 418,\n \'idiosyncratic\': 419,\n \'hayseed\': 420,\n \'tukur\': 421,\n \'advisedly\': 422,\n \'proposal\': 423,\n \'espeically\': 424,\n \'astrotech\': 425,\n \'shoufukutei\': 426,\n \'muncie\': 427,\n \'notoriety\': 428,\n \'escapism\': 429,\n \'outburst\': 430,\n \'hipper\': 431,\n \'condon\': 432,\n \'prix\': 433,\n \'glop\': 434,\n \'lespart\': 435,\n \'occupational\': 436,\n \'slacken\': 437,\n \'kerkhof\': 438,\n \'gymnasts\': 439,\n \'rigorous\': 440,\n \'jame\': 441,\n \'definetly\': 442,\n \'someway\': 443,\n \'caresses\': 444,\n \'deepak\': 445,\n \'sutdying\': 446,\n \'da\': 447,\n \'groundwork\': 448,\n \'ford\': 449,\n \'pentimento\': 450,\n \'hanns\': 451,\n \'drab\': 452,\n \'der\': 453,\n \'underwear\': 454,\n \'casper\': 455,\n \'puppetry\': 456,\n \'pakis\': 457,\n \'pearlman\': 458,\n \'bets\': 459,\n \'deservingly\': 460,\n \'hesitates\': 461,\n \'liberty\': 462,\n \'inconvenience\': 463,\n \'grosbard\': 464,\n \'steam\': 465,\n \'mounts\': 466,\n \'warnercolor\': 467,\n \'matt\': 468,\n \'beatific\': 469,\n \'colwell\': 470,\n \'slumping\': 471,\n \'doings\': 472,\n \'miswrote\': 473,\n \'jodoworsky\': 474,\n \'floods\': 475,\n \'enticement\': 476,\n \'rigueur\': 477,\n \'starsky\': 478,\n \'nick\': 479,\n \'monumentous\': 480,\n \'naffness\': 481,\n \'scratched\': 482,\n \'mays\': 483,\n \'starblazers\': 484,\n \'doves\': 485,\n \'wellpaced\': 486,\n \'growls\': 487,\n \'mist\': 488,\n \'ropes\': 489,\n \'baltimoreans\': 490,\n \'touch\': 491,\n \'aja\': 492,\n \'valga\': 493,\n \'recur\': 494,\n \'contreras\': 495,\n \'unbearded\': 496,\n \'cassetti\': 497,\n \'cascading\': 498,\n \'megapack\': 499,\n \'bandido\': 500,\n \'sprays\': 501,\n \'smuttiness\': 502,\n \'ladder\': 503,\n \'dosage\': 504,\n \'milwall\': 505,\n \'competent\': 506,\n \'hilltop\': 507,\n \'discomfort\': 508,\n \'stutter\': 509,\n \'draughtswoman\': 510,\n \'stockpile\': 511,\n \'littlekuriboh\': 512,\n \'bootie\': 513,\n \'disappoints\': 514,\n \'koz\': 515,\n \'proceeded\': 516,\n \'solimeno\': 517,\n \'avian\': 518,\n \'wicked\': 519,\n \'scales\': 520,\n \'howls\': 521,\n \'pleasaunces\': 522,\n \'shead\': 523,\n \'wickerman\': 524,\n \'xylophonist\': 525,\n \'companys\': 526,\n \'lorado\': 527,\n \'undertook\': 528,\n \'utopia\': 529,\n \'chihiro\': 530,\n \'courtesan\': 531,\n \'democratically\': 532,\n \'broad\': 533,\n \'conniving\': 534,\n \'photographic\': 535,\n \'davidbathsheba\': 536,\n \'glum\': 537,\n \'militaries\': 538,\n \'unfairly\': 539,\n \'ohio\': 540,\n \'talosian\': 541,\n \'grafted\': 542,\n \'cof\': 543,\n \'evers\': 544,\n \'bogglingly\': 545,\n \'overheating\': 546,\n \'mammothly\': 547,\n \'unfurnished\': 548,\n \'loves\': 549,\n \'battle\': 550,\n \'qi\': 551,\n \'tragedy\': 552,\n \'blonde\': 553,\n \'dystopic\': 554,\n \'cineasts\': 555,\n \'antonius\': 556,\n \'tarka\': 557,\n \'bloodthirst\': 558,\n \'milieu\': 559,\n \'vivant\': 560,\n \'censured\': 561,\n \'stinkpile\': 562,\n \'differential\': 563,\n \'affirmation\': 564,\n \'lydia\': 565,\n \'superlivemation\': 566,\n \'financially\': 567,\n \'pac\': 568,\n \'funiest\': 569,\n \'revolving\': 570,\n \'applauds\': 571,\n \'sperr\': 572,\n \'sybil\': 573,\n \'pedestrians\': 574,\n \'promise\': 575,\n \'elam\': 576,\n \'gazongas\': 577,\n \'categorised\': 578,\n \'tura\': 579,\n \'jeb\': 580,\n \'opportune\': 581,\n \'furgusson\': 582,\n \'irl\': 583,\n \'refuge\': 584,\n \'enacting\': 585,\n \'disenchantment\': 586,\n \'tis\': 587,\n \'breads\': 588,\n \'transposed\': 589,\n \'sivan\': 590,\n \'johan\': 591,\n \'siu\': 592,\n \'beswick\': 593,\n \'vlkava\': 594,\n \'auburn\': 595,\n \'gurl\': 596,\n \'figuring\': 597,\n \'numbingly\': 598,\n \'soft\': 599,\n \'centred\': 600,\n \'harrowed\': 601,\n \'hearkens\': 602,\n \'joeseph\': 603,\n \'moovies\': 604,\n \'witchie\': 605,\n \'cigs\': 606,\n \'stage\': 607,\n \'mitevska\': 608,\n \'roulette\': 609,\n \'rolly\': 610,\n \'ramchand\': 611,\n \'mulit\': 612,\n \'ameteurish\': 613,\n \'supplicant\': 614,\n \'compositor\': 615,\n \'pointer\': 616,\n \'dooooosie\': 617,\n \'rembrandt\': 618,\n \'skolimowski\': 619,\n \'vangelis\': 620,\n \'dzundza\': 621,\n \'cherri\': 622,\n \'harvested\': 623,\n \'filmmakers\': 624,\n \'essendon\': 625,\n \'nicolie\': 626,\n \'reassigned\': 627,\n \'calvins\': 628,\n \'refinery\': 629,\n \'amrish\': 630,\n \'lesson\': 631,\n \'nris\': 632,\n \'clerical\': 633,\n \'oooo\': 634,\n \'medication\': 635,\n \'phenomenons\': 636,\n \'santoni\': 637,\n \'moronfest\': 638,\n \'soviet\': 639,\n \'harden\': 640,\n \'relationsip\': 641,\n \'roofer\': 642,\n \'afar\': 643,\n \'neptune\': 644,\n \'unforgetable\': 645,\n \'sorcha\': 646,\n \'ditz\': 647,\n \'mehemet\': 648,\n \'advice\': 649,\n \'romantisised\': 650,\n \'ulcerating\': 651,\n \'millimeter\': 652,\n \'snorer\': 653,\n \'glady\': 654,\n \'daylights\': 655,\n \'anorexia\': 656,\n \'gettysburg\': 657,\n \'foe\': 658,\n \'suck\': 659,\n \'ising\': 660,\n \'johar\': 661,\n \'cradled\': 662,\n \'womennone\': 663,\n \'clampets\': 664,\n \'ishwar\': 665,\n \'dandies\': 666,\n \'jughead\': 667,\n \'themself\': 668,\n \'chundering\': 669,\n \'shipment\': 670,\n \'owed\': 671,\n \'wrestlemanias\': 672,\n \'commercisliation\': 673,\n \'vooren\': 674,\n \'shipped\': 675,\n \'brogues\': 676,\n \'nectar\': 677,\n \'kitties\': 678,\n \'buyer\': 679,\n \'tapers\': 680,\n \'leidner\': 681,\n \'perverted\': 682,\n \'vaticani\': 683,\n \'insouciance\': 684,\n \'iannaccone\': 685,\n \'succulently\': 686,\n \'apprehending\': 687,\n \'mitchel\': 688,\n \'workday\': 689,\n \'titty\': 690,\n \'oppenheimer\': 691,\n \'eser\': 692,\n \'tassel\': 693,\n \'sumptuousness\': 694,\n \'intonations\': 695,\n \'cherubic\': 696,\n \'franklin\': 697,\n \'propane\': 698,\n \'senegalese\': 699,\n \'compiled\': 700,\n \'arret\': 701,\n \'intrusively\': 702,\n \'wrinkle\': 703,\n \'urmila\': 704,\n \'buds\': 705,\n \'librarians\': 706,\n \'cubbyholes\': 707,\n \'portends\': 708,\n \'interconnecting\': 709,\n \'posterity\': 710,\n \'norseman\': 711,\n \'episodic\': 712,\n \'bleating\': 713,\n \'frumpy\': 714,\n \'ofcourse\': 715,\n \'rouged\': 716,\n \'voerhoven\': 717,\n \'stun\': 718,\n \'beret\': 719,\n \'scrutinized\': 720,\n \'sequenes\': 721,\n \'inhumanity\': 722,\n \'merkle\': 723,\n \'vomitum\': 724,\n \'gobbler\': 725,\n \'plastique\': 726,\n \'frownbuster\': 727,\n \'turaqui\': 728,\n \'sanju\': 729,\n \'x\': 730,\n \'chakraborty\': 731,\n \'curator\': 732,\n \'strategies\': 733,\n \'orientals\': 734,\n \'poorly\': 735,\n \'glass\': 736,\n \'fellowship\': 737,\n \'spaz\': 738,\n \'decomp\': 739,\n \'warbler\': 740,\n \'aonghas\': 741,\n \'withouts\': 742,\n \'bergqvist\': 743,\n \'dutt\': 744,\n \'maclaine\': 745,\n \'prowls\': 746,\n \'millie\': 747,\n \'turbulent\': 748,\n \'clunks\': 749,\n \'shards\': 750,\n \'conaughey\': 751,\n \'pounced\': 752,\n \'lineal\': 753,\n \'justicia\': 754,\n \'ksm\': 755,\n \'parnell\': 756,\n \'alcoholic\': 757,\n \'seafood\': 758,\n \'marienbad\': 759,\n \'mander\': 760,\n \'rowdy\': 761,\n \'designates\': 762,\n \'cheerless\': 763,\n \'hallgren\': 764,\n \'bastidge\': 765,\n \'aubrey\': 766,\n \'panoramas\': 767,\n \'ke\': 768,\n \'blige\': 769,\n \'nicks\': 770,\n \'taunts\': 771,\n \'thingie\': 772,\n \'zerifferelli\': 773,\n \'fisticuff\': 774,\n \'dakota\': 775,\n \'stettner\': 776,\n \'relaxers\': 777,\n \'cared\': 778,\n \'entrenchments\': 779,\n \'jaipur\': 780,\n \'rosco\': 781,\n \'murkily\': 782,\n \'karogi\': 783,\n \'sharpe\': 784,\n \'msb\': 785,\n \'kelemen\': 786,\n \'anal\': 787,\n \'entities\': 788,\n \'hagerthy\': 789,\n \'hyderabadi\': 790,\n \'indulgent\': 791,\n \'chicatillo\': 792,\n \'capabilities\': 793,\n \'leguizamo\': 794,\n \'couleur\': 795,\n \'apostrophe\': 796,\n \'uncynical\': 797,\n \'sadomasochism\': 798,\n \'retreated\': 799,\n \'kimmell\': 800,\n \'artistry\': 801,\n \'helen\': 802,\n \'stagnation\': 803,\n \'globalizing\': 804,\n \'puh\': 805,\n \'prosaically\': 806,\n \'redhead\': 807,\n \'footsteps\': 808,\n \'longtime\': 809,\n \'axiomatic\': 810,\n \'fans\': 811,\n \'xtianity\': 812,\n \'alucard\': 813,\n \'predominant\': 814,\n \'lynchings\': 815,\n \'fielding\': 816,\n \'contessa\': 817,\n \'fried\': 818,\n \'abortive\': 819,\n \'underscored\': 820,\n \'adroitly\': 821,\n \'awkwardness\': 822,\n \'sinese\': 823,\n \'travelcard\': 824,\n \'maryam\': 825,\n \'intact\': 826,\n \'ads\': 827,\n \'northam\': 828,\n \'nafta\': 829,\n \'matlock\': 830,\n \'madchen\': 831,\n \'swung\': 832,\n \'numero\': 833,\n \'genetics\': 834,\n \'ashley\': 835,\n \'scot\': 836,\n \'zeffirelli\': 837,\n \'slayers\': 838,\n \'duquenne\': 839,\n \'quibble\': 840,\n \'fumes\': 841,\n \'zues\': 842,\n \'pap\': 843,\n \'lasciviousness\': 844,\n \'cukor\': 845,\n \'lemuria\': 846,\n \'pejorative\': 847,\n \'toto\': 848,\n \'midway\': 849,\n \'vadis\': 850,\n \'sliced\': 851,\n \'businesspeople\': 852,\n \'homey\': 853,\n \'artisticly\': 854,\n \'refracted\': 855,\n \'dysfunctions\': 856,\n \'atlanteans\': 857,\n \'baby\': 858,\n \'bassis\': 859,\n \'reconstructions\': 860,\n \'johannesburg\': 861,\n \'jaret\': 862,\n \'hungarian\': 863,\n \'useless\': 864,\n \'indestructible\': 865,\n \'jacuzzi\': 866,\n \'kayyyy\': 867,\n \'mi\': 868,\n \'milinkovic\': 869,\n \'dioz\': 870,\n \'discombobulation\': 871,\n \'abm\': 872,\n \'vise\': 873,\n \'lovesick\': 874,\n \'faraway\': 875,\n \'unheated\': 876,\n \'bogie\': 877,\n \'messmer\': 878,\n \'foreseeing\': 879,\n \'labouf\': 880,\n \'phoenicia\': 881,\n \'overhears\': 882,\n \'remunda\': 883,\n \'unraveling\': 884,\n \'daisies\': 885,\n \'aristide\': 886,\n \'bedingfield\': 887,\n \'cheesecake\': 888,\n \'terrace\': 889,\n \'flagship\': 890,\n \'pickup\': 891,\n \'escalating\': 892,\n \'uttara\': 893,\n \'gunshots\': 894,\n \'meres\': 895,\n \'savales\': 896,\n \'horrorfilm\': 897,\n \'sheeple\': 898,\n \'twine\': 899,\n \'aboriginies\': 900,\n \'hoydenish\': 901,\n \'reshipping\': 902,\n \'wouln\': 903,\n \'speckle\': 904,\n \'befuddled\': 905,\n \'liebe\': 906,\n \'mopey\': 907,\n \'steffen\': 908,\n \'noises\': 909,\n \'install\': 910,\n \'barren\': 911,\n \'workaholics\': 912,\n \'alcoholism\': 913,\n \'bashing\': 914,\n \'ilu\': 915,\n \'disrupts\': 916,\n \'republics\': 917,\n \'briliant\': 918,\n \'rheubottom\': 919,\n \'eludes\': 920,\n \'endearing\': 921,\n \'alexanderplatz\': 922,\n \'judgment\': 923,\n \'colorfully\': 924,\n \'darlene\': 925,\n \'buckaroo\': 926,\n \'machettes\': 927,\n \'moncia\': 928,\n \'gaita\': 929,\n \'doctresses\': 930,\n \'thunderbolt\': 931,\n \'flak\': 932,\n \'vanquishes\': 933,\n \'supermen\': 934,\n \'shuddup\': 935,\n \'pinkie\': 936,\n \'sensations\': 937,\n \'elvira\': 938,\n \'guessed\': 939,\n \'consults\': 940,\n \'amatuerish\': 941,\n \'corsair\': 942,\n \'munitions\': 943,\n \'git\': 944,\n \'hectic\': 945,\n \'septic\': 946,\n \'flapper\': 947,\n \'atherton\': 948,\n \'anons\': 949,\n \'motos\': 950,\n \'bambou\': 951,\n \'childish\': 952,\n \'reviczky\': 953,\n \'graystone\': 954,\n \'mandate\': 955,\n \'heightens\': 956,\n \'petron\': 957,\n \'rods\': 958,\n \'excell\': 959,\n \'collection\': 960,\n \'macrae\': 961,\n \'wiping\': 962,\n \'delli\': 963,\n \'poltergeist\': 964,\n \'minutia\': 965,\n \'malikka\': 966,\n \'upbringings\': 967,\n \'noisier\': 968,\n \'ifyou\': 969,\n \'manchu\': 970,\n \'prophets\': 971,\n \'mice\': 972,\n \'leit\': 973,\n \'morrocco\': 974,\n \'korman\': 975,\n \'reviving\': 976,\n \'slowed\': 977,\n \'epstein\': 978,\n \'testified\': 979,\n \'bassett\': 980,\n \'bendan\': 981,\n \'punkris\': 982,\n \'scolded\': 983,\n \'okinawan\': 984,\n \'poisoning\': 985,\n \'blueprints\': 986,\n \'impalement\': 987,\n \'bethune\': 988,\n \'ted\': 989,\n \'dissertations\': 990,\n \'oops\': 991,\n \'deesh\': 992,\n \'chaya\': 993,\n \'gunnerside\': 994,\n \'mano\': 995,\n \'advision\': 996,\n \'negro\': 997,\n \'postino\': 998,\n \'dumbed\': 999,\n ...}\n\nCLASIFICADOR MEDIANTE RED NEURONAL\nYa nos acercamos al final de nuestro analizador de sentimientos. Hemos decidido utilizar una red neuronal clásica, que como ya hemos comentado en la introducción posee 3 capas. La primera que es de entradas y tiene la longitud de nuestro vocabulario. Cada review se codificará como un vector en el que cada cada elemento representa la frequencia de apariciones de esa palabra en el texto. La capa intermedia tiene 30 nodos, y finalmente la capa final tiene un solo nodo de salida.\nNuestro clasificador puede en modo entrenamiento y test devuelve 2 etiquetas Positivo o Negativo. Y en modo interactivo (logger), además devuelve el nivel de confianza y una tercera etiqueta Neutral.\nclass SentimentReviewClassifier:\n    def __init__(self, learning_rate = 0.01):\n        np.random.seed(7)\n        self.input_nodes = len(vocab)\n        self.middle_nodes = 30\n        self.final_nodes = 1\n        self.learning_rate = learning_rate\n\n        # Initialize net\n        self.hidden_0_1 = np.zeros((self.input_nodes,self.middle_nodes))\n        self.hidden_1_2 = np.random.normal(0.0, self.middle_nodes**-0.5, \n                                                (self.middle_nodes, self.final_nodes))\n        self.first_layer = np.zeros((1,self.middle_nodes))\n    \n    translate_label = {\n        \'POSITIVE\': 1,\n        \'NEGATIVE\': 0,\n    }\n        \n    def sigmoid(self,x):\n        return 1 / (1 + np.exp(-x))\n    \n    def sigmoid_output_2_derivative(self,output):\n        return output * (1 - output)\n    def show_progress(self, i, total, correct):\n        progress =  str(100 * i/float(total))[:4]\n        accuracy =  str(correct * 100 / float(i+1))[:4]\n        sys.stdout.write(""\\r - Progress:%s %%| Correct:%s | Accuracy:%s%%""%(progress, correct, accuracy))\n        \n    def train(self, reviews_corpus, labels_t):\n        """""" Update weights from corpus""""""\n        reviews = list()\n        for review in reviews_corpus:\n            indices = set()\n            for word in remove_stopwords(review):\n                if(word in word2index.keys()):\n                    indices.add(word2index[word])\n            reviews.append(list(indices))\n\n        correct = 0\n        for i in range(len(reviews)):\n            review = reviews[i]\n            label = labels_t[i]\n            # Training\n            self.first_layer *= 0\n            for index in review:\n                self.first_layer += self.hidden_0_1[index]\n            second_layer = self.sigmoid(self.first_layer.dot(self.hidden_1_2))            \n\n            # Output error\n            second_layer_error = second_layer - self.translate_label[label]\n            second_layer_delta = second_layer_error * self.sigmoid_output_2_derivative(second_layer)\n\n            # Backpropagated error\n            first_layer_error = second_layer_delta.dot(self.hidden_1_2.T)\n            first_layer_delta = first_layer_error\n            self.hidden_1_2 -= self.first_layer.T.dot(second_layer_delta) * self.learning_rate\n\n            for index in review:\n                self.hidden_0_1[index] -= first_layer_delta[0] * self.learning_rate\n\n            if(second_layer >= 0.5 and label == \'POSITIVE\'):\n                correct += 1\n            elif(second_layer < 0.5 and label == \'NEGATIVE\'):\n                correct += 1\n            self.show_progress(i, len(reviews), correct)\n            if(i % 2500 == 0):\n                print("""")\n    \n    def test(self, reviews, testing_labels):\n        """""" Test and don\'t update the weights """"""\n        correct = 0\n        for i in range(len(reviews)):\n            pred = self.run(reviews[i])\n            if(pred == testing_labels[i]):\n                correct += 1\n            self.show_progress(i, len(reviews), correct)\n    \n    def run(self, review, logger = False):\n        """""" Evaluate a single review""""""\n        self.first_layer *= 0\n        unique_indices = set()\n        for word in remove_stopwords(review):\n            if word in word2index.keys():\n                unique_indices.add(word2index[word])\n        for index in unique_indices:\n            self.first_layer += self.hidden_0_1[index]\n        second_layer = self.sigmoid(self.first_layer.dot(self.hidden_1_2))\n        out = second_layer[0]\n        threshold = 0\n        if logger:\n          print(out)\n          threshold = 0.05\n        if out >= 0.5 + threshold:\n            return ""POSITIVE""\n        elif out < 0.5 - threshold:\n            return ""NEGATIVE""\n        else:\n            return ""NEUTRAL""\n# Dividimos el dataset en 70% Training y 30% Testing\nDROPOUT_PARTITION = 0.7\nSPLIT_PART = int(len(df)*DROPOUT_PARTITION)\ndf_train = df.iloc[:SPLIT_PART]\ndf_test = df.iloc[SPLIT_PART:]\nprint(""DROPOUT SPLIT Train: %d, Test: %d, TOTAL: %d""%(len(df_train), len(df_test), len(df_train)+len(df_test)))\ndf_test.head()\nDROPOUT SPLIT Train: 17500, Test: 7500, TOTAL: 25000\n\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n</style>\n\n\n\n\nlabels\nreviews\n\n\n\n\n17500\nPOSITIVE\none reason pixar has endured so well  and been...\n\n\n17501\nNEGATIVE\ni saw the film and i got screwed  because the ...\n\n\n17502\nPOSITIVE\na scanner darkly  minority report  blade runne...\n\n\n17503\nNEGATIVE\nwhat  s happening to rgv  he seems to repeat h...\n\n\n17504\nPOSITIVE\ni  ve seen this film in avant  premiere at ima...\n\n\n\n\nnet = SentimentReviewClassifier(learning_rate=0.02)\nnet.train(df_train[\'reviews\'],df_train[\'labels\'])\n - Progress:0.0 %| Correct:1 | Accuracy:100.%\n - Progress:14.2 %| Correct:2028 | Accuracy:81.0%\n - Progress:28.5 %| Correct:4129 | Accuracy:82.5%\n - Progress:42.8 %| Correct:6277 | Accuracy:83.6%\n - Progress:57.1 %| Correct:8469 | Accuracy:84.6%\n - Progress:71.4 %| Correct:10629 | Accuracy:85.0%\n - Progress:85.7 %| Correct:12785 | Accuracy:85.2%\n - Progress:99.9 %| Correct:14934 | Accuracy:85.3%\n\n# Cambiamos la forma de indexar por problemas en algún dato en el dataframe.\nnet.test(df_test.iloc[:,1].values, df_test.iloc[:,0].values)\n - Progress:99.9 %| Correct:6453 | Accuracy:86.0%\n\n# Example of a single review\nnet.run(\'This a great film fantastic actors\', logger=True)\n[0.69505086]\n\n\n\n\n\n\'POSITIVE\'\n\n#@title ### Inserta un texto para probar el analizador de sentimientos\nreview = ""This movie is the best in the world"" #@param {type:""string""}\nprint(\'Review a analizar: ""%s""\'%review)\nprint(\'La Review es: %s\'% net.run(review, logger=True))\nReview a analizar: ""This movie is the best in the world""\n[0.6628306]\nLa Review es: POSITIVE\n\nCONCLUSIONES\nComo hemos podido observar, hemos obtenido resultados muy semejantes a los propuestos en el paper [1].\nExisten multitud de aproximaciones a un mismo problema.\nDeterminar la polaridad positiva o negativa de una reseña se puede conseguir con modelos relativamente sencillos.\nObtener datos más precisos, como qué tipo de sentimiento expresa, enfado, ira, amor, felicidad son un reto todavía en investigación.\nTécnicas muy similares propuestas en esta práctica se pueden utilizar para detectar reseñas fraudulentas, la dificultad está en conseguir un dataset etiquetado.\nUn analizador de sentimiento se puede simplificar a un clasificador de textos, en el que cada tópico es el sentimiento que queremos clasificar.\nPosibles mejoras, un mayor tratamiento en la reducción ruido, mediante la eliminación de palabras vacías aumentaría la precisión de nuestra red.\nExisten algoritmos más avanzados que posilemente den mejores resultados. Modelizar el corpus como word embeddings es una alternativa. Otra opción sería utilizar LSTM (Long short-term memory) [2]. Ambos sistemas tienen en cuenta las palabras que están cercanas y tenemos seguridad de que producirían mejores resultados.\nAunque nosotros hemos tomado una vía muy rudimentaria para ir comentando y describiendo la metodología paso a paso, existen varias librerías que pueden simplificar nuestro código. Nosotros aconsejamos la utilización de estas librerías en sistemas reales. Algunas de estas librerías son: sklearn, pytorch, tensorflow, keras, nltk, scipy entre otras.\nPor último quremos destacar que este analizador de sentimientos funcionará bien con el dominio de películas en el idioma inglés, pero no sería el más adecuado para corpus de otros dominios, y por supuesto el resultado no sería fiable en el caso de clasificar documentos que no tengan ninguna palabra de nuestro vocabulario.\nDe hecho al realizar esta prueba, se puede observar que el clasificador tiene un bias positio. Si no introduces ninguna palabra clasifica el texto como positivo con una confianza de 0.55.\n'], 'url_profile': 'https://github.com/sejas', 'info_list': ['Java', 'Updated Jan 20, 2020', 'Updated Jan 19, 2020', 'Python', 'GPL-3.0 license', 'Updated Aug 31, 2020', 'Python', 'Updated Jan 16, 2020', 'Python', 'Updated Apr 24, 2020', 'MIT license', 'Updated Jan 19, 2020', 'HTML', 'Updated Jan 16, 2020', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 19, 2020']}","{'location': 'Ottawa, Ontario, Canada', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['CanDev\n'], 'url_profile': 'https://github.com/sourabhagarwal07', 'info_list': ['Java', 'Updated Jan 20, 2020', 'Updated Jan 19, 2020', 'Python', 'GPL-3.0 license', 'Updated Aug 31, 2020', 'Python', 'Updated Jan 16, 2020', 'Python', 'Updated Apr 24, 2020', 'MIT license', 'Updated Jan 19, 2020', 'HTML', 'Updated Jan 16, 2020', 'Updated Jan 17, 2020', 'Jupyter Notebook', 'Updated Jan 16, 2020', 'Python', 'Updated Jan 19, 2020']}"
"{'location': 'Greece', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['8puzzle\nSolving the 8puzzle game using BFS and A* search algorithm and compare their effectivenes. Group project for Artificial Intelligence, UOWM Semester 2019-2020\n'], 'url_profile': 'https://github.com/AndrewManitsas', 'info_list': ['C', 'GPL-3.0 license', 'Updated Jan 23, 2020', 'Python', 'Updated Apr 18, 2020', 'Dart', 'Updated May 13, 2020', '5', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Updated Jan 13, 2020', 'Updated Sep 22, 2020', 'Python', 'Updated Apr 10, 2020', 'C#', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 19, 2020', 'Updated Jan 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '16 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/gabbirenee', 'info_list': ['C', 'GPL-3.0 license', 'Updated Jan 23, 2020', 'Python', 'Updated Apr 18, 2020', 'Dart', 'Updated May 13, 2020', '5', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Updated Jan 13, 2020', 'Updated Sep 22, 2020', 'Python', 'Updated Apr 10, 2020', 'C#', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 19, 2020', 'Updated Jan 17, 2020']}","{'location': 'NIT Durgapur', 'stats_list': [], 'contributions': '72 contributions\n        in the last year', 'description': ['Flutter App :\nADKit: Smartphone Based Artificial Intelligence Enabled Portable Low-cost Anemia Detection Kit based on Observation of Nail and Palm Pallor\nUnder MEITY\nStatus : Ongoing\nChange Log\n\n\nEnabled feature to upload video from gallery or record a new one\n\n\nAdded Different files for Home Page, Login Page, and Auth File.\n\n\nIntegrated Firebase to Primary Email\n\n\nAdded Firebase Authentication with Custom Email of Gmail\n\n\nEnabled Firebase Storage\n\n\nMinor UI Changes\n\n\nApp now Uploads videos to Firebase Storage\n\n\n'], 'url_profile': 'https://github.com/kulkarni-rajas', 'info_list': ['C', 'GPL-3.0 license', 'Updated Jan 23, 2020', 'Python', 'Updated Apr 18, 2020', 'Dart', 'Updated May 13, 2020', '5', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Updated Jan 13, 2020', 'Updated Sep 22, 2020', 'Python', 'Updated Apr 10, 2020', 'C#', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 19, 2020', 'Updated Jan 17, 2020']}","{'location': 'Southampton, UK', 'stats_list': [], 'contributions': '273 contributions\n        in the last year', 'description': ['Artificial Intelligence for Web Accessibility\nThis is the GitHub repository for my Masters dissertation titled: Artificial Intelligence for Web Accessibility which I completed as a part of my MSc in\nData Science course in the University of Southampton, UK under the supervision of Prof. Mike Wald\nThis project provided me an opportunity to apply my knowledge in machine Learning and Deep Learning to a problem that impacts people\'s lives.\nThe project mainly focuses on applying AI technologies to make the web more accessible to people who are differently abled. We take the knowledge and information\navailable on the internet for granted; but not everyone is so fortunate. This project is my modest attempt to work on this problem. There is a lot of scope to extend\nthis work and please feel free to contact me if you have any ideas/queries/suggestions!\nLinkedIn: Shaunak Sen\nEmail: shaunak1105@gmailcom\nThe project mainly focuses on two parts:\n\nAutomatic Image Captioning System\nContextual Hyperlink Detection\n\nThis document only provides an overview. For details please refer to the full report here\nAutomatic Image Captioning System\nThe Problem\nThe World Wide Web Consortium (W3C) is an organization responsible for developing and maintaining web standards such as HTML, CSS, etc. (57). The Web Content Accessibility Guidelines (WCAG) is developed through the W3C process and it aims to create and maintain a single set of guidelines and recommendations for individuals, organizations, and governments internationally to follow to make web content more accessible and inclusive, especially for people with disabilities (58; 64).\nGuideline H37 (56) of the WCAG focuses on the proper use of alternative (alt) texts for images to help visually impaired people understand the message the image is trying to convey. Often developers fail to provide the above-mentioned alt texts and even if they do, the text does not really convey the message of the image.\nAutomatic Image captioning is a challenging task because it combines the workings of both CNNs and RNNs together. The CNN must understand the high-level features of the image and the RNN must translate these features into relevant captions\nThe Dataset\nThere are several options for a dataset of images accompanied by their corresponding captions. Some of these are Flickr8k (17), Microsoft COCO: Common objects in context (MSCOCO) (27), Conceptual captions dataset by Google (44). I have used the Flickr8k dataset for this task\nData Cleaning and pre-processing\nIn this task, we are dealing with both image data as well as textual data, which has been crowdsourced (17). Data cleaning and preprocessing is very important for the performance of the deep learning model.\nThe pre-processing steps vary for images and text. The pre-processing for the images involves:\n\nResize the images to dimensions: 224x224x3 - 224 is the image height and width (in pixels). 3 denotes the number of color channels (RGB)\nNormalize the images by mean centering them. The mean RGB value was subtracted from each pixel value of the image\n\nFor natural language processing based tasks it is a good practice to clean text data and create a text cleaning pipeline using tools like python nltk (2; 6). The steps in the text cleaning pipeline suitable for our task include:\n\nTokenize the captions into separate words\nCase Normalization - Convert all words to lowercase\nRemove punctuations from the words\nRemove non-alphanumeric characters from the words\n\nFor more traditional machine learning tasks additional steps like stemming and lemmatization need to be carried out, but because our model is going to have an embedding layer, it does not make sense to perform additional preprocessing (6).\nModel for Image Classiﬁcation\nWe use CNN generally for image classification.In this task,  we strip away the last layer of the CNN model.  This is because we areonly interested in the high-level features that the CNN learns from the image, and noton the final classification.  These features can be fed into the RNN along with part ofthe corresponding text of the caption.  The features and the text together are used topredict the next text in the caption\n\nFor  the  purpose  of  this  task,  I  initially  tried  training  my  own  CNN  models,  but  theresults were not good.  Then,  I used Transfer learning,  where we re-use a model thathas already been developed for a certain task for a related but unidentical task (5).  Wecan use a pre-trained network for recognizing and classifying images and use it to getthe high-level features of the images.  Transfer learning helps reduce running time as themodel does not have to be trained from scratch.\nVGG 16 was selected as the final model and it was used it to extract the photofeatures.  Each feature has a dimensionality of 4096.\nOptimization of the VGG-16 model\nRunning each image through the entire VGG network takes a long time and had scopefor optimization.  The process was:\nAssign  a unique  id  to  each  image  in  the  dataset\n\nCreate  a  dictionary  of  form{i d    :  [ . . . ,  features ,]}\n\nFor  each  image\n    Run the  image  through  the VGG network  except  for  the  last  layer\n    Extract  the  features  for  that  image\n    Store  the  image  id  as  the  key and  the  list  of  features  as  the  value  in the  dictionary\n    Return  the  formed  dictionary\n\nOnce the dictionary is formed, we can easily look up the corresponding features for animage using the id of the image\nAt the end of this step,we have computed the high-level features of an image.Animportant point to note here is that the choice of stripping away exactly one layer fromthe model is experimental.  As discussed in the prev section 2.1, as we go deeper into theCNN, it learns more complicated features.  One can argue that it might be a good ideato explore the results after stripping away the last two layers so that slightly less specificfeatures are taken into account.  I tried this process, however, the performance of themodel by stripping away 2 layers reduced dramatically and the network became underfitted. That means by stripping away 2 layers the network could not understandthe complexities of the image well enough to associate the features with the captions.\n\nCreating the training set\nThe next step is to create the dataset that will combine the image features and the cap-tions together which we can then feed into our RNN model.  While generating captionswe have to set a limit for the model to stop predicting the next word in the caption.We do this by appending two special tokensstartseqandendseqto the beginning andend of each caption respectively.  These tokens tell the system when to start and whento stop predicting the sequence of words.\n\nX1, X2 and y are now our training lists.  The model should receive the pair[X1, X2]and predicty. [X1, X2] are like the photo features and the corresponding tokens fromthe caption combined andyis the next token that the model should learn to predict.Thesame algorithm is used to create the corresponding test sequences.\nThe figure represents X2 and y as  words  for  readability  and  understanding  purpose.However  neural  networks  cannot  understand  text  features.   We  have  to  encode  thesenumbers in some form.  The paper (31) discusses the benefits of using word embeddings.So we convert the input text into a one-hot vector and then feed them into an Embeddinglayer, which is built into Keras (8).  This layer basically converts these sparse one-hotvectors into a dense vector representation by embedding them into a lower dimension.\nThe  choice  of  how  many  dimensions  to  use  for  embedding  is  arbitrary,  and  throughexperimentation it was found that300dimensions gave the best results.\nModel for Image Captioning\nAs mentioned in (65), the task of automatically generating captions from an image is very similar to the task of machine translation.  Encoder-decoder based architectures have achieved  excellent  results  in  machine  translation  (7).   Encoder-decoder  based  modelsgenerally consist of an encoder RNN, which maps the input sequence to a vector of fixedlength, and the decoder maps this representation to a target sequence. This architecture,when incorporated with attention-based networks like LSTM (16) achieve state-of-the-art  results  in  machine  translation  tasks.   Also,  the  models  are  very  interpretable  andquite simple compared to other complex models for similar tasks. The final model has been generated absed on a number of experiments\n\n\nGenerating the captions\nNow,  we  have  our  final  model  which  has  been  trained  on  6000  images  and  their  cor-responding captions.  We can generate the captions on the test set (1000 images) andevaluate the results.  To generate the captions we use the following algorithm:\n\nAt this stage, we have the captions for all the 1000 images in the test set.\nEvaluating the model\n(50)  mentions  a  variety  of  metrics  to  evaluate  the  quality  of  the  generated  captions. Initially, the metric Bilingual Evaluation Understudy Score (BLEU) (36) was used forevaluating  the  generated  captions  against  the  real  captions  in  the  test  dataset.   As discussed in (4), BLEU offers some advantages that apply to this project like:\n\nSimple to understand\nEasy to implement - nltk (14) in python has an implementation of BLEU\nIt can be used to compare the performance of our model against the model de-scribed in (65) and (50)\nIt correlates highly with human evaluation\nThe score is calculated irrespective of the order of the words\nA cumulative BLEU score can be calculated based on N-gram (62) matches\n\nThe metric ranges from 0 to 1; 1 being a perfect match.  We can calculate cumulative BLEU score for N-grams.  For example, while considering the BLEU-2 score we see thepercentage of matching 2-grams in the real and generated caption.  Using this metric, our final model Figure 3.5 has the following scores:\n\nBLEU-1:  0.535031\nBLEU-2:  0.282928\nBLEU-3:  0.196293\nBLEU-4:  0.091624\n\nSample results\nSome examples of the captions generated by our model are shown in Figure 3.6.  It canbe seen that there are cases when the model gets the caption correct (green), partiallycorrect (yellow) and completely incorrect (red)\n\n\nBLEU metrics - The problem\nEven though the BLEU metrics for our model look promising a closer inspection revealeda problem.The  BLEU  scores  between  the  real  and  generated  captions  as  shown  in  table  3.3  arevery low although the captions are quite close to the real ones.  The reason why BLEUscore fails to capture this is because it tries to match every word exactly consideringvarious N-grams.  It fails to understand the words in their context and synonyms.\nA Proposed Solution\nIt is clear from the above discussion that we would require a more representative methodof evaluating the generated captions.  One possible solution is to use a word embeddingtechnique like word2vec (31).  The motivation behind using word embeddings for thisproject is:\n\n\nWhen words are trained by deep learning algorithms, they should be representedin a manner which can capture the context in some manner\n\n\nDetecting phrases which have close context is essential not only for evaluating the current model but also for developing the second part of this project.\n\n\nBy embedding the words in vector space we can apply metrics like Cosine similarity between  them  which  can  give  us  a  better  idea  about  the  similarity  than  BLEUs cores or Euclidean similarity.  Cosine similarity is insensitive to the relative sizesof the documents (28)\n\n\nWorking of a word embedding model - word2vec\nFor this project, we use the famous word embedding technique - word2vec (31). Word2vecis a shallow neural network that learns the association between the words (52).  FromFigure 3.7, we can see that the model takes as input the one-hot encoded form of thewords.  The size of this input vector is the same as our vocabulary size.  Then there is ahidden layer, the size of which determines the vector size in which all the words will beembedded.  The final output layer again is the same size as the vocabulary, but usuallyhas a softmax (63) activation function, which outputs a probability distribution of all possible words in the vocabulary.  By training this network on a large corpus of words,it gradually learns to maximize the probability of words which are in close proximity toit.  An important assumption that this model makes is that words in close proximity areoften similar or contextual.  Finally, after the network has learned the associations, wecan extract the hidden (embedding) dimension, and words which have similar contextwill represent similar vector space embeddings.\n\nWord Movers Distance\nNow  that  we  have  a  brief  idea  about  how  word2vec  works,  we  can  go  back  to  ourproblem of understanding if the generated captions are similar to the original captionsor not.  Basically, we have to compare two sentences, and in the field of natural languageprocessing, sentences are often referred to as documents.  So we need a metric that cangive us a similarity score based on the distances between the documents, incorporatingthe features of word2vec.\nWord Movers Distance (WMD) (23) is such a metric.  WMD utilized word2vec embed-dings.  The words and hence, the documents can be represented in vector space.  WMDcomputes the cumulative distance between two documents in vector space as the mini-mum distance the cloud of points for one document will need to travel to merge with thecloud of points for the above document.  Because it uses word2vec embeddings, similarwords will be close together in vector space, and, as a result, similar documents will haveless WMD between them.\nThe motivation for using WMD for our use case of identifyingthe similarity between captions are:\n\nCan leverage pre-trained embeddings of the word2vec model\nCan  capture  the  semantic  similarity  between  documents  that  other  metrics  likeTF-IDF (23; 37) fail to do\nThe algorithm is hyperparameter free - so a lot of time and memory for hyperpa-rameter optimization can be saved.  As discussed in section 1.3, RAM managementis a crucial factor\nThe algorithm can be easily implemented using the gensim package in python (39)\n\nImplementing the proposed solution\nTo implement WMD, we use pre-trained word2vec embeddings from the Google Newsdataset (12).  These are a set of pre-trained vectors, each representing a unique word,which have been extracted from Google news data.  There are about 100 billion uniquewords, and the vector (embedding) size is 300.  It is a safe assumption to consider that all of the words in our dataset is a part of this massive 100 billion words dataset, so wedo not have to train our own word2vec model for this.  Another important catch whilecomputing WMD is it considers Euclidean distance (60), and not Cosine Similarity (28).So if two documents have different lengths the Euclidean distance will be large, so wenormalize the embedded vectors so that they have the same lengths.\n\nThe implementation of the WMD distance on our test dataset is as follows:\n\nThe average WMD Distance score is 1.17, which suggests that the generated captionsare quite close to the original ones.  Through experimentation,  we can see that if thecaptions are not similar the WMD score is generally over 1.25, and often over 1.5\nResults\n\nIn  table  3.3,  we  had  observed  few  samples  of  real  and  generated  captions  for  whichthe N-gram BLEU scores were very less, even though the captions were quite similar.We compute the WMD Distance metric between these captions using pre-trained GoogleNews word embeddings (12), and the results are summarized in table 3.4.  It is clear fromthe above results that applying WMD metric to the captions generate better results.\n\nSome optimizations for deploying\nWe  optimized  the  CNN  part  of  the  model  by  pre-computing  and  storing  the  image  features.   When  we  deploy  the  model  to  the  web,we  need  to  consider  the  running  time  of  the  model.   For  that,  we  should  store  all the variables, data structures and weights of the models which have been trained and optimized on disk so that while predicting, the application can just read from these file sand  run  the  data  through  the  model  to  get  the  predictions.   We  should  not  have  to re-create the datasets or re-train the model every time.  Most importantly, the creation of the training dataset takes a long time (30 mins on 25GBRAM) and should be stored on disk.\n\nOptimizing word embeddings\nOur model has an embedding layer of fixed dimensionality which learns dense vector space embeddings of words. Once trained on the whole dataset,8763 words are learned by the model.  This is a significant number and it does not make sense to precompute the embeddings every time.  So, the embedding matrix once learned can be stored as a numpy array of dimensionality(vocabularysize, embeddingsize)and then be stored on disk as a pickle, which the model can refer to while training.  This significantly reduced the training time of the model (by almost 10 minutes on a batchof 1000 new images for 20 epochs).  Also, to make this approach work, we keep a list of words in our vocabulary.  If newer data comes in and the percentage of words that are out-of-vocabulary is beyond a particular threshold, we re-train the embedding.  This process is summarized in Figure 3.8.\nUsing model checkpoints\nAlso, because the model will be deployed on the cloud, we may need to re-train the model as we receive new data. So we should always ensure that we are using the best model for the predictions. An easy way to do this is to monitor the loss via callbacks. Callbacks allow us to monitor important statistics of a model like loss and accuracy while training [29]. Using callbacks we can create checkpoints of the model  The way we do this is:\nSet the format of the model file as: model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5\nCreate a new ModelCheckpoint instance\nSet the appropriate parameters:\n    monitor = ‘val_loss’\n    save_best_only = True: Only save the model with the lowest validation score\n    mode = ‘min’: overwrite the current file if the model has the minimum validation loss\nSet the checkpoint as a callback while training the model\n\n\nThus, only the model with the lowest validation loss will be stored on disk. If the current model has a lower validation loss, the model on disk will be replaced by the current one.\n\nFile management\nTable 3.5 shows a possible configuration of the files that have to be maintained on disk, their types and refresh rates. Generally files of sizes less than 1GB can be maintained as pickle files. Files larger than that have to be stored in HDF (HD5) format.\n\nNow that we have developed a system for automatically captioning images and we have also applied metrics to test the quality of the generated captions keeping in mind the complexities of natural language. Additionally, we explore some additional extended features that we can provide users to improve their experiences on the web.\n\nObject Detection\nImage captions provide a visually impaired user with an overview of what the image is trying to convey. However, what we noticed was that the captions generated were often vague, like “children playing in the park” instead of “three children playing with football in the park”. One way to do this would be to incorporate an attention mechanism (65) in our model. However, this would not solve the problem perfectly as the results in (65) and (50) are quite comparable.\nA solution is to use object detection. As mentioned, image captioning provides a general overview of what the image is trying to convey. The motivation behind this is that to get a detailed understanding of what is going on in the image, users can toggle through the objects in the image and the system will read out what that object is.\nThere are many object detection libraries and APIs available, but we use Microsoft Azure Vision API (30 ) for this purpose, due to the following reasons:\n\nThe API returns the objects detected as well as the coordinates of the bounding boxes\nThe API returns a confidence level of the detection\nThe API returns parent objects for any detected object. For e.g bicycle helmet - helmet - headwear. bicycle helmet is the detected object here.\nThe API returns the data in JSON (JavaScript Object Notation) format which is easy to interpret using JavaScript and Python\n\nA sample response from the API, when the model has detected the object \\textbf{dog} is shown below:\n{\n    ""url"": url for the image,\n    ""response"": {\n        ""objects"": [{\n          ""rectangle"": {\n            ""x"": 154,\n            ""y"": 23,\n            ""w"": 102,\n            ""h"": 122\n          },\n          ""object"": ""dog"",\n          ""confidence"": 0.845,\n          ""parent"": {\n            ""object"": ""mammal"",\n            ""confidence"": 0.849,\n            ""parent"": {\n              ""object"": ""animal"",\n              ""confidence"": 0.906\n            }\n          }\n        }],\n        ""requestId"": unique request id,\n        ""metadata"": {\n          ""width"": 356,\n          ""height"": 277,\n          ""format"": ""Png""\n        }\n      }\n}\nSome of the features that this application should have are the following:\n\nThe image should have a generic caption which will be the output of the image captioning model\nIf the user wants, they can explore more. On the click of a button, the object detection API should be executed\nThe bounding boxes for each object in the image should be drawn\nUser can hover over these objects and the information should be provided via text and speech\nUser can also toggle through the objects by pressing a specific key (completely blind users will not know where to hover on the image)\n\nKeeping these features in mind, a demonstration application was built. The link to this application is:\nhttps://codepen.io/shaunak1105/full/dybZEXa\n\nThe above figure shows how users can interact with the app. The information about the objects detected is provided both by text and speech. The bounding boxes are also overlaid on the image. Users can choose to hover over the objects or toggle through them by pressing the space key.\nRelevance of image on a web page\nOften, we come across web sites which are cluttered with images. These images may be present for the purpose of advertisements and they do not convey any real meaning to the topic being discussed. These images are thus confusing and distracting and for someone who is using screen readers, the experience will be worse.\nThe motivation behind this extension is to detect these irrelevant images from the text surrounding it and automatically flag these images so that they can be ignored by the screen readers.\nWe have already built an image captioning system and evaluated metrics for testing caption-to-caption similarity using WMD. To extract the text surrounding the image, we can consider a window of w words around the image tag. So we have the generated caption, the words surrounding the image and we want to apply WMD to detect of the text and image caption are in context to each other or not.\nWe initially pre-process both the caption text and the surrounding text . Then we computed the WMD score. however, we were not getting proper results (the scores were always higher than 1.25).\nThe caption text does not have a fixed length of words. So we cannot directly consider it as a document and compute WMD between the texts. Additionally, only a part of the surrounding text might be discussing the caption. So if we simply compute the WMD between the caption and surrounding text, it will return a high value, but that does not mean that they are not similar.\nHowever,at least one part of the surrounding text must be discussing the caption. We can detect this by splitting both the surrounding text and the caption into N-grams (62). For example, if the caption has 5 words, we can consider 2-gram sequences, 3-gram sequences, 4-gram sequences, and 5-gram sequences between the caption and the surrounding text and then compute WMD similarity. The intuition is that at least one of these N-grams between the caption and the surrounding text should have a close match i.e have less WMD score.\nThe algorithm to do this is described below:\nPreprocess both the caption text and surrounding text\nCompute length of caption\nFor i in range of 2 to length of caption:\n    Create i-grams of caption text\n    Create i-grams of surrounding text\n    For each such i-gram pair\n        Compute WMD between caption i-gram and surrounding i-gram\n            If WMD < 1.15\n                The caption is similar\n                    Return true\n            Else\n                continue\nReturn false - the caption is not similar\n\n\nThis process is visualized in teh figure below. It is clear for this scenario, simply computing the WMD score between the caption and surrounding text resulted in a high score of 1.34. However using the algorithm discussed, we get a much lower score and we can also visually see which parts of the text received a close match (shown in green in the figure). By removal of stopwords, we have ensured that common words are not taken into the formation of N-grams.\n\nIn progress; to be continued\n\n'], 'url_profile': 'https://github.com/ShaunakSen', 'info_list': ['C', 'GPL-3.0 license', 'Updated Jan 23, 2020', 'Python', 'Updated Apr 18, 2020', 'Dart', 'Updated May 13, 2020', '5', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Updated Jan 13, 2020', 'Updated Sep 22, 2020', 'Python', 'Updated Apr 10, 2020', 'C#', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 19, 2020', 'Updated Jan 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '10 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/jhuebotter', 'info_list': ['C', 'GPL-3.0 license', 'Updated Jan 23, 2020', 'Python', 'Updated Apr 18, 2020', 'Dart', 'Updated May 13, 2020', '5', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Updated Jan 13, 2020', 'Updated Sep 22, 2020', 'Python', 'Updated Apr 10, 2020', 'C#', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 19, 2020', 'Updated Jan 17, 2020']}","{'location': 'Nagercoil', 'stats_list': [], 'contributions': '6 contributions\n        in the last year', 'description': ['RPA WISHING ROBOT\nThis project was developed to help to send a mail Invitation or wishes like new year, Christmas, Pongal.etc. The project is automated in UI Path using Artificial Intelligence.\n'], 'url_profile': 'https://github.com/sherbin29', 'info_list': ['C', 'GPL-3.0 license', 'Updated Jan 23, 2020', 'Python', 'Updated Apr 18, 2020', 'Dart', 'Updated May 13, 2020', '5', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Updated Jan 13, 2020', 'Updated Sep 22, 2020', 'Python', 'Updated Apr 10, 2020', 'C#', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 19, 2020', 'Updated Jan 17, 2020']}","{'location': 'Louisville, KY', 'stats_list': [], 'contributions': '369 contributions\n        in the last year', 'description': ['UofL-Final-Undergrad-Semester\nIn prior semesters I have made a repository for each class.  This semester I will be keeping all classes in a single repository because none of these individual classes should be as large as previous ones such as Artificial Intelligence.\nClass Syllabi\n\nSemiconductor Development Fundamentals (ECE 542) - Prof. Shamus McNamara\nControl Systems (ECE 560/561) - Prof. Tamir Inanc\nIntroduction to Databases (CECS 535) - Prof. Antonio Badia\nEngineering Methods, Tools, and Practices (ENGR 110) - Prof. Campbell Bego\nDeep Learning (CECS 590) - Prof. Daniel Sierrasosa\n\n'], 'url_profile': 'https://github.com/jtcass01', 'info_list': ['C', 'GPL-3.0 license', 'Updated Jan 23, 2020', 'Python', 'Updated Apr 18, 2020', 'Dart', 'Updated May 13, 2020', '5', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Updated Jan 13, 2020', 'Updated Sep 22, 2020', 'Python', 'Updated Apr 10, 2020', 'C#', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 19, 2020', 'Updated Jan 17, 2020']}","{'location': 'Sydney', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Corvus AI\nCorvus est une Intelligence Artificielle capable de prédire les résultats de football en Ligue 1. Algorithme basé sur les 3 dernières saisons\n'], 'url_profile': 'https://github.com/nicolasguerin8', 'info_list': ['C', 'GPL-3.0 license', 'Updated Jan 23, 2020', 'Python', 'Updated Apr 18, 2020', 'Dart', 'Updated May 13, 2020', '5', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Updated Jan 13, 2020', 'Updated Sep 22, 2020', 'Python', 'Updated Apr 10, 2020', 'C#', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 19, 2020', 'Updated Jan 17, 2020']}","{'location': 'Hangzhou', 'stats_list': [], 'contributions': '153 contributions\n        in the last year', 'description': ['sokoban-game-AI-solver\nBreif Introduction of the project\nAn individual project from the course ""Introduction to Artificial Intelligence"", developped a “Sokoban Game” AI solver with multiple searching algorithms (Depth First Search, Breadth First Search, Uniform Cost Search and Heuristic A* Search)\nIn this project the game rule is a little bit different from the classic Sokoban Game, there can be multible ""workers"" in the game.\nThe original game state can be input as a SokobanState object in sokoban.py, the following example is corresponding to the case shown in picture below:\nSokobanState(""START"", 0, None, 6, 6, # dimensions\n                 ((0, 0), (0, 2), (0, 4), (5, 5)), #robots\n                 frozenset(((1, 0), (4, 1), (1, 2), (4, 3), (1, 4), (4, 5))), #boxes\n                 frozenset(((5, 0), (0, 1), (5, 2), (0, 3), (5, 4), (0, 5))), #storage\n                 frozenset() #obstacles\n                 )\n\nProgram solves for the solution and search for the final state\n\nFor a detailed explanation please check ""project_description.pdf""\n'], 'url_profile': 'https://github.com/YixiaoHong', 'info_list': ['C', 'GPL-3.0 license', 'Updated Jan 23, 2020', 'Python', 'Updated Apr 18, 2020', 'Dart', 'Updated May 13, 2020', '5', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Updated Jan 13, 2020', 'Updated Sep 22, 2020', 'Python', 'Updated Apr 10, 2020', 'C#', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 19, 2020', 'Updated Jan 17, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['NONE'], 'url_profile': 'https://github.com/globalcoach11', 'info_list': ['C', 'GPL-3.0 license', 'Updated Jan 23, 2020', 'Python', 'Updated Apr 18, 2020', 'Dart', 'Updated May 13, 2020', '5', 'Jupyter Notebook', 'Updated Feb 24, 2020', 'Updated Jan 13, 2020', 'Updated Sep 22, 2020', 'Python', 'Updated Apr 10, 2020', 'C#', 'Updated Jan 25, 2020', 'Python', 'Updated Jan 19, 2020', 'Updated Jan 17, 2020']}"
"{'location': 'NONE', 'stats_list': [], 'contributions': '0 contributions\n        in the last year', 'description': ['Global Coach IT Academy\nGlobal Coach IT Academy Hyderabad is a global leading  company that professionalizes SAP Training ,Digital Marketing Training and other Software Courses like Python , Big Data , Hadoop, Machine learning and Artificial Intelligence etc. Operating  since 2011.We have trained more than 5000+professionals across the globe  we provide both online and off line training in hyderabad.\n'], 'url_profile': 'https://github.com/globalcoach11', 'info_list': ['Updated Jan 17, 2020', '1', 'Python', 'GPL-3.0 license', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Updated Feb 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '8 contributions\n        in the last year', 'description': [""XAIIC-classify-the-characteristics-of-Xi-an\n\nXi'an was called Chang'an in the ancient times, and it is one of the birthplaces of the Chinese nation. The them of this competition is Xi'an tourism, using artificial intelligence technology to classify Xi'an popular attractions, food, special products, folk customs, crafts and other pictures.\n\n\n\n\ntraining\n\nRunning environment ubuntu 16.04 with pytorch 1.3.1 and torchvision 0.4.2.\n\n\nTo start the training procedure, just run python go.py. Then you could see something like this\n\n\n\n\nAfter finishing training procedure, there will be some .pth files:\n\n\nmention\n\nThis is only the preliminary version.\n\n\nWe download some pictures from internet using script to enlarge the train&val set, but there are still some problems(you do not need to take much care about this):\n\n\n\nAnother thing you should pay attention to is the plateform provided by HUAWEI which is called ModelArts, this version of our code can not be directly used on this plateform. We made some modificaions with the baseline using our developing environment and got a great improvement(but not the best, we do not pay much more time on finetuning). Meanwhile, we found that without our modification(just using more pictures), we can also get an excellent result.\n\n""], 'url_profile': 'https://github.com/WWWangHan', 'info_list': ['Updated Jan 17, 2020', '1', 'Python', 'GPL-3.0 license', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Updated Feb 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '30 contributions\n        in the last year', 'description': [""Feature Engineering And Statistical Analysis\nThis project was from my CSC3060 module (Artificial Intelligence and Data Analytics). It is the first assignment in a 2 part project.\nThere were 3 parts to this project:\n(1) Create a dataset of handwritten symbols.\n(2) Calculate features from the handwritten symbols which may be useful for distinguishing between the different symbols automatically.\n(3) Perform statistical analysis of the datasets, using methods of statistical inference.\nSection 1\nWe created the handwritten symbols by using a tool called GIMP, where we used the touchscreen software of the PCs in the computer science lab to draw our own handwritten symbols. Each separate symbol had 8 different attempts so as to have variance within each symbol group. We then had to export these symbols to a pgm file which were binary where 1 is black and 0 is whitespace. These pgm files were converted to csv files via the notebook in the directory section1_code.\nSection 2\nAfter this, our handwritten symbols were now more easily processable. We used this to our advantage by calculating a range of features for each handwritten symbol. This is done in the notebook in section2_code. In this notebook we calculate a range of features, such as how many neighbours are around each pixel or if the symbol has an 'eye' (for example letters like a and e have eyes but c and f do not and we assign calculated features to each training sample which gives each sample a feature vector.\nSection 3\nThese features were then statistically analysed and visualised in section 3 using R code. Comparison between the symbol groups was visualised using boxplots and also calculated using the ANOVA statistical method as well as t-tests for when the comparison was between two groups rather than multiple. The Tukey HSD method was used especially often as a statistical method to compare across multiple features for different groups of symbols.\n""], 'url_profile': 'https://github.com/fionnmcconville', 'info_list': ['Updated Jan 17, 2020', '1', 'Python', 'GPL-3.0 license', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Updated Feb 3, 2020']}","{'location': 'NONE', 'stats_list': [], 'contributions': '9 contributions\n        in the last year', 'description': ['Teach-a-Neural-Network-to-Read-Handwriting\nNeural networks and deep learning are two success stories in modern artificial intelligence. They’ve led to major advances in image recognition, automatic text generation, and even in self-driving cars. To get involved with this exciting field, you should start with a manageable dataset. The MNIST Handwritten Digit Classification Challenge is the classic entry point. Image data is generally harder to work with than “flat” relational data. The MNIST data is beginner-friendly and is small enough to fit on one computer. Handwriting recognition will challenge you, but it doesn’t need high computational power. Build a neural network from scratch that solves the MNIST challenge with high accuracy.\n'], 'url_profile': 'https://github.com/SRIKARREDDY-dotorg', 'info_list': ['Updated Jan 17, 2020', '1', 'Python', 'GPL-3.0 license', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Updated Feb 3, 2020']}","{'location': 'INDIA', 'stats_list': [], 'contributions': '17 contributions\n        in the last year', 'description': [""Machine-Learning\nThis repository contains sample codes that are coded and test by me using Python 3 and python 4. This repository hold foundational data explaining about the things and about's it's existence and why it really matter in machine learning before actually moving into Artificial intelligence or Machine Learning field.  Understanding Why, How, Who, When, Where, What about things is necessary before getting to start working right away in any field.  Here mostly i teach myself by relating math related stuff to real life. Math is main ground for ML.\nAditya Gurav\nTogether we learn and grow as time passes by...\n""], 'url_profile': 'https://github.com/CodeThinkingMachine', 'info_list': ['Updated Jan 17, 2020', '1', 'Python', 'GPL-3.0 license', 'Updated Feb 24, 2020', 'Jupyter Notebook', 'Updated Jan 20, 2020', 'Jupyter Notebook', 'Updated Jan 17, 2020', 'Updated Feb 3, 2020']}",,,,,
